{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Advanced Cloud Architecture \u00b6 Class Objectives: The power of containerization and its distinction from virtualization; features of the Linux kernel underpinning containerization; how to set up a Docker environment, build containers, and use an orchestration tool, namely, Kubernetes; additional tools for monitoring, sharing, and deploying applications.","title":"Home"},{"location":"#advanced-cloud-architecture","text":"Class Objectives: The power of containerization and its distinction from virtualization; features of the Linux kernel underpinning containerization; how to set up a Docker environment, build containers, and use an orchestration tool, namely, Kubernetes; additional tools for monitoring, sharing, and deploying applications.","title":"Advanced Cloud Architecture"},{"location":"020_Assignment_2_Production_GKE/","text":"Lab 2 Creating Production GKE Cluster Objective: 1 Creating Production GKE Cluster \u00b6 1.1 Locate Assignment 2 \u00b6 Step 1 Clone ycit020 repo with Kubernetes manifests, which going to use for our work: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit020 cd ~/ycit020/Assignment2/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 2 deploy_a2 folder to your repo: cp -r ~/ycit020/Assignment2/deploy_a2 deploy_ycit020_a2 Step 4 Create Document for Assignment 2 , that will be used for grading: cd ~/$MY_REPO mkdir docs cat > docs/production_gke.md << EOF # Creating Production GKE Cluster **Step 1** Create a vpc network: **Step 2** Create a subnet VPC design: **Step 3** Create a subnet with primary and secondary ranges: **Step 4** Create a Private GKE Cluster: **Step 5** Create a Cloud Router: **Step 6** Create a Cloud Nat Gateway: EOF Step 5 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding documentation for ycit020 assignment 2\" Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 1.2 Creating a GCP project \u00b6 Note We going to create a temporarily project for this assignment. While you going to store code in existing project that we've used so far in class Set variables: export ORG=<student-name> export PRODUCT=notepad export ENV=dev export PROJECT_PREFIX=1 # Project has to be unique export PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX Create a project: gcloud projects create $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX List billings and take note of ACCOUNT_ID : gcloud alpha billing accounts list Define a variable for ACCOUNT_ID : export ACCOUNT_ID=<Your Billing Account> Attach your project to the correct billing account: gcloud alpha billing accounts projects link $PROJECT_ID --billing-account=$ACCOUNT_ID Set the newly created project as default for gcloud: gcloud config set project $PROJECT_ID gcloud config set compute/region us-central1 Enable compute , container , cloudresourcemanager APIs: gcloud services enable container.googleapis.com gcloud services enable compute.googleapis.com gcloud services enable cloudresourcemanager.googleapis.com Note Enabling GCP Service APIs very important step in automation (e.g. terraform) Get a list of services that enabled in your project: gcloud services list Note Some services APIs enabled by default during project creation 1.2 Deleting Default VPC \u00b6 Observe Asset Inventory in GCP UI: Products -> IAM & Admin -> Asset Inventory -> Overview Result You can see that vpc default network spans across all GCP Regions, which for many companies will not be acceptable practice (e.g. GDPR) List all networks in a project: gcloud compute networks list Output: NAME SUBNET_MODE BGP_ROUTING_MODE IPV4_RANGE GATEWAY_IPV4 default AUTO REGIONAL Review existing firewall rules for default vpc: gcloud compute firewall-rules list Also check in Google cloud UI: Networking->Firewalls Delete firewall rules associated with default vpc network: gcloud compute firewall-rules delete default-allow-internal gcloud compute firewall-rules delete default-allow-ssh gcloud compute firewall-rules delete default-allow-rdp gcloud compute firewall-rules delete default-allow-icmp Delete the Default network, following best practices: gcloud compute networks delete default 1.3 Creating a custom mode network (VPC) \u00b6 Task N1: Using reference doc: Creating a custom mode network . Create a new custom mode VPC network using gcloud command with following parameters: Network name: $ORG-$PRODUCT-$ENV-vpc Subnet mode: custom Bgp routing mode: regional MTUs: default Document a command to create vpc network in docs/production_gke.md doc under step 1 . Step 1: Create a new custom mode VPC network: with name ORG-$PRODUCT-$ENV-vpc , with subnet mode custom , and regional . Step 1: Create a new custom mode VPC network: Set variables: export PRODUCT=notepad export ENV=dev TODO: gcloud create network Review created network: gcloud compute networks list Also check in Google cloud UI: Networking->VPC Networks Step 2: Create firewall rules default-allow-internal and default-allow-ssh : gcloud compute firewall-rules create $ORG-$PRODUCT-$ENV-allow-tcp-ssh-icmp --network $ORG-$PRODUCT-$ENV-vpc --allow tcp:22,tcp:3389,icmp gcloud compute firewall-rules create allow-internal --network $ORG-$PRODUCT-$ENV-vpc --allow tcp,udp,icmp --source-ranges 10.128.0.0/22 Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/firewall-rules Review created firewall rules: gcloud compute firewall-rules list Also check in Google cloud UI: Networking->Firewalls 1.4 Design and create a user-managed subnet \u00b6 After you've created VPC network, it is require to add subnet to it. Task N2: In order to ensure that GKE clusters in you organization doesn't overlap each other, design VPC subnet ranges for dev , stg , prd VPC-native GKE clusters, where Nodes primary range belongs to Class A (10.0.0.0/x) Pods secondary ranges belongs to Class B (172.20.0.0/x) Services secondary ranges belongs to Class B (172.100.0.0/x) CIDR range for master-ipv4-cidr (k8s api range) belongs to Class B (172.16.0.0/x) In your design assume that each cluster will have maximum of 59 Nodes with max 110 Pods per node and 1700 Service per cluster. Use following reference docs: VPC-native clusters GKE address management Using tables from VPC-native clusters document and online subnet calculator , create a table for dev , stg , prd in the following format and store result under docs/production_gke.md : env | subnet | pod range | srv range | kubectl api range dev | IP range | IP range | IP range | IP range stg | IP range | IP range | IP range | IP range prd | IP range | IP range | IP range | IP range Document a subnet VPC design in docs/production_gke.md doc under step 2 . Task N3: Create a user-managed subnet. Create a subnet for dev cluster, taking in consideration VPC subnet ranges created in above table, where: Subnet name: $ORG-$PRODUCT-$ENV-vpc Node Range: See column subnet in above table for dev cluster Secondary service range with name: services Secondary Ranges: Service range name: services Service range CIDR: See column srv range in above table for dev cluster Pods range name: pods Pods range CIDR: See column pod range in above table for dev cluster Features: Flow Logs Private IP Google Access TODO: gcloud subnet create Reference docs: Creating a private cluster with custom subnet VPC-native clusters Use gcloud subnets create command reference for all available options. Review created subnet: gcloud compute networks subnets list Also check in Google cloud UI: Networking->VPC Networks -> Click VPC network and check `Subnet` tab Document a subnet VPC creation command in docs/production_gke.md doc under step 3 . 1.5 Creating a Private, Regional and VPC Native GKE Cluster \u00b6 Task N4: Create dev Private GKE Cluster with no client access to the public endpoint and following values: Cluster name: $ORG-$PRODUCT-$ENV-cluster Secondary pod range with name: pods Secondary service range with name: services VM size: e2-small Node count: 1 per zone GKE Control plane is replicated across three zones of a region: us-central1 GKE version: \"1.20.8-gke.700\" Cluster Node Communication: VPC Native Cluster Nodes access: Private Node GKE Cluster with Public API endpoint Cluster K8s API access: with limited access to the public endpoint via authorized network Use following reference docs: Creating a private cluster GKE Release Channels Use gcloud container clusters create command reference for all available options. #TODO gcloud clusters create $ORG-$PRODUCT-$ENV-cluster Document a GKE cluster creation command in docs/production_gke.md doc under step 4 . Result The private cluster is now created. gcloud has set up kubectl to authenticate with the private cluster but the cluster's K8s API server will only accept connections from the primary range of your subnetwork, and the secondary range of your subnetwork that is used for pods. That means nobody can access K8s API at this point of time, until we specify allowed ranges. Step 3 Authenticate to the cluster. gcloud container clusters get-credentials $ORG-$PRODUCT-$ENV-cluster --region us-central1 Step 4: Connecting to a Private Cluster Let's try to connect to the cluster: kubectl get pods Unable to connect to the server i/o timeout Fail This fails because private clusters firewall traffic to the master by default. In order to connect to the cluster you need to make use of the master authorized networks feature. Step 4: Enable Master Authorized networks on you cluster Suppose you have a group of machines, outside of your VPC network, belongs to your organization. You could authorize those machines to access the public endpoint. Here we will enable master authorized networks and whitelist the IP address for our Cloud Shell instance, to allow access to the master: gcloud container clusters update $ORG-$PRODUCT-$ENV-cluster --enable-master-authorized-networks --master-authorized-networks $(curl ipinfo.io/ip)/32 --region us-central1 Now we can access the API server using kubectl from GCP console: Note In real life it could be CIDR Range for you company, so only engineers or CI/CD systems from you company can connect to kubectl apis in secure manner. Now we can access the API server using kubectl: kubectl get pods kubectl run hello-web --image=gcr.io/google-samples/hello-app:1.0 --port=8080 kubectl get pods Output: NAME READY STATUS RESTARTS AGE hello-web 1/1 Running 0 7s Result We can deploy Pods to our Private Cluster. kubectl delete pod hello-web Step 2: Testing Outbound Traffic Outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads. cat <<EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: wget spec: containers: - name: wget image: alpine command: ['wget', '-T', '5', 'http://www.example.com/'] restartPolicy: Never EOF kubectl get pods Output: NAME READY STATUS RESTARTS AGE wget 0/1 Error 0 4m41s kubectl logs wget Output: Connecting to www.example.com (93.184.216.34:80) wget: download timed out Results Pods can't access internet, because it's a private cluster Note Normally, if cluster is Private and doesn't have Cloud Nat configured, users can't even deploy images from Docker Hub. See troubleshooting details here 1.6 Create a Google Cloud Nat \u00b6 Private clusters give you the ability to isolate nodes from having inbound and outbound connectivity to the public internet. This isolation is achieved as the nodes have internal IP addresses only. If you want to provide outbound internet access for certain private nodes, you can use Cloud NAT Cloud NAT is a distributed, software-defined managed service. It's not based on proxy VMs or appliances. You configure a NAT gateway on a Cloud Router, which provides the control plane for NAT, holding configuration parameters that you specify. Google Cloud runs and maintains processes on the physical machines that run your Google Cloud VMs. Cloud NAT can be configured to automatically scale the number of NAT IP addresses that it uses, and it supports VMs that belong to managed instance groups, including those with autoscaling enabled. Step 1: Create a Cloud NAT configuration using Cloud Router Create Cloud Router in the same region as the instances that use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway. Step 1a: Create a Cloud Router : gcloud compute routers create gke-nat-router \\ --network $ORG-$PRODUCT-$ENV-vpc \\ --region us-central1 Verify created Cloud Router: Networking -> Hybrid Connectivity -> Cloud Routers Step 1b: Create a Cloud Nat Gateway: gcloud compute routers nats create nat-config \\ --router-region us-central1 \\ --router gke-nat-router \\ --nat-all-subnet-ip-ranges \\ --auto-allocate-nat-external-ips Verify created Cloud Nat : Networking -> Network Services -> Cloud NAT Note Cloud NAT uses Cloud Router only to group NAT configuration information (control plane). Cloud NAT does not direct a Cloud Router to use BGP or to add routes. NAT traffic does not pass through a Cloud Router (data plane). Document Cloud Router and Cloud Nat creation command in docs/production_gke.md doc under step 5 and 6 respectively. Step 2: Testing Outbound Traffic Most outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads. cat <<EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: wget spec: containers: - name: wget image: alpine command: ['wget', '-T', '5', 'http://www.example.com/'] restartPolicy: Never EOF kubectl get pods Output: NAME READY STATUS RESTARTS AGE wget 0/1 Completed 0 2m53s kubectl logs wget Output: Connecting to www.example.com (93.184.216.34:80) saving to 'index.html' index.html 100% |********************************| 1256 0:00:00 ETA 'index.html' saved Results Pods can access internet, thanks to our configured Cloud Nat. 2 Deploy NotePad Go webapp \u00b6 2.1 Create a Namespace dev \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 2.2 Create Mysql deployment \u00b6 Create Mysql deployment: Step 1 Deploy PVC, Deployment, Services and Network Policy: cd ~/$MY_REPO/deploy_ycit020_a2 kubectl apply -f gowebapp-mysql-pvc.yaml #Create PVC kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment kubectl apply -f default-deny-dev.yaml # deny-all Ingress Traffic to dev Namespace kubectl apply -f gowebapp-mysql-netpol.yaml # `mysql` pod can be accessed from the `gowebapp` pod Verify status of mysql: kubectl get deploy,secret,pvc 2.3 Create GoWebApp deployment \u00b6 Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_ycit020_a2/ cd ~/$MY_REPO/deploy_ycit020_a2/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl apply -f gowebapp-netpol.yaml #gowebapp pod from Internet on CIDR range kubectl apply -f gowebapp-ingress.yaml #Create Ingress Verify status of gowebapp: kubectl get pods,cm,ing 3 Running GKE in Production \u00b6 3.1 Deploy a New NodePool \u00b6 A Kubernetes Engine cluster consists of a master and nodes. Kubernetes doesn't handle provisioning of nodes, so Google Kubernetes Engine handles this for you with a concept called node pools . A node pool is a subset of node instances within a cluster that all have the same configuration. They map to instance templates in Google Compute Engine, which provides the VMs used by the cluster. By default a Kubernetes Engine cluster has a single node pool, but you can add or remove them as you wish to change the shape of your cluster. In the previous example, you created a Kubernetes Engine cluster. This gave us three nodes (three e2-small (2 vCPUs, 2 GB memory), 100 GB of disk each) in a single node pool (called default-pool). Let's inspect the node pool: gcloud config set compute/region us-central1 gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-cluster Output: NAME MACHINE_TYPE DISK_SIZE_GB NODE_VERSION default-pool e2-small 100 1.20.8-gke.700 If you want to add more nodes of this type, you can grow this node pool. If you want to add more nodes of a different type, you can add other node pools. A common method of moving a cluster to larger nodes is to add a new node pool, move the work from the old nodes to the new, and delete the old node pool. Let's add a second node pool, and migrate our workload over to it. This time we will use the larger e2-medium (2 vCPUs, 4 GB memory), 100 GB of disk each machine type. gcloud container node-pools create new-pool --cluster $ORG-$PRODUCT-$ENV-cluster \\ --machine-type e2-medium --num-nodes 1 Output: NAME MACHINE_TYPE DISK_SIZE_GB NODE_VERSION default-pool e2-small 100 1.20.8-gke.700 new-pool e2-medium 100 1.20.8-gke.700 gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-cluster Output: NAME STATUS ROLES AGE VERSION gke-ayrat-notepad-dev-cl-default-pool-34b29738-b1m8 Ready <none> 11m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-c22a1803-hh91 Ready <none> 11m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-f2a61828-zjgk Ready <none> 11m v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-2df77edb-thgj Ready <none> 104s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-40bedbb3-gsxr Ready <none> 101s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-d8f28859-hgqn Ready <none> 60s v1.20.8-gke.700 Kubernetes does not reschedule Pods as long as they are running and available, so your workload remains running on the nodes in the default pool. Look at one of your nodes using kubectl describe. Just like you can attach labels to pods, nodes are automatically labelled with useful information which lets the scheduler make decisions and the administrator perform action on groups of nodes. Replace \"[NODE NAME]\" with the name of one of your nodes from the previous step. kubectl describe node [NODE NAME] | head -n 20 Output: Name: gke-ayrat-notepad-dev-cluste-new-pool-d8f28859-hgqn Roles: <none> Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=e2-standard-4 beta.kubernetes.io/os=linux cloud.google.com/gke-boot-disk=pd-standard cloud.google.com/gke-container-runtime=containerd cloud.google.com/gke-nodepool=new-pool cloud.google.com/gke-os-distribution=cos cloud.google.com/machine-family=e2 failure-domain.beta.kubernetes.io/region=us-central1 failure-domain.beta.kubernetes.io/zone=us-central1-b kubernetes.io/arch=amd64 kubernetes.io/hostname=gke-ayrat-notepad-dev-cluste-new-pool-d8f28859-hgqn kubernetes.io/os=linux node.kubernetes.io/instance-type=e2-standard-4 node.kubernetes.io/masq-agent-ds-ready=true topology.gke.io/zone=us-central1-b topology.kubernetes.io/region=us-central1 topology.kubernetes.io/zone=us-central1-b <...> You can also select nodes by node pool using the cloud.google.com/gke-nodepool label. We'll use this powerful construct shortly. kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool Output: NAME STATUS ROLES AGE VERSION gke-ayrat-notepad-dev-cl-default-pool-34b29738-b1m8 Ready <none> 14m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-c22a1803-hh91 Ready <none> 14m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-f2a61828-zjgk Ready <none> 14m v1.20.8-gke.70 3.2 Migrating pods to the new Node Pool \u00b6 To migrate your pods to the new node pool, we will perform the following steps: Cordon the existing node pool: This operation marks the nodes in the existing node pool (default-pool) as unschedulable. Kubernetes stops scheduling new Pods to these nodes once you mark them as unschedulable. Drain the existing node pool: This operation evicts the workloads running on the nodes of the existing node pool (default-pool) gracefully. You could cordon an individual node using the kubectl cordon command, but running this command on each node individually would be tedious. To speed up the process, we can embed the command in a loop. Be sure you copy the whole line - it will have scrolled off the screen to the right! for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do kubectl cordon \"$node\"; done Output: node/gke-ayrat-notepad-dev-cl-default-pool-4a7a0d9d-d5q9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-4d5f96ee-nnv9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 cordoned Note This loop utilizes the command kubectl get nodes to select all nodes in the default pool (using the cloud.google.com/gke-nodepool=default-pool label), and then it iterates through and runs kubectl cordon on each one. After running the loop, you should see that the default-pool nodes have SchedulingDisabled status in the node list: kubectl get nodes Output: NAME STATUS ROLES AGE VERSION gke-ayrat-notepad-dev-cl-default-pool-4a7a0d9d-d5q9 Ready,SchedulingDisabled <none> 38m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-4d5f96ee-nnv9 Ready,SchedulingDisabled <none> 38m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 Ready,SchedulingDisabled <none> 38m v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-af1b690a-v6j9 Ready <none> 2m57s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-f3195336-k11r Ready <none> 2m44s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-f47e9e43-6qzb Ready <none> 2m50s v1.20.8-gke.700 Next, we want to evict the Pods already scheduled on each node. To do this, we will construct another loop, this time using the kubectl drain command: for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do kubectl drain --force --ignore-daemonsets --delete-emptydir-data \"$node\"; done Output: <...> pod/gowebapp-mysql-6ffb7f9586-prddc evicted pod/hello-web evicted node/gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 evicted <...> As each node is drained, the pods running on it are evicted. Eviction makes sure to follow rules to provide the least disruption to the applications as possible. Users in production may want to look at more advanced features like Pod Disruption Budgets . Because the default node pool is unschedulable, the pods are now running on the single machine in the new node pool: kubectl get pods -o wide Output: NAME READY STATUS RESTARTS AGE IP NODE node/gke-ayrat-notepad-dev-cl-default-pool-4a7a0d9d-d5q9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-4d5f96ee-nnv9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 cordoned You can now delete the original node pool: gcloud container node-pools delete default-pool --cluster $ORG-$PRODUCT-$ENV-cluster Check that node-pool has been deleted: gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-cluster Result GKE cluster running only on new-pool Node pool. 3.3 Configure Node auto-repair and Node auto-upgrades \u00b6 Kubernetes Engine's node auto-repair feature helps you keep the nodes in your cluster in a healthy, running state. When enabled, Kubernetes Engine makes periodic checks on the health state of each node in your cluster. If a node fails consecutive health checks over an extended time period (approximately 10 minutes), Kubernetes Engine initiates a repair process for that node. gcloud container node-pools update new-pool --cluster $ORG-$PRODUCT-$ENV-cluster --enable-autorepair This will enable the autorepair feature for nodes. Please see https://cloud.google.com/kubernetes-engine/docs/node-auto-repair for more information on node autorepairs. First, Enable and configure OS Login in GKE: gcloud compute project-info add-metadata --metadata enable-oslogin=TRUE Next, for some fun: let's break a VM! This gcloud command will find the VM in your regional node pool which is in the default zone, and SSH into it. If you are asked to generate an SSH key just answer 'Y' at the prompt and hit enter to not set a passphrase. gcloud compute ssh $(gcloud compute instances list | \\ grep -m 1 notepad-dev | \\ awk '{ print $1 }') \\ --zone us-central1-a You can simulate a node failure by removing the kubelet binary, which is responsible for running Pods on every Kubernetes node: sudo rm /home/kubernetes/bin/kubelet && sudo systemctl restart kubelet logout Now when we check the node status we see the node is NotReady. watch kubectl get nodes NAME STATUS ROLES AGE VERSION gke-gke-workshop-new-pool-42b33f8c-9grf Ready <none> 6m v1.9.6-gke.1 gke-gke-workshop-new-pool-847e18a1-f1bp Ready <none> 6m v1.9.6-gke.1 gke-gke-workshop-new-pool-8c4b26e9-fq8p NotReady <none> 6m v1.9.6-gke.1 The Kubernetes Engine node repair agent will wait a few minutes in case the problem is intermittent. We'll come back to this in a minute. Define a maintenance window You can configure a maintenance window to have more control over when automatic upgrades are applied to Kubernetes on your cluster. Creating a maintenance window instructs Kubernetes Engine to automatically trigger any automated tasks in your clusters, such as master upgrades, node pool upgrades, and maintenance of internal components, during a specific timeframe. The times are specified in UTC, so select an appropriate time and set up a maintenance window for your cluster. Open the cloud console and navigate to Kubernetes Engine. Click on the gke-workshop cluster and click the edit button at the top. Find the Maintenance Window option and select 3AM. Finally, click Save to update the cluster. Enable node auto-upgrades Whenever a new version of Kubernetes is released, Google upgrades your master to that version. You can then choose to upgrade your nodes to that version, bringing functionality and security updates to both the OS and the Kubernetes components. Node Auto-Upgrades use the same update mechanism as manual node upgrades, but does the scheduled upgrades during your maintenance window. Auto-upgrades are enabled per node pool. gcloud container node-pools update new-pool --cluster $ORG-$PRODUCT-$ENV-cluster --enable-autoupgrade This will enable the autoupgrade feature for nodes. Please see https://cloud.google.com/kubernetes-engine/docs/node-management for more information on node autoupgrades. ERROR: (gcloud.container.node-pools.update) ResponseError: code=400, message=Operation operation-1511896056410-dbda7f9f is currently operating on cluster gke-workshop. Please wait and try again once it's done. That's good - that means our broken node is being repaired! Try again in a few minutes. Check your node repair How is that node repair coming? After a few minutes, you will see that the master drained the node, and then removed it. watch kubectl get nodes NotReady,SchedulingDisabled A few minutes after that, a new node was turned on in its place: Ready kubectl get pods kubectl delete deployment hello-web 4 Commit Readme doc to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit docs folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Readme doc for Production GKE Creation using gcloud\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 5 Cleanup \u00b6 gcloud container clusters delete $ORG-$PRODUCT-$ENV-cluster Important We going to delete the $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX only, please do not delete your project with Source Code Repo. gcloud projects delete $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX","title":"Assignment2"},{"location":"020_Assignment_2_Production_GKE/#1-creating-production-gke-cluster","text":"","title":"1 Creating Production GKE Cluster"},{"location":"020_Assignment_2_Production_GKE/#11-locate-assignment-2","text":"Step 1 Clone ycit020 repo with Kubernetes manifests, which going to use for our work: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit020 cd ~/ycit020/Assignment2/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 2 deploy_a2 folder to your repo: cp -r ~/ycit020/Assignment2/deploy_a2 deploy_ycit020_a2 Step 4 Create Document for Assignment 2 , that will be used for grading: cd ~/$MY_REPO mkdir docs cat > docs/production_gke.md << EOF # Creating Production GKE Cluster **Step 1** Create a vpc network: **Step 2** Create a subnet VPC design: **Step 3** Create a subnet with primary and secondary ranges: **Step 4** Create a Private GKE Cluster: **Step 5** Create a Cloud Router: **Step 6** Create a Cloud Nat Gateway: EOF Step 5 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding documentation for ycit020 assignment 2\" Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"1.1 Locate Assignment 2"},{"location":"020_Assignment_2_Production_GKE/#12-creating-a-gcp-project","text":"Note We going to create a temporarily project for this assignment. While you going to store code in existing project that we've used so far in class Set variables: export ORG=<student-name> export PRODUCT=notepad export ENV=dev export PROJECT_PREFIX=1 # Project has to be unique export PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX Create a project: gcloud projects create $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX List billings and take note of ACCOUNT_ID : gcloud alpha billing accounts list Define a variable for ACCOUNT_ID : export ACCOUNT_ID=<Your Billing Account> Attach your project to the correct billing account: gcloud alpha billing accounts projects link $PROJECT_ID --billing-account=$ACCOUNT_ID Set the newly created project as default for gcloud: gcloud config set project $PROJECT_ID gcloud config set compute/region us-central1 Enable compute , container , cloudresourcemanager APIs: gcloud services enable container.googleapis.com gcloud services enable compute.googleapis.com gcloud services enable cloudresourcemanager.googleapis.com Note Enabling GCP Service APIs very important step in automation (e.g. terraform) Get a list of services that enabled in your project: gcloud services list Note Some services APIs enabled by default during project creation","title":"1.2 Creating a GCP project"},{"location":"020_Assignment_2_Production_GKE/#12-deleting-default-vpc","text":"Observe Asset Inventory in GCP UI: Products -> IAM & Admin -> Asset Inventory -> Overview Result You can see that vpc default network spans across all GCP Regions, which for many companies will not be acceptable practice (e.g. GDPR) List all networks in a project: gcloud compute networks list Output: NAME SUBNET_MODE BGP_ROUTING_MODE IPV4_RANGE GATEWAY_IPV4 default AUTO REGIONAL Review existing firewall rules for default vpc: gcloud compute firewall-rules list Also check in Google cloud UI: Networking->Firewalls Delete firewall rules associated with default vpc network: gcloud compute firewall-rules delete default-allow-internal gcloud compute firewall-rules delete default-allow-ssh gcloud compute firewall-rules delete default-allow-rdp gcloud compute firewall-rules delete default-allow-icmp Delete the Default network, following best practices: gcloud compute networks delete default","title":"1.2 Deleting Default VPC"},{"location":"020_Assignment_2_Production_GKE/#13-creating-a-custom-mode-network-vpc","text":"Task N1: Using reference doc: Creating a custom mode network . Create a new custom mode VPC network using gcloud command with following parameters: Network name: $ORG-$PRODUCT-$ENV-vpc Subnet mode: custom Bgp routing mode: regional MTUs: default Document a command to create vpc network in docs/production_gke.md doc under step 1 . Step 1: Create a new custom mode VPC network: with name ORG-$PRODUCT-$ENV-vpc , with subnet mode custom , and regional . Step 1: Create a new custom mode VPC network: Set variables: export PRODUCT=notepad export ENV=dev TODO: gcloud create network Review created network: gcloud compute networks list Also check in Google cloud UI: Networking->VPC Networks Step 2: Create firewall rules default-allow-internal and default-allow-ssh : gcloud compute firewall-rules create $ORG-$PRODUCT-$ENV-allow-tcp-ssh-icmp --network $ORG-$PRODUCT-$ENV-vpc --allow tcp:22,tcp:3389,icmp gcloud compute firewall-rules create allow-internal --network $ORG-$PRODUCT-$ENV-vpc --allow tcp,udp,icmp --source-ranges 10.128.0.0/22 Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/firewall-rules Review created firewall rules: gcloud compute firewall-rules list Also check in Google cloud UI: Networking->Firewalls","title":"1.3 Creating a custom mode network (VPC)"},{"location":"020_Assignment_2_Production_GKE/#14-design-and-create-a-user-managed-subnet","text":"After you've created VPC network, it is require to add subnet to it. Task N2: In order to ensure that GKE clusters in you organization doesn't overlap each other, design VPC subnet ranges for dev , stg , prd VPC-native GKE clusters, where Nodes primary range belongs to Class A (10.0.0.0/x) Pods secondary ranges belongs to Class B (172.20.0.0/x) Services secondary ranges belongs to Class B (172.100.0.0/x) CIDR range for master-ipv4-cidr (k8s api range) belongs to Class B (172.16.0.0/x) In your design assume that each cluster will have maximum of 59 Nodes with max 110 Pods per node and 1700 Service per cluster. Use following reference docs: VPC-native clusters GKE address management Using tables from VPC-native clusters document and online subnet calculator , create a table for dev , stg , prd in the following format and store result under docs/production_gke.md : env | subnet | pod range | srv range | kubectl api range dev | IP range | IP range | IP range | IP range stg | IP range | IP range | IP range | IP range prd | IP range | IP range | IP range | IP range Document a subnet VPC design in docs/production_gke.md doc under step 2 . Task N3: Create a user-managed subnet. Create a subnet for dev cluster, taking in consideration VPC subnet ranges created in above table, where: Subnet name: $ORG-$PRODUCT-$ENV-vpc Node Range: See column subnet in above table for dev cluster Secondary service range with name: services Secondary Ranges: Service range name: services Service range CIDR: See column srv range in above table for dev cluster Pods range name: pods Pods range CIDR: See column pod range in above table for dev cluster Features: Flow Logs Private IP Google Access TODO: gcloud subnet create Reference docs: Creating a private cluster with custom subnet VPC-native clusters Use gcloud subnets create command reference for all available options. Review created subnet: gcloud compute networks subnets list Also check in Google cloud UI: Networking->VPC Networks -> Click VPC network and check `Subnet` tab Document a subnet VPC creation command in docs/production_gke.md doc under step 3 .","title":"1.4 Design and create a user-managed subnet"},{"location":"020_Assignment_2_Production_GKE/#15-creating-a-private-regional-and-vpc-native-gke-cluster","text":"Task N4: Create dev Private GKE Cluster with no client access to the public endpoint and following values: Cluster name: $ORG-$PRODUCT-$ENV-cluster Secondary pod range with name: pods Secondary service range with name: services VM size: e2-small Node count: 1 per zone GKE Control plane is replicated across three zones of a region: us-central1 GKE version: \"1.20.8-gke.700\" Cluster Node Communication: VPC Native Cluster Nodes access: Private Node GKE Cluster with Public API endpoint Cluster K8s API access: with limited access to the public endpoint via authorized network Use following reference docs: Creating a private cluster GKE Release Channels Use gcloud container clusters create command reference for all available options. #TODO gcloud clusters create $ORG-$PRODUCT-$ENV-cluster Document a GKE cluster creation command in docs/production_gke.md doc under step 4 . Result The private cluster is now created. gcloud has set up kubectl to authenticate with the private cluster but the cluster's K8s API server will only accept connections from the primary range of your subnetwork, and the secondary range of your subnetwork that is used for pods. That means nobody can access K8s API at this point of time, until we specify allowed ranges. Step 3 Authenticate to the cluster. gcloud container clusters get-credentials $ORG-$PRODUCT-$ENV-cluster --region us-central1 Step 4: Connecting to a Private Cluster Let's try to connect to the cluster: kubectl get pods Unable to connect to the server i/o timeout Fail This fails because private clusters firewall traffic to the master by default. In order to connect to the cluster you need to make use of the master authorized networks feature. Step 4: Enable Master Authorized networks on you cluster Suppose you have a group of machines, outside of your VPC network, belongs to your organization. You could authorize those machines to access the public endpoint. Here we will enable master authorized networks and whitelist the IP address for our Cloud Shell instance, to allow access to the master: gcloud container clusters update $ORG-$PRODUCT-$ENV-cluster --enable-master-authorized-networks --master-authorized-networks $(curl ipinfo.io/ip)/32 --region us-central1 Now we can access the API server using kubectl from GCP console: Note In real life it could be CIDR Range for you company, so only engineers or CI/CD systems from you company can connect to kubectl apis in secure manner. Now we can access the API server using kubectl: kubectl get pods kubectl run hello-web --image=gcr.io/google-samples/hello-app:1.0 --port=8080 kubectl get pods Output: NAME READY STATUS RESTARTS AGE hello-web 1/1 Running 0 7s Result We can deploy Pods to our Private Cluster. kubectl delete pod hello-web Step 2: Testing Outbound Traffic Outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads. cat <<EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: wget spec: containers: - name: wget image: alpine command: ['wget', '-T', '5', 'http://www.example.com/'] restartPolicy: Never EOF kubectl get pods Output: NAME READY STATUS RESTARTS AGE wget 0/1 Error 0 4m41s kubectl logs wget Output: Connecting to www.example.com (93.184.216.34:80) wget: download timed out Results Pods can't access internet, because it's a private cluster Note Normally, if cluster is Private and doesn't have Cloud Nat configured, users can't even deploy images from Docker Hub. See troubleshooting details here","title":"1.5 Creating a Private, Regional and VPC Native GKE Cluster"},{"location":"020_Assignment_2_Production_GKE/#16-create-a-google-cloud-nat","text":"Private clusters give you the ability to isolate nodes from having inbound and outbound connectivity to the public internet. This isolation is achieved as the nodes have internal IP addresses only. If you want to provide outbound internet access for certain private nodes, you can use Cloud NAT Cloud NAT is a distributed, software-defined managed service. It's not based on proxy VMs or appliances. You configure a NAT gateway on a Cloud Router, which provides the control plane for NAT, holding configuration parameters that you specify. Google Cloud runs and maintains processes on the physical machines that run your Google Cloud VMs. Cloud NAT can be configured to automatically scale the number of NAT IP addresses that it uses, and it supports VMs that belong to managed instance groups, including those with autoscaling enabled. Step 1: Create a Cloud NAT configuration using Cloud Router Create Cloud Router in the same region as the instances that use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway. Step 1a: Create a Cloud Router : gcloud compute routers create gke-nat-router \\ --network $ORG-$PRODUCT-$ENV-vpc \\ --region us-central1 Verify created Cloud Router: Networking -> Hybrid Connectivity -> Cloud Routers Step 1b: Create a Cloud Nat Gateway: gcloud compute routers nats create nat-config \\ --router-region us-central1 \\ --router gke-nat-router \\ --nat-all-subnet-ip-ranges \\ --auto-allocate-nat-external-ips Verify created Cloud Nat : Networking -> Network Services -> Cloud NAT Note Cloud NAT uses Cloud Router only to group NAT configuration information (control plane). Cloud NAT does not direct a Cloud Router to use BGP or to add routes. NAT traffic does not pass through a Cloud Router (data plane). Document Cloud Router and Cloud Nat creation command in docs/production_gke.md doc under step 5 and 6 respectively. Step 2: Testing Outbound Traffic Most outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads. cat <<EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: wget spec: containers: - name: wget image: alpine command: ['wget', '-T', '5', 'http://www.example.com/'] restartPolicy: Never EOF kubectl get pods Output: NAME READY STATUS RESTARTS AGE wget 0/1 Completed 0 2m53s kubectl logs wget Output: Connecting to www.example.com (93.184.216.34:80) saving to 'index.html' index.html 100% |********************************| 1256 0:00:00 ETA 'index.html' saved Results Pods can access internet, thanks to our configured Cloud Nat.","title":"1.6 Create a Google Cloud Nat"},{"location":"020_Assignment_2_Production_GKE/#2-deploy-notepad-go-webapp","text":"","title":"2 Deploy NotePad Go webapp"},{"location":"020_Assignment_2_Production_GKE/#21-create-a-namespace-dev","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"2.1 Create a Namespace dev"},{"location":"020_Assignment_2_Production_GKE/#22-create-mysql-deployment","text":"Create Mysql deployment: Step 1 Deploy PVC, Deployment, Services and Network Policy: cd ~/$MY_REPO/deploy_ycit020_a2 kubectl apply -f gowebapp-mysql-pvc.yaml #Create PVC kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment kubectl apply -f default-deny-dev.yaml # deny-all Ingress Traffic to dev Namespace kubectl apply -f gowebapp-mysql-netpol.yaml # `mysql` pod can be accessed from the `gowebapp` pod Verify status of mysql: kubectl get deploy,secret,pvc","title":"2.2 Create Mysql deployment"},{"location":"020_Assignment_2_Production_GKE/#23-create-gowebapp-deployment","text":"Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_ycit020_a2/ cd ~/$MY_REPO/deploy_ycit020_a2/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl apply -f gowebapp-netpol.yaml #gowebapp pod from Internet on CIDR range kubectl apply -f gowebapp-ingress.yaml #Create Ingress Verify status of gowebapp: kubectl get pods,cm,ing","title":"2.3 Create GoWebApp deployment"},{"location":"020_Assignment_2_Production_GKE/#3-running-gke-in-production","text":"","title":"3 Running GKE in Production"},{"location":"020_Assignment_2_Production_GKE/#31-deploy-a-new-nodepool","text":"A Kubernetes Engine cluster consists of a master and nodes. Kubernetes doesn't handle provisioning of nodes, so Google Kubernetes Engine handles this for you with a concept called node pools . A node pool is a subset of node instances within a cluster that all have the same configuration. They map to instance templates in Google Compute Engine, which provides the VMs used by the cluster. By default a Kubernetes Engine cluster has a single node pool, but you can add or remove them as you wish to change the shape of your cluster. In the previous example, you created a Kubernetes Engine cluster. This gave us three nodes (three e2-small (2 vCPUs, 2 GB memory), 100 GB of disk each) in a single node pool (called default-pool). Let's inspect the node pool: gcloud config set compute/region us-central1 gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-cluster Output: NAME MACHINE_TYPE DISK_SIZE_GB NODE_VERSION default-pool e2-small 100 1.20.8-gke.700 If you want to add more nodes of this type, you can grow this node pool. If you want to add more nodes of a different type, you can add other node pools. A common method of moving a cluster to larger nodes is to add a new node pool, move the work from the old nodes to the new, and delete the old node pool. Let's add a second node pool, and migrate our workload over to it. This time we will use the larger e2-medium (2 vCPUs, 4 GB memory), 100 GB of disk each machine type. gcloud container node-pools create new-pool --cluster $ORG-$PRODUCT-$ENV-cluster \\ --machine-type e2-medium --num-nodes 1 Output: NAME MACHINE_TYPE DISK_SIZE_GB NODE_VERSION default-pool e2-small 100 1.20.8-gke.700 new-pool e2-medium 100 1.20.8-gke.700 gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-cluster Output: NAME STATUS ROLES AGE VERSION gke-ayrat-notepad-dev-cl-default-pool-34b29738-b1m8 Ready <none> 11m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-c22a1803-hh91 Ready <none> 11m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-f2a61828-zjgk Ready <none> 11m v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-2df77edb-thgj Ready <none> 104s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-40bedbb3-gsxr Ready <none> 101s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-d8f28859-hgqn Ready <none> 60s v1.20.8-gke.700 Kubernetes does not reschedule Pods as long as they are running and available, so your workload remains running on the nodes in the default pool. Look at one of your nodes using kubectl describe. Just like you can attach labels to pods, nodes are automatically labelled with useful information which lets the scheduler make decisions and the administrator perform action on groups of nodes. Replace \"[NODE NAME]\" with the name of one of your nodes from the previous step. kubectl describe node [NODE NAME] | head -n 20 Output: Name: gke-ayrat-notepad-dev-cluste-new-pool-d8f28859-hgqn Roles: <none> Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=e2-standard-4 beta.kubernetes.io/os=linux cloud.google.com/gke-boot-disk=pd-standard cloud.google.com/gke-container-runtime=containerd cloud.google.com/gke-nodepool=new-pool cloud.google.com/gke-os-distribution=cos cloud.google.com/machine-family=e2 failure-domain.beta.kubernetes.io/region=us-central1 failure-domain.beta.kubernetes.io/zone=us-central1-b kubernetes.io/arch=amd64 kubernetes.io/hostname=gke-ayrat-notepad-dev-cluste-new-pool-d8f28859-hgqn kubernetes.io/os=linux node.kubernetes.io/instance-type=e2-standard-4 node.kubernetes.io/masq-agent-ds-ready=true topology.gke.io/zone=us-central1-b topology.kubernetes.io/region=us-central1 topology.kubernetes.io/zone=us-central1-b <...> You can also select nodes by node pool using the cloud.google.com/gke-nodepool label. We'll use this powerful construct shortly. kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool Output: NAME STATUS ROLES AGE VERSION gke-ayrat-notepad-dev-cl-default-pool-34b29738-b1m8 Ready <none> 14m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-c22a1803-hh91 Ready <none> 14m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-f2a61828-zjgk Ready <none> 14m v1.20.8-gke.70","title":"3.1 Deploy a New NodePool"},{"location":"020_Assignment_2_Production_GKE/#32-migrating-pods-to-the-new-node-pool","text":"To migrate your pods to the new node pool, we will perform the following steps: Cordon the existing node pool: This operation marks the nodes in the existing node pool (default-pool) as unschedulable. Kubernetes stops scheduling new Pods to these nodes once you mark them as unschedulable. Drain the existing node pool: This operation evicts the workloads running on the nodes of the existing node pool (default-pool) gracefully. You could cordon an individual node using the kubectl cordon command, but running this command on each node individually would be tedious. To speed up the process, we can embed the command in a loop. Be sure you copy the whole line - it will have scrolled off the screen to the right! for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do kubectl cordon \"$node\"; done Output: node/gke-ayrat-notepad-dev-cl-default-pool-4a7a0d9d-d5q9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-4d5f96ee-nnv9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 cordoned Note This loop utilizes the command kubectl get nodes to select all nodes in the default pool (using the cloud.google.com/gke-nodepool=default-pool label), and then it iterates through and runs kubectl cordon on each one. After running the loop, you should see that the default-pool nodes have SchedulingDisabled status in the node list: kubectl get nodes Output: NAME STATUS ROLES AGE VERSION gke-ayrat-notepad-dev-cl-default-pool-4a7a0d9d-d5q9 Ready,SchedulingDisabled <none> 38m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-4d5f96ee-nnv9 Ready,SchedulingDisabled <none> 38m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 Ready,SchedulingDisabled <none> 38m v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-af1b690a-v6j9 Ready <none> 2m57s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-f3195336-k11r Ready <none> 2m44s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-f47e9e43-6qzb Ready <none> 2m50s v1.20.8-gke.700 Next, we want to evict the Pods already scheduled on each node. To do this, we will construct another loop, this time using the kubectl drain command: for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do kubectl drain --force --ignore-daemonsets --delete-emptydir-data \"$node\"; done Output: <...> pod/gowebapp-mysql-6ffb7f9586-prddc evicted pod/hello-web evicted node/gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 evicted <...> As each node is drained, the pods running on it are evicted. Eviction makes sure to follow rules to provide the least disruption to the applications as possible. Users in production may want to look at more advanced features like Pod Disruption Budgets . Because the default node pool is unschedulable, the pods are now running on the single machine in the new node pool: kubectl get pods -o wide Output: NAME READY STATUS RESTARTS AGE IP NODE node/gke-ayrat-notepad-dev-cl-default-pool-4a7a0d9d-d5q9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-4d5f96ee-nnv9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 cordoned You can now delete the original node pool: gcloud container node-pools delete default-pool --cluster $ORG-$PRODUCT-$ENV-cluster Check that node-pool has been deleted: gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-cluster Result GKE cluster running only on new-pool Node pool.","title":"3.2 Migrating pods to the new Node Pool"},{"location":"020_Assignment_2_Production_GKE/#33-configure-node-auto-repair-and-node-auto-upgrades","text":"Kubernetes Engine's node auto-repair feature helps you keep the nodes in your cluster in a healthy, running state. When enabled, Kubernetes Engine makes periodic checks on the health state of each node in your cluster. If a node fails consecutive health checks over an extended time period (approximately 10 minutes), Kubernetes Engine initiates a repair process for that node. gcloud container node-pools update new-pool --cluster $ORG-$PRODUCT-$ENV-cluster --enable-autorepair This will enable the autorepair feature for nodes. Please see https://cloud.google.com/kubernetes-engine/docs/node-auto-repair for more information on node autorepairs. First, Enable and configure OS Login in GKE: gcloud compute project-info add-metadata --metadata enable-oslogin=TRUE Next, for some fun: let's break a VM! This gcloud command will find the VM in your regional node pool which is in the default zone, and SSH into it. If you are asked to generate an SSH key just answer 'Y' at the prompt and hit enter to not set a passphrase. gcloud compute ssh $(gcloud compute instances list | \\ grep -m 1 notepad-dev | \\ awk '{ print $1 }') \\ --zone us-central1-a You can simulate a node failure by removing the kubelet binary, which is responsible for running Pods on every Kubernetes node: sudo rm /home/kubernetes/bin/kubelet && sudo systemctl restart kubelet logout Now when we check the node status we see the node is NotReady. watch kubectl get nodes NAME STATUS ROLES AGE VERSION gke-gke-workshop-new-pool-42b33f8c-9grf Ready <none> 6m v1.9.6-gke.1 gke-gke-workshop-new-pool-847e18a1-f1bp Ready <none> 6m v1.9.6-gke.1 gke-gke-workshop-new-pool-8c4b26e9-fq8p NotReady <none> 6m v1.9.6-gke.1 The Kubernetes Engine node repair agent will wait a few minutes in case the problem is intermittent. We'll come back to this in a minute. Define a maintenance window You can configure a maintenance window to have more control over when automatic upgrades are applied to Kubernetes on your cluster. Creating a maintenance window instructs Kubernetes Engine to automatically trigger any automated tasks in your clusters, such as master upgrades, node pool upgrades, and maintenance of internal components, during a specific timeframe. The times are specified in UTC, so select an appropriate time and set up a maintenance window for your cluster. Open the cloud console and navigate to Kubernetes Engine. Click on the gke-workshop cluster and click the edit button at the top. Find the Maintenance Window option and select 3AM. Finally, click Save to update the cluster. Enable node auto-upgrades Whenever a new version of Kubernetes is released, Google upgrades your master to that version. You can then choose to upgrade your nodes to that version, bringing functionality and security updates to both the OS and the Kubernetes components. Node Auto-Upgrades use the same update mechanism as manual node upgrades, but does the scheduled upgrades during your maintenance window. Auto-upgrades are enabled per node pool. gcloud container node-pools update new-pool --cluster $ORG-$PRODUCT-$ENV-cluster --enable-autoupgrade This will enable the autoupgrade feature for nodes. Please see https://cloud.google.com/kubernetes-engine/docs/node-management for more information on node autoupgrades. ERROR: (gcloud.container.node-pools.update) ResponseError: code=400, message=Operation operation-1511896056410-dbda7f9f is currently operating on cluster gke-workshop. Please wait and try again once it's done. That's good - that means our broken node is being repaired! Try again in a few minutes. Check your node repair How is that node repair coming? After a few minutes, you will see that the master drained the node, and then removed it. watch kubectl get nodes NotReady,SchedulingDisabled A few minutes after that, a new node was turned on in its place: Ready kubectl get pods kubectl delete deployment hello-web","title":"3.3 Configure Node auto-repair and Node auto-upgrades"},{"location":"020_Assignment_2_Production_GKE/#4-commit-readme-doc-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit docs folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Readme doc for Production GKE Creation using gcloud\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"4 Commit Readme doc to repository and share it with Instructor/Teacher"},{"location":"020_Assignment_2_Production_GKE/#5-cleanup","text":"gcloud container clusters delete $ORG-$PRODUCT-$ENV-cluster Important We going to delete the $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX only, please do not delete your project with Source Code Repo. gcloud projects delete $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX","title":"5 Cleanup"},{"location":"020_Assignment_2_sol_Production_GKE/","text":"Lab 2 Creating Production GKE Cluster Objective: 1 Creating Production GKE Cluster \u00b6 1.1 Locate Assignment 2 \u00b6 Step 1 Clone ycit020 repo with Kubernetes manifests, which going to use for our work: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit020 cd ~/ycit020/Assignment2/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 2 deploy_a2 folder to your repo: cp -r ~/ycit020/Assignment2/deploy_a2 deploy_ycit020_a2 Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding documentation for ycit020 assignment 2\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 1.2 Creating a GCP project \u00b6 Note We going to create a temporarily project for this assignment. While you going to store code in existing project that we've used so far in class. Set variables: export ORG=<student-name> export PRODUCT=notepad export ENV=dev export PROJECT_PREFIX=1 # Project has to be unique export PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX Create a project: gcloud projects create $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX List billings and take note of ACCOUNT_ID : gcloud alpha billing accounts list Define a variable for ACCOUNT_ID : export ACCOUNT_ID=<Your Billing Account> Attach your project to the correct billing account: gcloud alpha billing accounts projects link $PROJECT_ID --billing-account=$ACCOUNT_ID Set the newly created project as default for gcloud: gcloud config set project $PROJECT_ID gcloud config set compute/region us-central1 Enable compute , container , cloudresourcemanager APIs: gcloud services enable container.googleapis.com gcloud services enable compute.googleapis.com gcloud services enable cloudresourcemanager.googleapis.com Note Enabling GCP Service APIs very important step in automation (e.g. terraform) Get a list of services that enabled in your project: gcloud services list Note Some services APIs enabled by default during project creation 1.2 Deleting Default VPC \u00b6 Observe Asset Inventory in GCP UI: Products -> IAM & Admin -> Asset Inventory -> Overview Result You can see that vpc default network spans across all GCP Regions, which for many companies will not be acceptable practice (e.g. GDPR) List all networks in a project: gcloud compute networks list Output: NAME SUBNET_MODE BGP_ROUTING_MODE IPV4_RANGE GATEWAY_IPV4 default AUTO REGIONAL Review existing firewall rules for default vpc: gcloud compute firewall-rules list Also check in Google cloud UI: Networking->Firewalls Delete firewall rules associated with default vpc network: gcloud compute firewall-rules delete default-allow-internal gcloud compute firewall-rules delete default-allow-ssh gcloud compute firewall-rules delete default-allow-rdp gcloud compute firewall-rules delete default-allow-icmp Delete the Default network, following best practices: gcloud compute networks delete default 1.3 Creating a custom mode network (VPC) \u00b6 Task N1: Using reference doc: Creating a custom mode network . Create a new custom mode VPC network using gcloud command with following parameters: Network name: $ORG-$PRODUCT-$ENV-vpc Subnet mode: custom Bgp routing mode: regional MTUs: default Document a command to create vpc network in docs/production_gke.md doc under step 1 . Step 1: Create a new custom mode VPC network: with name ORG-$PRODUCT-$ENV-vpc , with subnet mode custom , and regional . Step 1: Create a new custom mode VPC network: Set variables: export PRODUCT=notepad export ENV=dev gcloud compute networks create $ORG-$PRODUCT-$ENV-vpc \\ --subnet-mode=custom \\ --bgp-routing-mode=regional \\ --mtu=1460 Review created network: gcloud compute networks list Also check in Google cloud UI: Networking->VPC Networks Step 2: Create firewall rules default-allow-internal and default-allow-ssh : gcloud compute firewall-rules create $ORG-$PRODUCT-$ENV-allow-tcp-ssh-icmp --network $ORG-$PRODUCT-$ENV-vpc --allow tcp:22,tcp:3389,icmp gcloud compute firewall-rules create allow-internal --network $ORG-$PRODUCT-$ENV-vpc --allow tcp,udp,icmp --source-ranges 10.128.0.0/22 Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/firewall-rules Review created firewall rules: gcloud compute firewall-rules list Also check in Google cloud UI: Networking->Firewalls 1.4 Design and create a user-managed subnet \u00b6 After you've created VPC network, it is require to add subnet to it. Task N2: In order to ensure that GKE clusters in you organization doesn't overlap each other, design VPC subnet ranges for dev , stg , prd VPC-native GKE clusters, where Nodes primary range belongs to Class A (10.0.0.0/x) Pods secondary ranges belongs to Class B (172.20.0.0/x) Services secondary ranges belongs to Class B (172.100.0.0/x) CIDR range for master-ipv4-cidr (k8s api range) belongs to Class B (172.16.0.0/x) In your design assume that each cluster will have maximum of 59 Nodes with max 110 Pods per node and 1700 Service per cluster. Use following reference docs: VPC-native clusters GKE address management Using tables from VPC-native clusters document and online subnet calculator , create a table for dev , stg , prd in the following format and store result under docs/creating_gke.md : env | subnet | pod range | srv range | kubectl api range dev | 10.128.1.0/26 | 172.20.0.0/18 | 172.100.0.0/21 | 172.16.0.0/28 stg | 10.128.2.0/26 | 172.21.0.0/18 | 172.101.0.0/21 | 172.16.0.16/28 prd | 10.128.3.0/26 | 172.22.0.0/18 | 172.102.0.0/21 | 172.16.0.32/28 Note Ranges must be with in Private (RFC1918) Address Space Document a subnet VPC design in docs/production_gke.md doc under step 2 . Task N3: Create a user-managed subnet. Create a subnet for dev cluster, taking in consideration VPC subnet ranges created in above table, where: Subnet name: $ORG-$PRODUCT-$ENV-vpc Node Range: See column subnet in above table for dev cluster Secondary service range with name: services Secondary Ranges: Service range name: services Service range CIDR: See column srv range in above table for dev cluster Pods range name: pods Pods range CIDR: See column pod range in above table for dev cluster Features: Flow Logs Private IP Google Access gcloud compute networks subnets create gke-priv-cluster-subnet \\ --network $ORG-$PRODUCT-$ENV-vpc \\ --range 10.128.0.0/26 \\ --region us-central1 --enable-flow-logs \\ --enable-private-ip-google-access \\ --secondary-range services=10.10.0.0/21,pods=172.10.0.0/18 Reference docs: Creating a private cluster with custom subnet VPC-native clusters Use gcloud subnets create command reference for all available options. Review created subnet: gcloud compute networks subnets list Also check in Google cloud UI: Networking->VPC Networks -> Click VPC network and check `Subnet` tab Document a subnet VPC creation command in docs/production_gke.md doc under step 3 . 1.5 Creating a Private, Regional and VPC Native GKE Cluster \u00b6 Task N4: Create dev Private GKE Cluster with no client access to the public endpoint and following values: Cluster name: $ORG-$PRODUCT-$ENV-cluster Secondary pod range with name: pods Secondary service range with name: services VM size: e2-small Node count: 1 per zone GKE Control plane is replicated across three zones of a region: us-central1 GKE version: \"1.20.8-gke.700\" Cluster Node Communication: VPC Native Cluster Nodes access: Private Node GKE Cluster with Public API endpoint Cluster K8s API access: with limited access to the public endpoint via authorized network Use following reference docs: Creating a private cluster GKE Release Channels Use gcloud container clusters create command reference for all available options. gcloud container clusters create $ORG-$PRODUCT-$ENV-cluster \\ --region us-central1 \\ --num-nodes 1 \\ --machine-type \"e2-small\" \\ --cluster-version \"1.20.8-gke.700\" \\ --release-channel \"rapid\" \\ --enable-network-policy \\ --network $ORG-$PRODUCT-$ENV-vpc \\ --subnetwork gke-priv-cluster-subnet \\ --cluster-secondary-range-name pods \\ --services-secondary-range-name services \\ --enable-ip-alias \\ --enable-private-nodes \\ --master-ipv4-cidr 172.16.0.0/28 Document a GKE cluster creation command in docs/production_gke.md doc under step 4 . Result The private cluster is now created. gcloud has set up kubectl to authenticate with the private cluster but the cluster's K8s API server will only accept connections from the primary range of your subnetwork, and the secondary range of your subnetwork that is used for pods. That means nobody can access K8s API at this point of time, until we specify allowed ranges. Step 3 Authenticate to the cluster. gcloud container clusters get-credentials $ORG-$PRODUCT-$ENV-cluster --region us-central1 Step 4: Connecting to a Private Cluster Let's try to connect to the cluster: kubectl get pods Unable to connect to the server i/o timeout Fail This fails because private clusters firewall traffic to the master by default. In order to connect to the cluster you need to make use of the master authorized networks feature. Step 4: Enable Master Authorized networks on you cluster Suppose you have a group of machines, outside of your VPC network, belongs to your organization. You could authorize those machines to access the public endpoint. Here we will enable master authorized networks and whitelist the IP address for our Cloud Shell instance, to allow access to the master: gcloud container clusters update $ORG-$PRODUCT-$ENV-cluster --enable-master-authorized-networks --master-authorized-networks $(curl ipinfo.io/ip)/32 --region us-central1 Now we can access the API server using kubectl from GCP console: Note In real life it could be CIDR Range for you company, so only engineers or CI/CD systems from you company can connect to kubectl apis in secure manner. Now we can access the API server using kubectl: kubectl get pods kubectl run hello-web --image=gcr.io/google-samples/hello-app:1.0 --port=8080 kubectl get pods Output: NAME READY STATUS RESTARTS AGE hello-web 1/1 Running 0 7s Result We can deploy Pods to our Private Cluster. kubectl delete pod hello-web Step 2: Testing Outbound Traffic Outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads. cat <<EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: wget spec: containers: - name: wget image: alpine command: ['wget', '-T', '5', 'http://www.example.com/'] restartPolicy: Never EOF kubectl get pods Output: NAME READY STATUS RESTARTS AGE wget 0/1 Error 0 4m41s kubectl logs wget Output: Connecting to www.example.com (93.184.216.34:80) wget: download timed out Results Pods can't access internet, because it's a private cluster Note Normally, if cluster is Private and doesn't have Cloud Nat configured, users can't even deploy images from Docker Hub. See troubleshooting details here 1.6 Create a Google Cloud Nat \u00b6 Private clusters give you the ability to isolate nodes from having inbound and outbound connectivity to the public internet. This isolation is achieved as the nodes have internal IP addresses only. If you want to provide outbound internet access for certain private nodes, you can use Cloud NAT Cloud NAT is a distributed, software-defined managed service. It's not based on proxy VMs or appliances. You configure a NAT gateway on a Cloud Router, which provides the control plane for NAT, holding configuration parameters that you specify. Google Cloud runs and maintains processes on the physical machines that run your Google Cloud VMs. Cloud NAT can be configured to automatically scale the number of NAT IP addresses that it uses, and it supports VMs that belong to managed instance groups, including those with autoscaling enabled. Step 1: Create a Cloud NAT configuration using Cloud Router Create Cloud Router in the same region as the instances that use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway. Step 1a: Create a Cloud Router : gcloud compute routers create gke-nat-router \\ --network $ORG-$PRODUCT-$ENV-vpc \\ --region us-central1 Verify created Cloud Router: Networking -> Hybrid Connectivity -> Cloud Routers Step 1b: Create a Cloud Nat Gateway: gcloud compute routers nats create nat-config \\ --router-region us-central1 \\ --router gke-nat-router \\ --nat-all-subnet-ip-ranges \\ --auto-allocate-nat-external-ips Verify created Cloud Nat : Networking -> Network Services -> Cloud NAT Note Cloud NAT uses Cloud Router only to group NAT configuration information (control plane). Cloud NAT does not direct a Cloud Router to use BGP or to add routes. NAT traffic does not pass through a Cloud Router (data plane). Document Cloud Router and Cloud Nat creation command in docs/production_gke.md doc under step 5 and 6 respectively. Step 2: Testing Outbound Traffic Most outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads. cat <<EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: wget spec: containers: - name: wget image: alpine command: ['wget', '-T', '5', 'http://www.example.com/'] restartPolicy: Never EOF kubectl get pods Output: NAME READY STATUS RESTARTS AGE wget 0/1 Completed 0 2m53s kubectl logs wget Output: Connecting to www.example.com (93.184.216.34:80) saving to 'index.html' index.html 100% |********************************| 1256 0:00:00 ETA 'index.html' saved Results Pods can access internet, thanks to our configured Cloud Nat. 2 Deploy NotePad Go webapp \u00b6 2.1 Create a Namespace dev \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 2.2 Create Mysql deployment \u00b6 Create Mysql deployment: Step 1 Deploy PVC, Deployment, Services and Network Policy: cd ~/$MY_REPO/deploy_ycit020_a2 kubectl apply -f gowebapp-mysql-pvc.yaml #Create PVC kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment kubectl apply -f default-deny-dev.yaml # deny-all Ingress Traffic to dev Namespace kubectl apply -f gowebapp-mysql-netpol.yaml # `mysql` pod can be accessed from the `gowebapp` pod Verify status of mysql: kubectl get deploy,secret,pvc 2.3 Create GoWebApp deployment \u00b6 Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_ycit020_a2/ cd ~/$MY_REPO/deploy_ycit020_a2/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl apply -f gowebapp-netpol.yaml #gowebapp pod from Internet on CIDR range kubectl apply -f gowebapp-ingress.yaml #Create Ingress Verify status of gowebapp: kubectl get pods,cm,ing 3 Running GKE in Production \u00b6 3.1 Deploy a New NodePool \u00b6 A Kubernetes Engine cluster consists of a master and nodes. Kubernetes doesn't handle provisioning of nodes, so Google Kubernetes Engine handles this for you with a concept called node pools . A node pool is a subset of node instances within a cluster that all have the same configuration. They map to instance templates in Google Compute Engine, which provides the VMs used by the cluster. By default a Kubernetes Engine cluster has a single node pool, but you can add or remove them as you wish to change the shape of your cluster. In the previous example, you created a Kubernetes Engine cluster. This gave us three nodes (three e2-small (2 vCPUs, 2 GB memory), 100 GB of disk each) in a single node pool (called default-pool). Let's inspect the node pool: gcloud config set compute/region us-central1 gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-cluster Output: NAME MACHINE_TYPE DISK_SIZE_GB NODE_VERSION default-pool e2-small 100 1.20.8-gke.700 If you want to add more nodes of this type, you can grow this node pool. If you want to add more nodes of a different type, you can add other node pools. A common method of moving a cluster to larger nodes is to add a new node pool, move the work from the old nodes to the new, and delete the old node pool. Let's add a second node pool, and migrate our workload over to it. This time we will use the larger e2-medium (2 vCPUs, 4 GB memory), 100 GB of disk each) machine type. gcloud container node-pools create new-pool --cluster $ORG-$PRODUCT-$ENV-cluster \\ --machine-type e2-medium --num-nodes 1 Output: NAME MACHINE_TYPE DISK_SIZE_GB NODE_VERSION default-pool e2-small 100 1.20.8-gke.700 new-pool e2-medium 100 1.20.8-gke.700 gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-cluster Output: NAME STATUS ROLES AGE VERSION gke-ayrat-notepad-dev-cl-default-pool-34b29738-b1m8 Ready <none> 11m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-c22a1803-hh91 Ready <none> 11m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-f2a61828-zjgk Ready <none> 11m v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-2df77edb-thgj Ready <none> 104s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-40bedbb3-gsxr Ready <none> 101s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-d8f28859-hgqn Ready <none> 60s v1.20.8-gke.700 Kubernetes does not reschedule Pods as long as they are running and available, so your workload remains running on the nodes in the default pool. Look at one of your nodes using kubectl describe. Just like you can attach labels to pods, nodes are automatically labelled with useful information which lets the scheduler make decisions and the administrator perform action on groups of nodes. Replace \"[NODE NAME]\" with the name of one of your nodes from the previous step. kubectl describe node [NODE NAME] | head -n 20 Output: Name: gke-ayrat-notepad-dev-cluste-new-pool-d8f28859-hgqn Roles: <none> Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=e2-standard-4 beta.kubernetes.io/os=linux cloud.google.com/gke-boot-disk=pd-standard cloud.google.com/gke-container-runtime=containerd cloud.google.com/gke-nodepool=new-pool cloud.google.com/gke-os-distribution=cos cloud.google.com/machine-family=e2 failure-domain.beta.kubernetes.io/region=us-central1 failure-domain.beta.kubernetes.io/zone=us-central1-b kubernetes.io/arch=amd64 kubernetes.io/hostname=gke-ayrat-notepad-dev-cluste-new-pool-d8f28859-hgqn kubernetes.io/os=linux node.kubernetes.io/instance-type=e2-standard-4 node.kubernetes.io/masq-agent-ds-ready=true topology.gke.io/zone=us-central1-b topology.kubernetes.io/region=us-central1 topology.kubernetes.io/zone=us-central1-b <...> You can also select nodes by node pool using the cloud.google.com/gke-nodepool label. We'll use this powerful construct shortly. kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool Output: NAME STATUS ROLES AGE VERSION gke-ayrat-notepad-dev-cl-default-pool-34b29738-b1m8 Ready <none> 14m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-c22a1803-hh91 Ready <none> 14m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-f2a61828-zjgk Ready <none> 14m v1.20.8-gke.70 3.2 Migrating pods to the new Node Pool \u00b6 To migrate your pods to the new node pool, we will perform the following steps: Cordon the existing node pool: This operation marks the nodes in the existing node pool (default-pool) as unschedulable. Kubernetes stops scheduling new Pods to these nodes once you mark them as unschedulable. Drain the existing node pool: This operation evicts the workloads running on the nodes of the existing node pool (default-pool) gracefully. You could cordon an individual node using the kubectl cordon command, but running this command on each node individually would be tedious. To speed up the process, we can embed the command in a loop. Be sure you copy the whole line - it will have scrolled off the screen to the right! for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do kubectl cordon \"$node\"; done Output: node/gke-ayrat-notepad-dev-cl-default-pool-4a7a0d9d-d5q9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-4d5f96ee-nnv9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 cordoned Note This loop utilizes the command kubectl get nodes to select all nodes in the default pool (using the cloud.google.com/gke-nodepool=default-pool label), and then it iterates through and runs kubectl cordon on each one. After running the loop, you should see that the default-pool nodes have SchedulingDisabled status in the node list: kubectl get nodes Output: NAME STATUS ROLES AGE VERSION gke-ayrat-notepad-dev-cl-default-pool-4a7a0d9d-d5q9 Ready,SchedulingDisabled <none> 38m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-4d5f96ee-nnv9 Ready,SchedulingDisabled <none> 38m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 Ready,SchedulingDisabled <none> 38m v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-af1b690a-v6j9 Ready <none> 2m57s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-f3195336-k11r Ready <none> 2m44s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-f47e9e43-6qzb Ready <none> 2m50s v1.20.8-gke.700 Next, we want to evict the Pods already scheduled on each node. To do this, we will construct another loop, this time using the kubectl drain command: for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do kubectl drain --force --ignore-daemonsets --delete-emptydir-data --grace-period=10 \"$node\"; done Output: <...> pod/gowebapp-mysql-6ffb7f9586-prddc evicted pod/hello-web evicted node/gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 evicted <...> As each node is drained, the pods running on it are evicted. Eviction makes sure to follow rules to provide the least disruption to the applications as possible. Users in production may want to look at more advanced features like Pod Disruption Budgets . Because the default node pool is unschedulable, the pods are now running on the single machine in the new node pool: kubectl get pods -o wide Output: NAME READY STATUS RESTARTS AGE IP NODE node/gke-ayrat-notepad-dev-cl-default-pool-4a7a0d9d-d5q9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-4d5f96ee-nnv9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 cordoned Important GKE cluster can be in reparing mode for some time. Please wait ~10 minutes before procceed forward. You can now delete the original node pool: gcloud container node-pools delete default-pool --cluster $ORG-$PRODUCT-$ENV-cluster 3.3 Configure Node auto-repair and Node auto-upgrades \u00b6 Kubernetes Engine's node auto-repair feature helps you keep the nodes in your cluster in a healthy, running state. When enabled, Kubernetes Engine makes periodic checks on the health state of each node in your cluster. If a node fails consecutive health checks over an extended time period (approximately 10 minutes), Kubernetes Engine initiates a repair process for that node. gcloud container node-pools update new-pool --cluster $ORG-$PRODUCT-$ENV-cluster --enable-autorepair This will enable the autorepair feature for nodes. Please see https://cloud.google.com/kubernetes-engine/docs/node-auto-repair for more information on node autorepairs. First, Enable and configure OS Login in GKE: gcloud compute project-info add-metadata --metadata enable-oslogin=TRUE Next, for some fun: let's break a VM! This gcloud command will find the VM in your regional node pool which is in the default zone, and SSH into it. If you are asked to generate an SSH key just answer 'Y' at the prompt and hit enter to not set a passphrase. gcloud compute ssh $(gcloud compute instances list | \\ grep -m 1 notepad-dev | \\ awk '{ print $1 }') \\ --zone us-central1-a You can simulate a node failure by removing the kubelet binary, which is responsible for running Pods on every Kubernetes node: sudo rm /home/kubernetes/bin/kubelet && sudo systemctl restart kubelet logout Now when we check the node status we see the node is NotReady. watch kubectl get nodes NAME STATUS ROLES AGE VERSION gke-gke-workshop-new-pool-42b33f8c-9grf Ready <none> 6m v1.9.6-gke.1 gke-gke-workshop-new-pool-847e18a1-f1bp Ready <none> 6m v1.9.6-gke.1 gke-gke-workshop-new-pool-8c4b26e9-fq8p NotReady <none> 6m v1.9.6-gke.1 The Kubernetes Engine node repair agent will wait a few minutes in case the problem is intermittent. We'll come back to this in a minute. Define a maintenance window You can configure a maintenance window to have more control over when automatic upgrades are applied to Kubernetes on your cluster. Creating a maintenance window instructs Kubernetes Engine to automatically trigger any automated tasks in your clusters, such as master upgrades, node pool upgrades, and maintenance of internal components, during a specific timeframe. The times are specified in UTC, so select an appropriate time and set up a maintenance window for your cluster. Open the cloud console and navigate to Kubernetes Engine. Click on the gke-workshop cluster and click the edit button at the top. Find the Maintenance Window option and select 3AM. Finally, click Save to update the cluster. Enable node auto-upgrades Whenever a new version of Kubernetes is released, Google upgrades your master to that version. You can then choose to upgrade your nodes to that version, bringing functionality and security updates to both the OS and the Kubernetes components. Node Auto-Upgrades use the same update mechanism as manual node upgrades, but does the scheduled upgrades during your maintenance window. Auto-upgrades are enabled per node pool. gcloud container node-pools update new-pool --cluster $ORG-$PRODUCT-$ENV-cluster --enable-autoupgrade This will enable the autoupgrade feature for nodes. Please see https://cloud.google.com/kubernetes-engine/docs/node-management for more information on node autoupgrades. ERROR: (gcloud.container.node-pools.update) ResponseError: code=400, message=Operation operation-1511896056410-dbda7f9f is currently operating on cluster gke-workshop. Please wait and try again once it's done. That's good - that means our broken node is being repaired! Try again in a few minutes. Check your node repair How is that node repair coming? After a few minutes, you will see that the master drained the node, and then removed it. watch kubectl get nodes NotReady,SchedulingDisabled A few minutes after that, a new node was turned on in its place: Ready kubectl get pods kubectl delete deployment hello-web 4 Commit Readme doc to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit docs folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Readme doc for Production GKE Creation using gcloud\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 5 Cleanup \u00b6 gcloud container clusters delete $ORG-$PRODUCT-$ENV-cluster Important We going to delete the $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX only, please do not delete your project with Source Code Repo. gcloud projects delete $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX","title":"Assignment2 Sol"},{"location":"020_Assignment_2_sol_Production_GKE/#1-creating-production-gke-cluster","text":"","title":"1 Creating Production GKE Cluster"},{"location":"020_Assignment_2_sol_Production_GKE/#11-locate-assignment-2","text":"Step 1 Clone ycit020 repo with Kubernetes manifests, which going to use for our work: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit020 cd ~/ycit020/Assignment2/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 2 deploy_a2 folder to your repo: cp -r ~/ycit020/Assignment2/deploy_a2 deploy_ycit020_a2 Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding documentation for ycit020 assignment 2\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"1.1 Locate Assignment 2"},{"location":"020_Assignment_2_sol_Production_GKE/#12-creating-a-gcp-project","text":"Note We going to create a temporarily project for this assignment. While you going to store code in existing project that we've used so far in class. Set variables: export ORG=<student-name> export PRODUCT=notepad export ENV=dev export PROJECT_PREFIX=1 # Project has to be unique export PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX Create a project: gcloud projects create $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX List billings and take note of ACCOUNT_ID : gcloud alpha billing accounts list Define a variable for ACCOUNT_ID : export ACCOUNT_ID=<Your Billing Account> Attach your project to the correct billing account: gcloud alpha billing accounts projects link $PROJECT_ID --billing-account=$ACCOUNT_ID Set the newly created project as default for gcloud: gcloud config set project $PROJECT_ID gcloud config set compute/region us-central1 Enable compute , container , cloudresourcemanager APIs: gcloud services enable container.googleapis.com gcloud services enable compute.googleapis.com gcloud services enable cloudresourcemanager.googleapis.com Note Enabling GCP Service APIs very important step in automation (e.g. terraform) Get a list of services that enabled in your project: gcloud services list Note Some services APIs enabled by default during project creation","title":"1.2 Creating a GCP project"},{"location":"020_Assignment_2_sol_Production_GKE/#12-deleting-default-vpc","text":"Observe Asset Inventory in GCP UI: Products -> IAM & Admin -> Asset Inventory -> Overview Result You can see that vpc default network spans across all GCP Regions, which for many companies will not be acceptable practice (e.g. GDPR) List all networks in a project: gcloud compute networks list Output: NAME SUBNET_MODE BGP_ROUTING_MODE IPV4_RANGE GATEWAY_IPV4 default AUTO REGIONAL Review existing firewall rules for default vpc: gcloud compute firewall-rules list Also check in Google cloud UI: Networking->Firewalls Delete firewall rules associated with default vpc network: gcloud compute firewall-rules delete default-allow-internal gcloud compute firewall-rules delete default-allow-ssh gcloud compute firewall-rules delete default-allow-rdp gcloud compute firewall-rules delete default-allow-icmp Delete the Default network, following best practices: gcloud compute networks delete default","title":"1.2 Deleting Default VPC"},{"location":"020_Assignment_2_sol_Production_GKE/#13-creating-a-custom-mode-network-vpc","text":"Task N1: Using reference doc: Creating a custom mode network . Create a new custom mode VPC network using gcloud command with following parameters: Network name: $ORG-$PRODUCT-$ENV-vpc Subnet mode: custom Bgp routing mode: regional MTUs: default Document a command to create vpc network in docs/production_gke.md doc under step 1 . Step 1: Create a new custom mode VPC network: with name ORG-$PRODUCT-$ENV-vpc , with subnet mode custom , and regional . Step 1: Create a new custom mode VPC network: Set variables: export PRODUCT=notepad export ENV=dev gcloud compute networks create $ORG-$PRODUCT-$ENV-vpc \\ --subnet-mode=custom \\ --bgp-routing-mode=regional \\ --mtu=1460 Review created network: gcloud compute networks list Also check in Google cloud UI: Networking->VPC Networks Step 2: Create firewall rules default-allow-internal and default-allow-ssh : gcloud compute firewall-rules create $ORG-$PRODUCT-$ENV-allow-tcp-ssh-icmp --network $ORG-$PRODUCT-$ENV-vpc --allow tcp:22,tcp:3389,icmp gcloud compute firewall-rules create allow-internal --network $ORG-$PRODUCT-$ENV-vpc --allow tcp,udp,icmp --source-ranges 10.128.0.0/22 Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/firewall-rules Review created firewall rules: gcloud compute firewall-rules list Also check in Google cloud UI: Networking->Firewalls","title":"1.3 Creating a custom mode network (VPC)"},{"location":"020_Assignment_2_sol_Production_GKE/#14-design-and-create-a-user-managed-subnet","text":"After you've created VPC network, it is require to add subnet to it. Task N2: In order to ensure that GKE clusters in you organization doesn't overlap each other, design VPC subnet ranges for dev , stg , prd VPC-native GKE clusters, where Nodes primary range belongs to Class A (10.0.0.0/x) Pods secondary ranges belongs to Class B (172.20.0.0/x) Services secondary ranges belongs to Class B (172.100.0.0/x) CIDR range for master-ipv4-cidr (k8s api range) belongs to Class B (172.16.0.0/x) In your design assume that each cluster will have maximum of 59 Nodes with max 110 Pods per node and 1700 Service per cluster. Use following reference docs: VPC-native clusters GKE address management Using tables from VPC-native clusters document and online subnet calculator , create a table for dev , stg , prd in the following format and store result under docs/creating_gke.md : env | subnet | pod range | srv range | kubectl api range dev | 10.128.1.0/26 | 172.20.0.0/18 | 172.100.0.0/21 | 172.16.0.0/28 stg | 10.128.2.0/26 | 172.21.0.0/18 | 172.101.0.0/21 | 172.16.0.16/28 prd | 10.128.3.0/26 | 172.22.0.0/18 | 172.102.0.0/21 | 172.16.0.32/28 Note Ranges must be with in Private (RFC1918) Address Space Document a subnet VPC design in docs/production_gke.md doc under step 2 . Task N3: Create a user-managed subnet. Create a subnet for dev cluster, taking in consideration VPC subnet ranges created in above table, where: Subnet name: $ORG-$PRODUCT-$ENV-vpc Node Range: See column subnet in above table for dev cluster Secondary service range with name: services Secondary Ranges: Service range name: services Service range CIDR: See column srv range in above table for dev cluster Pods range name: pods Pods range CIDR: See column pod range in above table for dev cluster Features: Flow Logs Private IP Google Access gcloud compute networks subnets create gke-priv-cluster-subnet \\ --network $ORG-$PRODUCT-$ENV-vpc \\ --range 10.128.0.0/26 \\ --region us-central1 --enable-flow-logs \\ --enable-private-ip-google-access \\ --secondary-range services=10.10.0.0/21,pods=172.10.0.0/18 Reference docs: Creating a private cluster with custom subnet VPC-native clusters Use gcloud subnets create command reference for all available options. Review created subnet: gcloud compute networks subnets list Also check in Google cloud UI: Networking->VPC Networks -> Click VPC network and check `Subnet` tab Document a subnet VPC creation command in docs/production_gke.md doc under step 3 .","title":"1.4 Design and create a user-managed subnet"},{"location":"020_Assignment_2_sol_Production_GKE/#15-creating-a-private-regional-and-vpc-native-gke-cluster","text":"Task N4: Create dev Private GKE Cluster with no client access to the public endpoint and following values: Cluster name: $ORG-$PRODUCT-$ENV-cluster Secondary pod range with name: pods Secondary service range with name: services VM size: e2-small Node count: 1 per zone GKE Control plane is replicated across three zones of a region: us-central1 GKE version: \"1.20.8-gke.700\" Cluster Node Communication: VPC Native Cluster Nodes access: Private Node GKE Cluster with Public API endpoint Cluster K8s API access: with limited access to the public endpoint via authorized network Use following reference docs: Creating a private cluster GKE Release Channels Use gcloud container clusters create command reference for all available options. gcloud container clusters create $ORG-$PRODUCT-$ENV-cluster \\ --region us-central1 \\ --num-nodes 1 \\ --machine-type \"e2-small\" \\ --cluster-version \"1.20.8-gke.700\" \\ --release-channel \"rapid\" \\ --enable-network-policy \\ --network $ORG-$PRODUCT-$ENV-vpc \\ --subnetwork gke-priv-cluster-subnet \\ --cluster-secondary-range-name pods \\ --services-secondary-range-name services \\ --enable-ip-alias \\ --enable-private-nodes \\ --master-ipv4-cidr 172.16.0.0/28 Document a GKE cluster creation command in docs/production_gke.md doc under step 4 . Result The private cluster is now created. gcloud has set up kubectl to authenticate with the private cluster but the cluster's K8s API server will only accept connections from the primary range of your subnetwork, and the secondary range of your subnetwork that is used for pods. That means nobody can access K8s API at this point of time, until we specify allowed ranges. Step 3 Authenticate to the cluster. gcloud container clusters get-credentials $ORG-$PRODUCT-$ENV-cluster --region us-central1 Step 4: Connecting to a Private Cluster Let's try to connect to the cluster: kubectl get pods Unable to connect to the server i/o timeout Fail This fails because private clusters firewall traffic to the master by default. In order to connect to the cluster you need to make use of the master authorized networks feature. Step 4: Enable Master Authorized networks on you cluster Suppose you have a group of machines, outside of your VPC network, belongs to your organization. You could authorize those machines to access the public endpoint. Here we will enable master authorized networks and whitelist the IP address for our Cloud Shell instance, to allow access to the master: gcloud container clusters update $ORG-$PRODUCT-$ENV-cluster --enable-master-authorized-networks --master-authorized-networks $(curl ipinfo.io/ip)/32 --region us-central1 Now we can access the API server using kubectl from GCP console: Note In real life it could be CIDR Range for you company, so only engineers or CI/CD systems from you company can connect to kubectl apis in secure manner. Now we can access the API server using kubectl: kubectl get pods kubectl run hello-web --image=gcr.io/google-samples/hello-app:1.0 --port=8080 kubectl get pods Output: NAME READY STATUS RESTARTS AGE hello-web 1/1 Running 0 7s Result We can deploy Pods to our Private Cluster. kubectl delete pod hello-web Step 2: Testing Outbound Traffic Outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads. cat <<EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: wget spec: containers: - name: wget image: alpine command: ['wget', '-T', '5', 'http://www.example.com/'] restartPolicy: Never EOF kubectl get pods Output: NAME READY STATUS RESTARTS AGE wget 0/1 Error 0 4m41s kubectl logs wget Output: Connecting to www.example.com (93.184.216.34:80) wget: download timed out Results Pods can't access internet, because it's a private cluster Note Normally, if cluster is Private and doesn't have Cloud Nat configured, users can't even deploy images from Docker Hub. See troubleshooting details here","title":"1.5 Creating a Private, Regional and VPC Native GKE Cluster"},{"location":"020_Assignment_2_sol_Production_GKE/#16-create-a-google-cloud-nat","text":"Private clusters give you the ability to isolate nodes from having inbound and outbound connectivity to the public internet. This isolation is achieved as the nodes have internal IP addresses only. If you want to provide outbound internet access for certain private nodes, you can use Cloud NAT Cloud NAT is a distributed, software-defined managed service. It's not based on proxy VMs or appliances. You configure a NAT gateway on a Cloud Router, which provides the control plane for NAT, holding configuration parameters that you specify. Google Cloud runs and maintains processes on the physical machines that run your Google Cloud VMs. Cloud NAT can be configured to automatically scale the number of NAT IP addresses that it uses, and it supports VMs that belong to managed instance groups, including those with autoscaling enabled. Step 1: Create a Cloud NAT configuration using Cloud Router Create Cloud Router in the same region as the instances that use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway. Step 1a: Create a Cloud Router : gcloud compute routers create gke-nat-router \\ --network $ORG-$PRODUCT-$ENV-vpc \\ --region us-central1 Verify created Cloud Router: Networking -> Hybrid Connectivity -> Cloud Routers Step 1b: Create a Cloud Nat Gateway: gcloud compute routers nats create nat-config \\ --router-region us-central1 \\ --router gke-nat-router \\ --nat-all-subnet-ip-ranges \\ --auto-allocate-nat-external-ips Verify created Cloud Nat : Networking -> Network Services -> Cloud NAT Note Cloud NAT uses Cloud Router only to group NAT configuration information (control plane). Cloud NAT does not direct a Cloud Router to use BGP or to add routes. NAT traffic does not pass through a Cloud Router (data plane). Document Cloud Router and Cloud Nat creation command in docs/production_gke.md doc under step 5 and 6 respectively. Step 2: Testing Outbound Traffic Most outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads. cat <<EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: wget spec: containers: - name: wget image: alpine command: ['wget', '-T', '5', 'http://www.example.com/'] restartPolicy: Never EOF kubectl get pods Output: NAME READY STATUS RESTARTS AGE wget 0/1 Completed 0 2m53s kubectl logs wget Output: Connecting to www.example.com (93.184.216.34:80) saving to 'index.html' index.html 100% |********************************| 1256 0:00:00 ETA 'index.html' saved Results Pods can access internet, thanks to our configured Cloud Nat.","title":"1.6 Create a Google Cloud Nat"},{"location":"020_Assignment_2_sol_Production_GKE/#2-deploy-notepad-go-webapp","text":"","title":"2 Deploy NotePad Go webapp"},{"location":"020_Assignment_2_sol_Production_GKE/#21-create-a-namespace-dev","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"2.1 Create a Namespace dev"},{"location":"020_Assignment_2_sol_Production_GKE/#22-create-mysql-deployment","text":"Create Mysql deployment: Step 1 Deploy PVC, Deployment, Services and Network Policy: cd ~/$MY_REPO/deploy_ycit020_a2 kubectl apply -f gowebapp-mysql-pvc.yaml #Create PVC kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment kubectl apply -f default-deny-dev.yaml # deny-all Ingress Traffic to dev Namespace kubectl apply -f gowebapp-mysql-netpol.yaml # `mysql` pod can be accessed from the `gowebapp` pod Verify status of mysql: kubectl get deploy,secret,pvc","title":"2.2 Create Mysql deployment"},{"location":"020_Assignment_2_sol_Production_GKE/#23-create-gowebapp-deployment","text":"Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_ycit020_a2/ cd ~/$MY_REPO/deploy_ycit020_a2/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl apply -f gowebapp-netpol.yaml #gowebapp pod from Internet on CIDR range kubectl apply -f gowebapp-ingress.yaml #Create Ingress Verify status of gowebapp: kubectl get pods,cm,ing","title":"2.3 Create GoWebApp deployment"},{"location":"020_Assignment_2_sol_Production_GKE/#3-running-gke-in-production","text":"","title":"3 Running GKE in Production"},{"location":"020_Assignment_2_sol_Production_GKE/#31-deploy-a-new-nodepool","text":"A Kubernetes Engine cluster consists of a master and nodes. Kubernetes doesn't handle provisioning of nodes, so Google Kubernetes Engine handles this for you with a concept called node pools . A node pool is a subset of node instances within a cluster that all have the same configuration. They map to instance templates in Google Compute Engine, which provides the VMs used by the cluster. By default a Kubernetes Engine cluster has a single node pool, but you can add or remove them as you wish to change the shape of your cluster. In the previous example, you created a Kubernetes Engine cluster. This gave us three nodes (three e2-small (2 vCPUs, 2 GB memory), 100 GB of disk each) in a single node pool (called default-pool). Let's inspect the node pool: gcloud config set compute/region us-central1 gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-cluster Output: NAME MACHINE_TYPE DISK_SIZE_GB NODE_VERSION default-pool e2-small 100 1.20.8-gke.700 If you want to add more nodes of this type, you can grow this node pool. If you want to add more nodes of a different type, you can add other node pools. A common method of moving a cluster to larger nodes is to add a new node pool, move the work from the old nodes to the new, and delete the old node pool. Let's add a second node pool, and migrate our workload over to it. This time we will use the larger e2-medium (2 vCPUs, 4 GB memory), 100 GB of disk each) machine type. gcloud container node-pools create new-pool --cluster $ORG-$PRODUCT-$ENV-cluster \\ --machine-type e2-medium --num-nodes 1 Output: NAME MACHINE_TYPE DISK_SIZE_GB NODE_VERSION default-pool e2-small 100 1.20.8-gke.700 new-pool e2-medium 100 1.20.8-gke.700 gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-cluster Output: NAME STATUS ROLES AGE VERSION gke-ayrat-notepad-dev-cl-default-pool-34b29738-b1m8 Ready <none> 11m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-c22a1803-hh91 Ready <none> 11m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-f2a61828-zjgk Ready <none> 11m v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-2df77edb-thgj Ready <none> 104s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-40bedbb3-gsxr Ready <none> 101s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-d8f28859-hgqn Ready <none> 60s v1.20.8-gke.700 Kubernetes does not reschedule Pods as long as they are running and available, so your workload remains running on the nodes in the default pool. Look at one of your nodes using kubectl describe. Just like you can attach labels to pods, nodes are automatically labelled with useful information which lets the scheduler make decisions and the administrator perform action on groups of nodes. Replace \"[NODE NAME]\" with the name of one of your nodes from the previous step. kubectl describe node [NODE NAME] | head -n 20 Output: Name: gke-ayrat-notepad-dev-cluste-new-pool-d8f28859-hgqn Roles: <none> Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=e2-standard-4 beta.kubernetes.io/os=linux cloud.google.com/gke-boot-disk=pd-standard cloud.google.com/gke-container-runtime=containerd cloud.google.com/gke-nodepool=new-pool cloud.google.com/gke-os-distribution=cos cloud.google.com/machine-family=e2 failure-domain.beta.kubernetes.io/region=us-central1 failure-domain.beta.kubernetes.io/zone=us-central1-b kubernetes.io/arch=amd64 kubernetes.io/hostname=gke-ayrat-notepad-dev-cluste-new-pool-d8f28859-hgqn kubernetes.io/os=linux node.kubernetes.io/instance-type=e2-standard-4 node.kubernetes.io/masq-agent-ds-ready=true topology.gke.io/zone=us-central1-b topology.kubernetes.io/region=us-central1 topology.kubernetes.io/zone=us-central1-b <...> You can also select nodes by node pool using the cloud.google.com/gke-nodepool label. We'll use this powerful construct shortly. kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool Output: NAME STATUS ROLES AGE VERSION gke-ayrat-notepad-dev-cl-default-pool-34b29738-b1m8 Ready <none> 14m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-c22a1803-hh91 Ready <none> 14m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-f2a61828-zjgk Ready <none> 14m v1.20.8-gke.70","title":"3.1 Deploy a New NodePool"},{"location":"020_Assignment_2_sol_Production_GKE/#32-migrating-pods-to-the-new-node-pool","text":"To migrate your pods to the new node pool, we will perform the following steps: Cordon the existing node pool: This operation marks the nodes in the existing node pool (default-pool) as unschedulable. Kubernetes stops scheduling new Pods to these nodes once you mark them as unschedulable. Drain the existing node pool: This operation evicts the workloads running on the nodes of the existing node pool (default-pool) gracefully. You could cordon an individual node using the kubectl cordon command, but running this command on each node individually would be tedious. To speed up the process, we can embed the command in a loop. Be sure you copy the whole line - it will have scrolled off the screen to the right! for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do kubectl cordon \"$node\"; done Output: node/gke-ayrat-notepad-dev-cl-default-pool-4a7a0d9d-d5q9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-4d5f96ee-nnv9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 cordoned Note This loop utilizes the command kubectl get nodes to select all nodes in the default pool (using the cloud.google.com/gke-nodepool=default-pool label), and then it iterates through and runs kubectl cordon on each one. After running the loop, you should see that the default-pool nodes have SchedulingDisabled status in the node list: kubectl get nodes Output: NAME STATUS ROLES AGE VERSION gke-ayrat-notepad-dev-cl-default-pool-4a7a0d9d-d5q9 Ready,SchedulingDisabled <none> 38m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-4d5f96ee-nnv9 Ready,SchedulingDisabled <none> 38m v1.20.8-gke.700 gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 Ready,SchedulingDisabled <none> 38m v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-af1b690a-v6j9 Ready <none> 2m57s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-f3195336-k11r Ready <none> 2m44s v1.20.8-gke.700 gke-ayrat-notepad-dev-cluste-new-pool-f47e9e43-6qzb Ready <none> 2m50s v1.20.8-gke.700 Next, we want to evict the Pods already scheduled on each node. To do this, we will construct another loop, this time using the kubectl drain command: for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do kubectl drain --force --ignore-daemonsets --delete-emptydir-data --grace-period=10 \"$node\"; done Output: <...> pod/gowebapp-mysql-6ffb7f9586-prddc evicted pod/hello-web evicted node/gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 evicted <...> As each node is drained, the pods running on it are evicted. Eviction makes sure to follow rules to provide the least disruption to the applications as possible. Users in production may want to look at more advanced features like Pod Disruption Budgets . Because the default node pool is unschedulable, the pods are now running on the single machine in the new node pool: kubectl get pods -o wide Output: NAME READY STATUS RESTARTS AGE IP NODE node/gke-ayrat-notepad-dev-cl-default-pool-4a7a0d9d-d5q9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-4d5f96ee-nnv9 cordoned node/gke-ayrat-notepad-dev-cl-default-pool-eb93c399-zcs8 cordoned Important GKE cluster can be in reparing mode for some time. Please wait ~10 minutes before procceed forward. You can now delete the original node pool: gcloud container node-pools delete default-pool --cluster $ORG-$PRODUCT-$ENV-cluster","title":"3.2 Migrating pods to the new Node Pool"},{"location":"020_Assignment_2_sol_Production_GKE/#33-configure-node-auto-repair-and-node-auto-upgrades","text":"Kubernetes Engine's node auto-repair feature helps you keep the nodes in your cluster in a healthy, running state. When enabled, Kubernetes Engine makes periodic checks on the health state of each node in your cluster. If a node fails consecutive health checks over an extended time period (approximately 10 minutes), Kubernetes Engine initiates a repair process for that node. gcloud container node-pools update new-pool --cluster $ORG-$PRODUCT-$ENV-cluster --enable-autorepair This will enable the autorepair feature for nodes. Please see https://cloud.google.com/kubernetes-engine/docs/node-auto-repair for more information on node autorepairs. First, Enable and configure OS Login in GKE: gcloud compute project-info add-metadata --metadata enable-oslogin=TRUE Next, for some fun: let's break a VM! This gcloud command will find the VM in your regional node pool which is in the default zone, and SSH into it. If you are asked to generate an SSH key just answer 'Y' at the prompt and hit enter to not set a passphrase. gcloud compute ssh $(gcloud compute instances list | \\ grep -m 1 notepad-dev | \\ awk '{ print $1 }') \\ --zone us-central1-a You can simulate a node failure by removing the kubelet binary, which is responsible for running Pods on every Kubernetes node: sudo rm /home/kubernetes/bin/kubelet && sudo systemctl restart kubelet logout Now when we check the node status we see the node is NotReady. watch kubectl get nodes NAME STATUS ROLES AGE VERSION gke-gke-workshop-new-pool-42b33f8c-9grf Ready <none> 6m v1.9.6-gke.1 gke-gke-workshop-new-pool-847e18a1-f1bp Ready <none> 6m v1.9.6-gke.1 gke-gke-workshop-new-pool-8c4b26e9-fq8p NotReady <none> 6m v1.9.6-gke.1 The Kubernetes Engine node repair agent will wait a few minutes in case the problem is intermittent. We'll come back to this in a minute. Define a maintenance window You can configure a maintenance window to have more control over when automatic upgrades are applied to Kubernetes on your cluster. Creating a maintenance window instructs Kubernetes Engine to automatically trigger any automated tasks in your clusters, such as master upgrades, node pool upgrades, and maintenance of internal components, during a specific timeframe. The times are specified in UTC, so select an appropriate time and set up a maintenance window for your cluster. Open the cloud console and navigate to Kubernetes Engine. Click on the gke-workshop cluster and click the edit button at the top. Find the Maintenance Window option and select 3AM. Finally, click Save to update the cluster. Enable node auto-upgrades Whenever a new version of Kubernetes is released, Google upgrades your master to that version. You can then choose to upgrade your nodes to that version, bringing functionality and security updates to both the OS and the Kubernetes components. Node Auto-Upgrades use the same update mechanism as manual node upgrades, but does the scheduled upgrades during your maintenance window. Auto-upgrades are enabled per node pool. gcloud container node-pools update new-pool --cluster $ORG-$PRODUCT-$ENV-cluster --enable-autoupgrade This will enable the autoupgrade feature for nodes. Please see https://cloud.google.com/kubernetes-engine/docs/node-management for more information on node autoupgrades. ERROR: (gcloud.container.node-pools.update) ResponseError: code=400, message=Operation operation-1511896056410-dbda7f9f is currently operating on cluster gke-workshop. Please wait and try again once it's done. That's good - that means our broken node is being repaired! Try again in a few minutes. Check your node repair How is that node repair coming? After a few minutes, you will see that the master drained the node, and then removed it. watch kubectl get nodes NotReady,SchedulingDisabled A few minutes after that, a new node was turned on in its place: Ready kubectl get pods kubectl delete deployment hello-web","title":"3.3 Configure Node auto-repair and Node auto-upgrades"},{"location":"020_Assignment_2_sol_Production_GKE/#4-commit-readme-doc-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit docs folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Readme doc for Production GKE Creation using gcloud\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"4 Commit Readme doc to repository and share it with Instructor/Teacher"},{"location":"020_Assignment_2_sol_Production_GKE/#5-cleanup","text":"gcloud container clusters delete $ORG-$PRODUCT-$ENV-cluster Important We going to delete the $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX only, please do not delete your project with Source Code Repo. gcloud projects delete $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX","title":"5 Cleanup"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/","text":"Lab 3 Creating Production GKE Cluster via IaC using Terraform Objective: Learn Terraform Commands Learn GCP Terraform Provider Learn Terraform variables Store TF State in GCS buckets Learn how to create GCP resources with Terraform Working with Git and IaC \u00b6 Git-based workflows, merge requests and peer reviews create a level of documented transparency that is great for security teams and audits. They ensure every change is documented as well as the metadata surrounding the change, answering the why, who, when and what questions. In this set of exercises you will continue using notepad Google Source Repository that already contains you application code, dockerfiles and kubernetes manifests. We will use so called Monorepo approach, and store our terraform IAC configuration in the same repo as our application code and Kubernetes manifests. Add your IaC code to your repository \u00b6 Step 1 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 2 Create foundation-infrastructure and notepad-infrastructure folders that will contains respective terraform configurations cd ~/$MY_REPO mkdir foundation-infrastructure mkdir notepad-infrastructure Step 3 Create a README.md file describing foundation-infrastructure folder purpose: cat <<EOF> foundation-infrastructure/README.md # Creation of GCP Foundation Layer This step, is focused on creating GCP Foundation: folders (if any) and service projects creation with their respictive GCS bucket to store terraform state and IAM Config. EOF Step 4 Create a README.md file describing notepad-infrastructure folder purpose: cat <<EOF> notepad-infrastructure/README.md # Creation of GCP Services Layer This step, is focused on creating gcp services such as GKE, VPC and etc. inside of existing project EOF Step 5 Create a .gitignore file in your working directory: cat<<EOF>> .gitignore .terraform.* .terraform terraform.tfstate* EOF Note Ignore files are used to tell git not to track files. You should also always include any files with credentials in a gitignore file. List created folder structure along with gitignore : ls -ltra Step 6 Commit deploy folder using the following Git commands: git status git add . git commit -m \"Terraform Folder structure for assignement 3\" Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 1 Build GCP Foundation Layer \u00b6 In this Lab we going to build GCP Foundation layer. As we learned in the class this includes Org Structure creation with Folders and Projects, creation IAM Roles and assigning them to users and groups in organization. This Layer usually build be DevOps or Infra team that. Since we don't have organization registered for each student, we going to skip creation of folders and IAM groups. We going to start from creation of a Project, deleting default VPC and configuring inside that project a gcs bucket that will be used in the next terraform layer to store a Terraform state. Part 1: Create Terraform Configurations file to create's GCP Foundation Layer: Layer 1 From existing (we going to call it seeding) project that you currently use to store code in Google Cloud Source Repository: Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf Create a new notepad-dev Project Delete Default VPC Create a bucket in this project to store terraform state Part 2: Create Terraform Configurations file that create's GCP Services Layer: Layer 2 From notepad-dev GCP Project: Enable Google Project Service APIs Create VPC (google_compute_network) and Subnet (google_compute_subnetwork) Create Cloud Nat (google_compute_router) and (google_compute_router_nat) Private GKE Private Nodes with Public API Endpoint (for simplicity) 1.1 Installing Terraform \u00b6 GCP Cloud Shell comes with many common tools pre-installed including terraform Verify and validate the version of Terraform that is installed: terraform --version If you want to use specific version of terraform or want to install terraform in you local machine use following link to download binary. 1.2 Configure GCP credentials for Terraform \u00b6 If you would like to use terraform on GCP you have 2 options: 1). Using you user credentials, great option for testing terraform from you laptop and for learning purposes. 2). Using service account , great option if you going to use terraform with CI/CD system and fully automate Terraform deployment. In this Lab we going to use Option 1 - using user credentials. Step 1: In order to make requests against the GCP API, you need to authenticate to prove that it's you making the request. The preferred method of provisioning resources with Terraform on your workstation is to use the Google Cloud SDK (Option 1) gcloud auth application-default login 1.3 Initializing Terraform \u00b6 Terraform relies on providers to interact with remote systems. Every resource type is implemented by a provider; without providers , Terraform can't manage any kind of infrastructure; in order for terraform to install and use a provider it must be declared. In this exercise you will declare and configure the Terraform provider(s) that will be used for the rest of the Lab. 1.3.1 Declare Terraform Providers \u00b6 The required_providers block defines the providers terraform will use to provision resources and their source. version : The version argument is optional and is used to tell terraform to pick a particular version from the available versions source : The source is the provider registry e.g. hashicorp/gcp is the short for registry.terraform.io/hashicorp/gcp cd ~/$MY_REPO/foundation-infrastructure/ cat << EOF>> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } } } EOF 1.3.2 Configure the Terraform Provider \u00b6 Providers often require configuration (like endpoint URLs or cloud regions) before they can be used. These configurations are defined by a provider block. Multiple provider configuration blocks can be declared for the same by adding the alias argument Set you current PROJECT_ID value here: export PROJECT_ID=<YOUR_PROJECT_ID> cd ~/$MY_REPO/foundation-infrastructure/ cat << EOF>> main.tf provider \"google\" { alias = \"gcp-provider\" project = \"$PROJECT_ID\" region = \"us-central1\" } EOF Result We configured provider , so that it can provision resource in specified gcp project in us-central1 region. 1.3.3 Initialize Terraform \u00b6 Now that you have declared and configured the GCP provider for terraform, initialize terraform: cd ~/$MY_REPO/foundation-infrastructure/ terraform init Explore your directory. What has changed? ls -ltra Result We can see that new directory .terraform and .terraform.lock.hcl file. Extra! Investigate available providers in the Terraform Provider Registry Select another provider from the list, add it to your required providers, and to your main.tf Run terraform init to load your new provider! 1.4 Terraform Variables \u00b6 Input variables are used to increase your Terraform configuration's flexibility by defining values that can be assigned to customize the configuration. They provide a consistent interface to change how a given configuration behaves. Input variables blocks have a defined format: Input variables blocks have a defined format: variable \"variable_name\" { type = \"The variable type eg. string , list , number\" description = \"A description to understand what the variable will be used for\" default = \"A default value can be provided, terraform will use this if no other value is provided at terraform apply\" } Terraform CLI defines the following optional arguments for variable declarations: default - A default value which then makes the variable optional. type - This argument specifies what value types are accepted for the variable. description - This specifies the input variable's documentation. validation - A block to define validation rules, usually in addition to type constraints. sensitive - Limits Terraform UI output when the variable is used in configuration. In our above example for main.tf we can actually declare project and region as variables, so let's do it. 1.4.1 Declare Input Variables \u00b6 Variables are declared in a variables.tf file inside your terraform working directory. The label after the variable keyword is a name for the variable, which must be unique among all variables in the same module. You can choose variable name based on you preference, or in some cases based on agreed in company naming convention. In our case we declaring variables names: gcp_region and gcp_project_id We are going to declare a type as a string variable for gcp_project_id and gcp_region . It is always good idea to specify a clear description for the variable for documentation purpose as well as code reusability and readability. Finally, let's configured default argument. In our case we going to use \"us-central1\" as a default for gcp_region and we going to keep default value empty for gcp_project_id . cat <<EOF> variables.tf variable \"gcp_region\" { type = string description = \"The GCP Region\" default = \"us-central1\" } variable \"gcp_project_id\" { type = string description = \"The GCP Seeding project ID\" default = \"\" } EOF Note Explore other type of variables such as number , bool and type constructors such as: list( ), set( ), map( ) 1.4.2 Using Variables \u00b6 Now that you have created your input variables, let's re-create main.tf that currently has the terraform configuration for GCP Provider. rm main.tf cat << EOF>> main.tf provider \"google\" { alias = \"gcp-provider\" project = var.gcp_project_id region = var.gcp_region } EOF Test out your terraform configuration: terraform plan 1.4.3 Working with Variables files. \u00b6 Create a terraform.tfvars file to hold the values for your variables: export gcp_project_id=<YOUR_PROJECT_ID> cat <<EOF >> terraform.tfvars gcp_project_id = \"$PROJECT_ID\" gcp_region = \"us-central1\" EOF Hint using different env-name.tfvars files you can create different set of terraform configuration for the same code. (e.g. code for dev, staging, prod) 1.4.4 Validate configuration and code syntax \u00b6 Let's Validate configuration and code syntax that we've added so far. There are several tools that can analyze your Terraform code without running it, including: Terraform has build in command that you can use to check your Terraform syntax and types (a bit like a compiler): terraform validate Result Seems the code is legit so far Is your terraform easy to read and follow? Terraform has a built-in function to lint your configuration manifests for readability and best practice spacing: terraform fmt --recursive The --recursive flag asks the fmt command to traverse all of your terraform directories and format the .tf files it finds. It will report the files it changed as part of the return information of the command Hint Use the git diff command to see what was changed. Result We can see that terraform.tfvars file had some spacing that been fixed for better code readability. Extra Another cool tool that you can use along you terraform development is tflint - framework and each feature is provided by plugins, the key features are as follows: Find possible errors (like illegal instance types) for Major Cloud providers (AWS/Azure/GCP). Warn about deprecated syntax, unused declarations. Enforce best practices, naming conventions. Finally let's run terraform plan : terraform plan Output: No changes. Your infrastructure matches the configuration. Result We don't have any errors, however we don't have any resources created so far. Let's create a GCP project resource to start with! 1.5 Create GCP project using Terraform \u00b6 1.5.1 Configure google_project resource \u00b6 Resources describe the infrastructure objects you want terraform to create. A resource block can be used to create any object such as virtual private cloud, security groups, DNS records etc. Let's create our first Terraform resource: GCP project. In order to accomplish this we going to use google_project resource, documented here In order to create a new Project we need to define following arguments: * name - (Required) The display name of the project. * project_id - (Required) The project ID. Changing this forces a new project to be created. * billing_account - The alphanumeric ID of the billing account this project belongs to. First, Let's declare actual values for name , project_id and billing_account with variables: cat <<EOF >> variables.tf variable \"billing_account\" { description = \"The billing account ID for this project\" } variable \"project_name\" { description = \"The human readable project name (min 4 letters)\" } variable \"project_id\" { description = \"The GCP project ID\" } EOF Then, update terraform.tfvars variables files, with actual values for name , project_id and billing_account : We going to use same naming structure for PROJECT_ID and PROJECT_NAME as in previous Assignment, in order to define your new GCP project: export ORG=<student-name> export ORG=ayrat export PRODUCT=notepad export ENV=dev export PROJECT_PREFIX=4 export PROJECT_NAME=$ORG-$PRODUCT-$ENV export PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX # Project ID has to be unique echo $PROJECT_NAME echo $PROJECT_ID In order to get Billing account run following command: ACCOUNT_ID=$(gcloud alpha billing accounts list | grep Education | grep True |awk '{ print $1 }') echo $ACCOUNT_ID cat <<EOF >> terraform.tfvars billing_account = \"$ACCOUNT_ID\" project_name = \"$PROJECT_NAME\" project_id = \"$PROJECT_ID\" EOF Verify if everything looks good, and correct values has been set: cat terraform.tfvars Finally, let's define google_project resource in project.tf file, we will replace actual values for name and billing_account with variables: cat <<EOF >> project.tf resource \"google_project\" \"project\" { name = var.project_name billing_account = var.billing_account project_id = var.project_id } EOF Run plan command: terraform plan -var-file terraform.tfvars Result Plan: 1 to add, 0 to change, 0 to destroy. Note Take notice of plan command and see how some values that you declared in *.tvfars are visible and some values will known after apply terraform apply -var-file terraform.tfvars Success We've created our first resource with terrraform 1.5.2 Making Project resource Immutable \u00b6 Very often when you developing IaC, you need to destroy and recreate your resorces, e.g. for troubleshooting or creating a new resources using same config. Let's destroy our project and try to recreate it again. terraform destroy -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Failed What happened? Did you project been able to create? If not why ? If we want to make our infrastructure to be Immutable and fully automated, we need to make sure that we can destroy our service and recreate it any time the same way. In our case we can't do that because Project ID always has to be unique. To tackle this problem we need to randomize our Project ID creation within the terraform. 1.5.3 Create a Project using Terraform Random Provider \u00b6 In order to create a GCP Project with Random name, we can use Terraform's random_integer resource with following arguments: max (Number) The maximum inclusive value of the range - 100 min (Number) The minimum inclusive value of the range - 999 cat <<EOF >> random.tf resource \"random_integer\" \"id\" { min = 100 max = 999 } EOF random_integer resource doesn't belongs to Google Provider, it requires Hashicorp Random Provider to be initialized. Since it's native hashicorp provider we can skip the step of defining and configuring that provider, as it will be automatically initialized. terraform init Result Installing hashicorp/random v3.1.0... Installed hashicorp/random v3.1.0 (signed by HashiCorp) 1.5.3 Create a Project_ID name using Local Values \u00b6 Local Values allow you to assign a name to an expression, so the expression can be used multiple times, without repeating it. We going to define value of project_id as local. And it will be a combination of project_name - random_number . Let's add local value for project_id and replace value var.project_id with local.project_id . edit project.tf Update code snippet of project.tf as following and save: locals { project_id = \"${var.project_name}-${random_integer.id.result}\" } resource \"google_project\" \"project\" { name = var.project_name billing_account = var.billing_account project_id = local.project_id } We can now remove project_id value from terraform.tfvars , as we going to randomly generate it using locals expression with random_integer edit terraform.tfvars Remove project_id line and save. We can now also remove project_id variable from variables.tf : rm variables.tf cat <<EOF >> variables.tf variable \"gcp_region\" { type = string description = \"The GCP Region\" default = \"us-central1\" } variable \"gcp_project_id\" { type = string description = \"The GCP Seeding project ID\" default = \"\" } variable \"billing_account\" { description = \"The billing account ID for this project\" } variable \"project_name\" { description = \"The human readable project name (min 4 letters)\" } EOF terraform plan -var-file terraform.tfvars Output: # random_integer.id will be created + resource \"random_integer\" \"id\" { + id = (known after apply) + max = 999 + min = 100 + result = (known after apply) } terraform apply -var-file terraform.tfvars Result Project has been created with Random project_id 1.5.4 Configure Terraform Output for GCP Project \u00b6 Outputs provide return values of a Terraform state at any given time. So you can get for example values of what GCP resources like project, VMs, GKE cluster been created and their parametres. Outputs can be used used to export structured data about resources. This data can be used to configure other parts of your infrastructure, or as a data source for another Terraform workspace. Outputs are also necessary to share data from a child module to your root module. Outputs follow a similar structure to variables: output \"output_name\" { description = \"A description to understand what information is provided by the output\" value = \"An expression and/or resource_name.attribute\" sensitive = \"Optional argument, marking an output sensitive will supress the value from plan/apply phases\" } Let's declare GCP Project output values for project_id and project_number . You can find available output in each respective resource documents, under ` Attributes Reference . For example for GCP project available outputs are: cat <<EOF >> outputs.tf output \"id\" { value = google_project.project.project_id description = \"GCP project ID\" } output \"number\" { value = google_project.project.number description = \"GCP project number\" sensitive = true } EOF terraform plan -var-file terraform.tfvars We can see that we have new changes to output: Output: Changes to Outputs: + id = \"ayrat-notepad-dev-631\" + number = (sensitive value) Let's apply this changes: terraform apply -var-file terraform.tfvars Output: Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Let's now run terraform output command: terraform output Result We can see value of project id , however we don't see project number as it's been marked as sensitive. Note Outputs can be useful when you want to provide results of terraform resource creation to CI/CD or next automation tool like helm to deploy application. 1.5.5 Recreate GCP Project without Default VPC \u00b6 One of the requirements for our solution is to create GCP project with custom VPC. However, when terraform creates GCP Project it creates DEFAULT vpc by default. gcloud compute networks list --project ayrat-notepad-dev-631 Output: NAME SUBNET_MODE BGP_ROUTING_MODE IPV4_RANGE GATEWAY_IPV4 default AUTO REGIONAL In order to remove automatically created default VPC, specify special attribute during google_project resource creation. Check documentation here and find this argument. Task N1: Find Attribute to remove default VPC during google_project resource creation. Define in it project.tf and set it's value in variables.tf as true by default. edit project.tf TODO edit variables.tf TODO After changes you need to re-create a project, as vpc get's deleted during project creation only. terraform destroy -var-file terraform.tfvars terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Notice How long project is now getting created. This is due to after project creation, default vpc is being removed. Verify that default VPC has been deleted: gcloud compute networks list --project ayrat-notepad-dev-631 Output: Listed 0 items. Success We now able to create a GCP project without Default VPC. 1.6 Create GCP Storage bucket \u00b6 1.6.1 Create GCP Storage bucket in New GCP Project \u00b6 Using reference doc for google_storage_bucket , let's create google_storage_bucket resource that will create MULTI_REGIONAL GCS bucket in newly created project. We going to give bucket name : $ORG-notepad-dev-tfstate , and we going to use this bucket to store Terraform state for GCP Service Layer. cat <<EOF >> bucket.tf resource \"google_storage_bucket\" \"state\" { name = var.bucket_name project = local.project_id storage_class = var.storage_class force_destroy = \"true\" } EOF cat <<EOF >> variables.tf variable \"bucket_name\" { description = \"The name of the bucket.\" } variable \"storage_class\" { description = default = \"MULTI_REGIONAL\" } EOF Set variable that will be used to build name for a bucket: export ORG=<student-name> cat <<EOF >> terraform.tfvars bucket_name = \"$ORG-notepad-dev-tfstate\" EOF cat <<EOF >> outputs.tf output \"bucket_name\" { value = google_storage_bucket.state.name } EOF Let's review the plan: terraform plan -var-file terraform.tfvars And create google_storage_bucket resource: terraform apply -var-file terraform.tfvars Verify created bucket in GCP UI: Storage -> Cloud Storage gsutil ls -L -b gs://$ORG-notepad-dev-tfstate Result GCS bucket for terraform state has been created 1.6.2 Configure versioning on GCP Storage bucket. \u00b6 Check if versioning enabled on the created bucket: gsutil versioning get gs://$ORG-notepad-dev-tfstate Result Versioning: Suspended It is highly recommended that if you going to use GCS bucket as Terraform storage backend you should enable Object Versioning on the GCS bucket to allow for state recovery in the case of accidental deletions and human error. Task N2: Using reference doc for google_storage_bucket find and configure argument that enables gcs bucket versioning feature. Edit bucket.tf with correct argument. edit bucket.tf TODO terraform plan -var-file terraform.tfvars Output: Plan: 0 to add, 1 to change, 0 to destroy. Result Configuration for Versioning looks correct terraform apply -var-file terraform.tfvars Verify versioning is ON: gsutil versioning get gs://$ORG-notepad-dev-tfstate Output: Versioning Enabled Result We've finished building the Foundation Layer. So far we able to accomplish following: Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf Create a new notepad-dev Project Delete Default VPC Create a bucket in this project to store terraform state 2 Build GCP Services Layer \u00b6 Once we finished building a GCP Foundation layer which is essentially our project, we can start building GCP Services Layer inside that project. This second layer will configure following items: Enable Google Project Service APIs Create VPC (google_compute_network) and Subnet (google_compute_subnetwork) Create Cloud Nat (google_compute_router) and (google_compute_router_nat) Private GKE Private Nodes with Public API Endpoint (for simplicity) 2.1 Initialize Terraform for GCP Services Layer \u00b6 2.1.1 Define and configure terraform provider \u00b6 Step 1: Take note of newly created GCP Project_ID and BUCKET_ID: cd ~/$MY_REPO/foundation-infrastructure terraform output | grep 'id' |awk '{ print $3}' terraform output | grep 'bucket_name' |awk '{ print $3}' Set it as variable: export PROJECT_ID=$(terraform output | grep 'id' |awk '{ print $3}') export BUCKET_ID=$(terraform output | grep 'bucket_name' |awk '{ print $3}') echo $PROJECT_ID echo $BUCKET_ID Step 2: Declare the Terraform Provider for GCP Services Layer: We now going to switch to notepad-infrastructure where we going to create a new GCP service Layer terraform configuration: cd ~/$MY_REPO/notepad-infrastructure cat << EOF>> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } } } EOF Step 4: Configure the Terraform Provider cat << EOF>> main.tf provider \"google\" { project = var.gcp_project_id region = var.gcp_region } EOF Step 5: Define variables: cat <<EOF> variables.tf variable \"gcp_region\" { type = string description = \"The GCP Region\" default = \"us-central1\" } variable \"gcp_project_id\" { type = string description = \"The newly created GCP project ID\" } EOF Step 5: Set variables in terraform.tfvars cat <<EOF >> terraform.tfvars gcp_project_id = $PROJECT_ID EOF Step 4: Now that you have declared and configured the GCP provider for terraform, initialize terraform: terraform init 2.1.2 Configure Terraform State backend \u00b6 Terraform records information about what infrastructure it created in a Terraform state file. This information, or state, is stored by default in a local file named terraform.tfstate. This allows Terraform to compare what's in your configurations with what's in the state file, and determine what changes need to be applied. Local vs Remote backend: Local state files are the default for terraform. Remote state allows for collaboration between members of your team, for example multiple users or systems can deploy terraform configuration to the same environment as state is stored remotely and can be pulled localy during the execution. This however may create issues if 2 users running terraform plan and apply at the same time, however some remote backends including gcs provide locking feature that we covered and enabled in previous section Configure versioning on GCP Storage bucket. In addition to that, Remote state is more secure storage of sensitive values that might be contained in variables or outputs. See documents for remote backend for reference: Step 1 Configure remote backend using gcs bucket: Let's configure a backend for your state, using the gcs bucket you previously created in Foundation Layer. Backends are configured with a nested backend block within the top-level terraform block While a backend can be declared anywhere, it is recommended to use a backend.tf . Since we running on GCP we going to use GCS remote backend . It stores the state as an object in a configurable prefix in a pre-existing bucket on Google Cloud Storage (GCS). This backend also supports state locking. The bucket must exist prior to configuring the backend. We going to use following arguments: bucket - (Required) The name of the GCS bucket. This name must be globally unique. For more information, see Bucket Naming Guidelines. prefix - (Optional) GCS prefix inside the bucket. Named states for workspaces are stored in an object called / .tfstate. cat <<EOF >> backend.tf terraform { backend \"gcs\" { bucket = $BUCKET_ID prefix = \"state\" } } EOF Step 2 When you change, configure or unconfigure a backend, terraform must be re-initialized: terraform init Verify if folder state has been created in our bucket: gsutil ls gs://$ORG-notepad-dev-tfstate Summary state Folder has been created in gcs bucket and terrafrom has been initialized with remote backend 2.2 Enable required GCP Services API \u00b6 Before we continue with creating GCP services like VPC, routers, Cloud Nat and GKE it is required to enable underlining GCP API services. When we creating a new project most of the services API's are disabled, and requires explicitly to be enabled. google_project_service resource allows management of a single API service for an existing Google Cloud Platform project. Let's enable compute engine API service with terraform: cat <<EOF >> services.tf resource \"google_project_service\" \"compute\" { service = \"compute.googleapis.com\" disable_on_destroy = false } EOF terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Verify Compute Engine API service has been enabled: gcloud services list Result Compute Engine API is enabled as it is listed. 2.2 Set variables to define standard for Naming Convention \u00b6 As per terraform best practices, when you creating terraform resources, you need to follow naming convention, that is clear for you organization. Configuration objects should be named using underscores to delimit multiple words. Object's name should be named using dashes This practice ensures consistency with the naming convention for resource types, data source types, and other predefined values and helps prevent accidental deletion or outages: Example: # Good resource \"google_compute_instance\" \"web_server\" { name = \u201cweb-server-$org-$app-$env\u201d # ... } # Bad resource \u201cgoogle_compute_instance\u201d \u201cweb-server\u201d { name = \u201cweb-server\u201d # \u2026 Create variables to define standard for Naming Convention: cat <<EOF >> variables.tf variable \"org\" { type = string } variable \"product\" { type = string } variable \"environment\" { type = string } EOF export ORG=ayrat export PRODUCT=notepad export ENV=dev cat <<EOF >> terraform.tfvars org = \"$ORG\" product = \"$PRODUCT\" environment = \"$ENV\" EOF Review if created files are correct. 2.3 Create a custom mode network (VPC) with terraform \u00b6 Using google_compute_network resource create a VPC network with terraform. Task N3: Create vpc.tf file and define custom mode network (VPC) with following requirements: * Description: VPC that will be used by the GKE private cluster on the related project * name - vpc name with following pattern: \"vpc-$ORG-$PRODUCT-$ENV\" * Subnet mode: custom * Bgp routing mode: regional * MTUs: default Hint To create \"vpc-$ORG-$PRODUCT-$ENV\" name, use format function , and use %s to convert variables to string values. Define variables in variables.tf and terraform.tfvars if required. Define Output for: Generated VPC name: google_compute_network.vpc_network.name self_link - The URI of the created resource. cat <<EOF >> vpc.tf TODO EOF cat <<EOF >> outputs.tf TODO EOF Review TF Plan: terraform plan -var-file terraform.tfvars Output: + create Terraform will perform the following actions: # google_compute_network.vpc_network will be created + resource \"google_compute_network\" \"vpc_network\" { + auto_create_subnetworks = false + delete_default_routes_on_create = false + description = \"VPC that will be used by the GKE private cluster on the related project\" + gateway_ipv4 = (known after apply) + id = (known after apply) + mtu = (known after apply) + name = \"vpc-ayrat-notepad-dev\" + project = (known after apply) + routing_mode = \"REGIONAL\" + self_link = (known after apply) Create VPC: terraform apply -var-file terraform.tfvars Verify that custom-mode VPC has been created: gcloud compute networks list gcloud compute networks describe $(gcloud compute networks list | grep CUSTOM |awk '{ print $1 }') Output: autoCreateSubnetworks: false description: VPC that will be used by the GKE private cluster on the related project kind: compute#network name: vpc-student_name-notepad-dev routingConfig: routingMode: REGIONAL selfLink: https://www.googleapis.com/compute/v1/projects/XXX x_gcloud_bgp_routing_mode: REGIONAL x_gcloud_subnet_mode: CUSTOM Result VPC Network has been created, without auto subnets. 2.4 Create a user-managed subnet with terraform \u00b6 Using google_compute_subnetwork resource create a user-managed subnet with terraform. cat <<EOF >> subnet.tf resource \"google_compute_subnetwork\" \"gke_private_subnet\" { name = format(\"subnet-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link region = var.gcp_region project = var.gcp_project_id ip_cidr_range = var.network_cidr secondary_ip_range { range_name = var.pods_cidr_name ip_cidr_range = var.pods_cidr } secondary_ip_range { range_name = var.services_cidr_name ip_cidr_range = var.services_cidr } } EOF Note Notice power of terraform outputs. Here we link subnet with our VPC network using google_compute_network.vpc_network.self_link output value of created network in previous step. Define variables: cat <<EOF >> variables.tf # variables used to create VPC variable \"network_cidr\" { type = string } variable \"pods_cidr\" { type = string } variable \"pods_cidr_name\" { type = string } variable \"services_cidr\" { type = string } variable \"services_cidr_name\" { type = string } EOF Define outputs: cat <<EOF >> outputs.tf output \"subnet_selflink\" { value = \"\\${google_compute_subnetwork.gke_private_subnet.self_link}\" } EOF Task N4: Update terraform.tfvars file values with following information: Node Range: See column subnet in above table for dev cluster Secondary service range with name: services Secondary Ranges: Service range name: services Service range CIDR: See column srv range in above table for dev cluster Pods range name: pods Pods range CIDR: See column pod range in above table for dev cluster env | subnet | pod range | srv range | kubectl api range dev | 10.128.1.0/26 | 172.0.0.0/18 | 172.10.0.0/21 | 172.16.0.0/28 stg | 10.128.2.0/26 | 172.1.0.0/18 | 172.11.0.0/21 | 172.16.0.16/28 prd | 10.128.3.0/26 | 172.2.0.0/18 | 172.12.0.0/21 | 172.16.0.32/28 Note Ranges must be with in Private (RFC1918) Address Space cat <<EOF >> terraform.tfvars #subnet vars network_cidr = \"TODO\" pods_cidr = \"TODO\" pods_cidr_name = \"TODO\" services_cidr = \"TODO\" services_cidr_name = \"TODO\" EOF Review TF Plan: terraform plan -var-file terraform.tfvars Create VPC: terraform apply -var-file terraform.tfvars Review created subnet: gcloud compute networks subnets list gcloud compute networks subnets describe $(gcloud compute networks subnets list | grep us-central1 |awk '{ print $1 }') --region us-central1 Output: enableFlowLogs: false gatewayAddress: 10.128.1.1 ipCidrRange: 10.128.1.0/26 kind: compute#subnetwork logConfig: enable: false name: subnet-student_name-notepad-dev privateIpGoogleAccess: false privateIpv6GoogleAccess: DISABLE_GOOGLE_ACCESS purpose: PRIVATE secondaryIpRanges: - ipCidrRange: 172.0.0.0/18 rangeName: pods - ipCidrRange: 172.10.0.0/21 rangeName: services stackType: IPV4_ONLY Also check in Google cloud UI: Networking->VPC Networks -> Click VPC network and check `Subnet` tab Task N5: Update subnet.tf so that google_compute_subnetwork resource supports following features: * Flow Logs * Aggregation interval: 15 min * Flow sampling: 0.1 * Metadata: \"INCLUDE_ALL_METADATA\" * Private IP Google Access edit subnet.tf TODO Review TF Plan: terraform plan -var-file terraform.tfvars Create VPC: terraform apply -var-file terraform.tfvars 2.5 Create a Cloud router \u00b6 Create Cloud Router for custom mode network (VPC), in the same region as the instances that will use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway. Task N6: Define a google_compute_router inside router.tf that will be able to create a NAT router so the nodes can reach DockerHub and external APIs from private cluster, using following parameters: Create router for custom vpc_network created above with terraform Same project as VPC Same region as VPC Router name: gke-net-router Local BGP Autonomous System Number (ASN): 64514 Hint You can automatically recover vpc name from terraform output like this: google_compute_network.vpc_network.self_link . cat <<EOF >> router.tf TODO EOF Review TF Plan: terraform plan -var-file terraform.tfvars Create Cloud Router: terraform apply -var-file terraform.tfvars Verify created Cloud Router: CLI: gcloud compute routers list gcloud compute routers describe $(gcloud compute routers list | grep gke-net-router |awk '{ print $1 }') --region us-central1 Output: bgp: advertiseMode: DEFAULT asn: 64514 keepaliveInterval: 20 kind: compute#router name: gke-net-router UI: Networking -> Hybrid Connectivity -> Cloud Routers Result Router resource has been created for VPC Network 2.6 Create a Cloud Nat \u00b6 Set up a simple Cloud Nat configuration using google_compute_router_nat resource, which will automatically allocates the necessary external IP addresses to provide NAT services to a region. When you use auto-allocation, Google Cloud reserves IP addresses in your project automatically. cat <<EOF >> cloudnat.tf resource \"google_compute_router_nat\" \"gke_cloud_nat\" { project = var.gcp_project_id name = \"gke-cloud-nat\" router = google_compute_router.gke_net_router.name region = var.gcp_region nat_ip_allocate_option = \"AUTO_ONLY\" source_subnetwork_ip_ranges_to_nat = \"ALL_SUBNETWORKS_ALL_IP_RANGES\" } EOF Review TF Plan: terraform plan -var-file terraform.tfvars Create Cloud Router: terraform apply -var-file terraform.tfvars Verify created Cloud Nat : CLI: # List available Cloud Nat Routers gcloud compute routers nats list --router gke-net-router --router-region us-central1 # Describe Cloud Nat Routers `gke-cloud-nat`: gcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1 Output: enableEndpointIndependentMapping: true icmpIdleTimeoutSec: 30 name: gke-cloud-nat natIpAllocateOption: AUTO_ONLY sourceSubnetworkIpRangesToNat: ALL_SUBNETWORKS_ALL_IP_RANGES tcpEstablishedIdleTimeoutSec: 1200 tcpTransitoryIdleTimeoutSec: 30 udpIdleTimeoutSec: 30 UI: Networking -> Network Services -> Cloud NAT Result A NAT service created in a router Task N7: Additionally turn ON logging feature for ALL log types of communication for Cloud Nat edit cloudnat.tf TODO Review TF Plan: terraform plan -var-file terraform.tfvars Update Cloud Nat Configuration: terraform apply -var-file terraform.tfvars Output: Apply complete! Resources: 0 added, 1 changed, 0 destroyed. gcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1 Result Cloud Nat now supports Logging. Cloud NAT logging allows you to log NAT connections and errors. When Cloud NAT logging is enabled, one log entry can be generated for each of the following scenarios: When a network connection using NAT is created. When a packet is dropped because no port was available for NAT. 2.7 Create SSH Firewall rules default-allow-internal and default-allow-ssh \u00b6 Let's create SSH Firewall rule with name allow-tcp-ssh-icmp-$ORG-$PRODUCT-$ENV to allow SSH, ping, using google_compute_firewall resource. Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/firewall-rules https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_firewall cat <<EOF>> firewall.tf resource \"google_compute_firewall\" \"ssh-rule\" { name = format(\"allow-tcp-ssh-icmp-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link allow { protocol = \"tcp\" ports = [\"22\"] } allow { protocol = \"icmp\" } } EOF Review created firewall rules: gcloud compute firewall-rules list Also check in Google cloud UI: Networking->Firewalls 2.8 Create a Private GKE Cluster and delete default node pool \u00b6 2.8.1 Enable GCP Beta Provider \u00b6 In order to create a GKE cluster with terraform we will be leveraging google_container_cluster resource. Some of google_container_cluster arguments, such VPC-Native networking mode, VPA, Istio, CSI Driver add-ons, requires google-beta Provider. The google-beta provider is distinct from the google provider in that it supports GCP products and features that are in beta, while google does not. Fields and resources that are only present in google-beta will be marked as such in the shared provider documentation. Task N8: Configure and Initialize GCP Beta Provider , similar to how we did it for GCP Provider in 1.3.3 Initialize Terraform update provider.tf and main.tf configuration files. edit provider.tf TODO edit main.tf TODO Initialize google-beta provider plugin: terraform init Success Terraform has been successfully initialized! 2.8.2 Enable Kubernetes Engine API \u00b6 Kubernetes Engine API used to build and manages container-based applications, powered by the open source Kubernetes technology. Before starting GKE cluster creation it is required to enable it. Task N9: Enable container.googleapis.com in services.tf file similar to what we already did in 2.2 Enable required GCP Services API . edit services.tf TODO Note Adding disable_on_destroy=false helps to prevent errors during redeployments of the system. Review TF Plan: terraform plan -var-file terraform.tfvars Update Cloud Nat Configuration: terraform apply -var-file terraform.tfvars 2.8.3 Create a Private GKE Cluster and delete default node pool \u00b6 Using google_container_cluster resource create a Regional, Private GKE cluster, with following characteristics: Cluster Configuration: Cluster name: gke-$ORG-$PRODUCT-$ENV GKE Control plane is replicated across three zones of a region: us-central1 Private cluster with unrestricted access to the public endpoint: Cluster Nodes access: Private Node GKE Cluster with Public API endpoint Cluster K8s API access: with unrestricted access to the public endpoint Cluster Node Communication: VPC Native Secondary pod range with name: pods Secondary service range with name: services GKE master and node version: \"1.20.8-gke.700\" Terraform Provider: google-beta Timeouts: 30M Node Pool Configuration: VM size: e2-small Node count: 1 per zone Node images: Container-Optimized OS The name of a GCE machine (VM) type: e2-small Note Why delete default node pool? The default node pools cause trouble with managing the cluster, when created with terraform as it is not part of the terraform lifecycle. GKE Architecture Best Practice recommends to delete default node pool and create a custom one instead and manage the node pools explicitly. Note Why define Timeouts for gke resource? Normally GKE creation takes few minutes. However, in our case we creating GKE Cluster, and then system cordon, drain and then destroy default node pool. This process may take 10-20 minutes and we want to make sure terraform will not time out during this time. Step 1: Let's define GKE resource first: cat <<EOF >> gke.tf resource \"google_container_cluster\" \"primary_cluster\" { provider = google-beta project = var.gcp_project_id name = format(\"gke-%s-%s-%s\", var.org, var.product, var.environment) min_master_version = var.kubernetes_version network = google_compute_network.vpc_network.self_link subnetwork = google_compute_subnetwork.gke_private_subnet.self_link location = var.gcp_region logging_service = var.logging_service monitoring_service = var.monitoring_service remove_default_node_pool = true initial_node_count = 1 private_cluster_config { enable_private_nodes = var.enable_private_nodes enable_private_endpoint = var.enable_private_endpoint master_ipv4_cidr_block = var.master_ipv4_cidr_block } network_policy { enabled = var.network_policy provider = var.network_policy ? \"CALICO\" : \"PROVIDER_UNSPECIFIED\" } addons_config { http_load_balancing { disabled = var.disable_http_load_balancing } network_policy_config { disabled = var.network_policy ? false : true } } ip_allocation_policy { cluster_secondary_range_name = var.pods_range_name services_secondary_range_name = var.services_range_name } timeouts { create = \"30m\" update = \"30m\" delete = \"30m\" } workload_identity_config { identity_namespace = \"${var.gcp_project_id}.svc.id.goog\" } } EOF Step 2: Next define GKE cluster specific variables: cat <<EOF >> gke_variables.tf variable \"kubernetes_version\" { default = \"\" type = string description = \"The GKE version of Kubernetes\" } variable \"logging_service\" { description = \"The logging service that the cluster should write logs to.\" default = \"logging.googleapis.com/kubernetes\" } variable \"monitoring_service\" { default = \"monitoring.googleapis.com/kubernetes\" description = \"The GCP monitoring service scope\" } variable \"disable_http_load_balancing\" { default = false description = \"Enable HTTP Load balancing GCP integration\" } variable \"network_policy\" { description = \"Enable network policy addon\" default = true } variable \"pods_range_name\" { description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to pods\" default = \"\" } variable \"services_range_name\" { description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to services\" default = \"\" } variable \"enable_private_nodes\" { default = false description = \"Enable Private-IP Only GKE Nodes\" } variable \"enable_private_endpoint\" { default = false description = \"When true, the cluster's private endpoint is used as the cluster endpoint and access through the public endpoint is disabled.\" } variable \"master_ipv4_cidr_block\" { description = \"The ipv4 cidr block that the GKE masters use\" } EOF Step 3: Define GKE cluster specific outputs: cat <<EOF >> outputs.tf output \"id\" { value = \"${google_container_cluster.primary_cluster.id}\" } output \"endpoint\" { value = \"${google_container_cluster.primary_cluster.endpoint}\" } output \"master_version\" { value = \"${google_container_cluster.primary_cluster.master_version}\" } EOF Task N9: Complete terraform.tfvars with required values to GKE Cluster specified above: cat <<EOF >> terraform.tfvars //gke specific enable_private_nodes = \"TODO\" master_ipv4_cidr_block = \"TODO\" pods_range_name = \"TODO\" services_range_name = \"TODO\" kubernetes_version = \"TODO\" EOF In the next step, we going to create a custom GKE Node Pool. 2.8.4 Create a GKE custom Node pool \u00b6 Using google_container_node_pool resource create a custom GKE Node Pool with following characteristics: Node Pool Configuration: VM size: e2-small Node count: 1 per zone Node images: Container-Optimized OS The name of a GCE machine (VM) type: e2-small Step 1: Let's define GKE resource first: cat <<EOF >> gke.tf #Node Pool Resource resource \"google_container_node_pool\" \"custom-node_pool\" { provider = google-beta name = \"main-pool\" location = var.gcp_region project = var.gcp_project_id cluster = google_container_cluster.primary_cluster.name node_count = var.gke_pool_node_count version = var.kubernetes_version node_config { image_type = var.gke_pool_image_type disk_size_gb = var.gke_pool_disk_size_gb disk_type = var.gke_pool_disk_type machine_type = var.gke_pool_machine_type } timeouts { create = \"10m\" delete = \"10m\" } lifecycle { ignore_changes = [ node_count ] } } EOF Step 2: Next define GKE cluster specific variables: cat <<EOF >> gke_variables.tf #Node Pool specific variables variable \"gke_pool_machine_type\" { type = string } variable \"gke_pool_node_count\" { type = number } variable \"gke_pool_disk_type\" { type = string default = \"pd-standard\" } variable \"gke_pool_disk_size_gb\" { type = string } variable \"gke_pool_image_type\" { type = string } EOF Task 9 (Continued): Complete terraform.tfvars with required values to GKE Node Pool values specified above: cat <<EOF >> terraform.tfvars #pool specific gke_pool_node_count = \"TODO\" gke_pool_image_type = \"TODO\" gke_pool_disk_size_gb = \"TODO\" gke_pool_machine_type = \"TODO\" EOF Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars Step 4: Create GKE Cluster and Node Pool: terraform apply -var-file terraform.tfvars Output: google_container_cluster.primary_cluster: Creating... ... google_container_cluster.primary_cluster: Creation complete after 20m9s google_container_node_pool.custom-node_pool: Creating... google_container_node_pool.custom-node_pool: Creation complete after 2m10s 2.8.5 Update GKE Node Pool to support Auto Upgrade and Auto Recovery features \u00b6 Note GKE Master Nodes are managed by Google and get's upgraded automatically. Users can only specify Maintenance Window if they have preference for that process to occur (e.g. after busy hours). Users can however control Node Pool upgrade lifecycle. The can choose to do it themselves or with Auto Upgrade. Task N10: Using google_container_node_pool resource update node pool to support Auto Upgrade and Auto Recovery features. edit gke.tf TODO Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars No errors. Step 4: Update GKE Cluster Node Pool configuration: terraform apply -var-file terraform.tfvars Summary Congrats! You've now learned how to deploy production grade GKE clusters. 2.9 (Optional) Repeatable Infrastructure \u00b6 When you doing IaC it is important to insure that you can both create and destroy resources consistently. This is especially important when doing CI/CD testing. Step 3: Destroy all resources: terraform destroy -var-file terraform.tfvars No errors. Step 4: Recreate all resources: terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars 3. Create Documentation for terraform code \u00b6 Documentation for your terraform code is an important part of IaC. Make sure all your variables have a good description! There are community tools that have been developed to make the documentation process smoother, in terms of documenting Terraform resources and requirements.Its good practice to also include a usage example snippet. Terraform-Docs is a good example of one tool that can generate some documentation based on the description argument of your Input Variables, Output Values, and from your required_providers configurations. Step 1 Install the terraform-docs cli to your Google CloudShell environment: curl -Lo ./terraform-docs.tar.gz https://github.com/terraform-docs/terraform-docs/releases/download/v0.14.1/terraform-docs-v0.14.1-$(uname)-amd64.tar.gz tar -xzf terraform-docs.tar.gz rm terraform-docs.tar.gz chmod +x terraform-docs sudo mv terraform-docs /usr/local/bin/ terraform-docs Generating terraform documentation with Terraform Docs: cd ~/$MY_REPO cd foundation-infrastructure terraform-docs markdown . > README.md cd ../notepad-infrastructure terraform-docs markdown . > README.md Verify created documentation: edit README.md 4. Workaround for Project Quota issue \u00b6 If you see following error during project creation in foundation layer: Error: Error setting billing account \"010BE6-CA1129-195D77\" for project \"projects/ayrat-notepad-dev-244\": googleapi: Error 400: Precondition check failed., failedPrecondition This is due to our Billing account has quota of 5 projects per account. To solve this issue find all unused accounts: gcloud beta billing projects list --billing-account $ACCOUNT_ID And unlink them, so you have less then 5 projects per account: gcloud beta billing projects unlink $PROJECT_ID 5 Commit Readme doc to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit docs folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Readme doc for Production GKE Creation using gcloud\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 6 Cleanup \u00b6 We only going to cleanup GCP Service foundation layer, as we going to use GCP project in future. cd ~/$MY_REPO/notepad-infrastructure terraform destroy -var-file terraform.tfvars","title":"Assignment3"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#working-with-git-and-iac","text":"Git-based workflows, merge requests and peer reviews create a level of documented transparency that is great for security teams and audits. They ensure every change is documented as well as the metadata surrounding the change, answering the why, who, when and what questions. In this set of exercises you will continue using notepad Google Source Repository that already contains you application code, dockerfiles and kubernetes manifests. We will use so called Monorepo approach, and store our terraform IAC configuration in the same repo as our application code and Kubernetes manifests.","title":"Working with Git and IaC"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#add-your-iac-code-to-your-repository","text":"Step 1 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 2 Create foundation-infrastructure and notepad-infrastructure folders that will contains respective terraform configurations cd ~/$MY_REPO mkdir foundation-infrastructure mkdir notepad-infrastructure Step 3 Create a README.md file describing foundation-infrastructure folder purpose: cat <<EOF> foundation-infrastructure/README.md # Creation of GCP Foundation Layer This step, is focused on creating GCP Foundation: folders (if any) and service projects creation with their respictive GCS bucket to store terraform state and IAM Config. EOF Step 4 Create a README.md file describing notepad-infrastructure folder purpose: cat <<EOF> notepad-infrastructure/README.md # Creation of GCP Services Layer This step, is focused on creating gcp services such as GKE, VPC and etc. inside of existing project EOF Step 5 Create a .gitignore file in your working directory: cat<<EOF>> .gitignore .terraform.* .terraform terraform.tfstate* EOF Note Ignore files are used to tell git not to track files. You should also always include any files with credentials in a gitignore file. List created folder structure along with gitignore : ls -ltra Step 6 Commit deploy folder using the following Git commands: git status git add . git commit -m \"Terraform Folder structure for assignement 3\" Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"Add your IaC code to your repository"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#1-build-gcp-foundation-layer","text":"In this Lab we going to build GCP Foundation layer. As we learned in the class this includes Org Structure creation with Folders and Projects, creation IAM Roles and assigning them to users and groups in organization. This Layer usually build be DevOps or Infra team that. Since we don't have organization registered for each student, we going to skip creation of folders and IAM groups. We going to start from creation of a Project, deleting default VPC and configuring inside that project a gcs bucket that will be used in the next terraform layer to store a Terraform state. Part 1: Create Terraform Configurations file to create's GCP Foundation Layer: Layer 1 From existing (we going to call it seeding) project that you currently use to store code in Google Cloud Source Repository: Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf Create a new notepad-dev Project Delete Default VPC Create a bucket in this project to store terraform state Part 2: Create Terraform Configurations file that create's GCP Services Layer: Layer 2 From notepad-dev GCP Project: Enable Google Project Service APIs Create VPC (google_compute_network) and Subnet (google_compute_subnetwork) Create Cloud Nat (google_compute_router) and (google_compute_router_nat) Private GKE Private Nodes with Public API Endpoint (for simplicity)","title":"1 Build GCP Foundation Layer"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#11-installing-terraform","text":"GCP Cloud Shell comes with many common tools pre-installed including terraform Verify and validate the version of Terraform that is installed: terraform --version If you want to use specific version of terraform or want to install terraform in you local machine use following link to download binary.","title":"1.1 Installing Terraform"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#12-configure-gcp-credentials-for-terraform","text":"If you would like to use terraform on GCP you have 2 options: 1). Using you user credentials, great option for testing terraform from you laptop and for learning purposes. 2). Using service account , great option if you going to use terraform with CI/CD system and fully automate Terraform deployment. In this Lab we going to use Option 1 - using user credentials. Step 1: In order to make requests against the GCP API, you need to authenticate to prove that it's you making the request. The preferred method of provisioning resources with Terraform on your workstation is to use the Google Cloud SDK (Option 1) gcloud auth application-default login","title":"1.2 Configure GCP credentials for Terraform"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#13-initializing-terraform","text":"Terraform relies on providers to interact with remote systems. Every resource type is implemented by a provider; without providers , Terraform can't manage any kind of infrastructure; in order for terraform to install and use a provider it must be declared. In this exercise you will declare and configure the Terraform provider(s) that will be used for the rest of the Lab.","title":"1.3 Initializing Terraform"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#131-declare-terraform-providers","text":"The required_providers block defines the providers terraform will use to provision resources and their source. version : The version argument is optional and is used to tell terraform to pick a particular version from the available versions source : The source is the provider registry e.g. hashicorp/gcp is the short for registry.terraform.io/hashicorp/gcp cd ~/$MY_REPO/foundation-infrastructure/ cat << EOF>> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } } } EOF","title":"1.3.1 Declare Terraform Providers"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#132-configure-the-terraform-provider","text":"Providers often require configuration (like endpoint URLs or cloud regions) before they can be used. These configurations are defined by a provider block. Multiple provider configuration blocks can be declared for the same by adding the alias argument Set you current PROJECT_ID value here: export PROJECT_ID=<YOUR_PROJECT_ID> cd ~/$MY_REPO/foundation-infrastructure/ cat << EOF>> main.tf provider \"google\" { alias = \"gcp-provider\" project = \"$PROJECT_ID\" region = \"us-central1\" } EOF Result We configured provider , so that it can provision resource in specified gcp project in us-central1 region.","title":"1.3.2 Configure the Terraform Provider"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#133-initialize-terraform","text":"Now that you have declared and configured the GCP provider for terraform, initialize terraform: cd ~/$MY_REPO/foundation-infrastructure/ terraform init Explore your directory. What has changed? ls -ltra Result We can see that new directory .terraform and .terraform.lock.hcl file. Extra! Investigate available providers in the Terraform Provider Registry Select another provider from the list, add it to your required providers, and to your main.tf Run terraform init to load your new provider!","title":"1.3.3 Initialize Terraform"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#14-terraform-variables","text":"Input variables are used to increase your Terraform configuration's flexibility by defining values that can be assigned to customize the configuration. They provide a consistent interface to change how a given configuration behaves. Input variables blocks have a defined format: Input variables blocks have a defined format: variable \"variable_name\" { type = \"The variable type eg. string , list , number\" description = \"A description to understand what the variable will be used for\" default = \"A default value can be provided, terraform will use this if no other value is provided at terraform apply\" } Terraform CLI defines the following optional arguments for variable declarations: default - A default value which then makes the variable optional. type - This argument specifies what value types are accepted for the variable. description - This specifies the input variable's documentation. validation - A block to define validation rules, usually in addition to type constraints. sensitive - Limits Terraform UI output when the variable is used in configuration. In our above example for main.tf we can actually declare project and region as variables, so let's do it.","title":"1.4 Terraform Variables"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#141-declare-input-variables","text":"Variables are declared in a variables.tf file inside your terraform working directory. The label after the variable keyword is a name for the variable, which must be unique among all variables in the same module. You can choose variable name based on you preference, or in some cases based on agreed in company naming convention. In our case we declaring variables names: gcp_region and gcp_project_id We are going to declare a type as a string variable for gcp_project_id and gcp_region . It is always good idea to specify a clear description for the variable for documentation purpose as well as code reusability and readability. Finally, let's configured default argument. In our case we going to use \"us-central1\" as a default for gcp_region and we going to keep default value empty for gcp_project_id . cat <<EOF> variables.tf variable \"gcp_region\" { type = string description = \"The GCP Region\" default = \"us-central1\" } variable \"gcp_project_id\" { type = string description = \"The GCP Seeding project ID\" default = \"\" } EOF Note Explore other type of variables such as number , bool and type constructors such as: list( ), set( ), map( )","title":"1.4.1 Declare Input Variables"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#142-using-variables","text":"Now that you have created your input variables, let's re-create main.tf that currently has the terraform configuration for GCP Provider. rm main.tf cat << EOF>> main.tf provider \"google\" { alias = \"gcp-provider\" project = var.gcp_project_id region = var.gcp_region } EOF Test out your terraform configuration: terraform plan","title":"1.4.2 Using Variables"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#143-working-with-variables-files","text":"Create a terraform.tfvars file to hold the values for your variables: export gcp_project_id=<YOUR_PROJECT_ID> cat <<EOF >> terraform.tfvars gcp_project_id = \"$PROJECT_ID\" gcp_region = \"us-central1\" EOF Hint using different env-name.tfvars files you can create different set of terraform configuration for the same code. (e.g. code for dev, staging, prod)","title":"1.4.3 Working with Variables files."},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#144-validate-configuration-and-code-syntax","text":"Let's Validate configuration and code syntax that we've added so far. There are several tools that can analyze your Terraform code without running it, including: Terraform has build in command that you can use to check your Terraform syntax and types (a bit like a compiler): terraform validate Result Seems the code is legit so far Is your terraform easy to read and follow? Terraform has a built-in function to lint your configuration manifests for readability and best practice spacing: terraform fmt --recursive The --recursive flag asks the fmt command to traverse all of your terraform directories and format the .tf files it finds. It will report the files it changed as part of the return information of the command Hint Use the git diff command to see what was changed. Result We can see that terraform.tfvars file had some spacing that been fixed for better code readability. Extra Another cool tool that you can use along you terraform development is tflint - framework and each feature is provided by plugins, the key features are as follows: Find possible errors (like illegal instance types) for Major Cloud providers (AWS/Azure/GCP). Warn about deprecated syntax, unused declarations. Enforce best practices, naming conventions. Finally let's run terraform plan : terraform plan Output: No changes. Your infrastructure matches the configuration. Result We don't have any errors, however we don't have any resources created so far. Let's create a GCP project resource to start with!","title":"1.4.4 Validate configuration and code syntax"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#15-create-gcp-project-using-terraform","text":"","title":"1.5 Create GCP project using Terraform"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#151-configure-google_project-resource","text":"Resources describe the infrastructure objects you want terraform to create. A resource block can be used to create any object such as virtual private cloud, security groups, DNS records etc. Let's create our first Terraform resource: GCP project. In order to accomplish this we going to use google_project resource, documented here In order to create a new Project we need to define following arguments: * name - (Required) The display name of the project. * project_id - (Required) The project ID. Changing this forces a new project to be created. * billing_account - The alphanumeric ID of the billing account this project belongs to. First, Let's declare actual values for name , project_id and billing_account with variables: cat <<EOF >> variables.tf variable \"billing_account\" { description = \"The billing account ID for this project\" } variable \"project_name\" { description = \"The human readable project name (min 4 letters)\" } variable \"project_id\" { description = \"The GCP project ID\" } EOF Then, update terraform.tfvars variables files, with actual values for name , project_id and billing_account : We going to use same naming structure for PROJECT_ID and PROJECT_NAME as in previous Assignment, in order to define your new GCP project: export ORG=<student-name> export ORG=ayrat export PRODUCT=notepad export ENV=dev export PROJECT_PREFIX=4 export PROJECT_NAME=$ORG-$PRODUCT-$ENV export PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX # Project ID has to be unique echo $PROJECT_NAME echo $PROJECT_ID In order to get Billing account run following command: ACCOUNT_ID=$(gcloud alpha billing accounts list | grep Education | grep True |awk '{ print $1 }') echo $ACCOUNT_ID cat <<EOF >> terraform.tfvars billing_account = \"$ACCOUNT_ID\" project_name = \"$PROJECT_NAME\" project_id = \"$PROJECT_ID\" EOF Verify if everything looks good, and correct values has been set: cat terraform.tfvars Finally, let's define google_project resource in project.tf file, we will replace actual values for name and billing_account with variables: cat <<EOF >> project.tf resource \"google_project\" \"project\" { name = var.project_name billing_account = var.billing_account project_id = var.project_id } EOF Run plan command: terraform plan -var-file terraform.tfvars Result Plan: 1 to add, 0 to change, 0 to destroy. Note Take notice of plan command and see how some values that you declared in *.tvfars are visible and some values will known after apply terraform apply -var-file terraform.tfvars Success We've created our first resource with terrraform","title":"1.5.1 Configure google_project resource"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#152-making-project-resource-immutable","text":"Very often when you developing IaC, you need to destroy and recreate your resorces, e.g. for troubleshooting or creating a new resources using same config. Let's destroy our project and try to recreate it again. terraform destroy -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Failed What happened? Did you project been able to create? If not why ? If we want to make our infrastructure to be Immutable and fully automated, we need to make sure that we can destroy our service and recreate it any time the same way. In our case we can't do that because Project ID always has to be unique. To tackle this problem we need to randomize our Project ID creation within the terraform.","title":"1.5.2 Making Project resource Immutable"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#153-create-a-project-using-terraform-random-provider","text":"In order to create a GCP Project with Random name, we can use Terraform's random_integer resource with following arguments: max (Number) The maximum inclusive value of the range - 100 min (Number) The minimum inclusive value of the range - 999 cat <<EOF >> random.tf resource \"random_integer\" \"id\" { min = 100 max = 999 } EOF random_integer resource doesn't belongs to Google Provider, it requires Hashicorp Random Provider to be initialized. Since it's native hashicorp provider we can skip the step of defining and configuring that provider, as it will be automatically initialized. terraform init Result Installing hashicorp/random v3.1.0... Installed hashicorp/random v3.1.0 (signed by HashiCorp)","title":"1.5.3 Create a Project using Terraform Random Provider"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#153-create-a-project_id-name-using-local-values","text":"Local Values allow you to assign a name to an expression, so the expression can be used multiple times, without repeating it. We going to define value of project_id as local. And it will be a combination of project_name - random_number . Let's add local value for project_id and replace value var.project_id with local.project_id . edit project.tf Update code snippet of project.tf as following and save: locals { project_id = \"${var.project_name}-${random_integer.id.result}\" } resource \"google_project\" \"project\" { name = var.project_name billing_account = var.billing_account project_id = local.project_id } We can now remove project_id value from terraform.tfvars , as we going to randomly generate it using locals expression with random_integer edit terraform.tfvars Remove project_id line and save. We can now also remove project_id variable from variables.tf : rm variables.tf cat <<EOF >> variables.tf variable \"gcp_region\" { type = string description = \"The GCP Region\" default = \"us-central1\" } variable \"gcp_project_id\" { type = string description = \"The GCP Seeding project ID\" default = \"\" } variable \"billing_account\" { description = \"The billing account ID for this project\" } variable \"project_name\" { description = \"The human readable project name (min 4 letters)\" } EOF terraform plan -var-file terraform.tfvars Output: # random_integer.id will be created + resource \"random_integer\" \"id\" { + id = (known after apply) + max = 999 + min = 100 + result = (known after apply) } terraform apply -var-file terraform.tfvars Result Project has been created with Random project_id","title":"1.5.3 Create a Project_ID name using Local Values"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#154-configure-terraform-output-for-gcp-project","text":"Outputs provide return values of a Terraform state at any given time. So you can get for example values of what GCP resources like project, VMs, GKE cluster been created and their parametres. Outputs can be used used to export structured data about resources. This data can be used to configure other parts of your infrastructure, or as a data source for another Terraform workspace. Outputs are also necessary to share data from a child module to your root module. Outputs follow a similar structure to variables: output \"output_name\" { description = \"A description to understand what information is provided by the output\" value = \"An expression and/or resource_name.attribute\" sensitive = \"Optional argument, marking an output sensitive will supress the value from plan/apply phases\" } Let's declare GCP Project output values for project_id and project_number . You can find available output in each respective resource documents, under ` Attributes Reference . For example for GCP project available outputs are: cat <<EOF >> outputs.tf output \"id\" { value = google_project.project.project_id description = \"GCP project ID\" } output \"number\" { value = google_project.project.number description = \"GCP project number\" sensitive = true } EOF terraform plan -var-file terraform.tfvars We can see that we have new changes to output: Output: Changes to Outputs: + id = \"ayrat-notepad-dev-631\" + number = (sensitive value) Let's apply this changes: terraform apply -var-file terraform.tfvars Output: Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Let's now run terraform output command: terraform output Result We can see value of project id , however we don't see project number as it's been marked as sensitive. Note Outputs can be useful when you want to provide results of terraform resource creation to CI/CD or next automation tool like helm to deploy application.","title":"1.5.4 Configure Terraform Output for GCP Project"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#155-recreate-gcp-project-without-default-vpc","text":"One of the requirements for our solution is to create GCP project with custom VPC. However, when terraform creates GCP Project it creates DEFAULT vpc by default. gcloud compute networks list --project ayrat-notepad-dev-631 Output: NAME SUBNET_MODE BGP_ROUTING_MODE IPV4_RANGE GATEWAY_IPV4 default AUTO REGIONAL In order to remove automatically created default VPC, specify special attribute during google_project resource creation. Check documentation here and find this argument. Task N1: Find Attribute to remove default VPC during google_project resource creation. Define in it project.tf and set it's value in variables.tf as true by default. edit project.tf TODO edit variables.tf TODO After changes you need to re-create a project, as vpc get's deleted during project creation only. terraform destroy -var-file terraform.tfvars terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Notice How long project is now getting created. This is due to after project creation, default vpc is being removed. Verify that default VPC has been deleted: gcloud compute networks list --project ayrat-notepad-dev-631 Output: Listed 0 items. Success We now able to create a GCP project without Default VPC.","title":"1.5.5 Recreate GCP Project without Default VPC"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#16-create-gcp-storage-bucket","text":"","title":"1.6 Create GCP Storage bucket"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#161-create-gcp-storage-bucket-in-new-gcp-project","text":"Using reference doc for google_storage_bucket , let's create google_storage_bucket resource that will create MULTI_REGIONAL GCS bucket in newly created project. We going to give bucket name : $ORG-notepad-dev-tfstate , and we going to use this bucket to store Terraform state for GCP Service Layer. cat <<EOF >> bucket.tf resource \"google_storage_bucket\" \"state\" { name = var.bucket_name project = local.project_id storage_class = var.storage_class force_destroy = \"true\" } EOF cat <<EOF >> variables.tf variable \"bucket_name\" { description = \"The name of the bucket.\" } variable \"storage_class\" { description = default = \"MULTI_REGIONAL\" } EOF Set variable that will be used to build name for a bucket: export ORG=<student-name> cat <<EOF >> terraform.tfvars bucket_name = \"$ORG-notepad-dev-tfstate\" EOF cat <<EOF >> outputs.tf output \"bucket_name\" { value = google_storage_bucket.state.name } EOF Let's review the plan: terraform plan -var-file terraform.tfvars And create google_storage_bucket resource: terraform apply -var-file terraform.tfvars Verify created bucket in GCP UI: Storage -> Cloud Storage gsutil ls -L -b gs://$ORG-notepad-dev-tfstate Result GCS bucket for terraform state has been created","title":"1.6.1 Create GCP Storage bucket in New GCP Project"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#162-configure-versioning-on-gcp-storage-bucket","text":"Check if versioning enabled on the created bucket: gsutil versioning get gs://$ORG-notepad-dev-tfstate Result Versioning: Suspended It is highly recommended that if you going to use GCS bucket as Terraform storage backend you should enable Object Versioning on the GCS bucket to allow for state recovery in the case of accidental deletions and human error. Task N2: Using reference doc for google_storage_bucket find and configure argument that enables gcs bucket versioning feature. Edit bucket.tf with correct argument. edit bucket.tf TODO terraform plan -var-file terraform.tfvars Output: Plan: 0 to add, 1 to change, 0 to destroy. Result Configuration for Versioning looks correct terraform apply -var-file terraform.tfvars Verify versioning is ON: gsutil versioning get gs://$ORG-notepad-dev-tfstate Output: Versioning Enabled Result We've finished building the Foundation Layer. So far we able to accomplish following: Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf Create a new notepad-dev Project Delete Default VPC Create a bucket in this project to store terraform state","title":"1.6.2 Configure versioning on GCP Storage bucket."},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#2-build-gcp-services-layer","text":"Once we finished building a GCP Foundation layer which is essentially our project, we can start building GCP Services Layer inside that project. This second layer will configure following items: Enable Google Project Service APIs Create VPC (google_compute_network) and Subnet (google_compute_subnetwork) Create Cloud Nat (google_compute_router) and (google_compute_router_nat) Private GKE Private Nodes with Public API Endpoint (for simplicity)","title":"2 Build GCP Services Layer"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#21-initialize-terraform-for-gcp-services-layer","text":"","title":"2.1  Initialize Terraform for GCP Services Layer"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#211-define-and-configure-terraform-provider","text":"Step 1: Take note of newly created GCP Project_ID and BUCKET_ID: cd ~/$MY_REPO/foundation-infrastructure terraform output | grep 'id' |awk '{ print $3}' terraform output | grep 'bucket_name' |awk '{ print $3}' Set it as variable: export PROJECT_ID=$(terraform output | grep 'id' |awk '{ print $3}') export BUCKET_ID=$(terraform output | grep 'bucket_name' |awk '{ print $3}') echo $PROJECT_ID echo $BUCKET_ID Step 2: Declare the Terraform Provider for GCP Services Layer: We now going to switch to notepad-infrastructure where we going to create a new GCP service Layer terraform configuration: cd ~/$MY_REPO/notepad-infrastructure cat << EOF>> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } } } EOF Step 4: Configure the Terraform Provider cat << EOF>> main.tf provider \"google\" { project = var.gcp_project_id region = var.gcp_region } EOF Step 5: Define variables: cat <<EOF> variables.tf variable \"gcp_region\" { type = string description = \"The GCP Region\" default = \"us-central1\" } variable \"gcp_project_id\" { type = string description = \"The newly created GCP project ID\" } EOF Step 5: Set variables in terraform.tfvars cat <<EOF >> terraform.tfvars gcp_project_id = $PROJECT_ID EOF Step 4: Now that you have declared and configured the GCP provider for terraform, initialize terraform: terraform init","title":"2.1.1 Define and configure terraform provider"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#212-configure-terraform-state-backend","text":"Terraform records information about what infrastructure it created in a Terraform state file. This information, or state, is stored by default in a local file named terraform.tfstate. This allows Terraform to compare what's in your configurations with what's in the state file, and determine what changes need to be applied. Local vs Remote backend: Local state files are the default for terraform. Remote state allows for collaboration between members of your team, for example multiple users or systems can deploy terraform configuration to the same environment as state is stored remotely and can be pulled localy during the execution. This however may create issues if 2 users running terraform plan and apply at the same time, however some remote backends including gcs provide locking feature that we covered and enabled in previous section Configure versioning on GCP Storage bucket. In addition to that, Remote state is more secure storage of sensitive values that might be contained in variables or outputs. See documents for remote backend for reference: Step 1 Configure remote backend using gcs bucket: Let's configure a backend for your state, using the gcs bucket you previously created in Foundation Layer. Backends are configured with a nested backend block within the top-level terraform block While a backend can be declared anywhere, it is recommended to use a backend.tf . Since we running on GCP we going to use GCS remote backend . It stores the state as an object in a configurable prefix in a pre-existing bucket on Google Cloud Storage (GCS). This backend also supports state locking. The bucket must exist prior to configuring the backend. We going to use following arguments: bucket - (Required) The name of the GCS bucket. This name must be globally unique. For more information, see Bucket Naming Guidelines. prefix - (Optional) GCS prefix inside the bucket. Named states for workspaces are stored in an object called / .tfstate. cat <<EOF >> backend.tf terraform { backend \"gcs\" { bucket = $BUCKET_ID prefix = \"state\" } } EOF Step 2 When you change, configure or unconfigure a backend, terraform must be re-initialized: terraform init Verify if folder state has been created in our bucket: gsutil ls gs://$ORG-notepad-dev-tfstate Summary state Folder has been created in gcs bucket and terrafrom has been initialized with remote backend","title":"2.1.2 Configure Terraform State backend"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#22-enable-required-gcp-services-api","text":"Before we continue with creating GCP services like VPC, routers, Cloud Nat and GKE it is required to enable underlining GCP API services. When we creating a new project most of the services API's are disabled, and requires explicitly to be enabled. google_project_service resource allows management of a single API service for an existing Google Cloud Platform project. Let's enable compute engine API service with terraform: cat <<EOF >> services.tf resource \"google_project_service\" \"compute\" { service = \"compute.googleapis.com\" disable_on_destroy = false } EOF terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Verify Compute Engine API service has been enabled: gcloud services list Result Compute Engine API is enabled as it is listed.","title":"2.2 Enable required GCP Services API"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#22-set-variables-to-define-standard-for-naming-convention","text":"As per terraform best practices, when you creating terraform resources, you need to follow naming convention, that is clear for you organization. Configuration objects should be named using underscores to delimit multiple words. Object's name should be named using dashes This practice ensures consistency with the naming convention for resource types, data source types, and other predefined values and helps prevent accidental deletion or outages: Example: # Good resource \"google_compute_instance\" \"web_server\" { name = \u201cweb-server-$org-$app-$env\u201d # ... } # Bad resource \u201cgoogle_compute_instance\u201d \u201cweb-server\u201d { name = \u201cweb-server\u201d # \u2026 Create variables to define standard for Naming Convention: cat <<EOF >> variables.tf variable \"org\" { type = string } variable \"product\" { type = string } variable \"environment\" { type = string } EOF export ORG=ayrat export PRODUCT=notepad export ENV=dev cat <<EOF >> terraform.tfvars org = \"$ORG\" product = \"$PRODUCT\" environment = \"$ENV\" EOF Review if created files are correct.","title":"2.2 Set variables to define standard for Naming Convention"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#23-create-a-custom-mode-network-vpc-with-terraform","text":"Using google_compute_network resource create a VPC network with terraform. Task N3: Create vpc.tf file and define custom mode network (VPC) with following requirements: * Description: VPC that will be used by the GKE private cluster on the related project * name - vpc name with following pattern: \"vpc-$ORG-$PRODUCT-$ENV\" * Subnet mode: custom * Bgp routing mode: regional * MTUs: default Hint To create \"vpc-$ORG-$PRODUCT-$ENV\" name, use format function , and use %s to convert variables to string values. Define variables in variables.tf and terraform.tfvars if required. Define Output for: Generated VPC name: google_compute_network.vpc_network.name self_link - The URI of the created resource. cat <<EOF >> vpc.tf TODO EOF cat <<EOF >> outputs.tf TODO EOF Review TF Plan: terraform plan -var-file terraform.tfvars Output: + create Terraform will perform the following actions: # google_compute_network.vpc_network will be created + resource \"google_compute_network\" \"vpc_network\" { + auto_create_subnetworks = false + delete_default_routes_on_create = false + description = \"VPC that will be used by the GKE private cluster on the related project\" + gateway_ipv4 = (known after apply) + id = (known after apply) + mtu = (known after apply) + name = \"vpc-ayrat-notepad-dev\" + project = (known after apply) + routing_mode = \"REGIONAL\" + self_link = (known after apply) Create VPC: terraform apply -var-file terraform.tfvars Verify that custom-mode VPC has been created: gcloud compute networks list gcloud compute networks describe $(gcloud compute networks list | grep CUSTOM |awk '{ print $1 }') Output: autoCreateSubnetworks: false description: VPC that will be used by the GKE private cluster on the related project kind: compute#network name: vpc-student_name-notepad-dev routingConfig: routingMode: REGIONAL selfLink: https://www.googleapis.com/compute/v1/projects/XXX x_gcloud_bgp_routing_mode: REGIONAL x_gcloud_subnet_mode: CUSTOM Result VPC Network has been created, without auto subnets.","title":"2.3 Create a custom mode network (VPC) with terraform"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#24-create-a-user-managed-subnet-with-terraform","text":"Using google_compute_subnetwork resource create a user-managed subnet with terraform. cat <<EOF >> subnet.tf resource \"google_compute_subnetwork\" \"gke_private_subnet\" { name = format(\"subnet-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link region = var.gcp_region project = var.gcp_project_id ip_cidr_range = var.network_cidr secondary_ip_range { range_name = var.pods_cidr_name ip_cidr_range = var.pods_cidr } secondary_ip_range { range_name = var.services_cidr_name ip_cidr_range = var.services_cidr } } EOF Note Notice power of terraform outputs. Here we link subnet with our VPC network using google_compute_network.vpc_network.self_link output value of created network in previous step. Define variables: cat <<EOF >> variables.tf # variables used to create VPC variable \"network_cidr\" { type = string } variable \"pods_cidr\" { type = string } variable \"pods_cidr_name\" { type = string } variable \"services_cidr\" { type = string } variable \"services_cidr_name\" { type = string } EOF Define outputs: cat <<EOF >> outputs.tf output \"subnet_selflink\" { value = \"\\${google_compute_subnetwork.gke_private_subnet.self_link}\" } EOF Task N4: Update terraform.tfvars file values with following information: Node Range: See column subnet in above table for dev cluster Secondary service range with name: services Secondary Ranges: Service range name: services Service range CIDR: See column srv range in above table for dev cluster Pods range name: pods Pods range CIDR: See column pod range in above table for dev cluster env | subnet | pod range | srv range | kubectl api range dev | 10.128.1.0/26 | 172.0.0.0/18 | 172.10.0.0/21 | 172.16.0.0/28 stg | 10.128.2.0/26 | 172.1.0.0/18 | 172.11.0.0/21 | 172.16.0.16/28 prd | 10.128.3.0/26 | 172.2.0.0/18 | 172.12.0.0/21 | 172.16.0.32/28 Note Ranges must be with in Private (RFC1918) Address Space cat <<EOF >> terraform.tfvars #subnet vars network_cidr = \"TODO\" pods_cidr = \"TODO\" pods_cidr_name = \"TODO\" services_cidr = \"TODO\" services_cidr_name = \"TODO\" EOF Review TF Plan: terraform plan -var-file terraform.tfvars Create VPC: terraform apply -var-file terraform.tfvars Review created subnet: gcloud compute networks subnets list gcloud compute networks subnets describe $(gcloud compute networks subnets list | grep us-central1 |awk '{ print $1 }') --region us-central1 Output: enableFlowLogs: false gatewayAddress: 10.128.1.1 ipCidrRange: 10.128.1.0/26 kind: compute#subnetwork logConfig: enable: false name: subnet-student_name-notepad-dev privateIpGoogleAccess: false privateIpv6GoogleAccess: DISABLE_GOOGLE_ACCESS purpose: PRIVATE secondaryIpRanges: - ipCidrRange: 172.0.0.0/18 rangeName: pods - ipCidrRange: 172.10.0.0/21 rangeName: services stackType: IPV4_ONLY Also check in Google cloud UI: Networking->VPC Networks -> Click VPC network and check `Subnet` tab Task N5: Update subnet.tf so that google_compute_subnetwork resource supports following features: * Flow Logs * Aggregation interval: 15 min * Flow sampling: 0.1 * Metadata: \"INCLUDE_ALL_METADATA\" * Private IP Google Access edit subnet.tf TODO Review TF Plan: terraform plan -var-file terraform.tfvars Create VPC: terraform apply -var-file terraform.tfvars","title":"2.4 Create a user-managed subnet with terraform"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#25-create-a-cloud-router","text":"Create Cloud Router for custom mode network (VPC), in the same region as the instances that will use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway. Task N6: Define a google_compute_router inside router.tf that will be able to create a NAT router so the nodes can reach DockerHub and external APIs from private cluster, using following parameters: Create router for custom vpc_network created above with terraform Same project as VPC Same region as VPC Router name: gke-net-router Local BGP Autonomous System Number (ASN): 64514 Hint You can automatically recover vpc name from terraform output like this: google_compute_network.vpc_network.self_link . cat <<EOF >> router.tf TODO EOF Review TF Plan: terraform plan -var-file terraform.tfvars Create Cloud Router: terraform apply -var-file terraform.tfvars Verify created Cloud Router: CLI: gcloud compute routers list gcloud compute routers describe $(gcloud compute routers list | grep gke-net-router |awk '{ print $1 }') --region us-central1 Output: bgp: advertiseMode: DEFAULT asn: 64514 keepaliveInterval: 20 kind: compute#router name: gke-net-router UI: Networking -> Hybrid Connectivity -> Cloud Routers Result Router resource has been created for VPC Network","title":"2.5 Create a Cloud router"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#26-create-a-cloud-nat","text":"Set up a simple Cloud Nat configuration using google_compute_router_nat resource, which will automatically allocates the necessary external IP addresses to provide NAT services to a region. When you use auto-allocation, Google Cloud reserves IP addresses in your project automatically. cat <<EOF >> cloudnat.tf resource \"google_compute_router_nat\" \"gke_cloud_nat\" { project = var.gcp_project_id name = \"gke-cloud-nat\" router = google_compute_router.gke_net_router.name region = var.gcp_region nat_ip_allocate_option = \"AUTO_ONLY\" source_subnetwork_ip_ranges_to_nat = \"ALL_SUBNETWORKS_ALL_IP_RANGES\" } EOF Review TF Plan: terraform plan -var-file terraform.tfvars Create Cloud Router: terraform apply -var-file terraform.tfvars Verify created Cloud Nat : CLI: # List available Cloud Nat Routers gcloud compute routers nats list --router gke-net-router --router-region us-central1 # Describe Cloud Nat Routers `gke-cloud-nat`: gcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1 Output: enableEndpointIndependentMapping: true icmpIdleTimeoutSec: 30 name: gke-cloud-nat natIpAllocateOption: AUTO_ONLY sourceSubnetworkIpRangesToNat: ALL_SUBNETWORKS_ALL_IP_RANGES tcpEstablishedIdleTimeoutSec: 1200 tcpTransitoryIdleTimeoutSec: 30 udpIdleTimeoutSec: 30 UI: Networking -> Network Services -> Cloud NAT Result A NAT service created in a router Task N7: Additionally turn ON logging feature for ALL log types of communication for Cloud Nat edit cloudnat.tf TODO Review TF Plan: terraform plan -var-file terraform.tfvars Update Cloud Nat Configuration: terraform apply -var-file terraform.tfvars Output: Apply complete! Resources: 0 added, 1 changed, 0 destroyed. gcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1 Result Cloud Nat now supports Logging. Cloud NAT logging allows you to log NAT connections and errors. When Cloud NAT logging is enabled, one log entry can be generated for each of the following scenarios: When a network connection using NAT is created. When a packet is dropped because no port was available for NAT.","title":"2.6 Create a Cloud Nat"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#27-create-ssh-firewall-rules-default-allow-internal-and-default-allow-ssh","text":"Let's create SSH Firewall rule with name allow-tcp-ssh-icmp-$ORG-$PRODUCT-$ENV to allow SSH, ping, using google_compute_firewall resource. Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/firewall-rules https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_firewall cat <<EOF>> firewall.tf resource \"google_compute_firewall\" \"ssh-rule\" { name = format(\"allow-tcp-ssh-icmp-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link allow { protocol = \"tcp\" ports = [\"22\"] } allow { protocol = \"icmp\" } } EOF Review created firewall rules: gcloud compute firewall-rules list Also check in Google cloud UI: Networking->Firewalls","title":"2.7 Create SSH Firewall rules default-allow-internal and default-allow-ssh"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#28-create-a-private-gke-cluster-and-delete-default-node-pool","text":"","title":"2.8 Create a Private GKE Cluster and delete default node pool"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#281-enable-gcp-beta-provider","text":"In order to create a GKE cluster with terraform we will be leveraging google_container_cluster resource. Some of google_container_cluster arguments, such VPC-Native networking mode, VPA, Istio, CSI Driver add-ons, requires google-beta Provider. The google-beta provider is distinct from the google provider in that it supports GCP products and features that are in beta, while google does not. Fields and resources that are only present in google-beta will be marked as such in the shared provider documentation. Task N8: Configure and Initialize GCP Beta Provider , similar to how we did it for GCP Provider in 1.3.3 Initialize Terraform update provider.tf and main.tf configuration files. edit provider.tf TODO edit main.tf TODO Initialize google-beta provider plugin: terraform init Success Terraform has been successfully initialized!","title":"2.8.1 Enable GCP Beta Provider"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#282-enable-kubernetes-engine-api","text":"Kubernetes Engine API used to build and manages container-based applications, powered by the open source Kubernetes technology. Before starting GKE cluster creation it is required to enable it. Task N9: Enable container.googleapis.com in services.tf file similar to what we already did in 2.2 Enable required GCP Services API . edit services.tf TODO Note Adding disable_on_destroy=false helps to prevent errors during redeployments of the system. Review TF Plan: terraform plan -var-file terraform.tfvars Update Cloud Nat Configuration: terraform apply -var-file terraform.tfvars","title":"2.8.2 Enable Kubernetes Engine API"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#283-create-a-private-gke-cluster-and-delete-default-node-pool","text":"Using google_container_cluster resource create a Regional, Private GKE cluster, with following characteristics: Cluster Configuration: Cluster name: gke-$ORG-$PRODUCT-$ENV GKE Control plane is replicated across three zones of a region: us-central1 Private cluster with unrestricted access to the public endpoint: Cluster Nodes access: Private Node GKE Cluster with Public API endpoint Cluster K8s API access: with unrestricted access to the public endpoint Cluster Node Communication: VPC Native Secondary pod range with name: pods Secondary service range with name: services GKE master and node version: \"1.20.8-gke.700\" Terraform Provider: google-beta Timeouts: 30M Node Pool Configuration: VM size: e2-small Node count: 1 per zone Node images: Container-Optimized OS The name of a GCE machine (VM) type: e2-small Note Why delete default node pool? The default node pools cause trouble with managing the cluster, when created with terraform as it is not part of the terraform lifecycle. GKE Architecture Best Practice recommends to delete default node pool and create a custom one instead and manage the node pools explicitly. Note Why define Timeouts for gke resource? Normally GKE creation takes few minutes. However, in our case we creating GKE Cluster, and then system cordon, drain and then destroy default node pool. This process may take 10-20 minutes and we want to make sure terraform will not time out during this time. Step 1: Let's define GKE resource first: cat <<EOF >> gke.tf resource \"google_container_cluster\" \"primary_cluster\" { provider = google-beta project = var.gcp_project_id name = format(\"gke-%s-%s-%s\", var.org, var.product, var.environment) min_master_version = var.kubernetes_version network = google_compute_network.vpc_network.self_link subnetwork = google_compute_subnetwork.gke_private_subnet.self_link location = var.gcp_region logging_service = var.logging_service monitoring_service = var.monitoring_service remove_default_node_pool = true initial_node_count = 1 private_cluster_config { enable_private_nodes = var.enable_private_nodes enable_private_endpoint = var.enable_private_endpoint master_ipv4_cidr_block = var.master_ipv4_cidr_block } network_policy { enabled = var.network_policy provider = var.network_policy ? \"CALICO\" : \"PROVIDER_UNSPECIFIED\" } addons_config { http_load_balancing { disabled = var.disable_http_load_balancing } network_policy_config { disabled = var.network_policy ? false : true } } ip_allocation_policy { cluster_secondary_range_name = var.pods_range_name services_secondary_range_name = var.services_range_name } timeouts { create = \"30m\" update = \"30m\" delete = \"30m\" } workload_identity_config { identity_namespace = \"${var.gcp_project_id}.svc.id.goog\" } } EOF Step 2: Next define GKE cluster specific variables: cat <<EOF >> gke_variables.tf variable \"kubernetes_version\" { default = \"\" type = string description = \"The GKE version of Kubernetes\" } variable \"logging_service\" { description = \"The logging service that the cluster should write logs to.\" default = \"logging.googleapis.com/kubernetes\" } variable \"monitoring_service\" { default = \"monitoring.googleapis.com/kubernetes\" description = \"The GCP monitoring service scope\" } variable \"disable_http_load_balancing\" { default = false description = \"Enable HTTP Load balancing GCP integration\" } variable \"network_policy\" { description = \"Enable network policy addon\" default = true } variable \"pods_range_name\" { description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to pods\" default = \"\" } variable \"services_range_name\" { description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to services\" default = \"\" } variable \"enable_private_nodes\" { default = false description = \"Enable Private-IP Only GKE Nodes\" } variable \"enable_private_endpoint\" { default = false description = \"When true, the cluster's private endpoint is used as the cluster endpoint and access through the public endpoint is disabled.\" } variable \"master_ipv4_cidr_block\" { description = \"The ipv4 cidr block that the GKE masters use\" } EOF Step 3: Define GKE cluster specific outputs: cat <<EOF >> outputs.tf output \"id\" { value = \"${google_container_cluster.primary_cluster.id}\" } output \"endpoint\" { value = \"${google_container_cluster.primary_cluster.endpoint}\" } output \"master_version\" { value = \"${google_container_cluster.primary_cluster.master_version}\" } EOF Task N9: Complete terraform.tfvars with required values to GKE Cluster specified above: cat <<EOF >> terraform.tfvars //gke specific enable_private_nodes = \"TODO\" master_ipv4_cidr_block = \"TODO\" pods_range_name = \"TODO\" services_range_name = \"TODO\" kubernetes_version = \"TODO\" EOF In the next step, we going to create a custom GKE Node Pool.","title":"2.8.3 Create a Private GKE Cluster and delete default node pool"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#284-create-a-gke-custom-node-pool","text":"Using google_container_node_pool resource create a custom GKE Node Pool with following characteristics: Node Pool Configuration: VM size: e2-small Node count: 1 per zone Node images: Container-Optimized OS The name of a GCE machine (VM) type: e2-small Step 1: Let's define GKE resource first: cat <<EOF >> gke.tf #Node Pool Resource resource \"google_container_node_pool\" \"custom-node_pool\" { provider = google-beta name = \"main-pool\" location = var.gcp_region project = var.gcp_project_id cluster = google_container_cluster.primary_cluster.name node_count = var.gke_pool_node_count version = var.kubernetes_version node_config { image_type = var.gke_pool_image_type disk_size_gb = var.gke_pool_disk_size_gb disk_type = var.gke_pool_disk_type machine_type = var.gke_pool_machine_type } timeouts { create = \"10m\" delete = \"10m\" } lifecycle { ignore_changes = [ node_count ] } } EOF Step 2: Next define GKE cluster specific variables: cat <<EOF >> gke_variables.tf #Node Pool specific variables variable \"gke_pool_machine_type\" { type = string } variable \"gke_pool_node_count\" { type = number } variable \"gke_pool_disk_type\" { type = string default = \"pd-standard\" } variable \"gke_pool_disk_size_gb\" { type = string } variable \"gke_pool_image_type\" { type = string } EOF Task 9 (Continued): Complete terraform.tfvars with required values to GKE Node Pool values specified above: cat <<EOF >> terraform.tfvars #pool specific gke_pool_node_count = \"TODO\" gke_pool_image_type = \"TODO\" gke_pool_disk_size_gb = \"TODO\" gke_pool_machine_type = \"TODO\" EOF Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars Step 4: Create GKE Cluster and Node Pool: terraform apply -var-file terraform.tfvars Output: google_container_cluster.primary_cluster: Creating... ... google_container_cluster.primary_cluster: Creation complete after 20m9s google_container_node_pool.custom-node_pool: Creating... google_container_node_pool.custom-node_pool: Creation complete after 2m10s","title":"2.8.4 Create a GKE custom Node pool"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#285-update-gke-node-pool-to-support-auto-upgrade-and-auto-recovery-features","text":"Note GKE Master Nodes are managed by Google and get's upgraded automatically. Users can only specify Maintenance Window if they have preference for that process to occur (e.g. after busy hours). Users can however control Node Pool upgrade lifecycle. The can choose to do it themselves or with Auto Upgrade. Task N10: Using google_container_node_pool resource update node pool to support Auto Upgrade and Auto Recovery features. edit gke.tf TODO Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars No errors. Step 4: Update GKE Cluster Node Pool configuration: terraform apply -var-file terraform.tfvars Summary Congrats! You've now learned how to deploy production grade GKE clusters.","title":"2.8.5 Update GKE Node Pool to support Auto Upgrade and Auto Recovery features"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#29-optional-repeatable-infrastructure","text":"When you doing IaC it is important to insure that you can both create and destroy resources consistently. This is especially important when doing CI/CD testing. Step 3: Destroy all resources: terraform destroy -var-file terraform.tfvars No errors. Step 4: Recreate all resources: terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars","title":"2.9 (Optional) Repeatable Infrastructure"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#3-create-documentation-for-terraform-code","text":"Documentation for your terraform code is an important part of IaC. Make sure all your variables have a good description! There are community tools that have been developed to make the documentation process smoother, in terms of documenting Terraform resources and requirements.Its good practice to also include a usage example snippet. Terraform-Docs is a good example of one tool that can generate some documentation based on the description argument of your Input Variables, Output Values, and from your required_providers configurations. Step 1 Install the terraform-docs cli to your Google CloudShell environment: curl -Lo ./terraform-docs.tar.gz https://github.com/terraform-docs/terraform-docs/releases/download/v0.14.1/terraform-docs-v0.14.1-$(uname)-amd64.tar.gz tar -xzf terraform-docs.tar.gz rm terraform-docs.tar.gz chmod +x terraform-docs sudo mv terraform-docs /usr/local/bin/ terraform-docs Generating terraform documentation with Terraform Docs: cd ~/$MY_REPO cd foundation-infrastructure terraform-docs markdown . > README.md cd ../notepad-infrastructure terraform-docs markdown . > README.md Verify created documentation: edit README.md","title":"3. Create Documentation for terraform code"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#4-workaround-for-project-quota-issue","text":"If you see following error during project creation in foundation layer: Error: Error setting billing account \"010BE6-CA1129-195D77\" for project \"projects/ayrat-notepad-dev-244\": googleapi: Error 400: Precondition check failed., failedPrecondition This is due to our Billing account has quota of 5 projects per account. To solve this issue find all unused accounts: gcloud beta billing projects list --billing-account $ACCOUNT_ID And unlink them, so you have less then 5 projects per account: gcloud beta billing projects unlink $PROJECT_ID","title":"4. Workaround for Project Quota issue"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#5-commit-readme-doc-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit docs folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Readme doc for Production GKE Creation using gcloud\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"5 Commit Readme doc to repository and share it with Instructor/Teacher"},{"location":"020_Assignment_3_Terraform_GCP_Foundation/#6-cleanup","text":"We only going to cleanup GCP Service foundation layer, as we going to use GCP project in future. cd ~/$MY_REPO/notepad-infrastructure terraform destroy -var-file terraform.tfvars","title":"6 Cleanup"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/","text":"Lab 3 Creating Production GKE Cluster via IaC using Terraform Objective: Learn Terraform Commands Learn GCP Terraform Provider Learn Terraform variables Store TF State in GCS buckets Learn how to create GCP resources with Terraform Working with Git and IaC \u00b6 Git-based workflows, merge requests and peer reviews create a level of documented transparency that is great for security teams and audits. They ensure every change is documented as well as the metadata surrounding the change, answering the why, who, when and what questions. In this set of exercises you will continue using notepad Google Source Repository that already contains you application code, dockerfiles and kubernetes manifests. We will use so called Monorepo approach, and store our terraform IAC configuration in the same repo as our application code and Kubernetes manifests. Add your IaC code to your repository \u00b6 Step 1 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 2 Create foundation-infrastructure and notepad-infrastructure folders that will contains respective terraform configurations cd ~/$MY_REPO mkdir foundation-infrastructure mkdir notepad-infrastructure Step 3 Create a README.md file describing foundation-infrastructure folder purpose: cat <<EOF> foundation-infrastructure/README.md # Creation of GCP Foundation Layer This step, is focused on creating GCP Foundation: folders (if any) and service projects creation with their respictive GCS bucket to store terraform state and IAM Config. EOF Step 4 Create a README.md file describing notepad-infrastructure folder purpose: cat <<EOF> notepad-infrastructure/README.md # Creation of GCP Services Layer This step, is focused on creating gcp services such as GKE, VPC and etc. inside of existing project EOF Step 5 Create a .gitignore file in your working directory: cat<<EOF>> .gitignore .terraform.* .terraform terraform.tfstate* EOF Note Ignore files are used to tell git not to track files. You should also always include any files with credentials in a gitignore file. List created folder structure along with gitignore : ls -ltra Step 6 Commit deploy folder using the following Git commands: git status git add . git commit -m \"Terraform Folder structure for assignement 3\" Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 1 Build GCP Foundation Layer \u00b6 In this Lab we going to build GCP Foundation layer. As we learned in the class this includes Org Structure creation with Folders and Projects, creation IAM Roles and assigning them to users and groups in organization. This Layer usually build be DevOps or Infra team that. Since we don't have organization registered for each student, we going to skip creation of folders and IAM groups. We going to start from creation of a Project, deleting default VPC and configuring inside that project a gcs bucket that will be used in the next terraform layer to store a Terraform state. Part 1: Create Terraform Configurations file to create's GCP Foundation Layer: Layer 1 From existing (we going to call it seeding) project that you currently use to store code in Google Cloud Source Repository: Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf Create a new notepad-dev Project Delete Default VPC Create a bucket in this project to store terraform state Part 2: Create Terraform Configurations file that create's GCP Services Layer: Layer 2 From notepad-dev GCP Project: Enable Google Project Service APIs Create VPC (google_compute_network) and Subnet (google_compute_subnetwork) Create Cloud Nat (google_compute_router) and (google_compute_router_nat) Private GKE Private Nodes with Public API Endpoint (for simplicity) 1.1 Installing Terraform \u00b6 GCP Cloud Shell comes with many common tools pre-installed including terraform Verify and validate the version of Terraform that is installed: terraform --version If you want to use specific version of terraform or want to install terraform in you local machine use following link to download binary. 1.2 Configure GCP credentials for Terraform \u00b6 If you would like to use terraform on GCP you have 2 options: 1). Using you user credentials, great option for testing terraform from you laptop and for learning purposes. 2). Using service account , great option if you going to use terraform with CI/CD system and fully automate Terraform deployment. In this Lab we going to use Option 1 - using user credentials. Step 1: In order to make requests against the GCP API, you need to authenticate to prove that it's you making the request. The preferred method of provisioning resources with Terraform on your workstation is to use the Google Cloud SDK (Option 1) gcloud auth application-default login 1.3 Initializing Terraform \u00b6 Terraform relies on providers to interact with remote systems. Every resource type is implemented by a provider; without providers , Terraform can't manage any kind of infrastructure; in order for terraform to install and use a provider it must be declared. In this exercise you will declare and configure the Terraform provider(s) that will be used for the rest of the Lab. 1.3.1 Declare GCP Terraform Providers \u00b6 The required_providers block defines the providers terraform will use to provision resources and their source. version : The version argument is optional and is used to tell terraform to pick a particular version from the available versions source : The source is the provider registry e.g. hashicorp/gcp is the short for registry.terraform.io/hashicorp/gcp cd ~/$MY_REPO/foundation-infrastructure/ cat << EOF>> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } } } EOF 1.3.2 Configure the GCP Terraform Provider \u00b6 Providers often require configuration (like endpoint URLs or cloud regions) before they can be used. These configurations are defined by a provider block. Multiple provider configuration blocks can be declared for the same by adding the alias argument Set you current PROJECT_ID value here: export PROJECT_ID=<YOUR_PROJECT_ID> cd ~/$MY_REPO/foundation-infrastructure/ cat << EOF>> main.tf provider \"google\" { alias = \"gcp-provider\" project = \"$PROJECT_ID\" region = \"us-central1\" } EOF Result We configured provider , so that it can provision resource in specified gcp project in us-central1 region. 1.3.3 Initialize GCP Terraform Provider \u00b6 Now that you have declared and configured the GCP provider for terraform, initialize terraform: cd ~/$MY_REPO/foundation-infrastructure/ terraform init Explore your directory. What has changed? ls -ltra Result We can see that new directory .terraform and .terraform.lock.hcl file. Extra! Investigate available providers in the Terraform Provider Registry Select another provider from the list, add it to your required providers, and to your main.tf Run terraform init to load your new provider! 1.4 Terraform Variables \u00b6 Input variables are used to increase your Terraform configuration's flexibility by defining values that can be assigned to customize the configuration. They provide a consistent interface to change how a given configuration behaves. Input variables blocks have a defined format: Input variables blocks have a defined format: variable \"variable_name\" { type = \"The variable type eg. string , list , number\" description = \"A description to understand what the variable will be used for\" default = \"A default value can be provided, terraform will use this if no other value is provided at terraform apply\" } Terraform CLI defines the following optional arguments for variable declarations: default - A default value which then makes the variable optional. type - This argument specifies what value types are accepted for the variable. description - This specifies the input variable's documentation. validation - A block to define validation rules, usually in addition to type constraints. sensitive - Limits Terraform UI output when the variable is used in configuration. In our above example for main.tf we can actually declare project and region as variables, so let's do it. 1.4.1 Declare Input Variables \u00b6 Variables are declared in a variables.tf file inside your terraform working directory. The label after the variable keyword is a name for the variable, which must be unique among all variables in the same module. You can choose variable name based on you preference, or in some cases based on agreed in company naming convention. In our case we declaring variables names: gcp_region and gcp_project_id We are going to declare a type as a string variable for gcp_project_id and gcp_region . It is always good idea to specify a clear description for the variable for documentation purpose as well as code reusability and readability. Finally, let's configured default argument. In our case we going to use \"us-central1\" as a default for gcp_region and we going to keep default value empty for gcp_project_id . cat <<EOF> variables.tf variable \"gcp_region\" { type = string description = \"The GCP Region\" default = \"us-central1\" } variable \"gcp_project_id\" { type = string description = \"The GCP Seeding project ID\" default = \"\" } EOF Note Explore other type of variables such as number , bool and type constructors such as: list( ), set( ), map( ) 1.4.2 Using Variables \u00b6 Now that you have created your input variables, let's re-create main.tf that currently has the terraform configuration for GCP Provider. rm main.tf cat << EOF>> main.tf provider \"google\" { alias = \"gcp-provider\" project = var.gcp_project_id region = var.gcp_region } EOF Test out your terraform configuration: terraform plan 1.4.3 Working with Variables files. \u00b6 Create a terraform.tfvars file to hold the values for your variables: export gcp_project_id=<YOUR_PROJECT_ID> cat <<EOF >> terraform.tfvars gcp_project_id = \"$PROJECT_ID\" gcp_region = \"us-central1\" EOF Hint using different env-name.tfvars files you can create different set of terraform configuration for the same code. (e.g. code for dev, staging, prod) 1.4.4 Validate configuration and code syntax \u00b6 Let's Validate configuration and code syntax that we've added so far. There are several tools that can analyze your Terraform code without running it, including: Terraform has build in command that you can use to check your Terraform syntax and types (a bit like a compiler): terraform validate Result Seems the code is legit so far Is your terraform easy to read and follow? Terraform has a built-in function to lint your configuration manifests for readability and best practice spacing: terraform fmt --recursive The --recursive flag asks the fmt command to traverse all of your terraform directories and format the .tf files it finds. It will report the files it changed as part of the return information of the command Hint Use the git diff command to see what was changed. Result We can see that terraform.tfvars file had some spacing that been fixed for better code readability. Extra Another cool tool that you can use along you terraform development is tflint - framework and each feature is provided by plugins, the key features are as follows: Find possible errors (like illegal instance types) for Major Cloud providers (AWS/Azure/GCP). Warn about deprecated syntax, unused declarations. Enforce best practices, naming conventions. Finally let's run terraform plan : terraform plan Output: No changes. Your infrastructure matches the configuration. Result We don't have any errors, however we don't have any resources created so far. Let's create a GCP project resource to start with! 1.5 Create GCP project using Terraform \u00b6 1.5.1 Configure google_project resource \u00b6 Resources describe the infrastructure objects you want terraform to create. A resource block can be used to create any object such as virtual private cloud, security groups, DNS records etc. Let's create our first Terraform resource: GCP project. In order to accomplish this we going to use google_project resource, documented here In order to create a new Project we need to define following arguments: * name - (Required) The display name of the project. * project_id - (Required) The project ID. Changing this forces a new project to be created. * billing_account - The alphanumeric ID of the billing account this project belongs to. First, Let's declare actual values for name , project_id and billing_account with variables: cat <<EOF >> variables.tf variable \"billing_account\" { description = \"The billing account ID for this project\" } variable \"project_name\" { description = \"The human readable project name (min 4 letters)\" } variable \"project_id\" { description = \"The GCP project ID\" } EOF Then, update terraform.tfvars variables files, with actual values for name , project_id and billing_account : We going to use same naming structure for PROJECT_ID and PROJECT_NAME as in previous Assignment, in order to define your new GCP project: export ORG=<student-name> export ORG=ayrat export PRODUCT=notepad export ENV=dev export PROJECT_PREFIX=4 export PROJECT_NAME=$ORG-$PRODUCT-$ENV export PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX # Project ID has to be unique echo $PROJECT_NAME echo $PROJECT_ID In order to get Billing account run following command: ACCOUNT_ID=$(gcloud alpha billing accounts list | grep Education | grep True |awk '{ print $1 }') echo $ACCOUNT_ID cat <<EOF >> terraform.tfvars billing_account = \"$ACCOUNT_ID\" project_name = \"$PROJECT_NAME\" project_id = \"$PROJECT_ID\" EOF Verify if everything looks good, and correct values has been set: cat terraform.tfvars Finally, let's define google_project resource in project.tf file, we will replace actual values for name and billing_account with variables: cat <<EOF >> project.tf resource \"google_project\" \"project\" { name = var.project_name billing_account = var.billing_account project_id = var.project_id } EOF Run plan command: terraform plan -var-file terraform.tfvars Result Plan: 1 to add, 0 to change, 0 to destroy. Note Take notice of plan command and see how some values that you declared in *.tvfars are visible and some values will known after apply terraform apply -var-file terraform.tfvars Success We've created our first resource with terrraform 1.5.2 Making Project resource Immutable \u00b6 Very often when you developing IaC, you need to destroy and recreate your resorces, e.g. for troubleshooting or creating a new resources using same config. Let's destroy our project and try to recreate it again. terraform destroy -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Failed What happened? Did you project been able to create? If not why ? If we want to make our infrastructure to be Immutable and fully automated, we need to make sure that we can destroy our service and recreate it any time the same way. In our case we can't do that because Project ID always has to be unique. To tackle this problem we need to randomize our Project ID creation within the terraform. 1.5.3 Create a Project using Terraform Random Provider \u00b6 In order to create a GCP Project with Random name, we can use Terraform's random_integer resource with following arguments: max (Number) The maximum inclusive value of the range - 100 min (Number) The minimum inclusive value of the range - 999 cat <<EOF >> random.tf resource \"random_integer\" \"id\" { min = 100 max = 999 } EOF random_integer resource doesn't belongs to Google Provider, it requires Hashicorp Random Provider to be initialized. Since it's native hashicorp provider we can skip the step of defining and configuring that provider, as it will be automatically initialized. terraform init Result Installing hashicorp/random v3.1.0... Installed hashicorp/random v3.1.0 (signed by HashiCorp) 1.5.3 Create a Project_ID name using Local Values \u00b6 Local Values allow you to assign a name to an expression, so the expression can be used multiple times, without repeating it. We going to define value of project_id as local. And it will be a combination of project_name - random_number . Let's add local value for project_id and replace value var.project_id with local.project_id . edit project.tf Update code snippet of project.tf as following and save: locals { project_id = \"${var.project_name}-${random_integer.id.result}\" } resource \"google_project\" \"project\" { name = var.project_name billing_account = var.billing_account project_id = local.project_id } We can now remove project_id value from terraform.tfvars , as we going to randomly generate it using locals expression with random_integer edit terraform.tfvars Remove project_id line and save. We can now also remove project_id variable from variables.tf : rm variables.tf cat <<EOF >> variables.tf variable \"gcp_region\" { type = string description = \"The GCP Region\" default = \"us-central1\" } variable \"gcp_project_id\" { type = string description = \"The GCP Seeding project ID\" default = \"\" } variable \"billing_account\" { description = \"The billing account ID for this project\" } variable \"project_name\" { description = \"The human readable project name (min 4 letters)\" } EOF terraform plan -var-file terraform.tfvars Output: # random_integer.id will be created + resource \"random_integer\" \"id\" { + id = (known after apply) + max = 999 + min = 100 + result = (known after apply) } terraform apply -var-file terraform.tfvars Result Project has been created with Random project_id 1.5.4 Configure Terraform Output for GCP Project \u00b6 Outputs provide return values of a Terraform state at any given time. So you can get for example values of what GCP resources like project, VMs, GKE cluster been created and their parametres. Outputs can be used used to export structured data about resources. This data can be used to configure other parts of your infrastructure, or as a data source for another Terraform workspace. Outputs are also necessary to share data from a child module to your root module. Outputs follow a similar structure to variables: output \"output_name\" { description = \"A description to understand what information is provided by the output\" value = \"An expression and/or resource_name.attribute\" sensitive = \"Optional argument, marking an output sensitive will supress the value from plan/apply phases\" } Let's declare GCP Project output values for project_id and project_number . You can find available output in each respective resource documents, under ` Attributes Reference . For example for GCP project available outputs are: cat <<EOF >> outputs.tf output \"id\" { value = google_project.project.project_id description = \"GCP project ID\" } output \"number\" { value = google_project.project.number description = \"GCP project number\" sensitive = true } EOF terraform plan -var-file terraform.tfvars We can see that we have new changes to output: Output: Changes to Outputs: + id = \"ayrat-notepad-dev-631\" + number = (sensitive value) Let's apply this changes: terraform apply -var-file terraform.tfvars Output: Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Let's now run terraform output command: terraform output Result We can see value of project id , however we don't see project number as it's been marked as sensitive. Note Outputs can be useful when you want to provide results of terraform resource creation to CI/CD or next automation tool like helm to deploy application. 1.5.5 Recreate GCP Project without Default VPC \u00b6 One of the requirements for our solution is to create GCP project with custom VPC. However, when terraform creates GCP Project it creates DEFAULT vpc by default. gcloud compute networks list --project ayrat-notepad-dev-631 Output: NAME SUBNET_MODE BGP_ROUTING_MODE IPV4_RANGE GATEWAY_IPV4 default AUTO REGIONAL In order to remove automatically created default VPC, specify special attribute during google_project resource creation. Check documentation here and find this argument. Task N1: Find Attribute to remove default VPC during google_project resource creation. Define in it project.tf and set it's value in variables.tf as true by default. Solution: Update code snippet of project.tf as following and save: locals { project_id = \"${var.project_name}-${random_integer.id.result}\" } resource \"google_project\" \"project\" { name = var.project_name billing_account = var.billing_account project_id = local.project_id auto_create_network = var.auto_create_network } cat <<EOF >> variables.tf variable \"auto_create_network\" { description = \"Automatically provision a default global VPC network\" default = false } EOF After changes you need to re-create a project, as vpc get's deleted during project creation only. terraform destroy -var-file terraform.tfvars terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Notice How long project is now getting created. This is due to after project creation, default vpc is being removed. Verify that default VPC has been deleted: gcloud compute networks list --project ayrat-notepad-dev-631 Output: Listed 0 items. Success We now able to create a GCP project without Default VPC. 1.6 Create GCP Storage bucket \u00b6 1.6.1 Create GCP Storage bucket in New GCP Project \u00b6 Using reference doc for google_storage_bucket , let's create google_storage_bucket resource that will create MULTI_REGIONAL GCS bucket in newly created project. We going to give bucket name : $ORG-notepad-dev-tfstate , and we going to use this bucket to store Terraform state for GCP Service Layer. cat <<EOF >> bucket.tf resource \"google_storage_bucket\" \"state\" { name = var.bucket_name project = local.project_id storage_class = var.storage_class force_destroy = \"true\" } EOF cat <<EOF >> variables.tf variable \"bucket_name\" { description = \"The name of the bucket.\" } variable \"storage_class\" { description = default = \"MULTI_REGIONAL\" } EOF Set variable that will be used to build name for a bucket: export ORG=<student-name> cat <<EOF >> terraform.tfvars bucket_name = \"$ORG-notepad-dev-tfstate\" EOF cat <<EOF >> outputs.tf output \"bucket_name\" { value = google_storage_bucket.state.name } EOF Let's review the plan: terraform plan -var-file terraform.tfvars And create google_storage_bucket resource: terraform apply -var-file terraform.tfvars Verify created bucket in GCP UI: Storage -> Cloud Storage gsutil ls -L -b gs://$ORG-notepad-dev-tfstate Result GCS bucket for terraform state has been created 1.6.2 Configure versioning on GCP Storage bucket. \u00b6 Check if versioning enabled on the created bucket: gsutil versioning get gs://$ORG-notepad-dev-tfstate Result Versioning: Suspended It is highly recommended that if you going to use GCS bucket as Terraform storage backend you should enable Object Versioning on the GCS bucket to allow for state recovery in the case of accidental deletions and human error. Task N2: Using reference doc for google_storage_bucket find and configure argument that enables gcs bucket versioning feature. Edit bucket.tf with correct argument. edit bucket.tf Solution: resource \"google_storage_bucket\" \"state\" { name = var.bucket_name project = local.project_id storage_class = var.storage_class versioning { enabled = false } force_destroy = \"true\" } terraform plan -var-file terraform.tfvars Output: Plan: 0 to add, 1 to change, 0 to destroy. Result Configuration for Versioning looks correct terraform apply -var-file terraform.tfvars Verify versioning is ON: gsutil versioning get gs://$ORG-notepad-dev-tfstate Output: Versioning Enabled Result We've finished building the Foundation Layer. So far we able to accomplish following: Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf Create a new notepad-dev Project Delete Default VPC Create a bucket in this project to store terraform state 2 Build GCP Services Layer \u00b6 Once we finished building a GCP Foundation layer which is essentially our project, we can start building GCP Services Layer inside that project. This second layer will configure following items: Enable Google Project Service APIs Create VPC (google_compute_network) and Subnet (google_compute_subnetwork) Create Cloud Nat (google_compute_router) and (google_compute_router_nat) Private GKE Private Nodes with Public API Endpoint (for simplicity) 2.1 Initialize Terraform for GCP Services Layer \u00b6 2.1.1 Define and configure terraform provider \u00b6 Step 1: Take note of newly created GCP Project_ID and BUCKET_ID: cd ~/$MY_REPO/foundation-infrastructure terraform output | grep 'id' |awk '{ print $3}' terraform output | grep 'bucket_name' |awk '{ print $3}' Set it as variable: export PROJECT_ID=$(terraform output | grep 'id' |awk '{ print $3}') export BUCKET_ID=$(terraform output | grep 'bucket_name' |awk '{ print $3}') echo $PROJECT_ID echo $BUCKET_ID Step 2: Declare the Terraform Provider for GCP Services Layer: We now going to switch to notepad-infrastructure where we going to create a new GCP service Layer terraform configuration: cd ~/$MY_REPO/notepad-infrastructure cat << EOF>> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } } } EOF Step 4: Configure the Terraform Provider cat << EOF>> main.tf provider \"google\" { project = var.gcp_project_id region = var.gcp_region } EOF Step 5: Define variables: cat <<EOF> variables.tf variable \"gcp_region\" { type = string description = \"The GCP Region\" default = \"us-central1\" } variable \"gcp_project_id\" { type = string description = \"The newly created GCP project ID\" } EOF Step 5: Set variables in terraform.tfvars cat <<EOF >> terraform.tfvars gcp_project_id = $PROJECT_ID EOF Step 4: Now that you have declared and configured the GCP provider for terraform, initialize terraform: terraform init 2.1.2 Configure Terraform State backend \u00b6 Terraform records information about what infrastructure it created in a Terraform state file. This information, or state, is stored by default in a local file named terraform.tfstate. This allows Terraform to compare what's in your configurations with what's in the state file, and determine what changes need to be applied. Local vs Remote backend: Local state files are the default for terraform. Remote state allows for collaboration between members of your team, for example multiple users or systems can deploy terraform configuration to the same environment as state is stored remotely and can be pulled localy during the execution. This however may create issues if 2 users running terraform plan and apply at the same time, however some remote backends including gcs provide locking feature that we covered and enabled in previous section Configure versioning on GCP Storage bucket. In addition to that, Remote state is more secure storage of sensitive values that might be contained in variables or outputs. See documents for remote backend for reference: Step 1 Configure remote backend using gcs bucket: Let's configure a backend for your state, using the gcs bucket you previously created in Foundation Layer. Backends are configured with a nested backend block within the top-level terraform block While a backend can be declared anywhere, it is recommended to use a backend.tf . Since we running on GCP we going to use GCS remote backend . It stores the state as an object in a configurable prefix in a pre-existing bucket on Google Cloud Storage (GCS). This backend also supports state locking. The bucket must exist prior to configuring the backend. We going to use following arguments: bucket - (Required) The name of the GCS bucket. This name must be globally unique. For more information, see Bucket Naming Guidelines. prefix - (Optional) GCS prefix inside the bucket. Named states for workspaces are stored in an object called / .tfstate. cat <<EOF >> backend.tf terraform { backend \"gcs\" { bucket = $BUCKET_ID prefix = \"state\" } } EOF Step 2 When you change, configure or unconfigure a backend, terraform must be re-initialized: terraform init Verify if folder state has been created in our bucket: gsutil ls gs://$ORG-notepad-dev-tfstate Summary state Folder has been created in gcs bucket and terrafrom has been initialized with remote backend 2.2 Enable required GCP Services API \u00b6 Before we continue with creating GCP services like VPC, routers, Cloud Nat and GKE it is required to enable underlining GCP API services. When we creating a new project most of the services API's are disabled, and requires explicitly to be enabled. google_project_service resource allows management of a single API service for an existing Google Cloud Platform project. Let's enable compute engine API service with terraform: cat <<EOF >> services.tf resource \"google_project_service\" \"compute\" { service = \"compute.googleapis.com\" disable_on_destroy = false } EOF terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Verify Compute Engine API service has been enabled: gcloud services list Result Compute Engine API is enabled as it is listed. 2.2 Set variables to define standard for Naming Convention \u00b6 As per terraform best practices, when you creating terraform resources, you need to follow naming convention, that is clear for you organization. Configuration objects should be named using underscores to delimit multiple words. Object's name should be named using dashes This practice ensures consistency with the naming convention for resource types, data source types, and other predefined values and helps prevent accidental deletion or outages: Example: # Good resource \"google_compute_instance\" \"web_server\" { name = \u201cweb-server-$org-$app-$env\u201d # ... } # Bad resource \u201cgoogle_compute_instance\u201d \u201cweb-server\u201d { name = \u201cweb-server\u201d # \u2026 Create variables to define standard for Naming Convention: cat <<EOF >> variables.tf variable \"org\" { type = string } variable \"product\" { type = string } variable \"environment\" { type = string } EOF export ORG=ayrat export PRODUCT=notepad export ENV=dev cat <<EOF >> terraform.tfvars org = \"$ORG\" product = \"$PRODUCT\" environment = \"$ENV\" EOF Review if created files are correct. 2.3 Create a custom mode network (VPC) with terraform \u00b6 Using google_compute_network resource create a VPC network with terraform. Task N3: Create vpc.tf file and define custom mode network (VPC) with following requirements: * Description: VPC that will be used by the GKE private cluster on the related project * name - vpc name with following pattern: \"vpc-$ORG-$PRODUCT-$ENV\" * Subnet mode: custom * Bgp routing mode: regional * MTUs: default Hint To create \"vpc-$ORG-$PRODUCT-$ENV\" name, use format function , and use %s to convert variables to string values. Define variables in variables.tf and terraform.tfvars if required. Define Output for: Generated VPC name: google_compute_network.vpc_network.name self_link - The URI of the created resource. Solution: cat <<EOF >> vpc.tf resource \"google_compute_network\" \"vpc_network\" { name = format(\"vpc-%s-%s-%s\", var.org, var.product, var.environment) routing_mode = \"REGIONAL\" description = \"VPC that will be used by the GKE private cluster on the related project\" auto_create_subnetworks = false } EOF cat <<EOF >> outputs.tf output \"vpc_name\" { value = google_compute_network.vpc_network.name } output \"vpc_selflink\" { value = \"\\${google_compute_network.vpc_network.self_link}\" } EOF Review TF Plan: terraform plan -var-file terraform.tfvars Output: + create Terraform will perform the following actions: # google_compute_network.vpc_network will be created + resource \"google_compute_network\" \"vpc_network\" { + auto_create_subnetworks = false + delete_default_routes_on_create = false + description = \"VPC that will be used by the GKE private cluster on the related project\" + gateway_ipv4 = (known after apply) + id = (known after apply) + mtu = (known after apply) + name = \"vpc-ayrat-notepad-dev\" + project = (known after apply) + routing_mode = \"REGIONAL\" + self_link = (known after apply) Create VPC: terraform apply -var-file terraform.tfvars Verify that custom-mode VPC has been created: gcloud compute networks list gcloud compute networks describe $(gcloud compute networks list | grep CUSTOM |awk '{ print $1 }') Output: autoCreateSubnetworks: false description: VPC that will be used by the GKE private cluster on the related project kind: compute#network name: vpc-student_name-notepad-dev routingConfig: routingMode: REGIONAL selfLink: https://www.googleapis.com/compute/v1/projects/XXX x_gcloud_bgp_routing_mode: REGIONAL x_gcloud_subnet_mode: CUSTOM Result VPC Network has been created, without auto subnets. 2.4 Create a user-managed subnet with terraform \u00b6 Using google_compute_subnetwork resource create a user-managed subnet with terraform. cat <<EOF >> subnet.tf resource \"google_compute_subnetwork\" \"gke_private_subnet\" { name = format(\"subnet-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link region = var.gcp_region project = var.gcp_project_id ip_cidr_range = var.network_cidr secondary_ip_range { range_name = var.pods_cidr_name ip_cidr_range = var.pods_cidr } secondary_ip_range { range_name = var.services_cidr_name ip_cidr_range = var.services_cidr } } EOF Note Notice power of terraform outputs. Here we link subnet with our VPC network using google_compute_network.vpc_network.self_link output value of created network in previous step. Define variables: cat <<EOF >> variables.tf # variables used to create VPC variable \"network_cidr\" { type = string } variable \"pods_cidr\" { type = string } variable \"pods_cidr_name\" { type = string } variable \"services_cidr\" { type = string } variable \"services_cidr_name\" { type = string } EOF Define outputs: cat <<EOF >> outputs.tf output \"subnet_selflink\" { value = \"\\${google_compute_subnetwork.gke_private_subnet.self_link}\" } EOF Task N4: Update terraform.tfvars file values with following information: Node Range: See column subnet in above table for dev cluster Secondary service range with name: services Secondary Ranges: Service range name: services Service range CIDR: See column srv range in above table for dev cluster Pods range name: pods Pods range CIDR: See column pod range in above table for dev cluster env | subnet | pod range | srv range | kubectl api range dev | 10.128.1.0/26 | 172.0.0.0/18 | 172.10.0.0/21 | 172.16.0.0/28 stg | 10.128.2.0/26 | 172.1.0.0/18 | 172.11.0.0/21 | 172.16.0.16/28 prd | 10.128.3.0/26 | 172.2.0.0/18 | 172.12.0.0/21 | 172.16.0.32/28 Note Ranges must be with in Private (RFC1918) Address Space Solution: cat <<EOF >> terraform.tfvars #subnet vars network_cidr = \"10.128.1.0/26\" pods_cidr = \"172.0.0.0/18\" pods_cidr_name = \"pods\" services_cidr = \"172.10.0.0/21\" services_cidr_name = \"services\" EOF Review TF Plan: terraform plan -var-file terraform.tfvars Create VPC: terraform apply -var-file terraform.tfvars Review created subnet: gcloud compute networks subnets list gcloud compute networks subnets describe $(gcloud compute networks subnets list | grep us-central1 |awk '{ print $1 }') --region us-central1 Output: enableFlowLogs: false gatewayAddress: 10.128.1.1 ipCidrRange: 10.128.1.0/26 kind: compute#subnetwork logConfig: enable: false name: subnet-student_name-notepad-dev privateIpGoogleAccess: false privateIpv6GoogleAccess: DISABLE_GOOGLE_ACCESS purpose: PRIVATE secondaryIpRanges: - ipCidrRange: 172.0.0.0/18 rangeName: pods - ipCidrRange: 172.10.0.0/21 rangeName: services stackType: IPV4_ONLY Also check in Google cloud UI: Networking->VPC Networks -> Click VPC network and check `Subnet` tab Task N5: Update subnet.tf so that google_compute_subnetwork resource supports following features: * Flow Logs * Aggregation interval: 15 min * Flow sampling: 0.1 * Metadata: \"INCLUDE_ALL_METADATA\" * Private IP Google Access Solution: private_ip_google_access = true log_config { aggregation_interval = \"INTERVAL_15_MIN\" flow_sampling = 0.1 metadata = \"INCLUDE_ALL_METADATA\" } Review TF Plan: terraform plan -var-file terraform.tfvars Create VPC: terraform apply -var-file terraform.tfvars 2.5 Create a Cloud router \u00b6 Create Cloud Router for custom mode network (VPC), in the same region as the instances that will use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway. Task N6: Define a google_compute_router inside router.tf that will be able to create a NAT router so the nodes can reach DockerHub and external APIs from private cluster, using following parameters: Create router for custom vpc_network created above with terraform Same project as VPC Same region as VPC Router name: gke-net-router Local BGP Autonomous System Number (ASN): 64514 Hint You can automatically recover vpc name from terraform output like this: google_compute_network.vpc_network.self_link . Solution: cat <<EOF >> router.tf resource \"google_compute_router\" \"gke_net_router\" { project = var.gcp_project_id name = \"gke-net-router\" region = var.gcp_region network = google_compute_network.vpc_network.self_link bgp { asn = 64514 } } EOF Review TF Plan: terraform plan -var-file terraform.tfvars Create Cloud Router: terraform apply -var-file terraform.tfvars Verify created Cloud Router: CLI: gcloud compute routers list gcloud compute routers describe $(gcloud compute routers list | grep gke-net-router |awk '{ print $1 }') --region us-central1 Output: bgp: advertiseMode: DEFAULT asn: 64514 keepaliveInterval: 20 kind: compute#router name: gke-net-router UI: Networking -> Hybrid Connectivity -> Cloud Routers Result Router resource has been created for VPC Network 2.6 Create a Cloud Nat \u00b6 Set up a simple Cloud Nat configuration using google_compute_router_nat resource, which will automatically allocates the necessary external IP addresses to provide NAT services to a region. When you use auto-allocation, Google Cloud reserves IP addresses in your project automatically. cat <<EOF >> cloudnat.tf resource \"google_compute_router_nat\" \"gke_cloud_nat\" { project = var.gcp_project_id name = \"gke-cloud-nat\" router = google_compute_router.gke_net_router.name region = var.gcp_region nat_ip_allocate_option = \"AUTO_ONLY\" source_subnetwork_ip_ranges_to_nat = \"ALL_SUBNETWORKS_ALL_IP_RANGES\" } EOF Review TF Plan: terraform plan -var-file terraform.tfvars Create Cloud Router: terraform apply -var-file terraform.tfvars Verify created Cloud Nat : CLI: # List available Cloud Nat Routers gcloud compute routers nats list --router gke-net-router --router-region us-central1 # Describe Cloud Nat Routers `gke-cloud-nat`: gcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1 Output: enableEndpointIndependentMapping: true icmpIdleTimeoutSec: 30 name: gke-cloud-nat natIpAllocateOption: AUTO_ONLY sourceSubnetworkIpRangesToNat: ALL_SUBNETWORKS_ALL_IP_RANGES tcpEstablishedIdleTimeoutSec: 1200 tcpTransitoryIdleTimeoutSec: 30 udpIdleTimeoutSec: 30 UI: Networking -> Network Services -> Cloud NAT Result A NAT service created in a router Task N7: Additionally turn ON logging feature for ALL log types of communication for Cloud Nat edit cloudnat.tf Solution: Correctly add following configuration: log_config { filter = \"ALL\" enable = true } Review TF Plan: terraform plan -var-file terraform.tfvars Update Cloud Nat Configuration: terraform apply -var-file terraform.tfvars Output: Apply complete! Resources: 0 added, 1 changed, 0 destroyed. gcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1 Result Cloud Nat now supports Logging. Cloud NAT logging allows you to log NAT connections and errors. When Cloud NAT logging is enabled, one log entry can be generated for each of the following scenarios: When a network connection using NAT is created. When a packet is dropped because no port was available for NAT. 2.7 Create SSH Firewall rules default-allow-internal and default-allow-ssh \u00b6 Let's create SSH Firewall rule with name allow-tcp-ssh-icmp-$ORG-$PRODUCT-$ENV to allow SSH, ping, using google_compute_firewall resource. Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/firewall-rules https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_firewall cat <<EOF>> firewall.tf resource \"google_compute_firewall\" \"ssh-rule\" { name = format(\"allow-tcp-ssh-icmp-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link allow { protocol = \"tcp\" ports = [\"22\"] } allow { protocol = \"icmp\" } } EOF Review created firewall rules: gcloud compute firewall-rules list Also check in Google cloud UI: Networking->Firewalls 2.8 Create a Private GKE Cluster and delete default node pool \u00b6 2.8.1 Enable GCP Beta Provider \u00b6 In order to create a GKE cluster with terraform we will be leveraging google_container_cluster resource. Some of google_container_cluster arguments, such VPC-Native networking mode, VPA, Istio, CSI Driver add-ons, requires google-beta Provider. The google-beta provider is distinct from the google provider in that it supports GCP products and features that are in beta, while google does not. Fields and resources that are only present in google-beta will be marked as such in the shared provider documentation. Task N8: Configure and Initialize GCP Beta Provider , similar to how we did it for GCP Provider in 1.3.3 Initialize Terraform update provider.tf and main.tf configuration files. Solution: rm provider.tf cat <<EOF >> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } google-beta = { source = \"hashicorp/google-beta\" version = \"~> 3.70.0\" } } } EOF cat <<EOF >> main.tf provider \"google-beta\" { project = var.gcp_project_id region = var.gcp_region } EOF Initialize google-beta provider plugin: terraform init Success Terraform has been successfully initialized! 2.8.2 Enable Kubernetes Engine API \u00b6 Kubernetes Engine API used to build and manages container-based applications, powered by the open source Kubernetes technology. Before starting GKE cluster creation it is required to enable it. Task N9: Enable container.googleapis.com in services.tf file similar to what we already did in 2.2 Enable required GCP Services API . Solution: cat <<EOF >> services.tf resource \"google_project_service\" \"gke_api\" { service = \"container.googleapis.com\" disable_on_destroy = false } EOF Note disable_on_destroy=false helps to prevent errors during redeployments of the system. Review TF Plan: terraform plan -var-file terraform.tfvars Update Cloud Nat Configuration: terraform apply -var-file terraform.tfvars 2.8.3 Create a Private GKE Cluster and delete default node pool \u00b6 Using google_container_cluster resource create a Regional, Private GKE cluster, with following characteristics: Cluster Configuration: Cluster name: gke-$ORG-$PRODUCT-$ENV GKE Control plane is replicated across three zones of a region: us-central1 Private cluster with unrestricted access to the public endpoint: Cluster Nodes access: Private Node GKE Cluster with Public API endpoint Cluster K8s API access: with unrestricted access to the public endpoint Cluster Node Communication: VPC Native Secondary pod range with name: pods Secondary service range with name: services GKE master and node version: \"1.20.8-gke.700\" Terraform Provider: google-beta Timeouts: 30M Node Pool Configuration: VM size: e2-small Node count: 1 per zone Node images: Container-Optimized OS The name of a GCE machine (VM) type: e2-small Note Why delete default node pool? The default node pools cause trouble with managing the cluster, when created with terraform as it is not part of the terraform lifecycle. GKE Architecture Best Practice recommends to delete default node pool and create a custom one instead and manage the node pools explicitly. Note Why define Timeouts for gke resource? Normally GKE creation takes few minutes. However, in our case we creating GKE Cluster, and then system cordon, drain and then destroy default node pool. This process may take 10-20 minutes and we want to make sure terraform will not time out during this time. Step 1: Let's define GKE resource first: cat <<EOF >> gke.tf resource \"google_container_cluster\" \"primary_cluster\" { provider = google-beta project = var.gcp_project_id name = format(\"gke-%s-%s-%s\", var.org, var.product, var.environment) min_master_version = var.kubernetes_version network = google_compute_network.vpc_network.self_link subnetwork = google_compute_subnetwork.gke_private_subnet.self_link location = var.gcp_region logging_service = var.logging_service monitoring_service = var.monitoring_service remove_default_node_pool = true initial_node_count = 1 private_cluster_config { enable_private_nodes = var.enable_private_nodes enable_private_endpoint = var.enable_private_endpoint master_ipv4_cidr_block = var.master_ipv4_cidr_block } network_policy { enabled = var.network_policy provider = var.network_policy ? \"CALICO\" : \"PROVIDER_UNSPECIFIED\" } addons_config { http_load_balancing { disabled = var.disable_http_load_balancing } network_policy_config { disabled = var.network_policy ? false : true } } ip_allocation_policy { cluster_secondary_range_name = var.pods_range_name services_secondary_range_name = var.services_range_name } timeouts { create = \"30m\" update = \"30m\" delete = \"30m\" } workload_identity_config { identity_namespace = \"${var.gcp_project_id}.svc.id.goog\" } } EOF Step 2: Next define GKE cluster specific variables: cat <<EOF >> gke_variables.tf variable \"kubernetes_version\" { default = \"\" type = string description = \"The GKE version of Kubernetes\" } variable \"logging_service\" { description = \"The logging service that the cluster should write logs to.\" default = \"logging.googleapis.com/kubernetes\" } variable \"monitoring_service\" { default = \"monitoring.googleapis.com/kubernetes\" description = \"The GCP monitoring service scope\" } variable \"disable_http_load_balancing\" { default = false description = \"Enable HTTP Load balancing GCP integration\" } variable \"network_policy\" { description = \"Enable network policy addon\" default = true } variable \"pods_range_name\" { description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to pods\" default = \"\" } variable \"services_range_name\" { description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to services\" default = \"\" } variable \"enable_private_nodes\" { default = false description = \"Enable Private-IP Only GKE Nodes\" } variable \"enable_private_endpoint\" { default = false description = \"When true, the cluster's private endpoint is used as the cluster endpoint and access through the public endpoint is disabled.\" } variable \"master_ipv4_cidr_block\" { description = \"The ipv4 cidr block that the GKE masters use\" } EOF Step 3: Define GKE cluster specific outputs: cat <<EOF >> outputs.tf output \"id\" { value = \"${google_container_cluster.primary_cluster.id}\" } output \"endpoint\" { value = \"${google_container_cluster.primary_cluster.endpoint}\" } output \"master_version\" { value = \"${google_container_cluster.primary_cluster.master_version}\" } EOF Task N9: Complete terraform.tfvars with required values to GKE Cluster specified above: cat <<EOF >> terraform.tfvars //gke specific enable_private_nodes = \"TODO\" master_ipv4_cidr_block = \"TODO\" pods_range_name = \"TODO\" services_range_name = \"TODO\" kubernetes_version = \"TODO\" EOF Solution: //gke specific enable_private_nodes = \"true\" master_ipv4_cidr_block = \"172.16.0.0/28\" pods_range_name = \"pods\" services_range_name = \"services\" kubernetes_version = \"1.20.8-gke.700\" In the next step, we going to create a custom GKE Node Pool. 2.8.4 Create a GKE custom Node pool \u00b6 Using google_container_node_pool resource create a custom GKE Node Pool with following characteristics: Node Pool Configuration: VM size: e2-small Node count: 1 per zone Node images: Container-Optimized OS The name of a GCE machine (VM) type: e2-small Step 1: Let's define GKE resource first: cat <<EOF >> gke.tf #Node Pool Resource resource \"google_container_node_pool\" \"custom-node_pool\" { provider = google-beta name = \"main-pool\" location = var.gcp_region project = var.gcp_project_id cluster = google_container_cluster.primary_cluster.name node_count = var.gke_pool_node_count version = var.kubernetes_version node_config { image_type = var.gke_pool_image_type disk_size_gb = var.gke_pool_disk_size_gb disk_type = var.gke_pool_disk_type machine_type = var.gke_pool_machine_type } timeouts { create = \"10m\" delete = \"10m\" } lifecycle { ignore_changes = [ node_count ] } } EOF Step 2: Next define GKE cluster specific variables: cat <<EOF >> gke_variables.tf #Node Pool specific variables variable \"gke_pool_machine_type\" { type = string } variable \"gke_pool_node_count\" { type = number } variable \"gke_pool_disk_type\" { type = string default = \"pd-standard\" } variable \"gke_pool_disk_size_gb\" { type = string } variable \"gke_pool_image_type\" { type = string } EOF Task 9 (Continued): Complete terraform.tfvars with required values to GKE Node Pool values specified above: cat <<EOF >> terraform.tfvars #pool specific gke_pool_node_count = \"TODO\" gke_pool_image_type = \"TODO\" gke_pool_disk_size_gb = \"TODO\" gke_pool_machine_type = \"TODO\" EOF Solution: #pool specific gke_pool_node_count = \"1\" gke_pool_image_type = \"COS\" gke_pool_disk_size_gb = \"100\" gke_pool_machine_type = \"e2-small\" Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars Step 4: Create GKE Cluster and Node Pool: terraform apply -var-file terraform.tfvars Output: google_container_cluster.primary_cluster: Creating... ... google_container_cluster.primary_cluster: Creation complete after 20m9s google_container_node_pool.custom-node_pool: Creating... google_container_node_pool.custom-node_pool: Creation complete after 2m10s 2.8.5 Update GKE Node Pool to support Auto Upgrade and Auto Recovery features \u00b6 Note GKE Master Nodes are managed by Google and get's upgraded automatically. Users can only specify Maintenance Window if they have preference for that process to occur (e.g. after busy hours). Users can however control Node Pool upgrade lifecycle. The can choose to do it themselves or with Auto Upgrade. Task N10: Using google_container_node_pool resource update node pool to support Auto Upgrade and Auto Recovery features. edit gke.tf Solution: management { auto_repair = true auto_upgrade = true } Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars No errors. Step 4: Update GKE Cluster Node Pool configuration: terraform apply -var-file terraform.tfvars Summary Congrats! You've now learned how to deploy production grade GKE clusters. 2.9 (Optional) Repeatable Infrastructure \u00b6 When you doing IaC it is important to insure that you can both create and destroy resources consistently. This is especially important when doing CI/CD testing. Step 3: Destroy all resources: terraform destroy -var-file terraform.tfvars No errors. Step 4: Recreate all resources: terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars 3. Create Documentation for terraform code \u00b6 Documentation for your terraform code is an important part of IaC. Make sure all your variables have a good description! There are community tools that have been developed to make the documentation process smoother, in terms of documenting Terraform resources and requirements.Its good practice to also include a usage example snippet. Terraform-Docs is a good example of one tool that can generate some documentation based on the description argument of your Input Variables, Output Values, and from your required_providers configurations. Step 1 Install the terraform-docs cli to your Google CloudShell environment: curl -Lo ./terraform-docs.tar.gz https://github.com/terraform-docs/terraform-docs/releases/download/v0.14.1/terraform-docs-v0.14.1-$(uname)-amd64.tar.gz tar -xzf terraform-docs.tar.gz rm terraform-docs.tar.gz chmod +x terraform-docs sudo mv terraform-docs /usr/local/bin/ terraform-docs Generating terraform documentation with Terraform Docs: cd ~/$MY_REPO cd foundation-infrastructure terraform-docs markdown . > README.md cd ../notepad-infrastructure terraform-docs markdown . > README.md Verify created documentation: edit README.md 4. Workaround for Project Quota issue \u00b6 If you see following error during project creation in foundation layer: Error: Error setting billing account \"010BE6-CA1129-195D77\" for project \"projects/ayrat-notepad-dev-244\": googleapi: Error 400: Precondition check failed., failedPrecondition This is due to our Billing account has quota of 5 projects per account. To solve this issue find all unused accounts: gcloud beta billing projects list --billing-account $ACCOUNT_ID And unlink them, so you have less then 5 projects per account: gcloud beta billing projects unlink $PROJECT_ID 5 Commit Readme doc to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit docs folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Readme doc for Production GKE Creation using gcloud\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 6 Cleanup \u00b6 We only going to cleanup GCP Service foundation layer, as we going to use GCP project in future. cd ~/$MY_REPO/notepad-infrastructure terraform destroy -var-file terraform.tfvars","title":"Assignment3 Sol"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#working-with-git-and-iac","text":"Git-based workflows, merge requests and peer reviews create a level of documented transparency that is great for security teams and audits. They ensure every change is documented as well as the metadata surrounding the change, answering the why, who, when and what questions. In this set of exercises you will continue using notepad Google Source Repository that already contains you application code, dockerfiles and kubernetes manifests. We will use so called Monorepo approach, and store our terraform IAC configuration in the same repo as our application code and Kubernetes manifests.","title":"Working with Git and IaC"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#add-your-iac-code-to-your-repository","text":"Step 1 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 2 Create foundation-infrastructure and notepad-infrastructure folders that will contains respective terraform configurations cd ~/$MY_REPO mkdir foundation-infrastructure mkdir notepad-infrastructure Step 3 Create a README.md file describing foundation-infrastructure folder purpose: cat <<EOF> foundation-infrastructure/README.md # Creation of GCP Foundation Layer This step, is focused on creating GCP Foundation: folders (if any) and service projects creation with their respictive GCS bucket to store terraform state and IAM Config. EOF Step 4 Create a README.md file describing notepad-infrastructure folder purpose: cat <<EOF> notepad-infrastructure/README.md # Creation of GCP Services Layer This step, is focused on creating gcp services such as GKE, VPC and etc. inside of existing project EOF Step 5 Create a .gitignore file in your working directory: cat<<EOF>> .gitignore .terraform.* .terraform terraform.tfstate* EOF Note Ignore files are used to tell git not to track files. You should also always include any files with credentials in a gitignore file. List created folder structure along with gitignore : ls -ltra Step 6 Commit deploy folder using the following Git commands: git status git add . git commit -m \"Terraform Folder structure for assignement 3\" Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"Add your IaC code to your repository"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#1-build-gcp-foundation-layer","text":"In this Lab we going to build GCP Foundation layer. As we learned in the class this includes Org Structure creation with Folders and Projects, creation IAM Roles and assigning them to users and groups in organization. This Layer usually build be DevOps or Infra team that. Since we don't have organization registered for each student, we going to skip creation of folders and IAM groups. We going to start from creation of a Project, deleting default VPC and configuring inside that project a gcs bucket that will be used in the next terraform layer to store a Terraform state. Part 1: Create Terraform Configurations file to create's GCP Foundation Layer: Layer 1 From existing (we going to call it seeding) project that you currently use to store code in Google Cloud Source Repository: Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf Create a new notepad-dev Project Delete Default VPC Create a bucket in this project to store terraform state Part 2: Create Terraform Configurations file that create's GCP Services Layer: Layer 2 From notepad-dev GCP Project: Enable Google Project Service APIs Create VPC (google_compute_network) and Subnet (google_compute_subnetwork) Create Cloud Nat (google_compute_router) and (google_compute_router_nat) Private GKE Private Nodes with Public API Endpoint (for simplicity)","title":"1 Build GCP Foundation Layer"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#11-installing-terraform","text":"GCP Cloud Shell comes with many common tools pre-installed including terraform Verify and validate the version of Terraform that is installed: terraform --version If you want to use specific version of terraform or want to install terraform in you local machine use following link to download binary.","title":"1.1 Installing Terraform"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#12-configure-gcp-credentials-for-terraform","text":"If you would like to use terraform on GCP you have 2 options: 1). Using you user credentials, great option for testing terraform from you laptop and for learning purposes. 2). Using service account , great option if you going to use terraform with CI/CD system and fully automate Terraform deployment. In this Lab we going to use Option 1 - using user credentials. Step 1: In order to make requests against the GCP API, you need to authenticate to prove that it's you making the request. The preferred method of provisioning resources with Terraform on your workstation is to use the Google Cloud SDK (Option 1) gcloud auth application-default login","title":"1.2 Configure GCP credentials for Terraform"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#13-initializing-terraform","text":"Terraform relies on providers to interact with remote systems. Every resource type is implemented by a provider; without providers , Terraform can't manage any kind of infrastructure; in order for terraform to install and use a provider it must be declared. In this exercise you will declare and configure the Terraform provider(s) that will be used for the rest of the Lab.","title":"1.3 Initializing Terraform"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#131-declare-gcp-terraform-providers","text":"The required_providers block defines the providers terraform will use to provision resources and their source. version : The version argument is optional and is used to tell terraform to pick a particular version from the available versions source : The source is the provider registry e.g. hashicorp/gcp is the short for registry.terraform.io/hashicorp/gcp cd ~/$MY_REPO/foundation-infrastructure/ cat << EOF>> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } } } EOF","title":"1.3.1 Declare GCP Terraform Providers"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#132-configure-the-gcp-terraform-provider","text":"Providers often require configuration (like endpoint URLs or cloud regions) before they can be used. These configurations are defined by a provider block. Multiple provider configuration blocks can be declared for the same by adding the alias argument Set you current PROJECT_ID value here: export PROJECT_ID=<YOUR_PROJECT_ID> cd ~/$MY_REPO/foundation-infrastructure/ cat << EOF>> main.tf provider \"google\" { alias = \"gcp-provider\" project = \"$PROJECT_ID\" region = \"us-central1\" } EOF Result We configured provider , so that it can provision resource in specified gcp project in us-central1 region.","title":"1.3.2 Configure the GCP Terraform Provider"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#133-initialize-gcp-terraform-provider","text":"Now that you have declared and configured the GCP provider for terraform, initialize terraform: cd ~/$MY_REPO/foundation-infrastructure/ terraform init Explore your directory. What has changed? ls -ltra Result We can see that new directory .terraform and .terraform.lock.hcl file. Extra! Investigate available providers in the Terraform Provider Registry Select another provider from the list, add it to your required providers, and to your main.tf Run terraform init to load your new provider!","title":"1.3.3 Initialize GCP Terraform Provider"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#14-terraform-variables","text":"Input variables are used to increase your Terraform configuration's flexibility by defining values that can be assigned to customize the configuration. They provide a consistent interface to change how a given configuration behaves. Input variables blocks have a defined format: Input variables blocks have a defined format: variable \"variable_name\" { type = \"The variable type eg. string , list , number\" description = \"A description to understand what the variable will be used for\" default = \"A default value can be provided, terraform will use this if no other value is provided at terraform apply\" } Terraform CLI defines the following optional arguments for variable declarations: default - A default value which then makes the variable optional. type - This argument specifies what value types are accepted for the variable. description - This specifies the input variable's documentation. validation - A block to define validation rules, usually in addition to type constraints. sensitive - Limits Terraform UI output when the variable is used in configuration. In our above example for main.tf we can actually declare project and region as variables, so let's do it.","title":"1.4 Terraform Variables"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#141-declare-input-variables","text":"Variables are declared in a variables.tf file inside your terraform working directory. The label after the variable keyword is a name for the variable, which must be unique among all variables in the same module. You can choose variable name based on you preference, or in some cases based on agreed in company naming convention. In our case we declaring variables names: gcp_region and gcp_project_id We are going to declare a type as a string variable for gcp_project_id and gcp_region . It is always good idea to specify a clear description for the variable for documentation purpose as well as code reusability and readability. Finally, let's configured default argument. In our case we going to use \"us-central1\" as a default for gcp_region and we going to keep default value empty for gcp_project_id . cat <<EOF> variables.tf variable \"gcp_region\" { type = string description = \"The GCP Region\" default = \"us-central1\" } variable \"gcp_project_id\" { type = string description = \"The GCP Seeding project ID\" default = \"\" } EOF Note Explore other type of variables such as number , bool and type constructors such as: list( ), set( ), map( )","title":"1.4.1 Declare Input Variables"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#142-using-variables","text":"Now that you have created your input variables, let's re-create main.tf that currently has the terraform configuration for GCP Provider. rm main.tf cat << EOF>> main.tf provider \"google\" { alias = \"gcp-provider\" project = var.gcp_project_id region = var.gcp_region } EOF Test out your terraform configuration: terraform plan","title":"1.4.2 Using Variables"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#143-working-with-variables-files","text":"Create a terraform.tfvars file to hold the values for your variables: export gcp_project_id=<YOUR_PROJECT_ID> cat <<EOF >> terraform.tfvars gcp_project_id = \"$PROJECT_ID\" gcp_region = \"us-central1\" EOF Hint using different env-name.tfvars files you can create different set of terraform configuration for the same code. (e.g. code for dev, staging, prod)","title":"1.4.3 Working with Variables files."},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#144-validate-configuration-and-code-syntax","text":"Let's Validate configuration and code syntax that we've added so far. There are several tools that can analyze your Terraform code without running it, including: Terraform has build in command that you can use to check your Terraform syntax and types (a bit like a compiler): terraform validate Result Seems the code is legit so far Is your terraform easy to read and follow? Terraform has a built-in function to lint your configuration manifests for readability and best practice spacing: terraform fmt --recursive The --recursive flag asks the fmt command to traverse all of your terraform directories and format the .tf files it finds. It will report the files it changed as part of the return information of the command Hint Use the git diff command to see what was changed. Result We can see that terraform.tfvars file had some spacing that been fixed for better code readability. Extra Another cool tool that you can use along you terraform development is tflint - framework and each feature is provided by plugins, the key features are as follows: Find possible errors (like illegal instance types) for Major Cloud providers (AWS/Azure/GCP). Warn about deprecated syntax, unused declarations. Enforce best practices, naming conventions. Finally let's run terraform plan : terraform plan Output: No changes. Your infrastructure matches the configuration. Result We don't have any errors, however we don't have any resources created so far. Let's create a GCP project resource to start with!","title":"1.4.4 Validate configuration and code syntax"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#15-create-gcp-project-using-terraform","text":"","title":"1.5 Create GCP project using Terraform"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#151-configure-google_project-resource","text":"Resources describe the infrastructure objects you want terraform to create. A resource block can be used to create any object such as virtual private cloud, security groups, DNS records etc. Let's create our first Terraform resource: GCP project. In order to accomplish this we going to use google_project resource, documented here In order to create a new Project we need to define following arguments: * name - (Required) The display name of the project. * project_id - (Required) The project ID. Changing this forces a new project to be created. * billing_account - The alphanumeric ID of the billing account this project belongs to. First, Let's declare actual values for name , project_id and billing_account with variables: cat <<EOF >> variables.tf variable \"billing_account\" { description = \"The billing account ID for this project\" } variable \"project_name\" { description = \"The human readable project name (min 4 letters)\" } variable \"project_id\" { description = \"The GCP project ID\" } EOF Then, update terraform.tfvars variables files, with actual values for name , project_id and billing_account : We going to use same naming structure for PROJECT_ID and PROJECT_NAME as in previous Assignment, in order to define your new GCP project: export ORG=<student-name> export ORG=ayrat export PRODUCT=notepad export ENV=dev export PROJECT_PREFIX=4 export PROJECT_NAME=$ORG-$PRODUCT-$ENV export PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX # Project ID has to be unique echo $PROJECT_NAME echo $PROJECT_ID In order to get Billing account run following command: ACCOUNT_ID=$(gcloud alpha billing accounts list | grep Education | grep True |awk '{ print $1 }') echo $ACCOUNT_ID cat <<EOF >> terraform.tfvars billing_account = \"$ACCOUNT_ID\" project_name = \"$PROJECT_NAME\" project_id = \"$PROJECT_ID\" EOF Verify if everything looks good, and correct values has been set: cat terraform.tfvars Finally, let's define google_project resource in project.tf file, we will replace actual values for name and billing_account with variables: cat <<EOF >> project.tf resource \"google_project\" \"project\" { name = var.project_name billing_account = var.billing_account project_id = var.project_id } EOF Run plan command: terraform plan -var-file terraform.tfvars Result Plan: 1 to add, 0 to change, 0 to destroy. Note Take notice of plan command and see how some values that you declared in *.tvfars are visible and some values will known after apply terraform apply -var-file terraform.tfvars Success We've created our first resource with terrraform","title":"1.5.1 Configure google_project resource"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#152-making-project-resource-immutable","text":"Very often when you developing IaC, you need to destroy and recreate your resorces, e.g. for troubleshooting or creating a new resources using same config. Let's destroy our project and try to recreate it again. terraform destroy -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Failed What happened? Did you project been able to create? If not why ? If we want to make our infrastructure to be Immutable and fully automated, we need to make sure that we can destroy our service and recreate it any time the same way. In our case we can't do that because Project ID always has to be unique. To tackle this problem we need to randomize our Project ID creation within the terraform.","title":"1.5.2 Making Project resource Immutable"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#153-create-a-project-using-terraform-random-provider","text":"In order to create a GCP Project with Random name, we can use Terraform's random_integer resource with following arguments: max (Number) The maximum inclusive value of the range - 100 min (Number) The minimum inclusive value of the range - 999 cat <<EOF >> random.tf resource \"random_integer\" \"id\" { min = 100 max = 999 } EOF random_integer resource doesn't belongs to Google Provider, it requires Hashicorp Random Provider to be initialized. Since it's native hashicorp provider we can skip the step of defining and configuring that provider, as it will be automatically initialized. terraform init Result Installing hashicorp/random v3.1.0... Installed hashicorp/random v3.1.0 (signed by HashiCorp)","title":"1.5.3 Create a Project using Terraform Random Provider"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#153-create-a-project_id-name-using-local-values","text":"Local Values allow you to assign a name to an expression, so the expression can be used multiple times, without repeating it. We going to define value of project_id as local. And it will be a combination of project_name - random_number . Let's add local value for project_id and replace value var.project_id with local.project_id . edit project.tf Update code snippet of project.tf as following and save: locals { project_id = \"${var.project_name}-${random_integer.id.result}\" } resource \"google_project\" \"project\" { name = var.project_name billing_account = var.billing_account project_id = local.project_id } We can now remove project_id value from terraform.tfvars , as we going to randomly generate it using locals expression with random_integer edit terraform.tfvars Remove project_id line and save. We can now also remove project_id variable from variables.tf : rm variables.tf cat <<EOF >> variables.tf variable \"gcp_region\" { type = string description = \"The GCP Region\" default = \"us-central1\" } variable \"gcp_project_id\" { type = string description = \"The GCP Seeding project ID\" default = \"\" } variable \"billing_account\" { description = \"The billing account ID for this project\" } variable \"project_name\" { description = \"The human readable project name (min 4 letters)\" } EOF terraform plan -var-file terraform.tfvars Output: # random_integer.id will be created + resource \"random_integer\" \"id\" { + id = (known after apply) + max = 999 + min = 100 + result = (known after apply) } terraform apply -var-file terraform.tfvars Result Project has been created with Random project_id","title":"1.5.3 Create a Project_ID name using Local Values"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#154-configure-terraform-output-for-gcp-project","text":"Outputs provide return values of a Terraform state at any given time. So you can get for example values of what GCP resources like project, VMs, GKE cluster been created and their parametres. Outputs can be used used to export structured data about resources. This data can be used to configure other parts of your infrastructure, or as a data source for another Terraform workspace. Outputs are also necessary to share data from a child module to your root module. Outputs follow a similar structure to variables: output \"output_name\" { description = \"A description to understand what information is provided by the output\" value = \"An expression and/or resource_name.attribute\" sensitive = \"Optional argument, marking an output sensitive will supress the value from plan/apply phases\" } Let's declare GCP Project output values for project_id and project_number . You can find available output in each respective resource documents, under ` Attributes Reference . For example for GCP project available outputs are: cat <<EOF >> outputs.tf output \"id\" { value = google_project.project.project_id description = \"GCP project ID\" } output \"number\" { value = google_project.project.number description = \"GCP project number\" sensitive = true } EOF terraform plan -var-file terraform.tfvars We can see that we have new changes to output: Output: Changes to Outputs: + id = \"ayrat-notepad-dev-631\" + number = (sensitive value) Let's apply this changes: terraform apply -var-file terraform.tfvars Output: Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Let's now run terraform output command: terraform output Result We can see value of project id , however we don't see project number as it's been marked as sensitive. Note Outputs can be useful when you want to provide results of terraform resource creation to CI/CD or next automation tool like helm to deploy application.","title":"1.5.4 Configure Terraform Output for GCP Project"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#155-recreate-gcp-project-without-default-vpc","text":"One of the requirements for our solution is to create GCP project with custom VPC. However, when terraform creates GCP Project it creates DEFAULT vpc by default. gcloud compute networks list --project ayrat-notepad-dev-631 Output: NAME SUBNET_MODE BGP_ROUTING_MODE IPV4_RANGE GATEWAY_IPV4 default AUTO REGIONAL In order to remove automatically created default VPC, specify special attribute during google_project resource creation. Check documentation here and find this argument. Task N1: Find Attribute to remove default VPC during google_project resource creation. Define in it project.tf and set it's value in variables.tf as true by default. Solution: Update code snippet of project.tf as following and save: locals { project_id = \"${var.project_name}-${random_integer.id.result}\" } resource \"google_project\" \"project\" { name = var.project_name billing_account = var.billing_account project_id = local.project_id auto_create_network = var.auto_create_network } cat <<EOF >> variables.tf variable \"auto_create_network\" { description = \"Automatically provision a default global VPC network\" default = false } EOF After changes you need to re-create a project, as vpc get's deleted during project creation only. terraform destroy -var-file terraform.tfvars terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Notice How long project is now getting created. This is due to after project creation, default vpc is being removed. Verify that default VPC has been deleted: gcloud compute networks list --project ayrat-notepad-dev-631 Output: Listed 0 items. Success We now able to create a GCP project without Default VPC.","title":"1.5.5 Recreate GCP Project without Default VPC"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#16-create-gcp-storage-bucket","text":"","title":"1.6 Create GCP Storage bucket"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#161-create-gcp-storage-bucket-in-new-gcp-project","text":"Using reference doc for google_storage_bucket , let's create google_storage_bucket resource that will create MULTI_REGIONAL GCS bucket in newly created project. We going to give bucket name : $ORG-notepad-dev-tfstate , and we going to use this bucket to store Terraform state for GCP Service Layer. cat <<EOF >> bucket.tf resource \"google_storage_bucket\" \"state\" { name = var.bucket_name project = local.project_id storage_class = var.storage_class force_destroy = \"true\" } EOF cat <<EOF >> variables.tf variable \"bucket_name\" { description = \"The name of the bucket.\" } variable \"storage_class\" { description = default = \"MULTI_REGIONAL\" } EOF Set variable that will be used to build name for a bucket: export ORG=<student-name> cat <<EOF >> terraform.tfvars bucket_name = \"$ORG-notepad-dev-tfstate\" EOF cat <<EOF >> outputs.tf output \"bucket_name\" { value = google_storage_bucket.state.name } EOF Let's review the plan: terraform plan -var-file terraform.tfvars And create google_storage_bucket resource: terraform apply -var-file terraform.tfvars Verify created bucket in GCP UI: Storage -> Cloud Storage gsutil ls -L -b gs://$ORG-notepad-dev-tfstate Result GCS bucket for terraform state has been created","title":"1.6.1 Create GCP Storage bucket in New GCP Project"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#162-configure-versioning-on-gcp-storage-bucket","text":"Check if versioning enabled on the created bucket: gsutil versioning get gs://$ORG-notepad-dev-tfstate Result Versioning: Suspended It is highly recommended that if you going to use GCS bucket as Terraform storage backend you should enable Object Versioning on the GCS bucket to allow for state recovery in the case of accidental deletions and human error. Task N2: Using reference doc for google_storage_bucket find and configure argument that enables gcs bucket versioning feature. Edit bucket.tf with correct argument. edit bucket.tf Solution: resource \"google_storage_bucket\" \"state\" { name = var.bucket_name project = local.project_id storage_class = var.storage_class versioning { enabled = false } force_destroy = \"true\" } terraform plan -var-file terraform.tfvars Output: Plan: 0 to add, 1 to change, 0 to destroy. Result Configuration for Versioning looks correct terraform apply -var-file terraform.tfvars Verify versioning is ON: gsutil versioning get gs://$ORG-notepad-dev-tfstate Output: Versioning Enabled Result We've finished building the Foundation Layer. So far we able to accomplish following: Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf Create a new notepad-dev Project Delete Default VPC Create a bucket in this project to store terraform state","title":"1.6.2 Configure versioning on GCP Storage bucket."},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#2-build-gcp-services-layer","text":"Once we finished building a GCP Foundation layer which is essentially our project, we can start building GCP Services Layer inside that project. This second layer will configure following items: Enable Google Project Service APIs Create VPC (google_compute_network) and Subnet (google_compute_subnetwork) Create Cloud Nat (google_compute_router) and (google_compute_router_nat) Private GKE Private Nodes with Public API Endpoint (for simplicity)","title":"2 Build GCP Services Layer"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#21-initialize-terraform-for-gcp-services-layer","text":"","title":"2.1  Initialize Terraform for GCP Services Layer"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#211-define-and-configure-terraform-provider","text":"Step 1: Take note of newly created GCP Project_ID and BUCKET_ID: cd ~/$MY_REPO/foundation-infrastructure terraform output | grep 'id' |awk '{ print $3}' terraform output | grep 'bucket_name' |awk '{ print $3}' Set it as variable: export PROJECT_ID=$(terraform output | grep 'id' |awk '{ print $3}') export BUCKET_ID=$(terraform output | grep 'bucket_name' |awk '{ print $3}') echo $PROJECT_ID echo $BUCKET_ID Step 2: Declare the Terraform Provider for GCP Services Layer: We now going to switch to notepad-infrastructure where we going to create a new GCP service Layer terraform configuration: cd ~/$MY_REPO/notepad-infrastructure cat << EOF>> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } } } EOF Step 4: Configure the Terraform Provider cat << EOF>> main.tf provider \"google\" { project = var.gcp_project_id region = var.gcp_region } EOF Step 5: Define variables: cat <<EOF> variables.tf variable \"gcp_region\" { type = string description = \"The GCP Region\" default = \"us-central1\" } variable \"gcp_project_id\" { type = string description = \"The newly created GCP project ID\" } EOF Step 5: Set variables in terraform.tfvars cat <<EOF >> terraform.tfvars gcp_project_id = $PROJECT_ID EOF Step 4: Now that you have declared and configured the GCP provider for terraform, initialize terraform: terraform init","title":"2.1.1 Define and configure terraform provider"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#212-configure-terraform-state-backend","text":"Terraform records information about what infrastructure it created in a Terraform state file. This information, or state, is stored by default in a local file named terraform.tfstate. This allows Terraform to compare what's in your configurations with what's in the state file, and determine what changes need to be applied. Local vs Remote backend: Local state files are the default for terraform. Remote state allows for collaboration between members of your team, for example multiple users or systems can deploy terraform configuration to the same environment as state is stored remotely and can be pulled localy during the execution. This however may create issues if 2 users running terraform plan and apply at the same time, however some remote backends including gcs provide locking feature that we covered and enabled in previous section Configure versioning on GCP Storage bucket. In addition to that, Remote state is more secure storage of sensitive values that might be contained in variables or outputs. See documents for remote backend for reference: Step 1 Configure remote backend using gcs bucket: Let's configure a backend for your state, using the gcs bucket you previously created in Foundation Layer. Backends are configured with a nested backend block within the top-level terraform block While a backend can be declared anywhere, it is recommended to use a backend.tf . Since we running on GCP we going to use GCS remote backend . It stores the state as an object in a configurable prefix in a pre-existing bucket on Google Cloud Storage (GCS). This backend also supports state locking. The bucket must exist prior to configuring the backend. We going to use following arguments: bucket - (Required) The name of the GCS bucket. This name must be globally unique. For more information, see Bucket Naming Guidelines. prefix - (Optional) GCS prefix inside the bucket. Named states for workspaces are stored in an object called / .tfstate. cat <<EOF >> backend.tf terraform { backend \"gcs\" { bucket = $BUCKET_ID prefix = \"state\" } } EOF Step 2 When you change, configure or unconfigure a backend, terraform must be re-initialized: terraform init Verify if folder state has been created in our bucket: gsutil ls gs://$ORG-notepad-dev-tfstate Summary state Folder has been created in gcs bucket and terrafrom has been initialized with remote backend","title":"2.1.2 Configure Terraform State backend"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#22-enable-required-gcp-services-api","text":"Before we continue with creating GCP services like VPC, routers, Cloud Nat and GKE it is required to enable underlining GCP API services. When we creating a new project most of the services API's are disabled, and requires explicitly to be enabled. google_project_service resource allows management of a single API service for an existing Google Cloud Platform project. Let's enable compute engine API service with terraform: cat <<EOF >> services.tf resource \"google_project_service\" \"compute\" { service = \"compute.googleapis.com\" disable_on_destroy = false } EOF terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Verify Compute Engine API service has been enabled: gcloud services list Result Compute Engine API is enabled as it is listed.","title":"2.2 Enable required GCP Services API"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#22-set-variables-to-define-standard-for-naming-convention","text":"As per terraform best practices, when you creating terraform resources, you need to follow naming convention, that is clear for you organization. Configuration objects should be named using underscores to delimit multiple words. Object's name should be named using dashes This practice ensures consistency with the naming convention for resource types, data source types, and other predefined values and helps prevent accidental deletion or outages: Example: # Good resource \"google_compute_instance\" \"web_server\" { name = \u201cweb-server-$org-$app-$env\u201d # ... } # Bad resource \u201cgoogle_compute_instance\u201d \u201cweb-server\u201d { name = \u201cweb-server\u201d # \u2026 Create variables to define standard for Naming Convention: cat <<EOF >> variables.tf variable \"org\" { type = string } variable \"product\" { type = string } variable \"environment\" { type = string } EOF export ORG=ayrat export PRODUCT=notepad export ENV=dev cat <<EOF >> terraform.tfvars org = \"$ORG\" product = \"$PRODUCT\" environment = \"$ENV\" EOF Review if created files are correct.","title":"2.2 Set variables to define standard for Naming Convention"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#23-create-a-custom-mode-network-vpc-with-terraform","text":"Using google_compute_network resource create a VPC network with terraform. Task N3: Create vpc.tf file and define custom mode network (VPC) with following requirements: * Description: VPC that will be used by the GKE private cluster on the related project * name - vpc name with following pattern: \"vpc-$ORG-$PRODUCT-$ENV\" * Subnet mode: custom * Bgp routing mode: regional * MTUs: default Hint To create \"vpc-$ORG-$PRODUCT-$ENV\" name, use format function , and use %s to convert variables to string values. Define variables in variables.tf and terraform.tfvars if required. Define Output for: Generated VPC name: google_compute_network.vpc_network.name self_link - The URI of the created resource. Solution: cat <<EOF >> vpc.tf resource \"google_compute_network\" \"vpc_network\" { name = format(\"vpc-%s-%s-%s\", var.org, var.product, var.environment) routing_mode = \"REGIONAL\" description = \"VPC that will be used by the GKE private cluster on the related project\" auto_create_subnetworks = false } EOF cat <<EOF >> outputs.tf output \"vpc_name\" { value = google_compute_network.vpc_network.name } output \"vpc_selflink\" { value = \"\\${google_compute_network.vpc_network.self_link}\" } EOF Review TF Plan: terraform plan -var-file terraform.tfvars Output: + create Terraform will perform the following actions: # google_compute_network.vpc_network will be created + resource \"google_compute_network\" \"vpc_network\" { + auto_create_subnetworks = false + delete_default_routes_on_create = false + description = \"VPC that will be used by the GKE private cluster on the related project\" + gateway_ipv4 = (known after apply) + id = (known after apply) + mtu = (known after apply) + name = \"vpc-ayrat-notepad-dev\" + project = (known after apply) + routing_mode = \"REGIONAL\" + self_link = (known after apply) Create VPC: terraform apply -var-file terraform.tfvars Verify that custom-mode VPC has been created: gcloud compute networks list gcloud compute networks describe $(gcloud compute networks list | grep CUSTOM |awk '{ print $1 }') Output: autoCreateSubnetworks: false description: VPC that will be used by the GKE private cluster on the related project kind: compute#network name: vpc-student_name-notepad-dev routingConfig: routingMode: REGIONAL selfLink: https://www.googleapis.com/compute/v1/projects/XXX x_gcloud_bgp_routing_mode: REGIONAL x_gcloud_subnet_mode: CUSTOM Result VPC Network has been created, without auto subnets.","title":"2.3 Create a custom mode network (VPC) with terraform"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#24-create-a-user-managed-subnet-with-terraform","text":"Using google_compute_subnetwork resource create a user-managed subnet with terraform. cat <<EOF >> subnet.tf resource \"google_compute_subnetwork\" \"gke_private_subnet\" { name = format(\"subnet-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link region = var.gcp_region project = var.gcp_project_id ip_cidr_range = var.network_cidr secondary_ip_range { range_name = var.pods_cidr_name ip_cidr_range = var.pods_cidr } secondary_ip_range { range_name = var.services_cidr_name ip_cidr_range = var.services_cidr } } EOF Note Notice power of terraform outputs. Here we link subnet with our VPC network using google_compute_network.vpc_network.self_link output value of created network in previous step. Define variables: cat <<EOF >> variables.tf # variables used to create VPC variable \"network_cidr\" { type = string } variable \"pods_cidr\" { type = string } variable \"pods_cidr_name\" { type = string } variable \"services_cidr\" { type = string } variable \"services_cidr_name\" { type = string } EOF Define outputs: cat <<EOF >> outputs.tf output \"subnet_selflink\" { value = \"\\${google_compute_subnetwork.gke_private_subnet.self_link}\" } EOF Task N4: Update terraform.tfvars file values with following information: Node Range: See column subnet in above table for dev cluster Secondary service range with name: services Secondary Ranges: Service range name: services Service range CIDR: See column srv range in above table for dev cluster Pods range name: pods Pods range CIDR: See column pod range in above table for dev cluster env | subnet | pod range | srv range | kubectl api range dev | 10.128.1.0/26 | 172.0.0.0/18 | 172.10.0.0/21 | 172.16.0.0/28 stg | 10.128.2.0/26 | 172.1.0.0/18 | 172.11.0.0/21 | 172.16.0.16/28 prd | 10.128.3.0/26 | 172.2.0.0/18 | 172.12.0.0/21 | 172.16.0.32/28 Note Ranges must be with in Private (RFC1918) Address Space Solution: cat <<EOF >> terraform.tfvars #subnet vars network_cidr = \"10.128.1.0/26\" pods_cidr = \"172.0.0.0/18\" pods_cidr_name = \"pods\" services_cidr = \"172.10.0.0/21\" services_cidr_name = \"services\" EOF Review TF Plan: terraform plan -var-file terraform.tfvars Create VPC: terraform apply -var-file terraform.tfvars Review created subnet: gcloud compute networks subnets list gcloud compute networks subnets describe $(gcloud compute networks subnets list | grep us-central1 |awk '{ print $1 }') --region us-central1 Output: enableFlowLogs: false gatewayAddress: 10.128.1.1 ipCidrRange: 10.128.1.0/26 kind: compute#subnetwork logConfig: enable: false name: subnet-student_name-notepad-dev privateIpGoogleAccess: false privateIpv6GoogleAccess: DISABLE_GOOGLE_ACCESS purpose: PRIVATE secondaryIpRanges: - ipCidrRange: 172.0.0.0/18 rangeName: pods - ipCidrRange: 172.10.0.0/21 rangeName: services stackType: IPV4_ONLY Also check in Google cloud UI: Networking->VPC Networks -> Click VPC network and check `Subnet` tab Task N5: Update subnet.tf so that google_compute_subnetwork resource supports following features: * Flow Logs * Aggregation interval: 15 min * Flow sampling: 0.1 * Metadata: \"INCLUDE_ALL_METADATA\" * Private IP Google Access Solution: private_ip_google_access = true log_config { aggregation_interval = \"INTERVAL_15_MIN\" flow_sampling = 0.1 metadata = \"INCLUDE_ALL_METADATA\" } Review TF Plan: terraform plan -var-file terraform.tfvars Create VPC: terraform apply -var-file terraform.tfvars","title":"2.4 Create a user-managed subnet with terraform"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#25-create-a-cloud-router","text":"Create Cloud Router for custom mode network (VPC), in the same region as the instances that will use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway. Task N6: Define a google_compute_router inside router.tf that will be able to create a NAT router so the nodes can reach DockerHub and external APIs from private cluster, using following parameters: Create router for custom vpc_network created above with terraform Same project as VPC Same region as VPC Router name: gke-net-router Local BGP Autonomous System Number (ASN): 64514 Hint You can automatically recover vpc name from terraform output like this: google_compute_network.vpc_network.self_link . Solution: cat <<EOF >> router.tf resource \"google_compute_router\" \"gke_net_router\" { project = var.gcp_project_id name = \"gke-net-router\" region = var.gcp_region network = google_compute_network.vpc_network.self_link bgp { asn = 64514 } } EOF Review TF Plan: terraform plan -var-file terraform.tfvars Create Cloud Router: terraform apply -var-file terraform.tfvars Verify created Cloud Router: CLI: gcloud compute routers list gcloud compute routers describe $(gcloud compute routers list | grep gke-net-router |awk '{ print $1 }') --region us-central1 Output: bgp: advertiseMode: DEFAULT asn: 64514 keepaliveInterval: 20 kind: compute#router name: gke-net-router UI: Networking -> Hybrid Connectivity -> Cloud Routers Result Router resource has been created for VPC Network","title":"2.5 Create a Cloud router"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#26-create-a-cloud-nat","text":"Set up a simple Cloud Nat configuration using google_compute_router_nat resource, which will automatically allocates the necessary external IP addresses to provide NAT services to a region. When you use auto-allocation, Google Cloud reserves IP addresses in your project automatically. cat <<EOF >> cloudnat.tf resource \"google_compute_router_nat\" \"gke_cloud_nat\" { project = var.gcp_project_id name = \"gke-cloud-nat\" router = google_compute_router.gke_net_router.name region = var.gcp_region nat_ip_allocate_option = \"AUTO_ONLY\" source_subnetwork_ip_ranges_to_nat = \"ALL_SUBNETWORKS_ALL_IP_RANGES\" } EOF Review TF Plan: terraform plan -var-file terraform.tfvars Create Cloud Router: terraform apply -var-file terraform.tfvars Verify created Cloud Nat : CLI: # List available Cloud Nat Routers gcloud compute routers nats list --router gke-net-router --router-region us-central1 # Describe Cloud Nat Routers `gke-cloud-nat`: gcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1 Output: enableEndpointIndependentMapping: true icmpIdleTimeoutSec: 30 name: gke-cloud-nat natIpAllocateOption: AUTO_ONLY sourceSubnetworkIpRangesToNat: ALL_SUBNETWORKS_ALL_IP_RANGES tcpEstablishedIdleTimeoutSec: 1200 tcpTransitoryIdleTimeoutSec: 30 udpIdleTimeoutSec: 30 UI: Networking -> Network Services -> Cloud NAT Result A NAT service created in a router Task N7: Additionally turn ON logging feature for ALL log types of communication for Cloud Nat edit cloudnat.tf Solution: Correctly add following configuration: log_config { filter = \"ALL\" enable = true } Review TF Plan: terraform plan -var-file terraform.tfvars Update Cloud Nat Configuration: terraform apply -var-file terraform.tfvars Output: Apply complete! Resources: 0 added, 1 changed, 0 destroyed. gcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1 Result Cloud Nat now supports Logging. Cloud NAT logging allows you to log NAT connections and errors. When Cloud NAT logging is enabled, one log entry can be generated for each of the following scenarios: When a network connection using NAT is created. When a packet is dropped because no port was available for NAT.","title":"2.6 Create a Cloud Nat"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#27-create-ssh-firewall-rules-default-allow-internal-and-default-allow-ssh","text":"Let's create SSH Firewall rule with name allow-tcp-ssh-icmp-$ORG-$PRODUCT-$ENV to allow SSH, ping, using google_compute_firewall resource. Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/firewall-rules https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_firewall cat <<EOF>> firewall.tf resource \"google_compute_firewall\" \"ssh-rule\" { name = format(\"allow-tcp-ssh-icmp-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link allow { protocol = \"tcp\" ports = [\"22\"] } allow { protocol = \"icmp\" } } EOF Review created firewall rules: gcloud compute firewall-rules list Also check in Google cloud UI: Networking->Firewalls","title":"2.7 Create SSH Firewall rules default-allow-internal and default-allow-ssh"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#28-create-a-private-gke-cluster-and-delete-default-node-pool","text":"","title":"2.8 Create a Private GKE Cluster and delete default node pool"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#281-enable-gcp-beta-provider","text":"In order to create a GKE cluster with terraform we will be leveraging google_container_cluster resource. Some of google_container_cluster arguments, such VPC-Native networking mode, VPA, Istio, CSI Driver add-ons, requires google-beta Provider. The google-beta provider is distinct from the google provider in that it supports GCP products and features that are in beta, while google does not. Fields and resources that are only present in google-beta will be marked as such in the shared provider documentation. Task N8: Configure and Initialize GCP Beta Provider , similar to how we did it for GCP Provider in 1.3.3 Initialize Terraform update provider.tf and main.tf configuration files. Solution: rm provider.tf cat <<EOF >> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } google-beta = { source = \"hashicorp/google-beta\" version = \"~> 3.70.0\" } } } EOF cat <<EOF >> main.tf provider \"google-beta\" { project = var.gcp_project_id region = var.gcp_region } EOF Initialize google-beta provider plugin: terraform init Success Terraform has been successfully initialized!","title":"2.8.1 Enable GCP Beta Provider"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#282-enable-kubernetes-engine-api","text":"Kubernetes Engine API used to build and manages container-based applications, powered by the open source Kubernetes technology. Before starting GKE cluster creation it is required to enable it. Task N9: Enable container.googleapis.com in services.tf file similar to what we already did in 2.2 Enable required GCP Services API . Solution: cat <<EOF >> services.tf resource \"google_project_service\" \"gke_api\" { service = \"container.googleapis.com\" disable_on_destroy = false } EOF Note disable_on_destroy=false helps to prevent errors during redeployments of the system. Review TF Plan: terraform plan -var-file terraform.tfvars Update Cloud Nat Configuration: terraform apply -var-file terraform.tfvars","title":"2.8.2 Enable Kubernetes Engine API"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#283-create-a-private-gke-cluster-and-delete-default-node-pool","text":"Using google_container_cluster resource create a Regional, Private GKE cluster, with following characteristics: Cluster Configuration: Cluster name: gke-$ORG-$PRODUCT-$ENV GKE Control plane is replicated across three zones of a region: us-central1 Private cluster with unrestricted access to the public endpoint: Cluster Nodes access: Private Node GKE Cluster with Public API endpoint Cluster K8s API access: with unrestricted access to the public endpoint Cluster Node Communication: VPC Native Secondary pod range with name: pods Secondary service range with name: services GKE master and node version: \"1.20.8-gke.700\" Terraform Provider: google-beta Timeouts: 30M Node Pool Configuration: VM size: e2-small Node count: 1 per zone Node images: Container-Optimized OS The name of a GCE machine (VM) type: e2-small Note Why delete default node pool? The default node pools cause trouble with managing the cluster, when created with terraform as it is not part of the terraform lifecycle. GKE Architecture Best Practice recommends to delete default node pool and create a custom one instead and manage the node pools explicitly. Note Why define Timeouts for gke resource? Normally GKE creation takes few minutes. However, in our case we creating GKE Cluster, and then system cordon, drain and then destroy default node pool. This process may take 10-20 minutes and we want to make sure terraform will not time out during this time. Step 1: Let's define GKE resource first: cat <<EOF >> gke.tf resource \"google_container_cluster\" \"primary_cluster\" { provider = google-beta project = var.gcp_project_id name = format(\"gke-%s-%s-%s\", var.org, var.product, var.environment) min_master_version = var.kubernetes_version network = google_compute_network.vpc_network.self_link subnetwork = google_compute_subnetwork.gke_private_subnet.self_link location = var.gcp_region logging_service = var.logging_service monitoring_service = var.monitoring_service remove_default_node_pool = true initial_node_count = 1 private_cluster_config { enable_private_nodes = var.enable_private_nodes enable_private_endpoint = var.enable_private_endpoint master_ipv4_cidr_block = var.master_ipv4_cidr_block } network_policy { enabled = var.network_policy provider = var.network_policy ? \"CALICO\" : \"PROVIDER_UNSPECIFIED\" } addons_config { http_load_balancing { disabled = var.disable_http_load_balancing } network_policy_config { disabled = var.network_policy ? false : true } } ip_allocation_policy { cluster_secondary_range_name = var.pods_range_name services_secondary_range_name = var.services_range_name } timeouts { create = \"30m\" update = \"30m\" delete = \"30m\" } workload_identity_config { identity_namespace = \"${var.gcp_project_id}.svc.id.goog\" } } EOF Step 2: Next define GKE cluster specific variables: cat <<EOF >> gke_variables.tf variable \"kubernetes_version\" { default = \"\" type = string description = \"The GKE version of Kubernetes\" } variable \"logging_service\" { description = \"The logging service that the cluster should write logs to.\" default = \"logging.googleapis.com/kubernetes\" } variable \"monitoring_service\" { default = \"monitoring.googleapis.com/kubernetes\" description = \"The GCP monitoring service scope\" } variable \"disable_http_load_balancing\" { default = false description = \"Enable HTTP Load balancing GCP integration\" } variable \"network_policy\" { description = \"Enable network policy addon\" default = true } variable \"pods_range_name\" { description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to pods\" default = \"\" } variable \"services_range_name\" { description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to services\" default = \"\" } variable \"enable_private_nodes\" { default = false description = \"Enable Private-IP Only GKE Nodes\" } variable \"enable_private_endpoint\" { default = false description = \"When true, the cluster's private endpoint is used as the cluster endpoint and access through the public endpoint is disabled.\" } variable \"master_ipv4_cidr_block\" { description = \"The ipv4 cidr block that the GKE masters use\" } EOF Step 3: Define GKE cluster specific outputs: cat <<EOF >> outputs.tf output \"id\" { value = \"${google_container_cluster.primary_cluster.id}\" } output \"endpoint\" { value = \"${google_container_cluster.primary_cluster.endpoint}\" } output \"master_version\" { value = \"${google_container_cluster.primary_cluster.master_version}\" } EOF Task N9: Complete terraform.tfvars with required values to GKE Cluster specified above: cat <<EOF >> terraform.tfvars //gke specific enable_private_nodes = \"TODO\" master_ipv4_cidr_block = \"TODO\" pods_range_name = \"TODO\" services_range_name = \"TODO\" kubernetes_version = \"TODO\" EOF Solution: //gke specific enable_private_nodes = \"true\" master_ipv4_cidr_block = \"172.16.0.0/28\" pods_range_name = \"pods\" services_range_name = \"services\" kubernetes_version = \"1.20.8-gke.700\" In the next step, we going to create a custom GKE Node Pool.","title":"2.8.3 Create a Private GKE Cluster and delete default node pool"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#284-create-a-gke-custom-node-pool","text":"Using google_container_node_pool resource create a custom GKE Node Pool with following characteristics: Node Pool Configuration: VM size: e2-small Node count: 1 per zone Node images: Container-Optimized OS The name of a GCE machine (VM) type: e2-small Step 1: Let's define GKE resource first: cat <<EOF >> gke.tf #Node Pool Resource resource \"google_container_node_pool\" \"custom-node_pool\" { provider = google-beta name = \"main-pool\" location = var.gcp_region project = var.gcp_project_id cluster = google_container_cluster.primary_cluster.name node_count = var.gke_pool_node_count version = var.kubernetes_version node_config { image_type = var.gke_pool_image_type disk_size_gb = var.gke_pool_disk_size_gb disk_type = var.gke_pool_disk_type machine_type = var.gke_pool_machine_type } timeouts { create = \"10m\" delete = \"10m\" } lifecycle { ignore_changes = [ node_count ] } } EOF Step 2: Next define GKE cluster specific variables: cat <<EOF >> gke_variables.tf #Node Pool specific variables variable \"gke_pool_machine_type\" { type = string } variable \"gke_pool_node_count\" { type = number } variable \"gke_pool_disk_type\" { type = string default = \"pd-standard\" } variable \"gke_pool_disk_size_gb\" { type = string } variable \"gke_pool_image_type\" { type = string } EOF Task 9 (Continued): Complete terraform.tfvars with required values to GKE Node Pool values specified above: cat <<EOF >> terraform.tfvars #pool specific gke_pool_node_count = \"TODO\" gke_pool_image_type = \"TODO\" gke_pool_disk_size_gb = \"TODO\" gke_pool_machine_type = \"TODO\" EOF Solution: #pool specific gke_pool_node_count = \"1\" gke_pool_image_type = \"COS\" gke_pool_disk_size_gb = \"100\" gke_pool_machine_type = \"e2-small\" Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars Step 4: Create GKE Cluster and Node Pool: terraform apply -var-file terraform.tfvars Output: google_container_cluster.primary_cluster: Creating... ... google_container_cluster.primary_cluster: Creation complete after 20m9s google_container_node_pool.custom-node_pool: Creating... google_container_node_pool.custom-node_pool: Creation complete after 2m10s","title":"2.8.4 Create a GKE custom Node pool"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#285-update-gke-node-pool-to-support-auto-upgrade-and-auto-recovery-features","text":"Note GKE Master Nodes are managed by Google and get's upgraded automatically. Users can only specify Maintenance Window if they have preference for that process to occur (e.g. after busy hours). Users can however control Node Pool upgrade lifecycle. The can choose to do it themselves or with Auto Upgrade. Task N10: Using google_container_node_pool resource update node pool to support Auto Upgrade and Auto Recovery features. edit gke.tf Solution: management { auto_repair = true auto_upgrade = true } Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars No errors. Step 4: Update GKE Cluster Node Pool configuration: terraform apply -var-file terraform.tfvars Summary Congrats! You've now learned how to deploy production grade GKE clusters.","title":"2.8.5 Update GKE Node Pool to support Auto Upgrade and Auto Recovery features"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#29-optional-repeatable-infrastructure","text":"When you doing IaC it is important to insure that you can both create and destroy resources consistently. This is especially important when doing CI/CD testing. Step 3: Destroy all resources: terraform destroy -var-file terraform.tfvars No errors. Step 4: Recreate all resources: terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars","title":"2.9 (Optional) Repeatable Infrastructure"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#3-create-documentation-for-terraform-code","text":"Documentation for your terraform code is an important part of IaC. Make sure all your variables have a good description! There are community tools that have been developed to make the documentation process smoother, in terms of documenting Terraform resources and requirements.Its good practice to also include a usage example snippet. Terraform-Docs is a good example of one tool that can generate some documentation based on the description argument of your Input Variables, Output Values, and from your required_providers configurations. Step 1 Install the terraform-docs cli to your Google CloudShell environment: curl -Lo ./terraform-docs.tar.gz https://github.com/terraform-docs/terraform-docs/releases/download/v0.14.1/terraform-docs-v0.14.1-$(uname)-amd64.tar.gz tar -xzf terraform-docs.tar.gz rm terraform-docs.tar.gz chmod +x terraform-docs sudo mv terraform-docs /usr/local/bin/ terraform-docs Generating terraform documentation with Terraform Docs: cd ~/$MY_REPO cd foundation-infrastructure terraform-docs markdown . > README.md cd ../notepad-infrastructure terraform-docs markdown . > README.md Verify created documentation: edit README.md","title":"3. Create Documentation for terraform code"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#4-workaround-for-project-quota-issue","text":"If you see following error during project creation in foundation layer: Error: Error setting billing account \"010BE6-CA1129-195D77\" for project \"projects/ayrat-notepad-dev-244\": googleapi: Error 400: Precondition check failed., failedPrecondition This is due to our Billing account has quota of 5 projects per account. To solve this issue find all unused accounts: gcloud beta billing projects list --billing-account $ACCOUNT_ID And unlink them, so you have less then 5 projects per account: gcloud beta billing projects unlink $PROJECT_ID","title":"4. Workaround for Project Quota issue"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#5-commit-readme-doc-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit docs folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Readme doc for Production GKE Creation using gcloud\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"5 Commit Readme doc to repository and share it with Instructor/Teacher"},{"location":"020_Assignment_3_sol_Terraform_GCP_Foundation/#6-cleanup","text":"We only going to cleanup GCP Service foundation layer, as we going to use GCP project in future. cd ~/$MY_REPO/notepad-infrastructure terraform destroy -var-file terraform.tfvars","title":"6 Cleanup"},{"location":"020_Assignment_4_Helm_Foundation/","text":"Lab 4 Learning Helm Objective: Installing and Configuring the Helm Client Deploy Kubernetes apps with Helm Charts Learn Helm Commands Learn Helm Repository Create Helm chart Learn Helm Plugins Learn HelmFile Prerequisite \u00b6 Locate Assignment 4 \u00b6 Step 1 Clone ycit020 repo with Kubernetes manifests, which going to use for our work: cd ~/ycit020 # Alternatively: cd ~ & git clone https://github.com/Cloud-Architects-Program/ycit020 git pull cd ~/ycit020/Assignment4/ ls Result You can see Kubernetes manifests and terraform configs Step 1 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 (Optional) If you terraform config is not working you can copy working config from Assignment4 folder to your notepad-infrastructure folder. Step 4 Create structure for Helm Deployments for supporting applications cd ~/$MY_REPO/notepad-infrastructure mkdir helm cat <<EOF> helm/README.md # Helm Files and values to deploy supporting applications EOF Step 5 Copy Assignment 4 deploy_a4 folder to your repo: cd ~/$MY_REPO/ cp -r ~/ycit020/Assignment4/deploy_a4 deploy_ycit020_a4 ls deploy_ycit020_a4 Result You should see k8s-manifest folder Step 6 Create structure for NotePad Helm Chart locations cd ~/$MY_REPO/deploy_ycit020_a4 mkdir helm cat <<EOF> helm/README.md # Helm Chart, HelmFiles and values to deploy `NotePad` application. EOF Reserve Static IP addresses \u00b6 In the next few assignments we might deploy many applications and so we will require expose them using Ingress. When you create an Ingress object, you get a stable external IP address that clients can use to access your Services and in turn, your running containers. The IP address is stable in the sense that it lasts for the lifetime of the Ingress object. If you delete your Ingress and create a new Ingress from the same manifest file, you are not guaranteed to get the same external IP address. We would like to have for each student a a permanent IP address that stays the same across deleting your Ingress and creating a new one For that you must reserve a Regional static External IP address and provide it teacher, so we can setup A Record on the DNS for your behalf. Reference: Reserving a static external IP address Terraform resource google_compute_address Step 1: Create folder for static ip terraform configuration: cd ~/$MY_REPO/ mkdir ip-infrastructure Important The reason we creating a new folder for static_ip creation is because we don't want to destroy or delete this IP throughout our training. Step 1: Create Terraform configuration: Set you current PROJECT_ID value here: export PROJECT_ID=<YOUR_PROJECT_ID> Declare Provider: cd ~/$MY_REPO/ip-infrastructure cat << EOF>> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } } } EOF Configure global address resource: cat << EOF>> static_ip.tf provider \"google\" { project = var.gcp_project_id } resource \"google_compute_address\" \"regional_external_ip\" { provider = google name = \"static-ingres-ip\" address_type = \"EXTERNAL\" region = \"us-central1\" } EOF Configure variables: cat <<EOF> variables.tf variable \"gcp_project_id\" { type = string description = \"The GCP Seeding project ID\" default = \"\" } EOF Configure tfvars : gcp_project_id = \"$PROJECT_ID\" Configure outputs: cat <<EOF >> outputs.tf output \"addresses\" { description = \"Global IPv4 address for proxy load balancing to the nearest Ingress controller\" value = google_compute_address.regional_external_ip.address } output \"name\" { description = \"Static IP Name\" value = google_compute_address.regional_external_ip.name } EOF Step 2: Apply Terraform configuration: Initialize: terraform init Plan and Deploy Infrastructure: terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Set variable: cd ~/$MY_REPO/ip-infrastructure export STATIC_IP_ADDRESS=$(terraform output | grep 'addresses' |awk '{ print $3}') export STATIC_IP_NAME=$(terraform output | grep 'name' |awk '{ print $3}') Create readme: cat <<EOF> README.md Generated Static IP for Ingress: * Name - $STATIC_IP_NAME # Used Ingress Manifest * Address - $STATIC_IP_ADDRESS # Used to Configure DNS A Record EOF Important Add information about your static_ip_address in column 3 under your name in this spreadsheet: https://docs.google.com/spreadsheets/d/1XV-wW9Z8KvfWo4io11V_ax2__OImF27Rmlfbyj03T0I/edit?usp=sharing Step 6 Commit deploy folder using the following Git commands: cd ~/$MY_REPO git status git add . git commit -m \"adding documentation for ycit020 assignment 4\" Commit to Repository \u00b6 Step 1 Commit ip-infrastructure and helm folders using the following Git commands: cd ~/$MY_REPO git status git add . git commit -m \"Assignement 4\" Step 2 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 1 Install and Configure Helm Client \u00b6 1.1 Install Helm 3 \u00b6 GCP Cloud Shell comes with many common tools pre-installed including helm . Step 1: Verify and validate the version of Helm that is installed: helm --version Output: version.BuildInfo{Version:\"v3.5.0\"} Result Helm 3 is installed Note Until November 2020, two different major versions of Helm were actively maintained. The current stable major version of Helm is version 3. Note Helm follows a versioning convention known as Semantic Versioning (SemVer). In Semantic Versioning, the version number conveys meaning about what you can expect in the release. Because Helm follows this specification, users can expect certain things out of releases simply by carefully reading the version number. At its core, a semantic version has three numerical components and an optional stability marker (for alphas, betas, and release candidates). Here are some examples: v1.0.0 v3.3.2 SemVer represents format of X.Y.Z , where X is a major version, Y is a minor version and Z is a patch release: The major release number tends to be incremented infrequently. It indicates that major changes have been made to Helm, and that some of those changes may break compatibility with previous versions. The difference between Helm 2 and Helm 3 is substantial, and there is work necessary to migrate between the versions. The minor release number indicates feature additions. The difference between 3.2.0 and 3.3.0 might be that a few small new features were added. However, there are no breaking changes between versions. (With one caveat: a security fix might necessitate a breaking change, but we announce boldly when that is the case.) The patch release number indicates that only backward compatible bug fixes have been made between this release and the last one. It is always recommended to stay at the latest patch release. (OPTIONAL) Step 2: If you want to use specific version of helm or want to install helm in you local machine (On macOS and Linux,) use following link to install Helm. The usual sequence of commands for installing this way is as follows: $ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 $ chmod 700 get_helm.sh $ ./get_helm.sh The preceding commands fetch the latest version of the get_helm.sh script, and then use that to find and install the latest version of Helm 3. Alternatively, you can download latest binary of you OS choice here . 1.3 Deploy GKE cluster \u00b6 We going to reuse Terraform configuration to deploy our GKE Cluster. And we assume that terraform.tfvars has been properly configured already with required values. Step 1: Locate Terraform Configuration directory. cd ~/$MY_REPO/notepad-infrastructure Step 2: Initialize Terraform Providers s terraform init Step 3: Increase GKE Node Pool VM size edit terraform.tfvars Update gke_pool_machine_type from e2-small to e2-highcpu-4 , to support larger workloads. Step 4: Review TF Plan: terraform plan -var-file terraform.tfvars Step 5: Create GKE Cluster and Node Pool: terraform apply -var-file terraform.tfvars !!! result GKE Clusters has been created 1.4 Configure Helm \u00b6 Helm interacts directly with the Kubernetes API server. For that reason, Helm needs to be able to connect to a Kubernetes cluster. Helm attempts to do this automatically by reading the same configuration files used by kubectl . Helm will try to find this information by reading the environment variable $KUBECONFIG. Step 1 Authenticate to the cluster. export STUDENT_NAME= gcloud container clusters get-credentials gke-$STUDENT_NAME-notepad-dev --region us-central1 Step 2 Test that kubectl client connected to GKE cluster: kubectl get pods Output: No resources found in default namespace. Step 3 Test that helm client connected to GKE cluster: helm list Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION Result As expected there are no Charts has been Installed on our cluster yet Summary Helm 3 has been installed and configured to work with our cluster. Let's deploy some charts! 2 Basic Helm Chart Installation \u00b6 2.1 Searching Chart \u00b6 A Helm chart is an package that can be installed into your Kubernetes cluster. During chart development, you will often just work with a chart that is stored on your local system and later pushed to GitHub. But when it comes to sharing charts, Helm describes a standard format for indexing and sharing information about Helm charts . A Helm chart repository is simply a set of files, reachable over the network, that conforms to the Helm specification for indexing packages. There are huge number of chart repositories on the internet. The easiest way to find the popular repositories is to use your web browser to navigate to the Artifact Hub . There you will find thousands of Helm charts, each hosted on an appropriate repository. Deprecation In the past all charts were located and maintained by Helm Kubernetes Community Git repositories, known as https://github.com/kubernetes/charts, and had 2 types: Stable Incubator All the charts were rendered in Web UI via Helm Hub page. While the central Git Repository to maintain charts was great idea, with fast growing Helm popularity, it become hard to impossible to manage and maintain it by small group of maintainers, as they had to aprove hundreds of PR per day and frustrating for chart contributors as they had to wait several weeks their PR to be reviewed and approved. As a result GitHub project for Helm stable and incubator charts as well as [Helm Hub] has been deprecated and archived. All the charts are now maintained by independent contributors in their subsequent repo's. (e.g vault, is under hashicorp/vault-helm repo) and the central place to find all active and official Charts can be foind in Artifact Hub . Important Helm 2 came with a Helm repository installed by default. The stable chart repository was at one time the official source of production-ready Helm charts. As we discussed above stable chart repository has been now deprecated. In Helm 3, there is no default repository. Users are encouraged to use the Artifact Hub to find what they are looking for and then add their preferred repositories. Step 1: Helm provides native way to search charts from CLI in artifacthub.io cd ~/$MY_REPO/notepad-infrastructure/helm helm search hub drupal Output: https://artifacthub.io/packages/helm/bitnami/drupal 10.2.30 9.2.3 One of the most versatile open source content m... https://artifacthub.io/packages/helm/cetic/drupal 0.1.0 1.16.0 Drupal is a free and open-source web content ma.. Result Search is a good way to find existing Helm packages Step 2: Use the link to navigate to Artifact Hub Drupal chart: https://artifacthub.io/packages/helm/bitnami/drupal Note Drupal is OSS content management systems that can be installed this days on K8s. Result: The Artifact Page opened with information about the chart, it's parameters and how to use the chart. Step 3: Review Github source Code of the chart: https://github.com/bitnami/charts/tree/master/bitnami/drupal Result: Github page with Drupal Helm Chart itself, where you can browse the templates , values.yaml , Chart.yaml to understand how this chart is actually working and if you have any issues with the chart this would be the right location to open the issue or send a PR with new feature if you decide to contribute. 2.2 Adding a Chart Repository \u00b6 Once you found a chart, it's logical to install it. However first step you need to do is to add a Chart Repository. Step 1: Adding a Helm chart is done with the helm repo add command: helm repo add bitnami https://charts.bitnami.com/bitnami Note Bitnami is company well known to package application for Any Platforms and Cloud environments. With popularity of Helm, Bitnami developers were among the core contributors who designed the Helm repository system. They have contributed to the establishment of Helm\u2019s best practices for chart development and have written many of the most widely used charts. Bitnami is now part of VMware, provides IT organizations with an enterprise offering that is secure, compliant, continuously maintained and customizable to your organizational policies. Step 2: Now, we can verify that the Bitnami repository exists by running a helm repo list command: helm repo list Output: NAME URL bitnami https://charts.bitnami.com/bitnami Result This command shows us all of the repositories installed for Helm. Right now, we see only the Bitnami repository that we just added. 2.3 Helm Chart Installation \u00b6 Step 1: At very minimum, installing a chart in Helm requires just 2 pieces of information: the name of the installation and the chart you want to install: helm install mywebsite bitnami/drupal Output: NAME: mywebsite LAST DEPLOYED: Sun Aug 8 08:25:29 2021 NAMESPACE: prometheus STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ******************************************************************* *** PLEASE BE PATIENT: Drupal may take a few minutes to install *** ******************************************************************* 1. Get the Drupal URL: NOTE: It may take a few minutes for the LoadBalancer IP to be available. Watch the status with: 'kubectl get svc --namespace prometheus -w mywebsite-drupal' export SERVICE_IP=$(kubectl get svc --namespace prometheus mywebsite-drupal --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\") echo \"Drupal URL: http://$SERVICE_IP/\" 2. Get your Drupal login credentials by running: echo Username: user echo Password: $(kubectl get secret --namespace prometheus mywebsite-drupal -o jsonpath=\"{.data.drupal-password}\" | base64 --decode) Result When using Helm, you will see that output for each installation. A good chart would provide helpful notes on how to connect to the deployed solution. Tip You can get notes information any time after helm installation using helm get notes mywebsite command. Step 2: Follow your Drupal Helm Chart notes Instruction to access Website 1. Get the Drupal URL: NOTE: It may take a few minutes for the LoadBalancer IP to be available. Watch the status with: 'kubectl get svc --namespace test -w mywebsite-drupal' export SERVICE_IP=$(kubectl get svc --namespace test mywebsite-drupal --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\") echo \"Drupal URL: http://$SERVICE_IP/\" Success We can access My blog website and login with provider user and password. You can start your own blog now, that runs on Kubernetes Step 3: List deployed Kubernetes resources: kubectl get all --namespace test kubectl get pvc --namespace test Summary Our Drupal website consist of MariaDB statefulset , Drupal deployment , 2 pvc s and services . Mywebsite Drupal service is type LoadBalancer and that's how we able to access it. Step 4: List installed chart: helm list Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION mywebsite default 1 2021-08-08 08:25:29.625862 -0400 EDT deployed drupal-10.2.30 9.2.3 Note Like other commands, helm list is namespace aware. By default, Helm uses the namespace your Kubernetes configuration file sets as the default. Usually this is the namespace named default . Step 5: Deploy same chart in namespace test : kubectl create ns test helm install --namespace test mywebsite bitnami/drupal --create-namespace Tip By adding --create-namespace , indicates to Helm that we acknowledge that there may not be a namespace with that name already, and we just want one to be created. List deployed chart in namespace test : helm list --namespace test Summary In Helm 2, instance names were cluster-wide. You could only have an instance named mywebsite once per cluster. In Helm 3, naming has been changed. Now instance names are scoped to Kubernetes namespaces. We could install 2 instances named mywebsite as long as they each lived in a different namespace. Step 5: Cleanup Helm Drupal deployments using helm uninstall command: helm uninstall mywebsite helm uninstall mywebsite -n test kubectl delete ns test 3 Advanced Helm Chart Installation \u00b6 3.2.1 Deploy NGINX Ingress Chart with Custom Configuration \u00b6 Let's deploy another Helm application on our Kubernetes cluster: Ingress Nginx - is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer. In the previous classes we used Google implementation of Ingress - GKE Ingress for HTTP(S) Load Balancing . While this solutions provides managed Ingress experience and advanced features like Cloud Armor, DDoS protection and Identity aware proxy. Ingress Nginx Controller is popular solution and has a lot of features and integrations. If you want to deploy Kubernetes Application on different cloud providers or On-prem the same way, Nginx Ingress Controller becomes a default option. Our task is to configure Ingress Nginx using type:LoadBalancer with Regional Static IP configured in the section ### Reserve Static IP addresses . Additionally we want to enable metrics service that will fetch Prometheus monitoring metrics from ingress and disable admissionWebhooks configuration. Step 1: Let's search ingress-nginx in artifacthub.io helm search hub ingress-nginx Output: URL CHART VERSION APP VERSION DESCRIPTION https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx 4.0.0 1.0.0-beta.1 Ingress controller for Kubernetes using NGINX https://artifacthub.io/packages/helm/api/ingress-nginx 3.29.1 0.45.0 Ingress controller for Kubernetes using NGINX Result We going to select the first ingress-nginx chart that is maintained by Kubernetes Community Step 2: Review Hub Page details about this chart and locate information how to add repository: https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx Result: The Artifact Page opened with information about the chart, it's parameters, and how to use the chart itself Step 3: Add ingress-nginx Repo: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update Step 4: Oftentimes, searching is a useful way to find not only what charts can be installed, but what versions are available: helm search repo ingress-nginx --versions | head Output: NAME CHART VERSION APP VERSION DESCRIPTION ingress-nginx/ingress-nginx 3.35.0 0.48.1 Ingress controller for Kubernetes using NGINX a... ingress-nginx/ingress-nginx 3.34.0 0.47.0 Ingress controller for Kubernetes using NGINX a... ingress-nginx/ingress-nginx 3.33.0 0.47.0 Ingress controller for Kubernetes using NGINX a... .... Note By default, Helm tries to install the latest stable release of a chart, but you can override this behavior and install a specific version of a chart. Thus it is often useful to see not just the summary info for a chart, but exactly which versions exist for a chart. Every new version of the chart can be bring fixes and new changes, so for production use it's better to go with tested version and pin installation version. Summary We going with latest listed official version of the chart 3.35.0 of ingress-nginx Chart. 3.2.2 Download and inspect Chart locally \u00b6 Step 1: Pull ingress-nginx Helm Chart of specific version to Local filesystem: cd ~/$MY_REPO/notepad-infrastructure/helm helm pull ingress-nginx/ingress-nginx --version 3.35.0 tar -xvzf ingress-nginx-3.35.0.tgz Step 2: See the tree structure of the chart sudo apt-get install tree tree -L 2 ingress-nginx Output: ingress-nginx \u251c\u2500\u2500 CHANGELOG.md \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 OWNERS \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ci \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 admission-webhooks \u2502 \u251c\u2500\u2500 clusterrole.yaml \u2502 \u251c\u2500\u2500 clusterrolebinding.yaml \u2502 \u251c\u2500\u2500 controller-configmap-addheaders.yaml \u2502 \u251c\u2500\u2500 controller-configmap-proxyheaders.yaml \u2502 \u251c\u2500\u2500 controller-configmap-tcp.yaml \u2502 \u251c\u2500\u2500 controller-configmap-udp.yaml \u2502 \u251c\u2500\u2500 controller-configmap.yaml \u2502 \u251c\u2500\u2500 controller-daemonset.yaml \u2502 \u251c\u2500\u2500 controller-deployment.yaml \u2502 \u251c\u2500\u2500 controller-hpa.yaml \u2502 \u251c\u2500\u2500 controller-ingressclass.yaml \u2502 \u251c\u2500\u2500 controller-keda.yaml \u2502 \u251c\u2500\u2500 controller-poddisruptionbudget.yaml \u2502 \u251c\u2500\u2500 controller-prometheusrules.yaml \u2502 \u251c\u2500\u2500 controller-psp.yaml \u2502 \u251c\u2500\u2500 controller-role.yaml \u2502 \u251c\u2500\u2500 controller-rolebinding.yaml \u2502 \u251c\u2500\u2500 controller-service-internal.yaml \u2502 \u251c\u2500\u2500 controller-service-metrics.yaml \u2502 \u251c\u2500\u2500 controller-service-webhook.yaml \u2502 \u251c\u2500\u2500 controller-service.yaml \u2502 \u251c\u2500\u2500 controller-serviceaccount.yaml \u2502 \u251c\u2500\u2500 controller-servicemonitor.yaml \u2502 \u251c\u2500\u2500 default-backend-deployment.yaml \u2502 \u251c\u2500\u2500 default-backend-hpa.yaml \u2502 \u251c\u2500\u2500 default-backend-poddisruptionbudget.yaml \u2502 \u251c\u2500\u2500 default-backend-psp.yaml \u2502 \u251c\u2500\u2500 default-backend-role.yaml \u2502 \u251c\u2500\u2500 default-backend-rolebinding.yaml \u2502 \u251c\u2500\u2500 default-backend-service.yaml \u2502 \u251c\u2500\u2500 default-backend-serviceaccount.yaml \u2502 \u2514\u2500\u2500 dh-param-secret.yaml \u2514\u2500\u2500 values.yaml Result Typical structure of the helm chart, where: The Chart.yaml file contains metadata and some functionality controls for the chart templates folder contains all yaml manifests in Go templates and used to generate Kubernetes manifests values.yaml contains default values applied during the rendering. The NOTES.txt file is a special template. When a chart is installed, the NOTES.txt template is rendered and displayed rather than being installed into a cluster. 3.2.2 Helm Template \u00b6 As a reminder our goal it customize Nginx deployment to configure Ingress Nginx using type:LoadBalancer with Regional Static IP configured in the section ### Reserve Static IP addresses . Additionally we want to enable metrics service that will fetch Prometheus monitoring metrics from ingress and disable admissionWebhooks configuration. Step 1: Review values.yaml cd ~/$MY_REPO/notepad-infrastructure/helm/ingress-nginx edit values.yaml Result It is pretty confusing at this point to understand what this chart is going to deploy, it would of been great to render to YAML format first. Step 2: Execute helm template command and store output in render.yaml helm template ingress-nginx/ingress-nginx --version 3.35.0 > render.yaml Note During helm template, Helm never contacts a remote Kubernetes server, hence the chart only has access to default Kubernetes kinds. The template command always acts like an installation. Review rendered values: edit render.yaml grep \"Source\" render.yaml Result helm template designed to isolate the template rendering process of Helm from the installation. The template command performs following phases of Helm Lifecycle: loads the chart determines the values renders the templates formats to YAML Step 3: Let's first create a configuration that will disable admissionWebhooks . Review looking in values.yaml for admissionWebhooks configuration: grep -A10 admissionWebhooks values.yaml Output: admissionWebhooks: annotations: {} enabled: true failurePolicy: Fail # timeoutSeconds: 10 port: 8443 certificate: \"/usr/local/certificates/cert\" key: \"/usr/local/certificates/key\" namespaceSelector: {} objectSelector: {} Result admissionWebhooks enabled: true Step 3: Let's create custom_values.yaml file with admissionWebhooks disabled cat << EOF>> custom_values.yaml controller: admissionWebhooks: enabled: false EOF Execute helm template and store output in new file render2.yaml : helm template ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 > render2.yaml Now comparing this 2 render files you can see the what change in configuration has resulted in the rendered file: diff render.yaml render2.yaml This is the YAML files that will be generated after this change: grep \"Source\" render2.yaml Output: # Source: ingress-nginx/templates/controller-serviceaccount.yaml # Source: ingress-nginx/templates/controller-configmap.yaml # Source: ingress-nginx/templates/clusterrole.yaml # Source: ingress-nginx/templates/clusterrolebinding.yaml # Source: ingress-nginx/templates/controller-role.yaml # Source: ingress-nginx/templates/controller-rolebinding.yaml # Source: ingress-nginx/templates/controller-service.yaml # Source: ingress-nginx/templates/controller-deployment.yaml Step 4: Now, let's add a custom Configuration to controller-service.yaml to use Static loadBalancerIP , we've configured in step ### Reserve Static IP addresses . First we need to locate the correct service parameters: grep -A20 \"service:\" values.yaml Output Shows that we have 4 different services in values file Let's narrow our search: grep -A20 \"service:\" values.yaml | grep \"List of IP address\" Output: ## List of IP addresses at which the controller services are available ## List of IP addresses at which the stats-exporter service is available ## List of IP addresses at which the default backend service is available Output: service: enabled: true annotations: {} labels: {} # clusterIP: \"\" ## List of IP addresses at which the controller services are available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] # loadBalancerIP: \"\" loadBalancerSourceRanges: [] Result It seems controller is first service: in values.yaml and currently is it has loadBalancerIP section commented Step 5: Let's add loadBalancerIP IP configuration to custom_values.yaml Set variable: cd ~/$MY_REPO/ip-infrastructure export STATIC_IP_ADDRESS=$(terraform output | grep 'addresses' |awk '{ print $3}') echo $STATIC_IP_ADDRESS Add custom values: cd ~/$MY_REPO/notepad-infrastructure/helm/ingress-nginx cat << EOF>> custom_values.yaml service: loadBalancerIP: $STATIC_IP_ADDRESS EOF Our final custom_values.yaml should look as following: controller: admissionWebhooks: enabled: false service: loadBalancerIP: 35.X.X.X metrics: enabled: true Execute helm template and store output in new file render3.yaml : helm template ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 > render3.yaml Show the difference in rendered file: diff render2.yaml render3.yaml Output: > loadBalancerIP: 35.X.X.X Ensure that this change is in fact belongs to controller-service.yaml : grep -C16 \"loadBalancerIP:\" render3.yaml Result We configured correct service that will Ingress Controller service with type: Loadbalancer and static IP with previously generated with terraform. Summary helm template is great tool to render you charts to YAML. Tip You can use Helm as packaging you charts only. And then use helm template to generate actual YAMLs and apply them with kubectl apply 3.2.3 Dry-Runs \u00b6 Before applying configuration to Kubernetes Cluster it is good idea to Dry-Run it for Errors. This is especially important if you doing Upgrades. The dry-run feature provides Helm users a way to debug the output of a chart before it is sent on to Kubernetes. With all of the templates rendered, you can inspect exactly what would have been submitted to your cluster. And with the release data, you can verify that the release would have been created as you expected. Here is some of the dry-run working principals: --dry-run mixes non-YAML information with the rendered templates. This means the data has to be cleaned up before being sent to tools like kubectl. A --dry-run on upgrade can produce different YAML output than a --dry-run on install, and this can be confusing. It contacts the Kubernetes API server for validation, which means Helm has to have Kubernetes credentials even if it is just used to --dry-run a release. It also inserts information into the template engine that is cluster-specific. Because of this, the output of some rendering processes may be cluster-specific. Difference between dry-run and template is that the dry-run contacts Kubernetes API. It is good idea to use dry-run prior deployment and upgrade as it creates mutated output, while template doesn't contacts API and can to pure rendering. Step 1: Execute our deployment with dry-run command: helm install ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --dry-run Output: NAME: ingress-nginx LAST DEPLOYED: Tue Aug 10 07:22:56 2021 NAMESPACE: default STATUS: pending-install REVISION: 1 TEST SUITE: None HOOKS: MANIFEST: --- # Source: ingress-nginx/templates/controller-serviceaccount.yaml ........ NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace default get services -o wide -w ingress-nginx-controller' An example Ingress that makes use of the controller: ... If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: <base64 encoded cert> tls.key: <base64 encoded key> type: kubernetes.io/tls At the top of the output, it will print some information about the release in our case it tells what phase of the release it is in (pending-install), and the revision number. Next, after the informational block, all of the rendered templates are dumped to standard output. Finally, at the bottom of the dry-run output, Helm prints the user-oriented release notes: Note --dry-run dumps the output validates, but doesn't deploy actual chart. Step 2: Finally let's deploy Nginx Ingress Charts with our parameters in ingress-nginx namespace kubectl create ns ingress-nginx helm install ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx Step 3: Verify Helm Deployment: helm list Follow Installation note and verify Ingress-Controller Service: kubectl --namespace ingress-nginx get services -o wide ingress-nginx-controller Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 172.10.0.213 35.192.101.76 80:30930/TCP,443:30360/TCP 3m9s 3.2.4 Deploy Drupal using Nginx Ingress Controller \u00b6 Let's test our newly configured ingress-nginx-controller and expose drupal application using ingress instead of LoadBalancer . Check if STATIC_IP_ADDRESS variable is set: echo $STATIC_IP_ADDRESS If not set it with you Static IP address value. cd ~/$MY_REPO/notepad-infrastructure/helm/ kubectl delete pvc data-mywebsite-mariadb-0 cat << EOF>> drupal_ing_values.yaml ingress: annotations: {kubernetes.io/ingress.class: \"nginx\"} enabled: true hostname: $STATIC_IP_ADDRESS.nip.io path: / pathType: ImplementationSpecific EOF Note Here we using nip.io domain that provides simple wildcard DNS for any IP Address, however if you provided you STATIC_IP to teacher, they will generate following DNS, and you can replace hostname: with value $student-name.cloud-montreal.ca helm install mywebsite bitnami/drupal --values drupal_ing_values.yaml 1. Get the Drupal URL: You should be able to access your new Drupal installation through http://35.X.X.X.nip.io/ Success Drupal site is now accessible via Ingress. Our Nginx Ingress Controller has been setup correctly! Uninstall Drupal: helm uninstall mywebsite kubectl delete pvc data-mywebsite-mariadb-0 3.2.5 Using --set values and Upgrading Charts \u00b6 In addition to --value option, there is a second flag that can be used to add individual parameters to an install or upgrade . The --set flag takes one or more values directly. They do not need to be stored in a YAML file. Step 1: Let's update our ingress-nginx release with new parameter --set controller.image.pullPolicy=Always , but first we want to render template with and without parameter to see what the change will be applied: helm template ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx > render4.yaml helm template ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=Always > render5.yaml diff render4.yaml render5.yaml Output: < imagePullPolicy: IfNotPresent --- > imagePullPolicy: Always Step 2: Let's apply update to our ingress-nginx release. For that we going to use helm upgrade command: helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=Always Verify that upgrade was successful: helm list --namespace ingress-nginx Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ingress-nginx ingress-nginx 2 2021-08-10 10:48:35.265506 -0400 EDT deployed ingress-nginx-3.35.0 0.48.1 Result We can see that Revision change to 2 Note When we talk about upgrading in Helm, we talk about upgrading an installation, not a chart. An installation is a particular instance of a chart in your cluster. When you run helm install , it creates the installation. To modify that installation, use helm upgrade . This is an important distinction to make in the present context because upgrading an installation can consist of two different kinds of changes: * You can upgrade the version of the chart * You can upgrade the configuration of the installation In this case we upgrading the configuration of the installation. Extra If you interested upgrade chart version of ingress-nginx to the next available release or 3.34.0 , give it a try. 3.2.5 Listing Releases, History, Rollbacks \u00b6 Step 1: Let's see if we can upgrade our release with wrong value pullPolicy=NoSuchPolicy that doesn't exist on Kubernetes. We will first run in dry-run mode helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=NoSuchPolicy --dry-run Result Release \"ingress-nginx\" has been upgraded. Happy Helming! Let's now apply same config without --dry-run mode: helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=NoSuchPolicy Output: Error: UPGRADE FAILED: cannot patch \"ingress-nginx-controller\" with kind Deployment: Deployment.apps \"ingress-nginx-controller\" is invalid: spec.template.spec.containers[0].imagePullPolicy: Unsupported value: \"NoSuchPolicy\": supported values: \"Always\", \"IfNotPresent\", \"Never\" Failed As the error message indicates, a pull policy cannot be set to NoSuchPolicy. This error came from the Kubernetes API server, which means Helm submitted the manifest, and Kubernetes rejected it. So our release should be in a failed state. Verify with helm list to confirm failed state: helm list Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ingress-nginx ingress-nginx 3 2021-08-10 11:03:05.224405 -0400 EDT failed ingress-nginx-3.35.0 0.48.1 Step 2: List all releases with helm history : helm history ingress-nginx -n ingress-nginx REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Tue Aug 10 08:47:32 2021 superseded ingress-nginx-3.35.0 0.48.1 Install complete 2 Tue Aug 10 10:48:35 2021 deployed ingress-nginx-3.35.0 0.48.1 Upgrade complete 3 Tue Aug 10 11:03:05 2021 failed ingress-nginx-3.35.0 0.48.1 Upgrade \"ingress-nginx\" failed: cannot patch \"ingress-nginx-controller\" with kind Deployment: Deployment.apps \"ingress-nginx-controller\" is invalid: spec.template.spec.containers[0].imagePullPolicy: Unsupported value: \"NoSuchPolicy\": supported values: \"Always\", \"IfNotPresent\", \"Never\" Info During the life cycle of a release, it can pass through several different statuses. Here they are, approximately in the order you would likely see them: pending-install Before sending the manifests to Kubernetes, Helm claims the installation by creating a release (marked version 1) whose status is set to pending-install. deployed As soon as Kubernetes accepts the manifest from Helm, Helm updates the release record, marking it as deployed. pending-upgrade When a Helm upgrade is begun, a new release is created for an installation (e.g., v2), and its status is set to pending-upgrade. superseded When an upgrade is run, the last deployed release is updated, marked as superseded, and the newly upgraded release is changed from pending-upgrade to deployed. pending-rollback If a rollback is created, a new release (e.g., v3) is created, and its status is set to pending-rollback until Kubernetes accepts the release manifest. Then it is marked deployed and the last release is marked superseded. uninstalling When a helm uninstall is executed, the most recent release is read and then its status is changed to uninstalling. uninstalled If history is preserved during deletion, then when the helm uninstall is complete, the last release\u2019s status is changed to uninstalled. failed Finally, if during any operation, Kubernetes rejects a manifest submitted by Helm, Helm will mark that release failed. Step 3: Rollback release with helm rollback . From the error, we know that the release failed because we supplied an invalid image pull policy. So of course we could correct this by simply running another helm upgrade. But imagine a case where the cause of error was not readily available. Rather than leave the application in a failed state while diagnosing the problem, it would be nice to simply revert back to the release that worked before. helm rollback ingress-nginx 2 -n ingress-nginx helm history ingress-nginx -n ingress-nginx helm list 4 Tue Aug 10 11:16:24 2021 deployed ingress-nginx-3.35.0 0.48.1 Rollback to 2 --- ingress-nginx ingress-nginx 4 2021-08-10 11:16:24.424371 -0400 EDT deployed ingress-nginx-3.35.0 0.48.1 Rollback was a success! Happy Helming! Success This command tells Helm to fetch the ingress-nginx version 2 release, and resubmit that manifest to Kubernetes. A rollback does not restore to a previous snapshot of the cluster. Helm does not track enough information to do that. What it does is resubmit the previous configuration, and Kubernetes attempts to reset the resources to match. 3.2.5 Upgrade Release with Helm Diff Plugin \u00b6 While Helm provides a lot of functionality out of the box. Community contributes a lot of great functionality via helm plugins. Plugins allow you to add extra functionality to Helm and integrate seamlessly with the CLI, making them a popular choice for users with unique workflow requirements. There are a number of third-party plugins available online for common use cases, such as secrets management. In addition, plugins are incredibly easy to build on your own for unique, one-off tasks. Helm plugins are external tools that are accessible directly from the Helm CLI. They allow you to add custom subcommands to Helm without making any modifications to Helm\u2019s Go source code. Many third-party plugins are made open source and publicly available on GitHub. Many of these plugins use the \u201chelm-plugin\u201d tag/topic to make them easy to find. Refer to the documentation for Helm plugins on GitHub Let's Upgrade our running Ingress Nginx chart with metrics service that will enable Prometheus to scrape metrics from our Nginx. Step 1: Update custom_values.yaml with following information: cat << EOF>> custom_values.yaml metrics: enabled: true EOF Step 2: So far we've used helm template to render and compare manifests. However most of the time you might not be able to do it during upgrades. However you want to have a clear understanding what will be upgraded if you change something. That's where Helm Diff Plugin can be handy. Helm Diff plugin giving your a preview of what a helm upgrade would change. It basically generates a diff between the latest deployed version of a release and a helm upgrade --dry-run Install Helm Diff Plugin: helm plugin install https://github.com/databus23/helm-diff helm diff upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx Output: - imagePullPolicy: Always + imagePullPolicy: IfNotPresent + protocol: TCP + - name: metrics + containerPort: 1025 + # Source: ingress-nginx/templates/controller-service-metrics.yaml + apiVersion: v1 + kind: Service ... Result Amazing! This looks like terraform plan but for helm :) Important Another discovery from above printout is that since we forgot to add --set imagePullPolicy command, our value will be reverted with upgrade. This is really important to understand as you configuration maybe lost if you use --set Step 3: Let's upgrade our ingress-nginx release: helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx helm list -n ingress-nginx Success Our ingress-nginx is ready to run. Going forward we going to always deploy this chart. Summary So far we've learned: How to customize helm chart deployment using --set and --values using values.yaml Using values.yaml preferable mehtod in order to achieve reproducible deployments How to upgrade and rollback releases Helm template and dry-run Helm Diff Plugin 3.2.6 Install Minio with Helm \u00b6 Let's deploy another Helm application on our Kubernetes cluster: Minio - high-performance, S3 compatible object storage. helm install myminio bitnami/minio Follow your Minio Helm Chart notes Instruction to access Website: To access the MinIO&reg; web UI: - Get the MinIO&reg; URL: echo \"MinIO&reg; web URL: http://127.0.0.1:9000/minio\" kubectl port-forward --namespace default svc/myminio 8080:9000 In Gcloud console execute kubectl port-forward : kubectl port-forward --namespace default svc/myminio 8080:9000 Result We can access Minio UI using private IP via tunnel and we have to use autogenerated ACCESS_KEY and SECRET_KEY. Uninstall the Minio chart: helm uninstall myminio bitnami/minio While it looks easy to start with helm and deploy any charts, most of the time you want to have a full control of the deployment process. You want to specify, tested version, safe values or configurations that make sense for you or your organization. 3.2.7 Customize Minio Installation Task \u00b6 Task N1: Customize Minio Installation with following values: * Deploy `Minio` of specific version: `7.1.7` * Expose Minio using `Ingress` resource: * ingress.class: \"nginx\" * path: \"/\" * pathType: \"ImplementationSpecific\" * hostname: $STATIC_IP_ADDRESS.nip.io * Specify custom `ACCESS_KEY` for `Minio` frontend: `myaccesskey` * Specify custom `SECRET_KEY` for `Minio` frontend: `mysecretkey` Step 1: Use the following Minio Chart: helm search hub minio URL CHART VERSION APP VERSION DESCRIPTION https://artifacthub.io/packages/helm/bitnami/minio 7.1.7 2021.6.17 Bitnami Object Storage based on MinIO&reg ..... Result We going to choose bitnami package again as it's top of the list and seems maintained the most. Step 2: Since we already added bitnami repository locally, let's verify if we can list minio in it: helm search repo minio Result We can now go ahead and install minio Step 4: Pull Minio Helm Chart of specific version to Local filesystem to work with values file. cd ~/$MY_REPO/notepad-infrastructure/helm helm pull bitnami/minio --version 7.1.7 cd minio cat <<EOF> custom_values.yaml TODO: Finish the remaining steps EOF TODO: Complete custom_values.yaml and test deployment of Minio . Test Minio by creating bucket and uploading object to it. Step 5: Test and Install Minio Chart with Custom Configuration helm install myminio bitnami/minio --values custom_values.yaml --version 7.1.6 Follow your Minio Helm Chart notes Instruction to access Website. Step 6 Commit deploy_ycit020_a4/helm and notepad-infrastructure/helm folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Helm values for Minio\" Step 7 Push commit to the Cloud Source Repositories: git push origin master Step 8 Uninstall the Minio chart: helm uninstall myminio bitnami/minio 4 Creating NotePad Helm Charts \u00b6 So far with learned how to deploy existing charts from the Artifact Hub. However sometimes you or your company needs to build software that require's to be distributed and shared externally, as well as have lifecycle and release management. Helm Charts becomes are valuable option for that, specifically if you application is based of containers! Imagine that our solution is Go-based NotePad has first customer that wants to deploy it on their system and they requested delivery via Helm Charts that available on GCP based Helm Repository. To achieve such task we going to create 2 Charts: * `gowebapp-mysql` chart * `gowebapp` chart And store them in Google Artifact Registry that provides Helm 3 OCI Repository. 4.1 Design gowebapp-mysql chart \u00b6 4.1.1 Create gowebapp-mysql chart \u00b6 As scary as it sounds creating a new basic helm chart is 5 minute thing! We going to learn 2 quick methods to create helm charts, that will help you to save time and get started with helm quickly. Helm includes the helm create command to make it easy for you to create a chart of your own, and it\u2019s a great way to get started. The create command creates a chart for you, with all the required chart structure and files. These files are documented to help you understand what is needed, and the templates it provides showcase multiple Kubernetes manifests working together to deploy an application. In addition, you can install and test this chart right out of the box. Step 1: Create a new chart: cd ~/$MY_REPO/deploy_ycit020_a4/helm helm create gowebapp-mysql cd gowebapp-mysql tree Output: \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 hpa.yaml \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u251c\u2500\u2500 service.yaml \u2502 \u251c\u2500\u2500 serviceaccount.yaml \u2502 \u2514\u2500\u2500 tests \u2502 \u2514\u2500\u2500 test-connection.yaml \u2514\u2500\u2500 values.yaml Result This command creates a new Nginx chart, with a name of your choice, following best practices for a chart layout. Since Kubernetes clusters can have different methods to expose an application, this chart makes the way Nginx is exposed to network traffic configurable so it can be exposed in a wide variety of clusters. The chart has following structure: The Chart.yaml file contains metadata and some functionality controls for the chart The charts folder currently empty may container charts dependency of the top level chart. For example we could of make our gowebapp-mysql as dependency chart for gowebapp templates folder contains all yaml manifests in Go templates and used to generate Kubernetes manifests values.yaml contains default values applied during the rendering. The NOTES.txt file is a special template. When a chart is installed, the NOTES.txt template is rendered and displayed rather than being installed into a cluster. _helpers.tpl - helper templates for your other templates (e.g creating same labers across all charts). Files with _ are not rendered to Kubernetes object definitions, but are available everywhere within other chart templates for use. Here we going to show a quick way to create a Helm chart without adding any templating. Step 2: Delete templates folder and create empty one: rm -rf templates mkdir templates cd templates Step 3: Copy existing gowebapp-mysql manifests to templates folder: cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-mysql-pvc.yaml . # PVC cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/secret-mysql.yaml . # Secret cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-mysql-service.yaml . # Service cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-mysql-deployment.yaml . # Deployment Step 4: Lint Helm Chart: When developing charts, especially when working with YAML templates, it can be easy to make a mistake or miss something. To help you catch errors, bugs, style issues, and other suspicious elements, the Helm client includes a linter. This linter can be used during chart development and as part of any testing processes. To use the linter, use the lint command on a chart as a directory or a packaged archive: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ helm lint gowebapp-mysql Output: ==> Linting gowebapp-mysql [INFO] Chart.yaml: icon is recommended 1 chart(s) linted, 0 chart(s) failed Step 5: Install Helm Chart locally: Create dev namespace: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp-mysql kubectl create ns dev kubectl config set-context --current --namespace=dev Render the manifest locally and compare to original manifests: helm template gowebapp-mysql . > render.yaml Install the chart: helm install gowebapp-mysql . Output: NAME: gowebapp-mysql LAST DEPLOYED: Tue Aug 10 15:25:44 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None helm list kubectl get all kubectl get pvc Success We've deployed our own helm chart locally. While we haven't used templating of the chart at all, we have a helm chart that can be installed and upgraded using helm release features. 4.2 Design gowebapp chart. \u00b6 4.2.1 Create gowebapp chart \u00b6 We going to use another method to deploy our second chart gowebapp . In this case we going to use nginx template provided by help team. Task N2: Create gowebapp Helm chart: * Configure `Deployment` template in `values.yaml` * Configure `Service` template in `values.yaml` * Disable `Service account` template in `values.yaml` * Configure `Ingress` template in `values.yaml`: * ingress.class: \"nginx\" * path: \"/\" * pathType: \"ImplementationSpecific\" * host: $STATIC_IP_ADDRESS.nip.io * Templatize the ConfigMap Resource * Ensure the chart is deployable * Ensure `gowebapp` in browser Step 1 Create a new chart: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ helm create gowebapp cd gowebapp tree Output: \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 hpa.yaml \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u251c\u2500\u2500 service.yaml \u2502 \u251c\u2500\u2500 serviceaccount.yaml \u2502 \u2514\u2500\u2500 tests \u2502 \u2514\u2500\u2500 test-connection.yaml \u2514\u2500\u2500 values.yaml edit values.yaml 4.2.2 Template the Deployment \u00b6 Step 1 Update the replicaCount value to 2 in the values.yaml file: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Step 2 Update the repository and tag section to point to your gowebapp docker image in the values.yaml file: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Step 3 Update the resources section in the values.yaml file to include the resource requests and limits the gowebapp application needs: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml See the reference gowebapp-deployment.yaml : cat ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-deployment.yaml Step 4 Update the livness and readiness sections to match what you have in the gowebapp deployment.yaml See the reference gowebapp-deployment.yaml : cat ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-deployment.yaml Update livness and readiness sections for template deployment.yaml : cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/deployment.yaml Step 5 Notice that the deployment.yaml file does not have an environment variable section for secrets , so let's add one. For this chart we will assume that this section is optional based on whether or not a secrets section exist in the Values.yaml file. Step 5-a Include the following in the deployment template: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/deployment.yaml And add following code snippet in the appropriate location: {{- if .Values.secrets }} - env: - name: {{.Values.secrets.name}} valueFrom: secretKeyRef: name: {{.Values.secrets.secretReference.name}} key: {{.Values.secrets.secretReference.key}} {{- end}} Step 5-b Include a section in the values.yaml file: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Add following snippet: secrets: enabled: true name: DB_PASSWORD secretReference: name: mysql key: password Step 6 For this lab, we will include the volumes and volumeMounts sections without templating, so just copy the required sections to the appropriate location in the deployment.yaml template. cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/deployment.yaml See the reference gowebapp-deployment.yaml : cat ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-deployment.yaml Step 7 Render the Chart locally and compare Deployment to original gowebapp-deployment.yaml manifests: helm template gowebapp-mysql . > render.yaml 4.2.3 Template the Service \u00b6 Step 1: The service.yaml template doesn't have an annotation section, so modify the template to add an annotation section, that looks following: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/service.yaml {{- with .Values.annotations }} annotations: {{- toYaml . | nindent 4 }} {{- end }} TODO: Next, modify the values.yaml file to allow chart users to add annotations for the service. Make sure to use the right section in the values.yaml file, based on how you modified your /templates/service.yaml file. Reference documentation: * https://helm.sh/docs/chart_template_guide/control_structures/#modifying-scope-using-with Step 2: Under the service: section in the values.yaml file, update the service port: to 9000. Step 3 In the values.yaml file, update the service type to NodePort Step 4 Render the Chart locally and compare Service to original gowebapp-service.yaml manifests: helm template gowebapp-mysql . > render.yaml 4.2.4 Disable the Service account \u00b6 Step 1 Update the values.yaml file to disable the service account creation for the gowebapp deployment. 4.2.5 Template the Ingress Resource \u00b6 Step 1 enable the ingress the values.yaml file and configur according requirements: : * Expose `gowebapp` using `Ingress` resource: * ingress.class: \"nginx\" * path: \"/\" * pathType: \"ImplementationSpecific\" * host: $STATIC_IP_ADDRESS.nip.io cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Step 2 Render the Chart locally and verify if any issues: helm template gowebapp-mysql . > render.yaml 4.2.5 Templatize the ConfigMap Resource \u00b6 Step 1 Create and templatize configmap resource for our gowebapp that provides connection to Mysql: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp/templates/ cat <<EOF>> configmap.yaml kind: ConfigMap apiVersion: v1 metadata: name: {{ .Values.configMap.name }} data: webapp-config-json: |- {{ .Files.Get \"config.json\" | indent 4 }} EOF Store the config.json inside the chart repository: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp cat <<EOF>> config.json { \"Database\": { \"Type\": \"MySQL\", \"Bolt\": { \"Path\": \"gowebapp.db\" }, \"MongoDB\": { \"URL\": \"127.0.0.1\", \"Database\": \"gowebapp\" }, \"MySQL\": { \"Username\": \"root\", \"Password\": \"rootpasswd\", \"Name\": \"gowebapp\", \"Hostname\": \"gowebapp-mysql\", \"Port\": 3306, \"Parameter\": \"?parseTime=true\" } }, \"Email\": { \"Username\": \"\", \"Password\": \"\", \"Hostname\": \"\", \"Port\": 25, \"From\": \"\" }, \"Recaptcha\": { \"Enabled\": false, \"Secret\": \"\", \"SiteKey\": \"\" }, \"Server\": { \"Hostname\": \"\", \"UseHTTP\": true, \"UseHTTPS\": false, \"HTTPPort\": 80, \"HTTPSPort\": 443, \"CertFile\": \"tls/server.crt\", \"KeyFile\": \"tls/server.key\" }, \"Session\": { \"SecretKey\": \"@r4B?EThaSEh_drudR7P_hub=s#s2Pah\", \"Name\": \"gosess\", \"Options\": { \"Path\": \"/\", \"Domain\": \"\", \"MaxAge\": 28800, \"Secure\": false, \"HttpOnly\": true } }, \"Template\": { \"Root\": \"base\", \"Children\": [ \"partial/menu\", \"partial/footer\" ] }, \"View\": { \"BaseURI\": \"/\", \"Extension\": \"tmpl\", \"Folder\": \"template\", \"Name\": \"blank\", \"Caching\": true } } EOF And finally add following snippet inside values.yaml : edit gowebapp/values.yaml configMap: name: gowebapp Step 2 Render the Chart locally and verify if any issues: helm template gowebapp-mysql . > render.yaml 4.2.6 Deploy gowebapp \u00b6 Before deployment make sure test chart with dry-run , template and lint the chart Lint: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ helm lint gowebapp Install: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp helm install gowebapp . Output: NAME: gowebapp LAST DEPLOYED: Tue Aug 10 15:25:44 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None helm ls kubectl get all kubectl get pvc kubectl get ing Access gowebapp with Ingress Success You've now became a chart developer! And learned, how to create basic helm charts with 2 methods. You can farther customize the chart as needed and add more templates as you go. The next step would be to look contribute back to Helm community, use charts in Artificatory, find issue or add missing feature, help grow help project! 4.2.7 Submit assignement \u00b6 Step 1 Commit chart configuration in deploy_ycit020_a4/helm folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Gowebapp Helm Chart\" Step 2 Push commit to the Cloud Source Repositories: git push origin master Step 3 Uninstall chart gowebapp and gowebapp-mysql chart helm install gowebapp helm uninstall gowebapp-mysql Step 4 Resize GKE Cluster to 0 nodes, to avoid charges: cd ~/$MY_REPO/notepad-infrastructure edit terraform.tfvars And set gke_pool_node_count = \"0\" Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars Step 4: Shutdown all Nodes in GKE Cluster Node Pool: terraform apply -var-file terraform.tfvars Result GKE Clusters has been scale down to 0 nodes. Part 2 WIP \u00b6 5 Chart Repositories \u00b6 6 HelmFile \u00b6 7 Install Istio on GKE \u00b6 Currently Istio community developing following options of Istio deployment on Kubernetes Clusters: Install with Istioctl installation via istioctl command line tool used to showcase Istio functionality. Using the CLI, we generate a YAML file with all Istio resources and then deploy it to the Kubernetes cluster Istio Operator Install Takes installation of Istio to the next level as it's managing not only Istio installation but overall lifecicle of the Istio deployment including Upgrades and configurations. Install with Helm (alpha) Allows to farther simplify Istio deployment and it's customization. 7.0 Prerequisite \u00b6 Scaleup cluster back to 3 nodes and Update VPC firewall rules to enable auto-injection and the istioctl version and istioctl ps commands. References: Opening ports on a private cluster Istio deployment on GKE Private Clusters Step 1: Locate Terraform Configuration directory. cd ~/$MY_REPO/notepad-infrastructure Step 2: Configure GKE Cluster to Scale back to 1 node per zone in a region. edit terraform.tfvars And set gke_pool_node_count = \"1\" Step 3: Create new firewall rule for source range (master-ipv4-cidr) of the cluster, that will open required ports for Istio installation: cat <<EOF> istio_firewall.tf resource \"google_compute_firewall\" \"istio_specific\" { name = format(\"allow-istio-in-privategke-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link source_ranges = [\"172.16.0.0/28\"] allow { protocol = \"tcp\" ports = [\"10250\", \"443\", \"15017\", \"15014\", \"8080\"] } } EOF Step 4: Review TF Plan: terraform plan -var-file terraform.tfvars Step 5: Scale up GKE Cluster Node Pool and update firewall rules for required range: terraform apply -var-file terraform.tfvars Result GKE Clusters has been scalled to 3 nodes and has firewall rules opened Step 6: Verify firewall rule: gcloud compute firewall-rules list --filter=\"name~allow-istio-in-privategke*\" Output: NAME NETWORK DIRECTION PRIORITY ALLOW DENY DISABLED allow-istio-in-privategke-$student-notepad-dev vpc-$student-notepad-dev INGRESS 1000 tcp:10250,tcp:443,tcp:15017,tcp:15014,tcp:8080 False 7.1 Deploy Istio using Custom Resources \u00b6 This installation guide uses the istioctl command line tool to provide rich customization of the Istio control plane and of the sidecars for the Istio data plane. Download and extract the latest release: curl -L https://istio.io/downloadIstio | sh - cd istio-1.11.0 export PATH=$PWD/bin:$PATH Success The above command will fetch Istio packages and untar them in the same folder. tree -L 1 . \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 bin \u251c\u2500\u2500 manifest.yaml \u251c\u2500\u2500 manifests \u251c\u2500\u2500 samples \u2514\u2500\u2500 tools Note Istio installation directory contains: bin directory contains istioctl client binary manifests Installation Profiles, configurations and Helm Charts samples directory contains sample applications deployment tools directory contains auto-completion tooling and etc. Step 2 Deploy Istio Custom Resource Definitions (CRDs) Istio extends Kubernetes using Custom Resource Definitions (CRDs). CRDs allow registration of new/non-default Kubernetes resources. When Istio CRDs are deployed, Istio\u2019s objects are registered as Kubernetes objects, providing a highly integrated experience with Kubernetes as a deployment platform and thus allowing Kubernetes to store configuration of Istio features such as routing, security and telemetry and etc. We going to install using using demo configuration profile. The demo configuration profile allows to experiment with most of Istio features with modest resource requirements. Since it enables high levels of tracing and access logging, it is not suitable for production use cases. istioctl install --set profile=demo -y Output: \u2714 Istio core installed \u2714 Istiod installed \u2714 Egress gateways installed \u2714 Ingress gateways installed \u2714 Installation complete Thank you for installing Istio 1.11. Please take a few minutes to tell us about your install/upgrade experience! Note Wait a few seconds for the CRDs to be committed in the Kubernetes API-server. Step 3 Verify Istio CRDs successfully applied to Kubernetes Cluster. kubectl get crds | grep istio authorizationpolicies.security.istio.io 2021-08-16T19:38:16Z destinationrules.networking.istio.io 2021-08-16T19:38:16Z envoyfilters.networking.istio.io 2021-08-16T19:38:16Z gateways.networking.istio.io 2021-08-16T19:38:17Z istiooperators.install.istio.io 2021-08-16T19:38:17Z peerauthentications.security.istio.io 2021-08-16T19:38:17Z requestauthentications.security.istio.io 2021-08-16T19:38:17Z serviceentries.networking.istio.io 2021-08-16T19:38:17Z sidecars.networking.istio.io 2021-08-16T19:38:17Z telemetries.telemetry.istio.io 2021-08-16T19:38:18Z virtualservices.networking.istio.io 2021-08-16T19:38:18Z workloadentries.networking.istio.io 2021-08-16T19:38:18Z workloadgroups.networking.istio.io 2021-08-16T19:38:19Z Info Above CRDs will be avaialable as a new Kubernetes Resources and stored in Kubernetes ETCD database. The Kubernetes API will represent these new resources as endpoints that can be used as other native Kubernetes object (such as Pod, Services) levereging kubectl, RBAC and other features and admission controllers of Kubernetes. Step 4 Count total number of Installed CRDs: kubectl get crds | grep istio | wc -l Note CRDs count will vary based on Istio version and profile deployed. Step 5 Verify that Istio control plane has been installed successfully. kubectl get pods -n istio-system Output: istio-egressgateway-9dc6cbc49-5wkqt 1/1 Running 0 2m54s istio-ingressgateway-7975cdb749-xjc5g 1/1 Running 0 2m54s istiod-77b4d7b55d-j6kb5 1/1 Running 0 3m9s Info Istio control-plane include following components: istiod - contains components such as Citadel and Pilot istio-ingressgateway Istio Ingress Gateway istio-ingressgateway Istio Egress Gateway Step 6 Verify installation with istioctl CLI: istioctl version istioctl verify-install Output: Checked 13 custom resource definitions Checked 3 Istio Deployments \u2714 Istio is installed and verified successfully Step 5 Deploy Bookinfo sample application: kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-cjj5b 1/1 Running 0 12s productpage-v1-6b746f74dc-sxdxz 1/1 Running 0 10s ratings-v1-b6994bb9-jd985 1/1 Running 0 11s reviews-v1-545db77b95-5kjr4 1/1 Running 0 11s reviews-v2-7bf8c9648f-5v2s9 1/1 Running 0 11s reviews-v3-84779c7bbc-5khlt 1/1 Running 0 11s Note This is a typical Kubernetes deployment, and has nothing specific to Istio. Each Pod has 1 container. Step 6 Remove Bookinfo sample application: kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 7 Enable automatic Envoy sidecar injection. Currently there are 2 method to inject Envoy sidecar inside Istio Mesh: Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment. Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update. Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later: kubectl label namespace default istio-injection=enabled Step 8 Deploy Bookinfo sample application on Istio with Auto Sidecar Injection kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Step 8 Verify deployed application: The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it. kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-zqjbg 2/2 Running 0 13m productpage-v1-6b746f74dc-dj959 2/2 Running 0 12m ratings-v1-b6994bb9-29n4w 2/2 Running 0 12m reviews-v1-545db77b95-p9pgf 2/2 Running 0 12m reviews-v2-7bf8c9648f-4ltkg 2/2 Running 0 12m reviews-v3-84779c7bbc-vhsmw 2/2 Running 0 12m Note Each Pod has now 2 containers. One application container and another is istio-proxy sidecar container. Step 9 Access UI productpage via console: kubectl get services In Gcloud console execute kubectl port-forward : kubectl port-forward --namespace default svc/productpage 8080:9080 Click Web-Preview button in Gcloud, and select preview on port 8080 Click Normal user URL. Success We can access Bookinfo application configured with Istio and Envoy sidecar using private IP via tunnel Step 10 Remove Bookinfo sample application: kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 11 Uninstall Istio istioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f - kubectl delete namespace istio-system kubectl label namespace default istio-injection- 7.2 Deploy Istio using Istio Operator \u00b6 Step 1: Deploy the Istio operator: istioctl operator init Note This command runs the operator by creating the following resources in the istio-operator namespace: The operator custom resource definition The operator controller deployment A service to access operator metrics Necessary Istio operator RBAC rules Step 2: To install the Istio demo configuration profile using the operator, run the following command: kubectl apply -f - <<EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: example-istiocontrolplane spec: profile: demo EOF Note The Istio operator controller begins the process of installing Istio within 90 seconds of the creation of the IstioOperator resource. The Istio installation completes within 120 seconds. Step 3 Verify Operator resource status on Kubernetes Cluster: kubectl get istiooperators -n istio-system Output: NAMESPACE NAME REVISION STATUS AGE istio-system example-istiocontrolplane HEALTHY 25m kubectl describe istiooperators example-istiocontrolplane -n istio-system Spec: Profile: demo Status: Component Status: Base: Status: HEALTHY Egress Gateways: Status: HEALTHY Ingress Gateways: Status: HEALTHY Pilot: Status: HEALTHY Status: HEALTHY Events: <none> kubectl get pods -n istio-system Output: istio-egressgateway-9dc6cbc49-5wkqt 1/1 Running 0 2m54s istio-ingressgateway-7975cdb749-xjc5g 1/1 Running 0 2m54s istiod-77b4d7b55d-j6kb5 1/1 Running 0 3m9s Info Istio control-plane include following components: istiod - contains components such as Citadel and Pilot istio-ingressgateway Istio Ingress Gateway istio-ingressgateway Istio Egress Gateway Verify installation with istioctl CLI: istioctl verify-install Step 5 Enable automatic Envoy sidecar injection. Currently there are 2 method to inject Envoy sidecar inside Istio Mesh: Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment. Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update. Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later: kubectl label namespace default istio-injection=enabled Step 8 Deploy Bookinfo sample application on Istio with Auto Sidecar Injection kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Step 8 Verify deployed application: The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it. kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-zqjbg 2/2 Running 0 13m productpage-v1-6b746f74dc-dj959 2/2 Running 0 12m ratings-v1-b6994bb9-29n4w 2/2 Running 0 12m reviews-v1-545db77b95-p9pgf 2/2 Running 0 12m reviews-v2-7bf8c9648f-4ltkg 2/2 Running 0 12m reviews-v3-84779c7bbc-vhsmw 2/2 Running 0 12m Summary We've leaned to deploy Istio Control plane using istioctl cli and using Istio Operator. We also deployed regular k8s application on the namespace marked with auto injection! Step 9 Cleanup Uninstall Istio Operator kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 10 Uninstall Istio Operator kubectl delete istiooperators.install.istio.io -n istio-system example-istiocontrolplane istioctl operator remove kubectl delete ns istio-system --grace-period=0 --force 8 Commit Readme doc to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy_ycit020_a4/helm and notepad-infrastructure/helm folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"HelmFile Configuration to deploy NotePad and Suppporting Applications\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 9 Cleanup \u00b6 We going to cleanup GCP Service foundation layer with GKE Cluster to avoid excessive cost. cd ~/$MY_REPO/notepad-infrastructure terraform destroy -var-file terraform.tfvars","title":"Assignmen4"},{"location":"020_Assignment_4_Helm_Foundation/#prerequisite","text":"","title":"Prerequisite"},{"location":"020_Assignment_4_Helm_Foundation/#locate-assignment-4","text":"Step 1 Clone ycit020 repo with Kubernetes manifests, which going to use for our work: cd ~/ycit020 # Alternatively: cd ~ & git clone https://github.com/Cloud-Architects-Program/ycit020 git pull cd ~/ycit020/Assignment4/ ls Result You can see Kubernetes manifests and terraform configs Step 1 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 (Optional) If you terraform config is not working you can copy working config from Assignment4 folder to your notepad-infrastructure folder. Step 4 Create structure for Helm Deployments for supporting applications cd ~/$MY_REPO/notepad-infrastructure mkdir helm cat <<EOF> helm/README.md # Helm Files and values to deploy supporting applications EOF Step 5 Copy Assignment 4 deploy_a4 folder to your repo: cd ~/$MY_REPO/ cp -r ~/ycit020/Assignment4/deploy_a4 deploy_ycit020_a4 ls deploy_ycit020_a4 Result You should see k8s-manifest folder Step 6 Create structure for NotePad Helm Chart locations cd ~/$MY_REPO/deploy_ycit020_a4 mkdir helm cat <<EOF> helm/README.md # Helm Chart, HelmFiles and values to deploy `NotePad` application. EOF","title":"Locate Assignment 4"},{"location":"020_Assignment_4_Helm_Foundation/#reserve-static-ip-addresses","text":"In the next few assignments we might deploy many applications and so we will require expose them using Ingress. When you create an Ingress object, you get a stable external IP address that clients can use to access your Services and in turn, your running containers. The IP address is stable in the sense that it lasts for the lifetime of the Ingress object. If you delete your Ingress and create a new Ingress from the same manifest file, you are not guaranteed to get the same external IP address. We would like to have for each student a a permanent IP address that stays the same across deleting your Ingress and creating a new one For that you must reserve a Regional static External IP address and provide it teacher, so we can setup A Record on the DNS for your behalf. Reference: Reserving a static external IP address Terraform resource google_compute_address Step 1: Create folder for static ip terraform configuration: cd ~/$MY_REPO/ mkdir ip-infrastructure Important The reason we creating a new folder for static_ip creation is because we don't want to destroy or delete this IP throughout our training. Step 1: Create Terraform configuration: Set you current PROJECT_ID value here: export PROJECT_ID=<YOUR_PROJECT_ID> Declare Provider: cd ~/$MY_REPO/ip-infrastructure cat << EOF>> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } } } EOF Configure global address resource: cat << EOF>> static_ip.tf provider \"google\" { project = var.gcp_project_id } resource \"google_compute_address\" \"regional_external_ip\" { provider = google name = \"static-ingres-ip\" address_type = \"EXTERNAL\" region = \"us-central1\" } EOF Configure variables: cat <<EOF> variables.tf variable \"gcp_project_id\" { type = string description = \"The GCP Seeding project ID\" default = \"\" } EOF Configure tfvars : gcp_project_id = \"$PROJECT_ID\" Configure outputs: cat <<EOF >> outputs.tf output \"addresses\" { description = \"Global IPv4 address for proxy load balancing to the nearest Ingress controller\" value = google_compute_address.regional_external_ip.address } output \"name\" { description = \"Static IP Name\" value = google_compute_address.regional_external_ip.name } EOF Step 2: Apply Terraform configuration: Initialize: terraform init Plan and Deploy Infrastructure: terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Set variable: cd ~/$MY_REPO/ip-infrastructure export STATIC_IP_ADDRESS=$(terraform output | grep 'addresses' |awk '{ print $3}') export STATIC_IP_NAME=$(terraform output | grep 'name' |awk '{ print $3}') Create readme: cat <<EOF> README.md Generated Static IP for Ingress: * Name - $STATIC_IP_NAME # Used Ingress Manifest * Address - $STATIC_IP_ADDRESS # Used to Configure DNS A Record EOF Important Add information about your static_ip_address in column 3 under your name in this spreadsheet: https://docs.google.com/spreadsheets/d/1XV-wW9Z8KvfWo4io11V_ax2__OImF27Rmlfbyj03T0I/edit?usp=sharing Step 6 Commit deploy folder using the following Git commands: cd ~/$MY_REPO git status git add . git commit -m \"adding documentation for ycit020 assignment 4\"","title":"Reserve Static IP addresses"},{"location":"020_Assignment_4_Helm_Foundation/#commit-to-repository","text":"Step 1 Commit ip-infrastructure and helm folders using the following Git commands: cd ~/$MY_REPO git status git add . git commit -m \"Assignement 4\" Step 2 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"Commit to Repository"},{"location":"020_Assignment_4_Helm_Foundation/#1-install-and-configure-helm-client","text":"","title":"1 Install and Configure Helm Client"},{"location":"020_Assignment_4_Helm_Foundation/#11-install-helm-3","text":"GCP Cloud Shell comes with many common tools pre-installed including helm . Step 1: Verify and validate the version of Helm that is installed: helm --version Output: version.BuildInfo{Version:\"v3.5.0\"} Result Helm 3 is installed Note Until November 2020, two different major versions of Helm were actively maintained. The current stable major version of Helm is version 3. Note Helm follows a versioning convention known as Semantic Versioning (SemVer). In Semantic Versioning, the version number conveys meaning about what you can expect in the release. Because Helm follows this specification, users can expect certain things out of releases simply by carefully reading the version number. At its core, a semantic version has three numerical components and an optional stability marker (for alphas, betas, and release candidates). Here are some examples: v1.0.0 v3.3.2 SemVer represents format of X.Y.Z , where X is a major version, Y is a minor version and Z is a patch release: The major release number tends to be incremented infrequently. It indicates that major changes have been made to Helm, and that some of those changes may break compatibility with previous versions. The difference between Helm 2 and Helm 3 is substantial, and there is work necessary to migrate between the versions. The minor release number indicates feature additions. The difference between 3.2.0 and 3.3.0 might be that a few small new features were added. However, there are no breaking changes between versions. (With one caveat: a security fix might necessitate a breaking change, but we announce boldly when that is the case.) The patch release number indicates that only backward compatible bug fixes have been made between this release and the last one. It is always recommended to stay at the latest patch release. (OPTIONAL) Step 2: If you want to use specific version of helm or want to install helm in you local machine (On macOS and Linux,) use following link to install Helm. The usual sequence of commands for installing this way is as follows: $ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 $ chmod 700 get_helm.sh $ ./get_helm.sh The preceding commands fetch the latest version of the get_helm.sh script, and then use that to find and install the latest version of Helm 3. Alternatively, you can download latest binary of you OS choice here .","title":"1.1 Install Helm 3"},{"location":"020_Assignment_4_Helm_Foundation/#13-deploy-gke-cluster","text":"We going to reuse Terraform configuration to deploy our GKE Cluster. And we assume that terraform.tfvars has been properly configured already with required values. Step 1: Locate Terraform Configuration directory. cd ~/$MY_REPO/notepad-infrastructure Step 2: Initialize Terraform Providers s terraform init Step 3: Increase GKE Node Pool VM size edit terraform.tfvars Update gke_pool_machine_type from e2-small to e2-highcpu-4 , to support larger workloads. Step 4: Review TF Plan: terraform plan -var-file terraform.tfvars Step 5: Create GKE Cluster and Node Pool: terraform apply -var-file terraform.tfvars !!! result GKE Clusters has been created","title":"1.3 Deploy GKE cluster"},{"location":"020_Assignment_4_Helm_Foundation/#14-configure-helm","text":"Helm interacts directly with the Kubernetes API server. For that reason, Helm needs to be able to connect to a Kubernetes cluster. Helm attempts to do this automatically by reading the same configuration files used by kubectl . Helm will try to find this information by reading the environment variable $KUBECONFIG. Step 1 Authenticate to the cluster. export STUDENT_NAME= gcloud container clusters get-credentials gke-$STUDENT_NAME-notepad-dev --region us-central1 Step 2 Test that kubectl client connected to GKE cluster: kubectl get pods Output: No resources found in default namespace. Step 3 Test that helm client connected to GKE cluster: helm list Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION Result As expected there are no Charts has been Installed on our cluster yet Summary Helm 3 has been installed and configured to work with our cluster. Let's deploy some charts!","title":"1.4 Configure Helm"},{"location":"020_Assignment_4_Helm_Foundation/#2-basic-helm-chart-installation","text":"","title":"2 Basic Helm Chart Installation"},{"location":"020_Assignment_4_Helm_Foundation/#21-searching-chart","text":"A Helm chart is an package that can be installed into your Kubernetes cluster. During chart development, you will often just work with a chart that is stored on your local system and later pushed to GitHub. But when it comes to sharing charts, Helm describes a standard format for indexing and sharing information about Helm charts . A Helm chart repository is simply a set of files, reachable over the network, that conforms to the Helm specification for indexing packages. There are huge number of chart repositories on the internet. The easiest way to find the popular repositories is to use your web browser to navigate to the Artifact Hub . There you will find thousands of Helm charts, each hosted on an appropriate repository. Deprecation In the past all charts were located and maintained by Helm Kubernetes Community Git repositories, known as https://github.com/kubernetes/charts, and had 2 types: Stable Incubator All the charts were rendered in Web UI via Helm Hub page. While the central Git Repository to maintain charts was great idea, with fast growing Helm popularity, it become hard to impossible to manage and maintain it by small group of maintainers, as they had to aprove hundreds of PR per day and frustrating for chart contributors as they had to wait several weeks their PR to be reviewed and approved. As a result GitHub project for Helm stable and incubator charts as well as [Helm Hub] has been deprecated and archived. All the charts are now maintained by independent contributors in their subsequent repo's. (e.g vault, is under hashicorp/vault-helm repo) and the central place to find all active and official Charts can be foind in Artifact Hub . Important Helm 2 came with a Helm repository installed by default. The stable chart repository was at one time the official source of production-ready Helm charts. As we discussed above stable chart repository has been now deprecated. In Helm 3, there is no default repository. Users are encouraged to use the Artifact Hub to find what they are looking for and then add their preferred repositories. Step 1: Helm provides native way to search charts from CLI in artifacthub.io cd ~/$MY_REPO/notepad-infrastructure/helm helm search hub drupal Output: https://artifacthub.io/packages/helm/bitnami/drupal 10.2.30 9.2.3 One of the most versatile open source content m... https://artifacthub.io/packages/helm/cetic/drupal 0.1.0 1.16.0 Drupal is a free and open-source web content ma.. Result Search is a good way to find existing Helm packages Step 2: Use the link to navigate to Artifact Hub Drupal chart: https://artifacthub.io/packages/helm/bitnami/drupal Note Drupal is OSS content management systems that can be installed this days on K8s. Result: The Artifact Page opened with information about the chart, it's parameters and how to use the chart. Step 3: Review Github source Code of the chart: https://github.com/bitnami/charts/tree/master/bitnami/drupal Result: Github page with Drupal Helm Chart itself, where you can browse the templates , values.yaml , Chart.yaml to understand how this chart is actually working and if you have any issues with the chart this would be the right location to open the issue or send a PR with new feature if you decide to contribute.","title":"2.1 Searching Chart"},{"location":"020_Assignment_4_Helm_Foundation/#22-adding-a-chart-repository","text":"Once you found a chart, it's logical to install it. However first step you need to do is to add a Chart Repository. Step 1: Adding a Helm chart is done with the helm repo add command: helm repo add bitnami https://charts.bitnami.com/bitnami Note Bitnami is company well known to package application for Any Platforms and Cloud environments. With popularity of Helm, Bitnami developers were among the core contributors who designed the Helm repository system. They have contributed to the establishment of Helm\u2019s best practices for chart development and have written many of the most widely used charts. Bitnami is now part of VMware, provides IT organizations with an enterprise offering that is secure, compliant, continuously maintained and customizable to your organizational policies. Step 2: Now, we can verify that the Bitnami repository exists by running a helm repo list command: helm repo list Output: NAME URL bitnami https://charts.bitnami.com/bitnami Result This command shows us all of the repositories installed for Helm. Right now, we see only the Bitnami repository that we just added.","title":"2.2 Adding a Chart Repository"},{"location":"020_Assignment_4_Helm_Foundation/#23-helm-chart-installation","text":"Step 1: At very minimum, installing a chart in Helm requires just 2 pieces of information: the name of the installation and the chart you want to install: helm install mywebsite bitnami/drupal Output: NAME: mywebsite LAST DEPLOYED: Sun Aug 8 08:25:29 2021 NAMESPACE: prometheus STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ******************************************************************* *** PLEASE BE PATIENT: Drupal may take a few minutes to install *** ******************************************************************* 1. Get the Drupal URL: NOTE: It may take a few minutes for the LoadBalancer IP to be available. Watch the status with: 'kubectl get svc --namespace prometheus -w mywebsite-drupal' export SERVICE_IP=$(kubectl get svc --namespace prometheus mywebsite-drupal --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\") echo \"Drupal URL: http://$SERVICE_IP/\" 2. Get your Drupal login credentials by running: echo Username: user echo Password: $(kubectl get secret --namespace prometheus mywebsite-drupal -o jsonpath=\"{.data.drupal-password}\" | base64 --decode) Result When using Helm, you will see that output for each installation. A good chart would provide helpful notes on how to connect to the deployed solution. Tip You can get notes information any time after helm installation using helm get notes mywebsite command. Step 2: Follow your Drupal Helm Chart notes Instruction to access Website 1. Get the Drupal URL: NOTE: It may take a few minutes for the LoadBalancer IP to be available. Watch the status with: 'kubectl get svc --namespace test -w mywebsite-drupal' export SERVICE_IP=$(kubectl get svc --namespace test mywebsite-drupal --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\") echo \"Drupal URL: http://$SERVICE_IP/\" Success We can access My blog website and login with provider user and password. You can start your own blog now, that runs on Kubernetes Step 3: List deployed Kubernetes resources: kubectl get all --namespace test kubectl get pvc --namespace test Summary Our Drupal website consist of MariaDB statefulset , Drupal deployment , 2 pvc s and services . Mywebsite Drupal service is type LoadBalancer and that's how we able to access it. Step 4: List installed chart: helm list Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION mywebsite default 1 2021-08-08 08:25:29.625862 -0400 EDT deployed drupal-10.2.30 9.2.3 Note Like other commands, helm list is namespace aware. By default, Helm uses the namespace your Kubernetes configuration file sets as the default. Usually this is the namespace named default . Step 5: Deploy same chart in namespace test : kubectl create ns test helm install --namespace test mywebsite bitnami/drupal --create-namespace Tip By adding --create-namespace , indicates to Helm that we acknowledge that there may not be a namespace with that name already, and we just want one to be created. List deployed chart in namespace test : helm list --namespace test Summary In Helm 2, instance names were cluster-wide. You could only have an instance named mywebsite once per cluster. In Helm 3, naming has been changed. Now instance names are scoped to Kubernetes namespaces. We could install 2 instances named mywebsite as long as they each lived in a different namespace. Step 5: Cleanup Helm Drupal deployments using helm uninstall command: helm uninstall mywebsite helm uninstall mywebsite -n test kubectl delete ns test","title":"2.3 Helm Chart Installation"},{"location":"020_Assignment_4_Helm_Foundation/#3-advanced-helm-chart-installation","text":"","title":"3 Advanced Helm Chart Installation"},{"location":"020_Assignment_4_Helm_Foundation/#321-deploy-nginx-ingress-chart-with-custom-configuration","text":"Let's deploy another Helm application on our Kubernetes cluster: Ingress Nginx - is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer. In the previous classes we used Google implementation of Ingress - GKE Ingress for HTTP(S) Load Balancing . While this solutions provides managed Ingress experience and advanced features like Cloud Armor, DDoS protection and Identity aware proxy. Ingress Nginx Controller is popular solution and has a lot of features and integrations. If you want to deploy Kubernetes Application on different cloud providers or On-prem the same way, Nginx Ingress Controller becomes a default option. Our task is to configure Ingress Nginx using type:LoadBalancer with Regional Static IP configured in the section ### Reserve Static IP addresses . Additionally we want to enable metrics service that will fetch Prometheus monitoring metrics from ingress and disable admissionWebhooks configuration. Step 1: Let's search ingress-nginx in artifacthub.io helm search hub ingress-nginx Output: URL CHART VERSION APP VERSION DESCRIPTION https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx 4.0.0 1.0.0-beta.1 Ingress controller for Kubernetes using NGINX https://artifacthub.io/packages/helm/api/ingress-nginx 3.29.1 0.45.0 Ingress controller for Kubernetes using NGINX Result We going to select the first ingress-nginx chart that is maintained by Kubernetes Community Step 2: Review Hub Page details about this chart and locate information how to add repository: https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx Result: The Artifact Page opened with information about the chart, it's parameters, and how to use the chart itself Step 3: Add ingress-nginx Repo: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update Step 4: Oftentimes, searching is a useful way to find not only what charts can be installed, but what versions are available: helm search repo ingress-nginx --versions | head Output: NAME CHART VERSION APP VERSION DESCRIPTION ingress-nginx/ingress-nginx 3.35.0 0.48.1 Ingress controller for Kubernetes using NGINX a... ingress-nginx/ingress-nginx 3.34.0 0.47.0 Ingress controller for Kubernetes using NGINX a... ingress-nginx/ingress-nginx 3.33.0 0.47.0 Ingress controller for Kubernetes using NGINX a... .... Note By default, Helm tries to install the latest stable release of a chart, but you can override this behavior and install a specific version of a chart. Thus it is often useful to see not just the summary info for a chart, but exactly which versions exist for a chart. Every new version of the chart can be bring fixes and new changes, so for production use it's better to go with tested version and pin installation version. Summary We going with latest listed official version of the chart 3.35.0 of ingress-nginx Chart.","title":"3.2.1 Deploy NGINX Ingress Chart with Custom Configuration"},{"location":"020_Assignment_4_Helm_Foundation/#322-download-and-inspect-chart-locally","text":"Step 1: Pull ingress-nginx Helm Chart of specific version to Local filesystem: cd ~/$MY_REPO/notepad-infrastructure/helm helm pull ingress-nginx/ingress-nginx --version 3.35.0 tar -xvzf ingress-nginx-3.35.0.tgz Step 2: See the tree structure of the chart sudo apt-get install tree tree -L 2 ingress-nginx Output: ingress-nginx \u251c\u2500\u2500 CHANGELOG.md \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 OWNERS \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ci \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 admission-webhooks \u2502 \u251c\u2500\u2500 clusterrole.yaml \u2502 \u251c\u2500\u2500 clusterrolebinding.yaml \u2502 \u251c\u2500\u2500 controller-configmap-addheaders.yaml \u2502 \u251c\u2500\u2500 controller-configmap-proxyheaders.yaml \u2502 \u251c\u2500\u2500 controller-configmap-tcp.yaml \u2502 \u251c\u2500\u2500 controller-configmap-udp.yaml \u2502 \u251c\u2500\u2500 controller-configmap.yaml \u2502 \u251c\u2500\u2500 controller-daemonset.yaml \u2502 \u251c\u2500\u2500 controller-deployment.yaml \u2502 \u251c\u2500\u2500 controller-hpa.yaml \u2502 \u251c\u2500\u2500 controller-ingressclass.yaml \u2502 \u251c\u2500\u2500 controller-keda.yaml \u2502 \u251c\u2500\u2500 controller-poddisruptionbudget.yaml \u2502 \u251c\u2500\u2500 controller-prometheusrules.yaml \u2502 \u251c\u2500\u2500 controller-psp.yaml \u2502 \u251c\u2500\u2500 controller-role.yaml \u2502 \u251c\u2500\u2500 controller-rolebinding.yaml \u2502 \u251c\u2500\u2500 controller-service-internal.yaml \u2502 \u251c\u2500\u2500 controller-service-metrics.yaml \u2502 \u251c\u2500\u2500 controller-service-webhook.yaml \u2502 \u251c\u2500\u2500 controller-service.yaml \u2502 \u251c\u2500\u2500 controller-serviceaccount.yaml \u2502 \u251c\u2500\u2500 controller-servicemonitor.yaml \u2502 \u251c\u2500\u2500 default-backend-deployment.yaml \u2502 \u251c\u2500\u2500 default-backend-hpa.yaml \u2502 \u251c\u2500\u2500 default-backend-poddisruptionbudget.yaml \u2502 \u251c\u2500\u2500 default-backend-psp.yaml \u2502 \u251c\u2500\u2500 default-backend-role.yaml \u2502 \u251c\u2500\u2500 default-backend-rolebinding.yaml \u2502 \u251c\u2500\u2500 default-backend-service.yaml \u2502 \u251c\u2500\u2500 default-backend-serviceaccount.yaml \u2502 \u2514\u2500\u2500 dh-param-secret.yaml \u2514\u2500\u2500 values.yaml Result Typical structure of the helm chart, where: The Chart.yaml file contains metadata and some functionality controls for the chart templates folder contains all yaml manifests in Go templates and used to generate Kubernetes manifests values.yaml contains default values applied during the rendering. The NOTES.txt file is a special template. When a chart is installed, the NOTES.txt template is rendered and displayed rather than being installed into a cluster.","title":"3.2.2 Download and inspect Chart locally"},{"location":"020_Assignment_4_Helm_Foundation/#322-helm-template","text":"As a reminder our goal it customize Nginx deployment to configure Ingress Nginx using type:LoadBalancer with Regional Static IP configured in the section ### Reserve Static IP addresses . Additionally we want to enable metrics service that will fetch Prometheus monitoring metrics from ingress and disable admissionWebhooks configuration. Step 1: Review values.yaml cd ~/$MY_REPO/notepad-infrastructure/helm/ingress-nginx edit values.yaml Result It is pretty confusing at this point to understand what this chart is going to deploy, it would of been great to render to YAML format first. Step 2: Execute helm template command and store output in render.yaml helm template ingress-nginx/ingress-nginx --version 3.35.0 > render.yaml Note During helm template, Helm never contacts a remote Kubernetes server, hence the chart only has access to default Kubernetes kinds. The template command always acts like an installation. Review rendered values: edit render.yaml grep \"Source\" render.yaml Result helm template designed to isolate the template rendering process of Helm from the installation. The template command performs following phases of Helm Lifecycle: loads the chart determines the values renders the templates formats to YAML Step 3: Let's first create a configuration that will disable admissionWebhooks . Review looking in values.yaml for admissionWebhooks configuration: grep -A10 admissionWebhooks values.yaml Output: admissionWebhooks: annotations: {} enabled: true failurePolicy: Fail # timeoutSeconds: 10 port: 8443 certificate: \"/usr/local/certificates/cert\" key: \"/usr/local/certificates/key\" namespaceSelector: {} objectSelector: {} Result admissionWebhooks enabled: true Step 3: Let's create custom_values.yaml file with admissionWebhooks disabled cat << EOF>> custom_values.yaml controller: admissionWebhooks: enabled: false EOF Execute helm template and store output in new file render2.yaml : helm template ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 > render2.yaml Now comparing this 2 render files you can see the what change in configuration has resulted in the rendered file: diff render.yaml render2.yaml This is the YAML files that will be generated after this change: grep \"Source\" render2.yaml Output: # Source: ingress-nginx/templates/controller-serviceaccount.yaml # Source: ingress-nginx/templates/controller-configmap.yaml # Source: ingress-nginx/templates/clusterrole.yaml # Source: ingress-nginx/templates/clusterrolebinding.yaml # Source: ingress-nginx/templates/controller-role.yaml # Source: ingress-nginx/templates/controller-rolebinding.yaml # Source: ingress-nginx/templates/controller-service.yaml # Source: ingress-nginx/templates/controller-deployment.yaml Step 4: Now, let's add a custom Configuration to controller-service.yaml to use Static loadBalancerIP , we've configured in step ### Reserve Static IP addresses . First we need to locate the correct service parameters: grep -A20 \"service:\" values.yaml Output Shows that we have 4 different services in values file Let's narrow our search: grep -A20 \"service:\" values.yaml | grep \"List of IP address\" Output: ## List of IP addresses at which the controller services are available ## List of IP addresses at which the stats-exporter service is available ## List of IP addresses at which the default backend service is available Output: service: enabled: true annotations: {} labels: {} # clusterIP: \"\" ## List of IP addresses at which the controller services are available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] # loadBalancerIP: \"\" loadBalancerSourceRanges: [] Result It seems controller is first service: in values.yaml and currently is it has loadBalancerIP section commented Step 5: Let's add loadBalancerIP IP configuration to custom_values.yaml Set variable: cd ~/$MY_REPO/ip-infrastructure export STATIC_IP_ADDRESS=$(terraform output | grep 'addresses' |awk '{ print $3}') echo $STATIC_IP_ADDRESS Add custom values: cd ~/$MY_REPO/notepad-infrastructure/helm/ingress-nginx cat << EOF>> custom_values.yaml service: loadBalancerIP: $STATIC_IP_ADDRESS EOF Our final custom_values.yaml should look as following: controller: admissionWebhooks: enabled: false service: loadBalancerIP: 35.X.X.X metrics: enabled: true Execute helm template and store output in new file render3.yaml : helm template ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 > render3.yaml Show the difference in rendered file: diff render2.yaml render3.yaml Output: > loadBalancerIP: 35.X.X.X Ensure that this change is in fact belongs to controller-service.yaml : grep -C16 \"loadBalancerIP:\" render3.yaml Result We configured correct service that will Ingress Controller service with type: Loadbalancer and static IP with previously generated with terraform. Summary helm template is great tool to render you charts to YAML. Tip You can use Helm as packaging you charts only. And then use helm template to generate actual YAMLs and apply them with kubectl apply","title":"3.2.2 Helm Template"},{"location":"020_Assignment_4_Helm_Foundation/#323-dry-runs","text":"Before applying configuration to Kubernetes Cluster it is good idea to Dry-Run it for Errors. This is especially important if you doing Upgrades. The dry-run feature provides Helm users a way to debug the output of a chart before it is sent on to Kubernetes. With all of the templates rendered, you can inspect exactly what would have been submitted to your cluster. And with the release data, you can verify that the release would have been created as you expected. Here is some of the dry-run working principals: --dry-run mixes non-YAML information with the rendered templates. This means the data has to be cleaned up before being sent to tools like kubectl. A --dry-run on upgrade can produce different YAML output than a --dry-run on install, and this can be confusing. It contacts the Kubernetes API server for validation, which means Helm has to have Kubernetes credentials even if it is just used to --dry-run a release. It also inserts information into the template engine that is cluster-specific. Because of this, the output of some rendering processes may be cluster-specific. Difference between dry-run and template is that the dry-run contacts Kubernetes API. It is good idea to use dry-run prior deployment and upgrade as it creates mutated output, while template doesn't contacts API and can to pure rendering. Step 1: Execute our deployment with dry-run command: helm install ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --dry-run Output: NAME: ingress-nginx LAST DEPLOYED: Tue Aug 10 07:22:56 2021 NAMESPACE: default STATUS: pending-install REVISION: 1 TEST SUITE: None HOOKS: MANIFEST: --- # Source: ingress-nginx/templates/controller-serviceaccount.yaml ........ NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace default get services -o wide -w ingress-nginx-controller' An example Ingress that makes use of the controller: ... If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: <base64 encoded cert> tls.key: <base64 encoded key> type: kubernetes.io/tls At the top of the output, it will print some information about the release in our case it tells what phase of the release it is in (pending-install), and the revision number. Next, after the informational block, all of the rendered templates are dumped to standard output. Finally, at the bottom of the dry-run output, Helm prints the user-oriented release notes: Note --dry-run dumps the output validates, but doesn't deploy actual chart. Step 2: Finally let's deploy Nginx Ingress Charts with our parameters in ingress-nginx namespace kubectl create ns ingress-nginx helm install ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx Step 3: Verify Helm Deployment: helm list Follow Installation note and verify Ingress-Controller Service: kubectl --namespace ingress-nginx get services -o wide ingress-nginx-controller Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 172.10.0.213 35.192.101.76 80:30930/TCP,443:30360/TCP 3m9s","title":"3.2.3 Dry-Runs"},{"location":"020_Assignment_4_Helm_Foundation/#324-deploy-drupal-using-nginx-ingress-controller","text":"Let's test our newly configured ingress-nginx-controller and expose drupal application using ingress instead of LoadBalancer . Check if STATIC_IP_ADDRESS variable is set: echo $STATIC_IP_ADDRESS If not set it with you Static IP address value. cd ~/$MY_REPO/notepad-infrastructure/helm/ kubectl delete pvc data-mywebsite-mariadb-0 cat << EOF>> drupal_ing_values.yaml ingress: annotations: {kubernetes.io/ingress.class: \"nginx\"} enabled: true hostname: $STATIC_IP_ADDRESS.nip.io path: / pathType: ImplementationSpecific EOF Note Here we using nip.io domain that provides simple wildcard DNS for any IP Address, however if you provided you STATIC_IP to teacher, they will generate following DNS, and you can replace hostname: with value $student-name.cloud-montreal.ca helm install mywebsite bitnami/drupal --values drupal_ing_values.yaml 1. Get the Drupal URL: You should be able to access your new Drupal installation through http://35.X.X.X.nip.io/ Success Drupal site is now accessible via Ingress. Our Nginx Ingress Controller has been setup correctly! Uninstall Drupal: helm uninstall mywebsite kubectl delete pvc data-mywebsite-mariadb-0","title":"3.2.4 Deploy Drupal using Nginx Ingress Controller"},{"location":"020_Assignment_4_Helm_Foundation/#325-using-set-values-and-upgrading-charts","text":"In addition to --value option, there is a second flag that can be used to add individual parameters to an install or upgrade . The --set flag takes one or more values directly. They do not need to be stored in a YAML file. Step 1: Let's update our ingress-nginx release with new parameter --set controller.image.pullPolicy=Always , but first we want to render template with and without parameter to see what the change will be applied: helm template ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx > render4.yaml helm template ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=Always > render5.yaml diff render4.yaml render5.yaml Output: < imagePullPolicy: IfNotPresent --- > imagePullPolicy: Always Step 2: Let's apply update to our ingress-nginx release. For that we going to use helm upgrade command: helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=Always Verify that upgrade was successful: helm list --namespace ingress-nginx Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ingress-nginx ingress-nginx 2 2021-08-10 10:48:35.265506 -0400 EDT deployed ingress-nginx-3.35.0 0.48.1 Result We can see that Revision change to 2 Note When we talk about upgrading in Helm, we talk about upgrading an installation, not a chart. An installation is a particular instance of a chart in your cluster. When you run helm install , it creates the installation. To modify that installation, use helm upgrade . This is an important distinction to make in the present context because upgrading an installation can consist of two different kinds of changes: * You can upgrade the version of the chart * You can upgrade the configuration of the installation In this case we upgrading the configuration of the installation. Extra If you interested upgrade chart version of ingress-nginx to the next available release or 3.34.0 , give it a try.","title":"3.2.5  Using  --set values and Upgrading Charts"},{"location":"020_Assignment_4_Helm_Foundation/#325-listing-releases-history-rollbacks","text":"Step 1: Let's see if we can upgrade our release with wrong value pullPolicy=NoSuchPolicy that doesn't exist on Kubernetes. We will first run in dry-run mode helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=NoSuchPolicy --dry-run Result Release \"ingress-nginx\" has been upgraded. Happy Helming! Let's now apply same config without --dry-run mode: helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=NoSuchPolicy Output: Error: UPGRADE FAILED: cannot patch \"ingress-nginx-controller\" with kind Deployment: Deployment.apps \"ingress-nginx-controller\" is invalid: spec.template.spec.containers[0].imagePullPolicy: Unsupported value: \"NoSuchPolicy\": supported values: \"Always\", \"IfNotPresent\", \"Never\" Failed As the error message indicates, a pull policy cannot be set to NoSuchPolicy. This error came from the Kubernetes API server, which means Helm submitted the manifest, and Kubernetes rejected it. So our release should be in a failed state. Verify with helm list to confirm failed state: helm list Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ingress-nginx ingress-nginx 3 2021-08-10 11:03:05.224405 -0400 EDT failed ingress-nginx-3.35.0 0.48.1 Step 2: List all releases with helm history : helm history ingress-nginx -n ingress-nginx REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Tue Aug 10 08:47:32 2021 superseded ingress-nginx-3.35.0 0.48.1 Install complete 2 Tue Aug 10 10:48:35 2021 deployed ingress-nginx-3.35.0 0.48.1 Upgrade complete 3 Tue Aug 10 11:03:05 2021 failed ingress-nginx-3.35.0 0.48.1 Upgrade \"ingress-nginx\" failed: cannot patch \"ingress-nginx-controller\" with kind Deployment: Deployment.apps \"ingress-nginx-controller\" is invalid: spec.template.spec.containers[0].imagePullPolicy: Unsupported value: \"NoSuchPolicy\": supported values: \"Always\", \"IfNotPresent\", \"Never\" Info During the life cycle of a release, it can pass through several different statuses. Here they are, approximately in the order you would likely see them: pending-install Before sending the manifests to Kubernetes, Helm claims the installation by creating a release (marked version 1) whose status is set to pending-install. deployed As soon as Kubernetes accepts the manifest from Helm, Helm updates the release record, marking it as deployed. pending-upgrade When a Helm upgrade is begun, a new release is created for an installation (e.g., v2), and its status is set to pending-upgrade. superseded When an upgrade is run, the last deployed release is updated, marked as superseded, and the newly upgraded release is changed from pending-upgrade to deployed. pending-rollback If a rollback is created, a new release (e.g., v3) is created, and its status is set to pending-rollback until Kubernetes accepts the release manifest. Then it is marked deployed and the last release is marked superseded. uninstalling When a helm uninstall is executed, the most recent release is read and then its status is changed to uninstalling. uninstalled If history is preserved during deletion, then when the helm uninstall is complete, the last release\u2019s status is changed to uninstalled. failed Finally, if during any operation, Kubernetes rejects a manifest submitted by Helm, Helm will mark that release failed. Step 3: Rollback release with helm rollback . From the error, we know that the release failed because we supplied an invalid image pull policy. So of course we could correct this by simply running another helm upgrade. But imagine a case where the cause of error was not readily available. Rather than leave the application in a failed state while diagnosing the problem, it would be nice to simply revert back to the release that worked before. helm rollback ingress-nginx 2 -n ingress-nginx helm history ingress-nginx -n ingress-nginx helm list 4 Tue Aug 10 11:16:24 2021 deployed ingress-nginx-3.35.0 0.48.1 Rollback to 2 --- ingress-nginx ingress-nginx 4 2021-08-10 11:16:24.424371 -0400 EDT deployed ingress-nginx-3.35.0 0.48.1 Rollback was a success! Happy Helming! Success This command tells Helm to fetch the ingress-nginx version 2 release, and resubmit that manifest to Kubernetes. A rollback does not restore to a previous snapshot of the cluster. Helm does not track enough information to do that. What it does is resubmit the previous configuration, and Kubernetes attempts to reset the resources to match.","title":"3.2.5  Listing Releases, History, Rollbacks"},{"location":"020_Assignment_4_Helm_Foundation/#325-upgrade-release-with-helm-diff-plugin","text":"While Helm provides a lot of functionality out of the box. Community contributes a lot of great functionality via helm plugins. Plugins allow you to add extra functionality to Helm and integrate seamlessly with the CLI, making them a popular choice for users with unique workflow requirements. There are a number of third-party plugins available online for common use cases, such as secrets management. In addition, plugins are incredibly easy to build on your own for unique, one-off tasks. Helm plugins are external tools that are accessible directly from the Helm CLI. They allow you to add custom subcommands to Helm without making any modifications to Helm\u2019s Go source code. Many third-party plugins are made open source and publicly available on GitHub. Many of these plugins use the \u201chelm-plugin\u201d tag/topic to make them easy to find. Refer to the documentation for Helm plugins on GitHub Let's Upgrade our running Ingress Nginx chart with metrics service that will enable Prometheus to scrape metrics from our Nginx. Step 1: Update custom_values.yaml with following information: cat << EOF>> custom_values.yaml metrics: enabled: true EOF Step 2: So far we've used helm template to render and compare manifests. However most of the time you might not be able to do it during upgrades. However you want to have a clear understanding what will be upgraded if you change something. That's where Helm Diff Plugin can be handy. Helm Diff plugin giving your a preview of what a helm upgrade would change. It basically generates a diff between the latest deployed version of a release and a helm upgrade --dry-run Install Helm Diff Plugin: helm plugin install https://github.com/databus23/helm-diff helm diff upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx Output: - imagePullPolicy: Always + imagePullPolicy: IfNotPresent + protocol: TCP + - name: metrics + containerPort: 1025 + # Source: ingress-nginx/templates/controller-service-metrics.yaml + apiVersion: v1 + kind: Service ... Result Amazing! This looks like terraform plan but for helm :) Important Another discovery from above printout is that since we forgot to add --set imagePullPolicy command, our value will be reverted with upgrade. This is really important to understand as you configuration maybe lost if you use --set Step 3: Let's upgrade our ingress-nginx release: helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx helm list -n ingress-nginx Success Our ingress-nginx is ready to run. Going forward we going to always deploy this chart. Summary So far we've learned: How to customize helm chart deployment using --set and --values using values.yaml Using values.yaml preferable mehtod in order to achieve reproducible deployments How to upgrade and rollback releases Helm template and dry-run Helm Diff Plugin","title":"3.2.5  Upgrade Release with Helm Diff Plugin"},{"location":"020_Assignment_4_Helm_Foundation/#326-install-minio-with-helm","text":"Let's deploy another Helm application on our Kubernetes cluster: Minio - high-performance, S3 compatible object storage. helm install myminio bitnami/minio Follow your Minio Helm Chart notes Instruction to access Website: To access the MinIO&reg; web UI: - Get the MinIO&reg; URL: echo \"MinIO&reg; web URL: http://127.0.0.1:9000/minio\" kubectl port-forward --namespace default svc/myminio 8080:9000 In Gcloud console execute kubectl port-forward : kubectl port-forward --namespace default svc/myminio 8080:9000 Result We can access Minio UI using private IP via tunnel and we have to use autogenerated ACCESS_KEY and SECRET_KEY. Uninstall the Minio chart: helm uninstall myminio bitnami/minio While it looks easy to start with helm and deploy any charts, most of the time you want to have a full control of the deployment process. You want to specify, tested version, safe values or configurations that make sense for you or your organization.","title":"3.2.6 Install Minio with Helm"},{"location":"020_Assignment_4_Helm_Foundation/#327-customize-minio-installation-task","text":"Task N1: Customize Minio Installation with following values: * Deploy `Minio` of specific version: `7.1.7` * Expose Minio using `Ingress` resource: * ingress.class: \"nginx\" * path: \"/\" * pathType: \"ImplementationSpecific\" * hostname: $STATIC_IP_ADDRESS.nip.io * Specify custom `ACCESS_KEY` for `Minio` frontend: `myaccesskey` * Specify custom `SECRET_KEY` for `Minio` frontend: `mysecretkey` Step 1: Use the following Minio Chart: helm search hub minio URL CHART VERSION APP VERSION DESCRIPTION https://artifacthub.io/packages/helm/bitnami/minio 7.1.7 2021.6.17 Bitnami Object Storage based on MinIO&reg ..... Result We going to choose bitnami package again as it's top of the list and seems maintained the most. Step 2: Since we already added bitnami repository locally, let's verify if we can list minio in it: helm search repo minio Result We can now go ahead and install minio Step 4: Pull Minio Helm Chart of specific version to Local filesystem to work with values file. cd ~/$MY_REPO/notepad-infrastructure/helm helm pull bitnami/minio --version 7.1.7 cd minio cat <<EOF> custom_values.yaml TODO: Finish the remaining steps EOF TODO: Complete custom_values.yaml and test deployment of Minio . Test Minio by creating bucket and uploading object to it. Step 5: Test and Install Minio Chart with Custom Configuration helm install myminio bitnami/minio --values custom_values.yaml --version 7.1.6 Follow your Minio Helm Chart notes Instruction to access Website. Step 6 Commit deploy_ycit020_a4/helm and notepad-infrastructure/helm folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Helm values for Minio\" Step 7 Push commit to the Cloud Source Repositories: git push origin master Step 8 Uninstall the Minio chart: helm uninstall myminio bitnami/minio","title":"3.2.7 Customize Minio Installation Task"},{"location":"020_Assignment_4_Helm_Foundation/#4-creating-notepad-helm-charts","text":"So far with learned how to deploy existing charts from the Artifact Hub. However sometimes you or your company needs to build software that require's to be distributed and shared externally, as well as have lifecycle and release management. Helm Charts becomes are valuable option for that, specifically if you application is based of containers! Imagine that our solution is Go-based NotePad has first customer that wants to deploy it on their system and they requested delivery via Helm Charts that available on GCP based Helm Repository. To achieve such task we going to create 2 Charts: * `gowebapp-mysql` chart * `gowebapp` chart And store them in Google Artifact Registry that provides Helm 3 OCI Repository.","title":"4 Creating NotePad Helm Charts"},{"location":"020_Assignment_4_Helm_Foundation/#41-design-gowebapp-mysql-chart","text":"","title":"4.1 Design gowebapp-mysql chart"},{"location":"020_Assignment_4_Helm_Foundation/#411-create-gowebapp-mysql-chart","text":"As scary as it sounds creating a new basic helm chart is 5 minute thing! We going to learn 2 quick methods to create helm charts, that will help you to save time and get started with helm quickly. Helm includes the helm create command to make it easy for you to create a chart of your own, and it\u2019s a great way to get started. The create command creates a chart for you, with all the required chart structure and files. These files are documented to help you understand what is needed, and the templates it provides showcase multiple Kubernetes manifests working together to deploy an application. In addition, you can install and test this chart right out of the box. Step 1: Create a new chart: cd ~/$MY_REPO/deploy_ycit020_a4/helm helm create gowebapp-mysql cd gowebapp-mysql tree Output: \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 hpa.yaml \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u251c\u2500\u2500 service.yaml \u2502 \u251c\u2500\u2500 serviceaccount.yaml \u2502 \u2514\u2500\u2500 tests \u2502 \u2514\u2500\u2500 test-connection.yaml \u2514\u2500\u2500 values.yaml Result This command creates a new Nginx chart, with a name of your choice, following best practices for a chart layout. Since Kubernetes clusters can have different methods to expose an application, this chart makes the way Nginx is exposed to network traffic configurable so it can be exposed in a wide variety of clusters. The chart has following structure: The Chart.yaml file contains metadata and some functionality controls for the chart The charts folder currently empty may container charts dependency of the top level chart. For example we could of make our gowebapp-mysql as dependency chart for gowebapp templates folder contains all yaml manifests in Go templates and used to generate Kubernetes manifests values.yaml contains default values applied during the rendering. The NOTES.txt file is a special template. When a chart is installed, the NOTES.txt template is rendered and displayed rather than being installed into a cluster. _helpers.tpl - helper templates for your other templates (e.g creating same labers across all charts). Files with _ are not rendered to Kubernetes object definitions, but are available everywhere within other chart templates for use. Here we going to show a quick way to create a Helm chart without adding any templating. Step 2: Delete templates folder and create empty one: rm -rf templates mkdir templates cd templates Step 3: Copy existing gowebapp-mysql manifests to templates folder: cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-mysql-pvc.yaml . # PVC cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/secret-mysql.yaml . # Secret cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-mysql-service.yaml . # Service cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-mysql-deployment.yaml . # Deployment Step 4: Lint Helm Chart: When developing charts, especially when working with YAML templates, it can be easy to make a mistake or miss something. To help you catch errors, bugs, style issues, and other suspicious elements, the Helm client includes a linter. This linter can be used during chart development and as part of any testing processes. To use the linter, use the lint command on a chart as a directory or a packaged archive: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ helm lint gowebapp-mysql Output: ==> Linting gowebapp-mysql [INFO] Chart.yaml: icon is recommended 1 chart(s) linted, 0 chart(s) failed Step 5: Install Helm Chart locally: Create dev namespace: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp-mysql kubectl create ns dev kubectl config set-context --current --namespace=dev Render the manifest locally and compare to original manifests: helm template gowebapp-mysql . > render.yaml Install the chart: helm install gowebapp-mysql . Output: NAME: gowebapp-mysql LAST DEPLOYED: Tue Aug 10 15:25:44 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None helm list kubectl get all kubectl get pvc Success We've deployed our own helm chart locally. While we haven't used templating of the chart at all, we have a helm chart that can be installed and upgraded using helm release features.","title":"4.1.1  Create gowebapp-mysql chart"},{"location":"020_Assignment_4_Helm_Foundation/#42-design-gowebapp-chart","text":"","title":"4.2  Design gowebapp chart."},{"location":"020_Assignment_4_Helm_Foundation/#421-create-gowebapp-chart","text":"We going to use another method to deploy our second chart gowebapp . In this case we going to use nginx template provided by help team. Task N2: Create gowebapp Helm chart: * Configure `Deployment` template in `values.yaml` * Configure `Service` template in `values.yaml` * Disable `Service account` template in `values.yaml` * Configure `Ingress` template in `values.yaml`: * ingress.class: \"nginx\" * path: \"/\" * pathType: \"ImplementationSpecific\" * host: $STATIC_IP_ADDRESS.nip.io * Templatize the ConfigMap Resource * Ensure the chart is deployable * Ensure `gowebapp` in browser Step 1 Create a new chart: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ helm create gowebapp cd gowebapp tree Output: \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 hpa.yaml \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u251c\u2500\u2500 service.yaml \u2502 \u251c\u2500\u2500 serviceaccount.yaml \u2502 \u2514\u2500\u2500 tests \u2502 \u2514\u2500\u2500 test-connection.yaml \u2514\u2500\u2500 values.yaml edit values.yaml","title":"4.2.1  Create gowebapp chart"},{"location":"020_Assignment_4_Helm_Foundation/#422-template-the-deployment","text":"Step 1 Update the replicaCount value to 2 in the values.yaml file: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Step 2 Update the repository and tag section to point to your gowebapp docker image in the values.yaml file: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Step 3 Update the resources section in the values.yaml file to include the resource requests and limits the gowebapp application needs: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml See the reference gowebapp-deployment.yaml : cat ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-deployment.yaml Step 4 Update the livness and readiness sections to match what you have in the gowebapp deployment.yaml See the reference gowebapp-deployment.yaml : cat ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-deployment.yaml Update livness and readiness sections for template deployment.yaml : cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/deployment.yaml Step 5 Notice that the deployment.yaml file does not have an environment variable section for secrets , so let's add one. For this chart we will assume that this section is optional based on whether or not a secrets section exist in the Values.yaml file. Step 5-a Include the following in the deployment template: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/deployment.yaml And add following code snippet in the appropriate location: {{- if .Values.secrets }} - env: - name: {{.Values.secrets.name}} valueFrom: secretKeyRef: name: {{.Values.secrets.secretReference.name}} key: {{.Values.secrets.secretReference.key}} {{- end}} Step 5-b Include a section in the values.yaml file: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Add following snippet: secrets: enabled: true name: DB_PASSWORD secretReference: name: mysql key: password Step 6 For this lab, we will include the volumes and volumeMounts sections without templating, so just copy the required sections to the appropriate location in the deployment.yaml template. cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/deployment.yaml See the reference gowebapp-deployment.yaml : cat ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-deployment.yaml Step 7 Render the Chart locally and compare Deployment to original gowebapp-deployment.yaml manifests: helm template gowebapp-mysql . > render.yaml","title":"4.2.2 Template the Deployment"},{"location":"020_Assignment_4_Helm_Foundation/#423-template-the-service","text":"Step 1: The service.yaml template doesn't have an annotation section, so modify the template to add an annotation section, that looks following: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/service.yaml {{- with .Values.annotations }} annotations: {{- toYaml . | nindent 4 }} {{- end }} TODO: Next, modify the values.yaml file to allow chart users to add annotations for the service. Make sure to use the right section in the values.yaml file, based on how you modified your /templates/service.yaml file. Reference documentation: * https://helm.sh/docs/chart_template_guide/control_structures/#modifying-scope-using-with Step 2: Under the service: section in the values.yaml file, update the service port: to 9000. Step 3 In the values.yaml file, update the service type to NodePort Step 4 Render the Chart locally and compare Service to original gowebapp-service.yaml manifests: helm template gowebapp-mysql . > render.yaml","title":"4.2.3 Template the Service"},{"location":"020_Assignment_4_Helm_Foundation/#424-disable-the-service-account","text":"Step 1 Update the values.yaml file to disable the service account creation for the gowebapp deployment.","title":"4.2.4 Disable the Service account"},{"location":"020_Assignment_4_Helm_Foundation/#425-template-the-ingress-resource","text":"Step 1 enable the ingress the values.yaml file and configur according requirements: : * Expose `gowebapp` using `Ingress` resource: * ingress.class: \"nginx\" * path: \"/\" * pathType: \"ImplementationSpecific\" * host: $STATIC_IP_ADDRESS.nip.io cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Step 2 Render the Chart locally and verify if any issues: helm template gowebapp-mysql . > render.yaml","title":"4.2.5 Template the Ingress Resource"},{"location":"020_Assignment_4_Helm_Foundation/#425-templatize-the-configmap-resource","text":"Step 1 Create and templatize configmap resource for our gowebapp that provides connection to Mysql: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp/templates/ cat <<EOF>> configmap.yaml kind: ConfigMap apiVersion: v1 metadata: name: {{ .Values.configMap.name }} data: webapp-config-json: |- {{ .Files.Get \"config.json\" | indent 4 }} EOF Store the config.json inside the chart repository: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp cat <<EOF>> config.json { \"Database\": { \"Type\": \"MySQL\", \"Bolt\": { \"Path\": \"gowebapp.db\" }, \"MongoDB\": { \"URL\": \"127.0.0.1\", \"Database\": \"gowebapp\" }, \"MySQL\": { \"Username\": \"root\", \"Password\": \"rootpasswd\", \"Name\": \"gowebapp\", \"Hostname\": \"gowebapp-mysql\", \"Port\": 3306, \"Parameter\": \"?parseTime=true\" } }, \"Email\": { \"Username\": \"\", \"Password\": \"\", \"Hostname\": \"\", \"Port\": 25, \"From\": \"\" }, \"Recaptcha\": { \"Enabled\": false, \"Secret\": \"\", \"SiteKey\": \"\" }, \"Server\": { \"Hostname\": \"\", \"UseHTTP\": true, \"UseHTTPS\": false, \"HTTPPort\": 80, \"HTTPSPort\": 443, \"CertFile\": \"tls/server.crt\", \"KeyFile\": \"tls/server.key\" }, \"Session\": { \"SecretKey\": \"@r4B?EThaSEh_drudR7P_hub=s#s2Pah\", \"Name\": \"gosess\", \"Options\": { \"Path\": \"/\", \"Domain\": \"\", \"MaxAge\": 28800, \"Secure\": false, \"HttpOnly\": true } }, \"Template\": { \"Root\": \"base\", \"Children\": [ \"partial/menu\", \"partial/footer\" ] }, \"View\": { \"BaseURI\": \"/\", \"Extension\": \"tmpl\", \"Folder\": \"template\", \"Name\": \"blank\", \"Caching\": true } } EOF And finally add following snippet inside values.yaml : edit gowebapp/values.yaml configMap: name: gowebapp Step 2 Render the Chart locally and verify if any issues: helm template gowebapp-mysql . > render.yaml","title":"4.2.5 Templatize the ConfigMap Resource"},{"location":"020_Assignment_4_Helm_Foundation/#426-deploy-gowebapp","text":"Before deployment make sure test chart with dry-run , template and lint the chart Lint: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ helm lint gowebapp Install: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp helm install gowebapp . Output: NAME: gowebapp LAST DEPLOYED: Tue Aug 10 15:25:44 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None helm ls kubectl get all kubectl get pvc kubectl get ing Access gowebapp with Ingress Success You've now became a chart developer! And learned, how to create basic helm charts with 2 methods. You can farther customize the chart as needed and add more templates as you go. The next step would be to look contribute back to Helm community, use charts in Artificatory, find issue or add missing feature, help grow help project!","title":"4.2.6 Deploy gowebapp"},{"location":"020_Assignment_4_Helm_Foundation/#427-submit-assignement","text":"Step 1 Commit chart configuration in deploy_ycit020_a4/helm folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Gowebapp Helm Chart\" Step 2 Push commit to the Cloud Source Repositories: git push origin master Step 3 Uninstall chart gowebapp and gowebapp-mysql chart helm install gowebapp helm uninstall gowebapp-mysql Step 4 Resize GKE Cluster to 0 nodes, to avoid charges: cd ~/$MY_REPO/notepad-infrastructure edit terraform.tfvars And set gke_pool_node_count = \"0\" Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars Step 4: Shutdown all Nodes in GKE Cluster Node Pool: terraform apply -var-file terraform.tfvars Result GKE Clusters has been scale down to 0 nodes.","title":"4.2.7 Submit assignement"},{"location":"020_Assignment_4_Helm_Foundation/#part-2-wip","text":"","title":"Part 2 WIP"},{"location":"020_Assignment_4_Helm_Foundation/#5-chart-repositories","text":"","title":"5 Chart Repositories"},{"location":"020_Assignment_4_Helm_Foundation/#6-helmfile","text":"","title":"6 HelmFile"},{"location":"020_Assignment_4_Helm_Foundation/#7-install-istio-on-gke","text":"Currently Istio community developing following options of Istio deployment on Kubernetes Clusters: Install with Istioctl installation via istioctl command line tool used to showcase Istio functionality. Using the CLI, we generate a YAML file with all Istio resources and then deploy it to the Kubernetes cluster Istio Operator Install Takes installation of Istio to the next level as it's managing not only Istio installation but overall lifecicle of the Istio deployment including Upgrades and configurations. Install with Helm (alpha) Allows to farther simplify Istio deployment and it's customization.","title":"7 Install Istio on GKE"},{"location":"020_Assignment_4_Helm_Foundation/#70-prerequisite","text":"Scaleup cluster back to 3 nodes and Update VPC firewall rules to enable auto-injection and the istioctl version and istioctl ps commands. References: Opening ports on a private cluster Istio deployment on GKE Private Clusters Step 1: Locate Terraform Configuration directory. cd ~/$MY_REPO/notepad-infrastructure Step 2: Configure GKE Cluster to Scale back to 1 node per zone in a region. edit terraform.tfvars And set gke_pool_node_count = \"1\" Step 3: Create new firewall rule for source range (master-ipv4-cidr) of the cluster, that will open required ports for Istio installation: cat <<EOF> istio_firewall.tf resource \"google_compute_firewall\" \"istio_specific\" { name = format(\"allow-istio-in-privategke-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link source_ranges = [\"172.16.0.0/28\"] allow { protocol = \"tcp\" ports = [\"10250\", \"443\", \"15017\", \"15014\", \"8080\"] } } EOF Step 4: Review TF Plan: terraform plan -var-file terraform.tfvars Step 5: Scale up GKE Cluster Node Pool and update firewall rules for required range: terraform apply -var-file terraform.tfvars Result GKE Clusters has been scalled to 3 nodes and has firewall rules opened Step 6: Verify firewall rule: gcloud compute firewall-rules list --filter=\"name~allow-istio-in-privategke*\" Output: NAME NETWORK DIRECTION PRIORITY ALLOW DENY DISABLED allow-istio-in-privategke-$student-notepad-dev vpc-$student-notepad-dev INGRESS 1000 tcp:10250,tcp:443,tcp:15017,tcp:15014,tcp:8080 False","title":"7.0 Prerequisite"},{"location":"020_Assignment_4_Helm_Foundation/#71-deploy-istio-using-custom-resources","text":"This installation guide uses the istioctl command line tool to provide rich customization of the Istio control plane and of the sidecars for the Istio data plane. Download and extract the latest release: curl -L https://istio.io/downloadIstio | sh - cd istio-1.11.0 export PATH=$PWD/bin:$PATH Success The above command will fetch Istio packages and untar them in the same folder. tree -L 1 . \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 bin \u251c\u2500\u2500 manifest.yaml \u251c\u2500\u2500 manifests \u251c\u2500\u2500 samples \u2514\u2500\u2500 tools Note Istio installation directory contains: bin directory contains istioctl client binary manifests Installation Profiles, configurations and Helm Charts samples directory contains sample applications deployment tools directory contains auto-completion tooling and etc. Step 2 Deploy Istio Custom Resource Definitions (CRDs) Istio extends Kubernetes using Custom Resource Definitions (CRDs). CRDs allow registration of new/non-default Kubernetes resources. When Istio CRDs are deployed, Istio\u2019s objects are registered as Kubernetes objects, providing a highly integrated experience with Kubernetes as a deployment platform and thus allowing Kubernetes to store configuration of Istio features such as routing, security and telemetry and etc. We going to install using using demo configuration profile. The demo configuration profile allows to experiment with most of Istio features with modest resource requirements. Since it enables high levels of tracing and access logging, it is not suitable for production use cases. istioctl install --set profile=demo -y Output: \u2714 Istio core installed \u2714 Istiod installed \u2714 Egress gateways installed \u2714 Ingress gateways installed \u2714 Installation complete Thank you for installing Istio 1.11. Please take a few minutes to tell us about your install/upgrade experience! Note Wait a few seconds for the CRDs to be committed in the Kubernetes API-server. Step 3 Verify Istio CRDs successfully applied to Kubernetes Cluster. kubectl get crds | grep istio authorizationpolicies.security.istio.io 2021-08-16T19:38:16Z destinationrules.networking.istio.io 2021-08-16T19:38:16Z envoyfilters.networking.istio.io 2021-08-16T19:38:16Z gateways.networking.istio.io 2021-08-16T19:38:17Z istiooperators.install.istio.io 2021-08-16T19:38:17Z peerauthentications.security.istio.io 2021-08-16T19:38:17Z requestauthentications.security.istio.io 2021-08-16T19:38:17Z serviceentries.networking.istio.io 2021-08-16T19:38:17Z sidecars.networking.istio.io 2021-08-16T19:38:17Z telemetries.telemetry.istio.io 2021-08-16T19:38:18Z virtualservices.networking.istio.io 2021-08-16T19:38:18Z workloadentries.networking.istio.io 2021-08-16T19:38:18Z workloadgroups.networking.istio.io 2021-08-16T19:38:19Z Info Above CRDs will be avaialable as a new Kubernetes Resources and stored in Kubernetes ETCD database. The Kubernetes API will represent these new resources as endpoints that can be used as other native Kubernetes object (such as Pod, Services) levereging kubectl, RBAC and other features and admission controllers of Kubernetes. Step 4 Count total number of Installed CRDs: kubectl get crds | grep istio | wc -l Note CRDs count will vary based on Istio version and profile deployed. Step 5 Verify that Istio control plane has been installed successfully. kubectl get pods -n istio-system Output: istio-egressgateway-9dc6cbc49-5wkqt 1/1 Running 0 2m54s istio-ingressgateway-7975cdb749-xjc5g 1/1 Running 0 2m54s istiod-77b4d7b55d-j6kb5 1/1 Running 0 3m9s Info Istio control-plane include following components: istiod - contains components such as Citadel and Pilot istio-ingressgateway Istio Ingress Gateway istio-ingressgateway Istio Egress Gateway Step 6 Verify installation with istioctl CLI: istioctl version istioctl verify-install Output: Checked 13 custom resource definitions Checked 3 Istio Deployments \u2714 Istio is installed and verified successfully Step 5 Deploy Bookinfo sample application: kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-cjj5b 1/1 Running 0 12s productpage-v1-6b746f74dc-sxdxz 1/1 Running 0 10s ratings-v1-b6994bb9-jd985 1/1 Running 0 11s reviews-v1-545db77b95-5kjr4 1/1 Running 0 11s reviews-v2-7bf8c9648f-5v2s9 1/1 Running 0 11s reviews-v3-84779c7bbc-5khlt 1/1 Running 0 11s Note This is a typical Kubernetes deployment, and has nothing specific to Istio. Each Pod has 1 container. Step 6 Remove Bookinfo sample application: kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 7 Enable automatic Envoy sidecar injection. Currently there are 2 method to inject Envoy sidecar inside Istio Mesh: Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment. Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update. Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later: kubectl label namespace default istio-injection=enabled Step 8 Deploy Bookinfo sample application on Istio with Auto Sidecar Injection kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Step 8 Verify deployed application: The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it. kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-zqjbg 2/2 Running 0 13m productpage-v1-6b746f74dc-dj959 2/2 Running 0 12m ratings-v1-b6994bb9-29n4w 2/2 Running 0 12m reviews-v1-545db77b95-p9pgf 2/2 Running 0 12m reviews-v2-7bf8c9648f-4ltkg 2/2 Running 0 12m reviews-v3-84779c7bbc-vhsmw 2/2 Running 0 12m Note Each Pod has now 2 containers. One application container and another is istio-proxy sidecar container. Step 9 Access UI productpage via console: kubectl get services In Gcloud console execute kubectl port-forward : kubectl port-forward --namespace default svc/productpage 8080:9080 Click Web-Preview button in Gcloud, and select preview on port 8080 Click Normal user URL. Success We can access Bookinfo application configured with Istio and Envoy sidecar using private IP via tunnel Step 10 Remove Bookinfo sample application: kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 11 Uninstall Istio istioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f - kubectl delete namespace istio-system kubectl label namespace default istio-injection-","title":"7.1 Deploy Istio using Custom Resources"},{"location":"020_Assignment_4_Helm_Foundation/#72-deploy-istio-using-istio-operator","text":"Step 1: Deploy the Istio operator: istioctl operator init Note This command runs the operator by creating the following resources in the istio-operator namespace: The operator custom resource definition The operator controller deployment A service to access operator metrics Necessary Istio operator RBAC rules Step 2: To install the Istio demo configuration profile using the operator, run the following command: kubectl apply -f - <<EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: example-istiocontrolplane spec: profile: demo EOF Note The Istio operator controller begins the process of installing Istio within 90 seconds of the creation of the IstioOperator resource. The Istio installation completes within 120 seconds. Step 3 Verify Operator resource status on Kubernetes Cluster: kubectl get istiooperators -n istio-system Output: NAMESPACE NAME REVISION STATUS AGE istio-system example-istiocontrolplane HEALTHY 25m kubectl describe istiooperators example-istiocontrolplane -n istio-system Spec: Profile: demo Status: Component Status: Base: Status: HEALTHY Egress Gateways: Status: HEALTHY Ingress Gateways: Status: HEALTHY Pilot: Status: HEALTHY Status: HEALTHY Events: <none> kubectl get pods -n istio-system Output: istio-egressgateway-9dc6cbc49-5wkqt 1/1 Running 0 2m54s istio-ingressgateway-7975cdb749-xjc5g 1/1 Running 0 2m54s istiod-77b4d7b55d-j6kb5 1/1 Running 0 3m9s Info Istio control-plane include following components: istiod - contains components such as Citadel and Pilot istio-ingressgateway Istio Ingress Gateway istio-ingressgateway Istio Egress Gateway Verify installation with istioctl CLI: istioctl verify-install Step 5 Enable automatic Envoy sidecar injection. Currently there are 2 method to inject Envoy sidecar inside Istio Mesh: Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment. Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update. Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later: kubectl label namespace default istio-injection=enabled Step 8 Deploy Bookinfo sample application on Istio with Auto Sidecar Injection kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Step 8 Verify deployed application: The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it. kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-zqjbg 2/2 Running 0 13m productpage-v1-6b746f74dc-dj959 2/2 Running 0 12m ratings-v1-b6994bb9-29n4w 2/2 Running 0 12m reviews-v1-545db77b95-p9pgf 2/2 Running 0 12m reviews-v2-7bf8c9648f-4ltkg 2/2 Running 0 12m reviews-v3-84779c7bbc-vhsmw 2/2 Running 0 12m Summary We've leaned to deploy Istio Control plane using istioctl cli and using Istio Operator. We also deployed regular k8s application on the namespace marked with auto injection! Step 9 Cleanup Uninstall Istio Operator kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 10 Uninstall Istio Operator kubectl delete istiooperators.install.istio.io -n istio-system example-istiocontrolplane istioctl operator remove kubectl delete ns istio-system --grace-period=0 --force","title":"7.2 Deploy Istio using Istio Operator"},{"location":"020_Assignment_4_Helm_Foundation/#8-commit-readme-doc-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy_ycit020_a4/helm and notepad-infrastructure/helm folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"HelmFile Configuration to deploy NotePad and Suppporting Applications\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"8 Commit Readme doc to repository and share it with Instructor/Teacher"},{"location":"020_Assignment_4_Helm_Foundation/#9-cleanup","text":"We going to cleanup GCP Service foundation layer with GKE Cluster to avoid excessive cost. cd ~/$MY_REPO/notepad-infrastructure terraform destroy -var-file terraform.tfvars","title":"9 Cleanup"},{"location":"020_Assignment_4_sol_Helm_Foundation/","text":"Lab 4 Learning Helm Objective: Installing and Configuring the Helm Client Deploy Kubernetes apps with Helm Charts Learn Helm Commands Learn Helm Repository Create Helm chart Learn Helm Plugins Learn HelmFile Prerequisite \u00b6 Locate Assignment 4 \u00b6 Step 1 Clone ycit020 repo with Kubernetes manifests, which going to use for our work: cd ~/ycit020 # Alternatively: cd ~ & git clone https://github.com/Cloud-Architects-Program/ycit020 git pull cd ~/ycit020/Assignment4/ ls Result You can see Kubernetes manifests and terraform configs Step 1 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 (Optional) If you terraform config is not working you can copy working config from Assignment4 folder to your notepad-infrastructure folder. Step 4 Create structure for Helm Deployments for supporting applications cd ~/$MY_REPO/notepad-infrastructure mkdir helm cat <<EOF> helm/README.md # Helm Files and values to deploy supporting applications EOF Step 5 Copy Assignment 4 deploy_a4 folder to your repo: cd ~/$MY_REPO/ cp -r ~/ycit020/Assignment4/deploy_a4 deploy_ycit020_a4 ls deploy_ycit020_a4 Result You should see k8s-manifest folder Step 6 Create structure for NotePad Helm Chart locations cd ~/$MY_REPO/deploy_ycit020_a4 mkdir helm cat <<EOF> helm/README.md # Helm Chart, HelmFiles and values to deploy `NotePad` application. EOF Reserve Static IP addresses \u00b6 In the next few assignments we might deploy many applications and so we will require expose them using Ingress. When you create an Ingress object, you get a stable external IP address that clients can use to access your Services and in turn, your running containers. The IP address is stable in the sense that it lasts for the lifetime of the Ingress object. If you delete your Ingress and create a new Ingress from the same manifest file, you are not guaranteed to get the same external IP address. We would like to have for each student a a permanent IP address that stays the same across deleting your Ingress and creating a new one For that you must reserve a Regional static External IP address and provide it teacher, so we can setup A Record on the DNS for your behalf. Reference: Reserving a static external IP address Terraform resource google_compute_address Step 1: Create folder for static ip terraform configuration: cd ~/$MY_REPO/ mkdir ip-infrastructure Important The reason we creating a new folder for static_ip creation is because we don't want to destroy or delete this IP throughout our training. Step 1: Create Terraform configuration: Set you current PROJECT_ID value here: export PROJECT_ID=<YOUR_PROJECT_ID> Declare Provider: cd ~/$MY_REPO/ip-infrastructure cat << EOF>> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } } } EOF Configure global address resource: cat << EOF>> static_ip.tf provider \"google\" { project = var.gcp_project_id } resource \"google_compute_address\" \"regional_external_ip\" { provider = google name = \"static-ingres-ip\" address_type = \"EXTERNAL\" region = \"us-central1\" } EOF Configure variables: cat <<EOF> variables.tf variable \"gcp_project_id\" { type = string description = \"The GCP Seeding project ID\" default = \"\" } EOF Configure tfvars : gcp_project_id = \"$PROJECT_ID\" Configure outputs: cat <<EOF >> outputs.tf output \"addresses\" { description = \"Global IPv4 address for proxy load balancing to the nearest Ingress controller\" value = google_compute_address.regional_external_ip.address } output \"name\" { description = \"Static IP Name\" value = google_compute_address.regional_external_ip.name } EOF Step 2: Apply Terraform configuration: Initialize: terraform init Plan and Deploy Infrastructure: terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Set variable: cd ~/$MY_REPO/ip-infrastructure export STATIC_IP_ADDRESS=$(terraform output | grep 'addresses' |awk '{ print $3}') export STATIC_IP_NAME=$(terraform output | grep 'name' |awk '{ print $3}') Create readme: cat <<EOF> README.md Generated Static IP for Ingress: * Name - $STATIC_IP_NAME # Used Ingress Manifest * Address - $STATIC_IP_ADDRESS # Used to Configure DNS A Record EOF Step 6 Commit deploy folder using the following Git commands: cd ~/$MY_REPO git status git add . git commit -m \"adding documentation for ycit020 assignment 4\" Commit to Repository \u00b6 Step 1 Commit ip-infrastructure and helm folders using the following Git commands: cd ~/$MY_REPO git status git add . git commit -m \"Assignement 4\" Step 2 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 1 Install and Configure Helm Client \u00b6 1.1 Install Helm 3 \u00b6 GCP Cloud Shell comes with many common tools pre-installed including helm . Step 1: Verify and validate the version of Helm that is installed: helm --version Output: version.BuildInfo{Version:\"v3.5.0\"} Result Helm 3 is installed Note Until November 2020, two different major versions of Helm were actively maintained. The current stable major version of Helm is version 3. Note Helm follows a versioning convention known as Semantic Versioning (SemVer). In Semantic Versioning, the version number conveys meaning about what you can expect in the release. Because Helm follows this specification, users can expect certain things out of releases simply by carefully reading the version number. At its core, a semantic version has three numerical components and an optional stability marker (for alphas, betas, and release candidates). Here are some examples: v1.0.0 v3.3.2 SemVer represents format of X.Y.Z , where X is a major version, Y is a minor version and Z is a patch release: The major release number tends to be incremented infrequently. It indicates that major changes have been made to Helm, and that some of those changes may break compatibility with previous versions. The difference between Helm 2 and Helm 3 is substantial, and there is work necessary to migrate between the versions. The minor release number indicates feature additions. The difference between 3.2.0 and 3.3.0 might be that a few small new features were added. However, there are no breaking changes between versions. (With one caveat: a security fix might necessitate a breaking change, but we announce boldly when that is the case.) The patch release number indicates that only backward compatible bug fixes have been made between this release and the last one. It is always recommended to stay at the latest patch release. (OPTIONAL) Step 2: If you want to use specific version of helm or want to install helm in you local machine (On macOS and Linux,) use following link to install Helm. The usual sequence of commands for installing this way is as follows: $ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 $ chmod 700 get_helm.sh $ ./get_helm.sh The preceding commands fetch the latest version of the get_helm.sh script, and then use that to find and install the latest version of Helm 3. Alternatively, you can download latest binary of you OS choice here . 1.3 Deploy GKE cluster \u00b6 We going to reuse Terraform configuration to deploy our GKE Cluster. And we assume that terraform.tfvars has been properly configured already with required values. Step 1: Locate Terraform Configuration directory. cd ~/$MY_REPO/notepad-infrastructure Step 2: Initialize Terraform Providers terraform init Step 3: Increase GKE Node Pool VM size edit terraform.tfvars Update gke_pool_machine_type from e2-small to e2-highcpu-4 , to support larger workloads. Step 4: Review TF Plan: terraform plan -var-file terraform.tfvars Step 5: Create GKE Cluster and Node Pool: terraform apply -var-file terraform.tfvars !!! result GKE Clusters has been created 1.4 Configure Helm \u00b6 Helm interacts directly with the Kubernetes API server. For that reason, Helm needs to be able to connect to a Kubernetes cluster. Helm attempts to do this automatically by reading the same configuration files used by kubectl . Helm will try to find this information by reading the environment variable $KUBECONFIG. Step 1 Authenticate to the cluster. export STUDENT_NAME= gcloud container clusters get-credentials gke-$STUDENT_NAME-notepad-dev --region us-central1 Step 2 Test that kubectl client connected to GKE cluster: kubectl get pods Output: No resources found in default namespace. Step 3 Test that helm client connected to GKE cluster: helm list Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION Result As expected there are no Charts has been Installed on our cluster yet Summary Helm 3 has been installed and configured to work with our cluster. Let's deploy some charts! 2 Basic Helm Chart Installation \u00b6 2.1 Searching Chart \u00b6 A Helm chart is an package that can be installed into your Kubernetes cluster. During chart development, you will often just work with a chart that is stored on your local system and later pushed to GitHub. But when it comes to sharing charts, Helm describes a standard format for indexing and sharing information about Helm charts . A Helm chart repository is simply a set of files, reachable over the network, that conforms to the Helm specification for indexing packages. There are huge number of chart repositories on the internet. The easiest way to find the popular repositories is to use your web browser to navigate to the Artifact Hub . There you will find thousands of Helm charts, each hosted on an appropriate repository. Deprecation In the past all charts were located and maintained by Helm Kubernetes Community Git repositories, known as https://github.com/kubernetes/charts, and had 2 types: Stable Incubator All the charts were rendered in Web UI via Helm Hub page. While the central Git Repository to maintain charts was great idea, with fast growing Helm popularity, it become hard to impossible to manage and maintain it by small group of maintainers, as they had to aprove hundreds of PR per day and frustrating for chart contributors as they had to wait several weeks their PR to be reviewed and approved. As a result GitHub project for Helm stable and incubator charts as well as [Helm Hub] has been deprecated and archived. All the charts are now maintained by independent contributors in their subsequent repo's. (e.g vault, is under hashicorp/vault-helm repo) and the central place to find all active and official Charts can be foind in Artifact Hub . Important Helm 2 came with a Helm repository installed by default. The stable chart repository was at one time the official source of production-ready Helm charts. As we discussed above stable chart repository has been now deprecated. In Helm 3, there is no default repository. Users are encouraged to use the Artifact Hub to find what they are looking for and then add their preferred repositories. Step 1: Helm provides native way to search charts from CLI in artifacthub.io cd ~/$MY_REPO/notepad-infrastructure/helm helm search hub drupal Output: https://artifacthub.io/packages/helm/bitnami/drupal 10.2.30 9.2.3 One of the most versatile open source content m... https://artifacthub.io/packages/helm/cetic/drupal 0.1.0 1.16.0 Drupal is a free and open-source web content ma.. Result Search is a good way to find existing Helm packages Step 2: Use the link to navigate to Artifact Hub Drupal chart: https://artifacthub.io/packages/helm/bitnami/drupal Note Drupal is OSS content management systems that can be installed this days on K8s. Result: The Artifact Page opened with information about the chart, it's parameters and how to use the chart. Step 3: Review Github source Code of the chart: https://github.com/bitnami/charts/tree/master/bitnami/drupal Result: Github page with Drupal Helm Chart itself, where you can browse the templates , values.yaml , Chart.yaml to understand how this chart is actually working and if you have any issues with the chart this would be the right location to open the issue or send a PR with new feature if you decide to contribute. 2.2 Adding a Chart Repository \u00b6 Once you found a chart, it's logical to install it. However first step you need to do is to add a Chart Repository. Step 1: Adding a Helm chart is done with the helm repo add command: helm repo add bitnami https://charts.bitnami.com/bitnami Note Bitnami is company well known to package application for Any Platforms and Cloud environments. With popularity of Helm, Bitnami developers were among the core contributors who designed the Helm repository system. They have contributed to the establishment of Helm\u2019s best practices for chart development and have written many of the most widely used charts. Bitnami is now part of VMware, provides IT organizations with an enterprise offering that is secure, compliant, continuously maintained and customizable to your organizational policies. Step 2: Now, we can verify that the Bitnami repository exists by running a helm repo list command: helm repo list Output: NAME URL bitnami https://charts.bitnami.com/bitnami Result This command shows us all of the repositories installed for Helm. Right now, we see only the Bitnami repository that we just added. 2.3 Helm Chart Installation \u00b6 Step 1: At very minimum, installing a chart in Helm requires just 2 pieces of information: the name of the installation and the chart you want to install: helm install mywebsite bitnami/drupal Output: NAME: mywebsite LAST DEPLOYED: Sun Aug 8 08:25:29 2021 NAMESPACE: prometheus STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ******************************************************************* *** PLEASE BE PATIENT: Drupal may take a few minutes to install *** ******************************************************************* 1. Get the Drupal URL: NOTE: It may take a few minutes for the LoadBalancer IP to be available. Watch the status with: 'kubectl get svc --namespace prometheus -w mywebsite-drupal' export SERVICE_IP=$(kubectl get svc --namespace prometheus mywebsite-drupal --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\") echo \"Drupal URL: http://$SERVICE_IP/\" 2. Get your Drupal login credentials by running: echo Username: user echo Password: $(kubectl get secret --namespace prometheus mywebsite-drupal -o jsonpath=\"{.data.drupal-password}\" | base64 --decode) Result When using Helm, you will see that output for each installation. A good chart would provide helpful notes on how to connect to the deployed solution. Tip You can get notes information any time after helm installation using helm get notes mywebsite command. Step 2: Follow your Drupal Helm Chart notes Instruction to access Website 1. Get the Drupal URL: NOTE: It may take a few minutes for the LoadBalancer IP to be available. Watch the status with: 'kubectl get svc --namespace test -w mywebsite-drupal' export SERVICE_IP=$(kubectl get svc --namespace test mywebsite-drupal --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\") echo \"Drupal URL: http://$SERVICE_IP/\" Success We can access My blog website and login with provider user and password. You can start your own blog now, that runs on Kubernetes Step 3: List deployed Kubernetes resources: kubectl get all --namespace test kubectl get pvc --namespace test Summary Our Drupal website consist of MariaDB statefulset , Drupal deployment , 2 pvc s and services . Mywebsite Drupal service is type LoadBalancer and that's how we able to access it. Step 4: List installed chart: helm list Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION mywebsite default 1 2021-08-08 08:25:29.625862 -0400 EDT deployed drupal-10.2.30 9.2.3 Note Like other commands, helm list is namespace aware. By default, Helm uses the namespace your Kubernetes configuration file sets as the default. Usually this is the namespace named default . Step 5: Deploy same chart in namespace test : kubectl create ns test helm install --namespace test mywebsite bitnami/drupal --create-namespace Tip By adding --create-namespace , indicates to Helm that we acknowledge that there may not be a namespace with that name already, and we just want one to be created. List deployed chart in namespace test : helm list --namespace test Summary In Helm 2, instance names were cluster-wide. You could only have an instance named mywebsite once per cluster. In Helm 3, naming has been changed. Now instance names are scoped to Kubernetes namespaces. We could install 2 instances named mywebsite as long as they each lived in a different namespace. Step 5: Cleanup Helm Drupal deployments using helm uninstall command: helm uninstall mywebsite helm uninstall mywebsite -n test kubectl delete ns test 3 Advanced Helm Chart Installation \u00b6 3.2.1 Deploy NGINX Ingress Chart with Custom Configuration \u00b6 Let's deploy another Helm application on our Kubernetes cluster: Ingress Nginx - is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer. In the previous classes we used Google implementation of Ingress - GKE Ingress for HTTP(S) Load Balancing . While this solutions provides managed Ingress experience and advanced features like Cloud Armor, DDoS protection and Identity aware proxy. Ingress Nginx Controller is popular solution and has a lot of features and integrations. If you want to deploy Kubernetes Application on different cloud providers or On-prem the same way, Nginx Ingress Controller becomes a default option. Our task is to configure Ingress Nginx using type:LoadBalancer with Regional Static IP configured in the section ### Reserve Static IP addresses . Additionally we want to enable metrics service that will fetch Prometheus monitoring metrics from ingress and disable admissionWebhooks configuration. Step 1: Let's search ingress-nginx in artifacthub.io helm search hub ingress-nginx Output: URL CHART VERSION APP VERSION DESCRIPTION https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx 4.0.0 1.0.0-beta.1 Ingress controller for Kubernetes using NGINX https://artifacthub.io/packages/helm/api/ingress-nginx 3.29.1 0.45.0 Ingress controller for Kubernetes using NGINX Result We going to select the first ingress-nginx chart that is maintained by Kubernetes Community Step 2: Review Hub Page details about this chart and locate information how to add repository: https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx Result: The Artifact Page opened with information about the chart, it's parameters, and how to use the chart itself Step 3: Add ingress-nginx Repo: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update Step 4: Oftentimes, searching is a useful way to find not only what charts can be installed, but what versions are available: helm search repo ingress-nginx --versions | head Output: NAME CHART VERSION APP VERSION DESCRIPTION ingress-nginx/ingress-nginx 3.35.0 0.48.1 Ingress controller for Kubernetes using NGINX a... ingress-nginx/ingress-nginx 3.34.0 0.47.0 Ingress controller for Kubernetes using NGINX a... ingress-nginx/ingress-nginx 3.33.0 0.47.0 Ingress controller for Kubernetes using NGINX a... .... Note By default, Helm tries to install the latest stable release of a chart, but you can override this behavior and install a specific version of a chart. Thus it is often useful to see not just the summary info for a chart, but exactly which versions exist for a chart. Every new version of the chart can be bring fixes and new changes, so for production use it's better to go with tested version and pin installation version. Summary We going with latest listed official version of the chart 3.35.0 of ingress-nginx Chart. 3.2.2 Download and inspect Chart locally \u00b6 Step 1: Pull ingress-nginx Helm Chart of specific version to Local filesystem: cd ~/$MY_REPO/notepad-infrastructure/helm helm pull ingress-nginx/ingress-nginx --version 3.35.0 tar -xvzf ingress-nginx-3.35.0.tgz Step 2: See the tree structure of the chart sudo apt-get install tree tree -L 2 ingress-nginx Output: ingress-nginx \u251c\u2500\u2500 CHANGELOG.md \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 OWNERS \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ci \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 admission-webhooks \u2502 \u251c\u2500\u2500 clusterrole.yaml \u2502 \u251c\u2500\u2500 clusterrolebinding.yaml \u2502 \u251c\u2500\u2500 controller-configmap-addheaders.yaml \u2502 \u251c\u2500\u2500 controller-configmap-proxyheaders.yaml \u2502 \u251c\u2500\u2500 controller-configmap-tcp.yaml \u2502 \u251c\u2500\u2500 controller-configmap-udp.yaml \u2502 \u251c\u2500\u2500 controller-configmap.yaml \u2502 \u251c\u2500\u2500 controller-daemonset.yaml \u2502 \u251c\u2500\u2500 controller-deployment.yaml \u2502 \u251c\u2500\u2500 controller-hpa.yaml \u2502 \u251c\u2500\u2500 controller-ingressclass.yaml \u2502 \u251c\u2500\u2500 controller-keda.yaml \u2502 \u251c\u2500\u2500 controller-poddisruptionbudget.yaml \u2502 \u251c\u2500\u2500 controller-prometheusrules.yaml \u2502 \u251c\u2500\u2500 controller-psp.yaml \u2502 \u251c\u2500\u2500 controller-role.yaml \u2502 \u251c\u2500\u2500 controller-rolebinding.yaml \u2502 \u251c\u2500\u2500 controller-service-internal.yaml \u2502 \u251c\u2500\u2500 controller-service-metrics.yaml \u2502 \u251c\u2500\u2500 controller-service-webhook.yaml \u2502 \u251c\u2500\u2500 controller-service.yaml \u2502 \u251c\u2500\u2500 controller-serviceaccount.yaml \u2502 \u251c\u2500\u2500 controller-servicemonitor.yaml \u2502 \u251c\u2500\u2500 default-backend-deployment.yaml \u2502 \u251c\u2500\u2500 default-backend-hpa.yaml \u2502 \u251c\u2500\u2500 default-backend-poddisruptionbudget.yaml \u2502 \u251c\u2500\u2500 default-backend-psp.yaml \u2502 \u251c\u2500\u2500 default-backend-role.yaml \u2502 \u251c\u2500\u2500 default-backend-rolebinding.yaml \u2502 \u251c\u2500\u2500 default-backend-service.yaml \u2502 \u251c\u2500\u2500 default-backend-serviceaccount.yaml \u2502 \u2514\u2500\u2500 dh-param-secret.yaml \u2514\u2500\u2500 values.yaml Result Typical structure of the helm chart, where: The Chart.yaml file contains metadata and some functionality controls for the chart templates folder contains all yaml manifests in Go templates and used to generate Kubernetes manifests values.yaml contains default values applied during the rendering. The NOTES.txt file is a special template. When a chart is installed, the NOTES.txt template is rendered and displayed rather than being installed into a cluster. 3.2.2 Helm Template \u00b6 As a reminder our goal it customize Nginx deployment to configure Ingress Nginx using type:LoadBalancer with Regional Static IP configured in the section ### Reserve Static IP addresses . Additionally we want to enable metrics service that will fetch Prometheus monitoring metrics from ingress and disable admissionWebhooks configuration. Step 1: Review values.yaml cd ~/$MY_REPO/notepad-infrastructure/helm/ingress-nginx edit values.yaml Result It is pretty confusing at this point to understand what this chart is going to deploy, it would of been great to render to YAML format first. Step 2: Execute helm template command and store output in render.yaml helm template ingress-nginx/ingress-nginx --version 3.35.0 > render.yaml Note During helm template, Helm never contacts a remote Kubernetes server, hence the chart only has access to default Kubernetes kinds. The template command always acts like an installation. Review rendered values: edit render.yaml grep \"Source\" render.yaml Result helm template designed to isolate the template rendering process of Helm from the installation. The template command performs following phases of Helm Lifecycle: loads the chart determines the values renders the templates formats to YAML Step 3: Let's first create a configuration that will disable admissionWebhooks . Review looking in values.yaml for admissionWebhooks configuration: grep -A10 admissionWebhooks values.yaml Output: admissionWebhooks: annotations: {} enabled: true failurePolicy: Fail # timeoutSeconds: 10 port: 8443 certificate: \"/usr/local/certificates/cert\" key: \"/usr/local/certificates/key\" namespaceSelector: {} objectSelector: {} Result admissionWebhooks enabled: true Step 3: Let's create custom_values.yaml file with admissionWebhooks disabled cat << EOF>> custom_values.yaml controller: admissionWebhooks: enabled: false EOF Execute helm template and store output in new file render2.yaml : helm template ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 > render2.yaml Now comparing this 2 render files you can see the what change in configuration has resulted in the rendered file: diff render.yaml render2.yaml This is the YAML files that will be generated after this change: grep \"Source\" render2.yaml Output: # Source: ingress-nginx/templates/controller-serviceaccount.yaml # Source: ingress-nginx/templates/controller-configmap.yaml # Source: ingress-nginx/templates/clusterrole.yaml # Source: ingress-nginx/templates/clusterrolebinding.yaml # Source: ingress-nginx/templates/controller-role.yaml # Source: ingress-nginx/templates/controller-rolebinding.yaml # Source: ingress-nginx/templates/controller-service.yaml # Source: ingress-nginx/templates/controller-deployment.yaml Step 4: Now, let's add a custom Configuration to controller-service.yaml to use Static loadBalancerIP , we've configured in step ### Reserve Static IP addresses . First we need to locate the correct service parameters: grep -A20 \"service:\" values.yaml Output Shows that we have 4 different services in values file Let's narrow our search: grep -A20 \"service:\" values.yaml | grep \"List of IP address\" Output: ## List of IP addresses at which the controller services are available ## List of IP addresses at which the stats-exporter service is available ## List of IP addresses at which the default backend service is available Output: service: enabled: true annotations: {} labels: {} # clusterIP: \"\" ## List of IP addresses at which the controller services are available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] # loadBalancerIP: \"\" loadBalancerSourceRanges: [] Result It seems controller is first service: in values.yaml and currently is it has loadBalancerIP section commented Step 5: Let's add loadBalancerIP IP configuration to custom_values.yaml Set variable: cd ~/$MY_REPO/ip-infrastructure export STATIC_IP_ADDRESS=$(terraform output | grep 'addresses' |awk '{ print $3}') echo $STATIC_IP_ADDRESS Add custom values: cd ~/$MY_REPO/notepad-infrastructure/helm/ingress-nginx cat << EOF>> custom_values.yaml service: loadBalancerIP: $STATIC_IP_ADDRESS EOF Our final custom_values.yaml should look as following: controller: admissionWebhooks: enabled: false service: loadBalancerIP: 35.X.X.X metrics: enabled: true Execute helm template and store output in new file render3.yaml : helm template ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 > render3.yaml Show the difference in rendered file: diff render2.yaml render3.yaml Output: > loadBalancerIP: 35.X.X.X Ensure that this change is in fact belongs to controller-service.yaml : grep -C16 \"loadBalancerIP:\" render3.yaml Result We configured correct service that will Ingress Controller service with type: Loadbalancer and static IP with previously generated with terraform. Summary helm template is great tool to render you charts to YAML. Tip You can use Helm as packaging you charts only. And then use helm template to generate actual YAMLs and apply them with kubectl apply 3.2.3 Dry-Runs \u00b6 Before applying configuration to Kubernetes Cluster it is good idea to Dry-Run it for Errors. This is especially important if you doing Upgrades. The dry-run feature provides Helm users a way to debug the output of a chart before it is sent on to Kubernetes. With all of the templates rendered, you can inspect exactly what would have been submitted to your cluster. And with the release data, you can verify that the release would have been created as you expected. Here is some of the dry-run working principals: --dry-run mixes non-YAML information with the rendered templates. This means the data has to be cleaned up before being sent to tools like kubectl. A --dry-run on upgrade can produce different YAML output than a --dry-run on install, and this can be confusing. It contacts the Kubernetes API server for validation, which means Helm has to have Kubernetes credentials even if it is just used to --dry-run a release. It also inserts information into the template engine that is cluster-specific. Because of this, the output of some rendering processes may be cluster-specific. Difference between dry-run and template is that the dry-run contacts Kubernetes API. It is good idea to use dry-run prior deployment and upgrade as it creates mutated output, while template doesn't contacts API and can to pure rendering. Step 1: Execute our deployment with dry-run command: helm install ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --dry-run Output: NAME: ingress-nginx LAST DEPLOYED: Tue Aug 10 07:22:56 2021 NAMESPACE: default STATUS: pending-install REVISION: 1 TEST SUITE: None HOOKS: MANIFEST: --- # Source: ingress-nginx/templates/controller-serviceaccount.yaml ........ NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace default get services -o wide -w ingress-nginx-controller' An example Ingress that makes use of the controller: ... If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: <base64 encoded cert> tls.key: <base64 encoded key> type: kubernetes.io/tls At the top of the output, it will print some information about the release in our case it tells what phase of the release it is in (pending-install), and the revision number. Next, after the informational block, all of the rendered templates are dumped to standard output. Finally, at the bottom of the dry-run output, Helm prints the user-oriented release notes: Note --dry-run dumps the output validates, but doesn't deploy actual chart. Step 2: Finally let's deploy Nginx Ingress Charts with our parameters in ingress-nginx namespace kubectl create ns ingress-nginx helm install ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx Step 3: Verify Helm Deployment: helm list Follow Installation note and verify Ingress-Controller Service: kubectl --namespace ingress-nginx get services -o wide ingress-nginx-controller Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 172.10.0.213 35.192.101.76 80:30930/TCP,443:30360/TCP 3m9s 3.2.4 Deploy Drupal using Nginx Ingress Controller \u00b6 Let's test our newly configured ingress-nginx-controller and expose drupal application using ingress instead of LoadBalancer . Check if STATIC_IP_ADDRESS variable is set: echo $STATIC_IP_ADDRESS If not set it with you Static IP address value. cd ~/$MY_REPO/notepad-infrastructure/helm/ kubectl delete pvc data-mywebsite-mariadb-0 cat << EOF>> drupal_ing_values.yaml ingress: annotations: {kubernetes.io/ingress.class: \"nginx\"} enabled: true hostname: $STATIC_IP_ADDRESS.nip.io path: / pathType: ImplementationSpecific EOF Note Here we using nip.io domain that provides simple wildcard DNS for any IP Address, however if you provided you STATIC_IP to teacher, they will generate following DNS, and you can replace hostname: with value $student-name.cloud-montreal.ca helm install mywebsite bitnami/drupal --values drupal_ing_values.yaml 1. Get the Drupal URL: You should be able to access your new Drupal installation through http://35.X.X.X.nip.io/ Success Drupal site is now accessible via Ingress. Our Nginx Ingress Controller has been setup correctly! Uninstall Drupal: helm uninstall mywebsite kubectl delete pvc data-mywebsite-mariadb-0 3.2.5 Using --set values and Upgrading Charts \u00b6 In addition to --value option, there is a second flag that can be used to add individual parameters to an install or upgrade . The --set flag takes one or more values directly. They do not need to be stored in a YAML file. Step 1: Let's update our ingress-nginx release with new parameter --set controller.image.pullPolicy=Always , but first we want to render template with and without parameter to see what the change will be applied: helm template ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx > render4.yaml helm template ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=Always > render5.yaml diff render4.yaml render5.yaml Output: < imagePullPolicy: IfNotPresent --- > imagePullPolicy: Always Step 2: Let's apply update to our ingress-nginx release. For that we going to use helm upgrade command: helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=Always Verify that upgrade was successful: helm list --namespace ingress-nginx Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ingress-nginx ingress-nginx 2 2021-08-10 10:48:35.265506 -0400 EDT deployed ingress-nginx-3.35.0 0.48.1 Result We can see that Revision change to 2 Note When we talk about upgrading in Helm, we talk about upgrading an installation, not a chart. An installation is a particular instance of a chart in your cluster. When you run helm install , it creates the installation. To modify that installation, use helm upgrade . This is an important distinction to make in the present context because upgrading an installation can consist of two different kinds of changes: * You can upgrade the version of the chart * You can upgrade the configuration of the installation In this case we upgrading the configuration of the installation. Extra If you interested upgrade chart version of ingress-nginx to the next available release or 3.34.0 , give it a try. 3.2.5 Listing Releases, History, Rollbacks \u00b6 Step 1: Let's see if we can upgrade our release with wrong value pullPolicy=NoSuchPolicy that doesn't exist on Kubernetes. We will first run in dry-run mode helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=NoSuchPolicy --dry-run Result Release \"ingress-nginx\" has been upgraded. Happy Helming! Let's now apply same config without --dry-run mode: helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=NoSuchPolicy Output: Error: UPGRADE FAILED: cannot patch \"ingress-nginx-controller\" with kind Deployment: Deployment.apps \"ingress-nginx-controller\" is invalid: spec.template.spec.containers[0].imagePullPolicy: Unsupported value: \"NoSuchPolicy\": supported values: \"Always\", \"IfNotPresent\", \"Never\" Failed As the error message indicates, a pull policy cannot be set to NoSuchPolicy. This error came from the Kubernetes API server, which means Helm submitted the manifest, and Kubernetes rejected it. So our release should be in a failed state. Verify with helm list to confirm failed state: helm list Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ingress-nginx ingress-nginx 3 2021-08-10 11:03:05.224405 -0400 EDT failed ingress-nginx-3.35.0 0.48.1 Step 2: List all releases with helm history : helm history ingress-nginx -n ingress-nginx REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Tue Aug 10 08:47:32 2021 superseded ingress-nginx-3.35.0 0.48.1 Install complete 2 Tue Aug 10 10:48:35 2021 deployed ingress-nginx-3.35.0 0.48.1 Upgrade complete 3 Tue Aug 10 11:03:05 2021 failed ingress-nginx-3.35.0 0.48.1 Upgrade \"ingress-nginx\" failed: cannot patch \"ingress-nginx-controller\" with kind Deployment: Deployment.apps \"ingress-nginx-controller\" is invalid: spec.template.spec.containers[0].imagePullPolicy: Unsupported value: \"NoSuchPolicy\": supported values: \"Always\", \"IfNotPresent\", \"Never\" Info During the life cycle of a release, it can pass through several different statuses. Here they are, approximately in the order you would likely see them: pending-install Before sending the manifests to Kubernetes, Helm claims the installation by creating a release (marked version 1) whose status is set to pending-install. deployed As soon as Kubernetes accepts the manifest from Helm, Helm updates the release record, marking it as deployed. pending-upgrade When a Helm upgrade is begun, a new release is created for an installation (e.g., v2), and its status is set to pending-upgrade. superseded When an upgrade is run, the last deployed release is updated, marked as superseded, and the newly upgraded release is changed from pending-upgrade to deployed. pending-rollback If a rollback is created, a new release (e.g., v3) is created, and its status is set to pending-rollback until Kubernetes accepts the release manifest. Then it is marked deployed and the last release is marked superseded. uninstalling When a helm uninstall is executed, the most recent release is read and then its status is changed to uninstalling. uninstalled If history is preserved during deletion, then when the helm uninstall is complete, the last release\u2019s status is changed to uninstalled. failed Finally, if during any operation, Kubernetes rejects a manifest submitted by Helm, Helm will mark that release failed. Step 3: Rollback release with helm rollback . From the error, we know that the release failed because we supplied an invalid image pull policy. So of course we could correct this by simply running another helm upgrade. But imagine a case where the cause of error was not readily available. Rather than leave the application in a failed state while diagnosing the problem, it would be nice to simply revert back to the release that worked before. helm rollback ingress-nginx 2 -n ingress-nginx helm history ingress-nginx -n ingress-nginx helm list 4 Tue Aug 10 11:16:24 2021 deployed ingress-nginx-3.35.0 0.48.1 Rollback to 2 --- ingress-nginx ingress-nginx 4 2021-08-10 11:16:24.424371 -0400 EDT deployed ingress-nginx-3.35.0 0.48.1 Rollback was a success! Happy Helming! Success This command tells Helm to fetch the ingress-nginx version 2 release, and resubmit that manifest to Kubernetes. A rollback does not restore to a previous snapshot of the cluster. Helm does not track enough information to do that. What it does is resubmit the previous configuration, and Kubernetes attempts to reset the resources to match. 3.2.5 Upgrade Release with Helm Diff Plugin \u00b6 While Helm provides a lot of functionality out of the box. Community contributes a lot of great functionality via helm plugins. Plugins allow you to add extra functionality to Helm and integrate seamlessly with the CLI, making them a popular choice for users with unique workflow requirements. There are a number of third-party plugins available online for common use cases, such as secrets management. In addition, plugins are incredibly easy to build on your own for unique, one-off tasks. Helm plugins are external tools that are accessible directly from the Helm CLI. They allow you to add custom subcommands to Helm without making any modifications to Helm\u2019s Go source code. Many third-party plugins are made open source and publicly available on GitHub. Many of these plugins use the \u201chelm-plugin\u201d tag/topic to make them easy to find. Refer to the documentation for Helm plugins on GitHub Let's Upgrade our running Ingress Nginx chart with metrics service that will enable Prometheus to scrape metrics from our Nginx. Step 1: Update custom_values.yaml with following information: cat << EOF>> custom_values.yaml metrics: enabled: true EOF Step 2: So far we've used helm template to render and compare manifests. However most of the time you might not be able to do it during upgrades. However you want to have a clear understanding what will be upgraded if you change something. That's where Helm Diff Plugin can be handy. Helm Diff plugin giving your a preview of what a helm upgrade would change. It basically generates a diff between the latest deployed version of a release and a helm upgrade --dry-run Install Helm Diff Plugin: helm plugin install https://github.com/databus23/helm-diff helm diff upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx Output: - imagePullPolicy: Always + imagePullPolicy: IfNotPresent + protocol: TCP + - name: metrics + containerPort: 1025 + # Source: ingress-nginx/templates/controller-service-metrics.yaml + apiVersion: v1 + kind: Service ... Result Amazing! This looks like terraform plan but for helm :) Important Another discovery from above printout is that since we forgot to add --set imagePullPolicy command, our value will be reverted with upgrade. This is really important to understand as you configuration maybe lost if you use --set Step 3: Let's upgrade our ingress-nginx release: helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx helm list -n ingress-nginx Success Our ingress-nginx is ready to run. Going forward we going to always deploy this chart. Summary So far we've learned: How to customize helm chart deployment using --set and --values using values.yaml Using values.yaml preferable mehtod in order to achieve reproducible deployments How to upgrade and rollback releases Helm template and dry-run Helm Diff Plugin 3.2.6 Install Minio with Helm \u00b6 Let's deploy another Helm application on our Kubernetes cluster: Minio - high-performance, S3 compatible object storage. helm install myminio bitnami/minio Follow your Minio Helm Chart notes Instruction to access Website: To access the MinIO&reg; web UI: - Get the MinIO&reg; URL: echo \"MinIO&reg; web URL: http://127.0.0.1:9000/minio\" kubectl port-forward --namespace default svc/myminio 8080:9000 In Gcloud console execute kubectl port-forward : kubectl port-forward --namespace default svc/myminio 8080:9000 Result We can access Minio UI using private IP via tunnel and we have to use autogenerated ACCESS_KEY and SECRET_KEY. While it looks easy to start with helm and deploy any charts, most of the time you want to have a full control of the deployment process. You want to specify, tested version, safe values or configurations that make sense for you or your organization. 3.2.7 Customize Minio Installation Task \u00b6 Task N1: Customize Minio Installation with following values: * Deploy `Minio` of specific version: `7.1.7` * Expose Minio using `Ingress` resource: * ingress.class: \"nginx\" * path: \"/\" * pathType: \"ImplementationSpecific\" * * Specify custom `ACCESS_KEY` for `Minio` frontend: `myaccesskey` * Specify custom `SECRET_KEY` for `Minio` frontend: `mysecretkey` Step 1: Use the following Minio Chart: helm search hub minio URL CHART VERSION APP VERSION DESCRIPTION https://artifacthub.io/packages/helm/bitnami/minio 7.1.7 2021.6.17 Bitnami Object Storage based on MinIO&reg ..... Result We going to choose bitnami package again as it's top of the list and seems maintained the most. Step 2: Since we already added bitnami repository locally, let's verify if we can list minio in it: helm search repo minio Result We can now go ahead and install minio Step 4: Pull Minio Helm Chart of specific version to Local filesystem to work with values file. cd ~/$MY_REPO/notepad-infrastructure/helm helm pull bitnami/minio --version 7.1.7 cd minio cat <<EOF> custom_values.yaml TODO: Finish the remaining steps EOF TODO: Complete custom_values.yaml and test deployment of Minio . Test Minio by creating bucket and uploading object to it. Step 5 Commit deploy_ycit020_a4/helm and notepad-infrastructure/helm folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Helm values for Minio\" Step 6 Push commit to the Cloud Source Repositories: git push origin master Solution: Step 5: Let's review values.yaml : cd minio grep -A60 \"service:\" values.yaml | grep -v \"#\" Output: service: type: ClusterIP port: 9000 nodePort: \"\" loadBalancerIP: \"\" loadBalancerSourceRanges: [] externalTrafficPolicy: Cluster annotations: {} ingress: enabled: false certManager: false apiVersion: \"\" hostname: foo.example.com path: / pathType: ImplementationSpecific servicePort: minio Result By default Minio frontend will have ClusterIP type, let's try it with LoadBalancer Step 6: Let's review values.yaml and search Minio Access specific information: grep -A20 \"mode:\" values.yaml | grep -v \"#\" Output: mode: standalone accessKey: password: \"\" forcePassword: false secretKey: password: \"\" forcePassword: false Result By default Minio will generate accessKey.password and secretKey.password, let's create our own. Step 7: Create custom_values.yaml which will enable Ingress: cat <<EOF> custom_values.yaml accessKey: password: myaccesskey forcePassword: false secretKey: password: mysecretkey forcePassword: false ingress: annotations: {kubernetes.io/ingress.class: \"nginx\"} enabled: true hostname: 35.192.101.76.nip.io path: / pathType: ImplementationSpecific servicePort: minio EOF Step 8: Install Minio Chart with Custom Configuration helm install myminio bitnami/minio --values custom_values.yaml --version 7.1.6 4 Creating NotePad Helm Charts \u00b6 So far with learned how to deploy existing charts from the Artifact Hub. However sometimes you or your company needs to build software that require's to be distributed and shared externally, as well as have lifecycle and release management. Helm Charts becomes are valuable option for that, specifically if you application is based of containers! Imagine that our solution is Go-based NotePad has first customer that wants to deploy it on their system and they requested delivery via Helm Charts that available on GCP based Helm Repository. To achieve such task we going to create 2 Charts: * `gowebapp-mysql` chart * `gowebapp` chart And store them in Google Artifact Registry that provides Helm 3 OCI Repository. 4.1 Design gowebapp-mysql chart \u00b6 4.1.1 Create gowebapp-mysql chart \u00b6 As scary as it sounds creating a new basic helm chart is 5 minute thing! We going to learn 2 quick methods to create helm charts, that will help you to save time and get started with helm quickly. Helm includes the helm create command to make it easy for you to create a chart of your own, and it\u2019s a great way to get started. The create command creates a chart for you, with all the required chart structure and files. These files are documented to help you understand what is needed, and the templates it provides showcase multiple Kubernetes manifests working together to deploy an application. In addition, you can install and test this chart right out of the box. Step 1 Create a new chart: cd ~/$MY_REPO/deploy_ycit020_a4/helm helm create gowebapp-mysql cd gowebapp-mysql tree Output: \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 hpa.yaml \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u251c\u2500\u2500 service.yaml \u2502 \u251c\u2500\u2500 serviceaccount.yaml \u2502 \u2514\u2500\u2500 tests \u2502 \u2514\u2500\u2500 test-connection.yaml \u2514\u2500\u2500 values.yaml Result This command creates a new Nginx chart, with a name of your choice, following best practices for a chart layout. Since Kubernetes clusters can have different methods to expose an application, this chart makes the way Nginx is exposed to network traffic configurable so it can be exposed in a wide variety of clusters. The chart has following structure: The Chart.yaml file contains metadata and some functionality controls for the chart The charts folder currently empty may container charts dependency of the top level chart. For example we could of make our gowebapp-mysql as dependency chart for gowebapp templates folder contains all yaml manifests in Go templates and used to generate Kubernetes manifests values.yaml contains default values applied during the rendering. The NOTES.txt file is a special template. When a chart is installed, the NOTES.txt template is rendered and displayed rather than being installed into a cluster. _helpers.tpl - helper templates for your other templates (e.g creating same labers across all charts). Files with _ are not rendered to Kubernetes object definitions, but are available everywhere within other chart templates for use. Here we going to show a quick way to create a Helm chart without adding any templating. Step 2 Delete templates folder and create empty one: rm -rf templates mkdir templates cd templates Step 3 Copy existing gowebapp-mysql manifests to templates folder: cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-mysql-pvc.yaml . # PVC cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/secret-mysql.yaml . # Secret cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-mysql-service.yaml . # Service cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-mysql-deployment.yaml . # Deployment Step 4 Install Helm Chart locally: Create dev namespace: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp-mysql kubectl create ns dev kubectl config set-context --current --namespace=dev Render the manifest locally and compare to original manifests: helm template gowebapp-mysql . > render.yaml Install the chart: helm install gowebapp-mysql . Output: NAME: gowebapp-mysql LAST DEPLOYED: Tue Aug 10 15:25:44 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None helm list kubectl get all kubectl get pvc Success We've deployed our own helm chart locally. While we haven't used templating of the chart at all, we have a helm chart that can be installed and upgraded using helm release features. 4.2 Design gowebapp chart. \u00b6 4.2.1 Create gowebapp chart \u00b6 We going to use another method to deploy our second chart gowebapp . In this case we going to use nginx template provided by help team. Task N2: Create gowebapp Helm chart: * Configure `Deployment` template in `values.yaml` * Configure `Service` template in `values.yaml` * Disable `Service account` template in `values.yaml` * Configure `Ingress` template in `values.yaml`: * ingress.class: \"nginx\" * path: \"/\" * pathType: \"ImplementationSpecific\" * host: $STATIC_IP_ADDRESS.nip.io * Templatize the ConfigMap Resource * Ensure the chart is deployable * Ensure `gowebapp` in browser Step 1 Create a new chart: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ helm create gowebapp cd gowebapp tree Output: \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 hpa.yaml \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u251c\u2500\u2500 service.yaml \u2502 \u251c\u2500\u2500 serviceaccount.yaml \u2502 \u2514\u2500\u2500 tests \u2502 \u2514\u2500\u2500 test-connection.yaml \u2514\u2500\u2500 values.yaml edit values.yaml 4.2.2 Template the Deployment \u00b6 Step 1 Update the replicaCount value to 2 in the values.yaml file: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Step 2 Update the repository and tag section to point to your gowebapp docker image in the values.yaml file: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Step 3 Update the resources section in the values.yaml file to include the resource requests and limits the gowebapp application needs: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml See the reference gowebapp-deployment.yaml : cat ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-deployment.yaml Step 4 Update the livness and readiness sections to match what you have in the gowebapp deployment.yaml See the reference gowebapp-deployment.yaml : cat ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-deployment.yaml Update livness and readiness sections for template deployment.yaml : cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/deployment.yaml Step 5 Notice that the deployment.yaml file does not have an environment variable section for secrets , so let's add one. For this chart we will assume that this section is optional based on whether or not a secrets section exist in the Values.yaml file. Step 5-a Include the following in the deployment template: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/deployment.yaml And add following code snippet in the appropriate location: {{- if .Values.secrets }} - env: - name: {{.Values.secrets.name}} valueFrom: secretKeyRef: name: {{.Values.secrets.secretReference.name}} key: {{.Values.secrets.secretReference.key}} {{- end}} Step 5-b Include a section in the values.yaml file: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Add following snippet: secrets: enabled: true name: DB_PASSWORD secretReference: name: mysql key: password Step 6 For this lab, we will include the volumes and volumeMounts sections without templating, so just copy the required sections to the appropriate location in the deployment.yaml template. cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/deployment.yaml See the reference gowebapp-deployment.yaml : cat ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-deployment.yaml Step 7 Render the Chart locally and compare Deployment to original gowebapp-deployment.yaml manifests: helm template gowebapp-mysql . > render.yaml 4.2.3 Template the Service \u00b6 Step 1: The service.yaml template doesn't have an annotation section, so modify the template to add an annotation section, that looks following: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/service.yaml {{- with .Values.annotations }} annotations: {{- toYaml . | nindent 4 }} {{- end }} TODO: Next, modify the values.yaml file to allow chart users to add annotations for the service. Make sure to use the right section in the values.yaml file, based on how you modified your /templates/service.yaml file. Reference documentation: * https://helm.sh/docs/chart_template_guide/control_structures/#modifying-scope-using-with Step 2: Under the service: section in the values.yaml file, update the service port: to 9000. Step 3 In the values.yaml file, update the service type to NodePort Step 4 Render the Chart locally and compare Service to original gowebapp-service.yaml manifests: helm template gowebapp-mysql . > render.yaml 4.2.4 Disable the Service account \u00b6 Step 1 Update the values.yaml file to disable the service account creation for the gowebapp deployment. 4.2.5 Template the Ingress Resource \u00b6 Step 1 enable the ingress the values.yaml file and configur according requirements: : * Expose `gowebapp` using `Ingress` resource: * ingress.class: \"nginx\" * path: \"/\" * pathType: \"ImplementationSpecific\" * host: $STATIC_IP_ADDRESS.nip.io cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Step 2 Render the Chart locally and verify if any issues: helm template gowebapp-mysql . > render.yaml 4.2.5 Templatize the ConfigMap Resource \u00b6 Step 1 Create and templatize configmap resource for our gowebapp that provides connection to Mysql: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp/templates/ cat <<EOF>> configmap.yaml kind: ConfigMap apiVersion: v1 metadata: name: {{ .Values.configMap.name }} data: webapp-config-json: |- {{ .Files.Get \"config.json\" | indent 4 }} EOF Store the config.json inside the chart repository: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp cat <<EOF>> config.json { \"Database\": { \"Type\": \"MySQL\", \"Bolt\": { \"Path\": \"gowebapp.db\" }, \"MongoDB\": { \"URL\": \"127.0.0.1\", \"Database\": \"gowebapp\" }, \"MySQL\": { \"Username\": \"root\", \"Password\": \"rootpasswd\", \"Name\": \"gowebapp\", \"Hostname\": \"gowebapp-mysql\", \"Port\": 3306, \"Parameter\": \"?parseTime=true\" } }, \"Email\": { \"Username\": \"\", \"Password\": \"\", \"Hostname\": \"\", \"Port\": 25, \"From\": \"\" }, \"Recaptcha\": { \"Enabled\": false, \"Secret\": \"\", \"SiteKey\": \"\" }, \"Server\": { \"Hostname\": \"\", \"UseHTTP\": true, \"UseHTTPS\": false, \"HTTPPort\": 80, \"HTTPSPort\": 443, \"CertFile\": \"tls/server.crt\", \"KeyFile\": \"tls/server.key\" }, \"Session\": { \"SecretKey\": \"@r4B?EThaSEh_drudR7P_hub=s#s2Pah\", \"Name\": \"gosess\", \"Options\": { \"Path\": \"/\", \"Domain\": \"\", \"MaxAge\": 28800, \"Secure\": false, \"HttpOnly\": true } }, \"Template\": { \"Root\": \"base\", \"Children\": [ \"partial/menu\", \"partial/footer\" ] }, \"View\": { \"BaseURI\": \"/\", \"Extension\": \"tmpl\", \"Folder\": \"template\", \"Name\": \"blank\", \"Caching\": true } } EOF And finally add following snippet inside values.yaml : edit gowebapp/values.yaml configMap: name: gowebapp Step 2 Render the Chart locally and verify if any issues: helm template gowebapp-mysql . > render.yaml 4.2.6 Deploy gowebapp \u00b6 Before deployment make sure test chart with dry-run and template . cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp helm install gowebapp . Output: NAME: gowebapp LAST DEPLOYED: Tue Aug 10 15:25:44 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None helm ls kubectl get all kubectl get pvc kubectl get ing Access gowebapp with Ingress 4.2.7 Submit assignement \u00b6 Step 1 Commit chart configuration in deploy_ycit020_a4/helm folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Gowebapp Helm Chart\" Step 2 Push commit to the Cloud Source Repositories: git push origin master Step 3 Uninstall chart gowebapp and gowebapp-mysql chart helm install gowebapp helm uninstall gowebapp-mysql Step 4 Resize GKE Cluster to 0 nodes, to avoid charges: cd ~/$MY_REPO/notepad-infrastructure edit terraform.tfvars And set gke_pool_node_count = \"0\" Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars Step 4: Shutdown all Nodes in GKE Cluster Node Pool: terraform apply -var-file terraform.tfvars !!! result GKE Clusters has been scale down to 0 nodes. Part 2 WIP \u00b6 5 Chart Repositories \u00b6 Imagine that our NotePad applicaiton proved to be succesfull and our company wants to sell the software to end customers and enable continious delivery of the applicaiton to the customer, and address software feature requests and bugs in timely manner. In order to distribute our Helm Charts we need to de 6 HelmFile \u00b6 7 Install Istio on GKE \u00b6 Currently Istio community developing following options of Istio deployment on Kubernetes Clusters: Install with Istioctl installation via istioctl command line tool used to showcase Istio functionality. Using the CLI, we generate a YAML file with all Istio resources and then deploy it to the Kubernetes cluster Istio Operator Install Takes installation of Istio to the next level as it's managing not only Istio installation but overall lifecicle of the Istio deployment including Upgrades and configurations. Install with Helm (alpha) Allows to farther simplify Istio deployment and it's customization. 7.0 Prerequisite \u00b6 Scaleup cluster back to 3 nodes and Update VPC firewall rules to enable auto-injection and the istioctl version and istioctl ps commands. References: Opening ports on a private cluster Istio deployment on GKE Private Clusters Step 1: Locate Terraform Configuration directory. cd ~/$MY_REPO/notepad-infrastructure Step 2: Configure GKE Cluster to Scale back to 1 node per zone in a region. edit terraform.tfvars And set gke_pool_node_count = \"1\" Step 3: Create new firewall rule for source range (master-ipv4-cidr) of the cluster, that will open required ports for Istio installation: cat <<EOF> istio_firewall.tf resource \"google_compute_firewall\" \"istio_specific\" { name = format(\"allow-istio-in-privategke-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link source_ranges = [\"172.16.0.0/28\"] allow { protocol = \"tcp\" ports = [\"10250\", \"443\", \"15017\", \"15014\", \"8080\"] } } EOF Step 4: Review TF Plan: terraform plan -var-file terraform.tfvars Step 5: Scale up GKE Cluster Node Pool and update firewall rules for required range: terraform apply -var-file terraform.tfvars Result GKE Clusters has been scalled to 3 nodes and has firewall rules opened Step 6: Verify firewall rule: gcloud compute firewall-rules list --filter=\"name~allow-istio-in-privategke*\" Output: NAME NETWORK DIRECTION PRIORITY ALLOW DENY DISABLED allow-istio-in-privategke-$student-notepad-dev vpc-$student-notepad-dev INGRESS 1000 tcp:10250,tcp:443,tcp:15017,tcp:15014,tcp:8080 False 7.1 Deploy Istio using Custom Resources \u00b6 This installation guide uses the istioctl command line tool to provide rich customization of the Istio control plane and of the sidecars for the Istio data plane. Download and extract the latest release: curl -L https://istio.io/downloadIstio | sh - cd istio-1.11.0 export PATH=$PWD/bin:$PATH Success The above command will fetch Istio packages and untar them in the same folder. tree -L 1 . \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 bin \u251c\u2500\u2500 manifest.yaml \u251c\u2500\u2500 manifests \u251c\u2500\u2500 samples \u2514\u2500\u2500 tools Note Istio installation directory contains: bin directory contains istioctl client binary manifests Installation Profiles, configurations and Helm Charts samples directory contains sample applications deployment tools directory contains auto-completion tooling and etc. Step 2 Deploy Istio Custom Resource Definitions (CRDs) Istio extends Kubernetes using Custom Resource Definitions (CRDs). CRDs allow registration of new/non-default Kubernetes resources. When Istio CRDs are deployed, Istio\u2019s objects are registered as Kubernetes objects, providing a highly integrated experience with Kubernetes as a deployment platform and thus allowing Kubernetes to store configuration of Istio features such as routing, security and telemetry and etc. We going to install using using demo configuration profile. The demo configuration profile allows to experiment with most of Istio features with modest resource requirements. Since it enables high levels of tracing and access logging, it is not suitable for production use cases. istioctl install --set profile=demo -y Output: \u2714 Istio core installed \u2714 Istiod installed \u2714 Egress gateways installed \u2714 Ingress gateways installed \u2714 Installation complete Thank you for installing Istio 1.11. Please take a few minutes to tell us about your install/upgrade experience! Note Wait a few seconds for the CRDs to be committed in the Kubernetes API-server. Step 3 Verify Istio CRDs successfully applied to Kubernetes Cluster. kubectl get crds | grep istio authorizationpolicies.security.istio.io 2021-08-16T19:38:16Z destinationrules.networking.istio.io 2021-08-16T19:38:16Z envoyfilters.networking.istio.io 2021-08-16T19:38:16Z gateways.networking.istio.io 2021-08-16T19:38:17Z istiooperators.install.istio.io 2021-08-16T19:38:17Z peerauthentications.security.istio.io 2021-08-16T19:38:17Z requestauthentications.security.istio.io 2021-08-16T19:38:17Z serviceentries.networking.istio.io 2021-08-16T19:38:17Z sidecars.networking.istio.io 2021-08-16T19:38:17Z telemetries.telemetry.istio.io 2021-08-16T19:38:18Z virtualservices.networking.istio.io 2021-08-16T19:38:18Z workloadentries.networking.istio.io 2021-08-16T19:38:18Z workloadgroups.networking.istio.io 2021-08-16T19:38:19Z Info Above CRDs will be avaialable as a new Kubernetes Resources and stored in Kubernetes ETCD database. The Kubernetes API will represent these new resources as endpoints that can be used as other native Kubernetes object (such as Pod, Services) levereging kubectl, RBAC and other features and admission controllers of Kubernetes. Step 4 Count total number of Installed CRDs: kubectl get crds | grep istio | wc -l Note CRDs count will vary based on Istio version and profile deployed. Step 5 Verify that Istio control plane has been installed successfully. kubectl get pods -n istio-system Output: istio-egressgateway-9dc6cbc49-5wkqt 1/1 Running 0 2m54s istio-ingressgateway-7975cdb749-xjc5g 1/1 Running 0 2m54s istiod-77b4d7b55d-j6kb5 1/1 Running 0 3m9s Info Istio control-plane include following components: istiod - contains components such as Citadel and Pilot istio-ingressgateway Istio Ingress Gateway istio-ingressgateway Istio Egress Gateway Step 6 Verify installation with istioctl CLI: istioctl version istioctl verify-install Output: Checked 13 custom resource definitions Checked 3 Istio Deployments \u2714 Istio is installed and verified successfully Step 5 Deploy Bookinfo sample application: kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-cjj5b 1/1 Running 0 12s productpage-v1-6b746f74dc-sxdxz 1/1 Running 0 10s ratings-v1-b6994bb9-jd985 1/1 Running 0 11s reviews-v1-545db77b95-5kjr4 1/1 Running 0 11s reviews-v2-7bf8c9648f-5v2s9 1/1 Running 0 11s reviews-v3-84779c7bbc-5khlt 1/1 Running 0 11s Note This is a typical Kubernetes deployment, and has nothing specific to Istio. Each Pod has 1 container. Step 6 Remove Bookinfo sample application: kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 7 Enable automatic Envoy sidecar injection. Currently there are 2 method to inject Envoy sidecar inside Istio Mesh: Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment. Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update. Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later: kubectl label namespace default istio-injection=enabled Step 8 Deploy Bookinfo sample application on Istio with Auto Sidecar Injection kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Step 8 Verify deployed application: The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it. kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-zqjbg 2/2 Running 0 13m productpage-v1-6b746f74dc-dj959 2/2 Running 0 12m ratings-v1-b6994bb9-29n4w 2/2 Running 0 12m reviews-v1-545db77b95-p9pgf 2/2 Running 0 12m reviews-v2-7bf8c9648f-4ltkg 2/2 Running 0 12m reviews-v3-84779c7bbc-vhsmw 2/2 Running 0 12m Note Each Pod has now 2 containers. One application container and another is istio-proxy sidecar container. Step 9 Access UI productpage via console: kubectl get services In Gcloud console execute kubectl port-forward : kubectl port-forward --namespace default svc/productpage 8080:9080 Click Web-Preview button in Gcloud, and select preview on port 8080 Click Normal user URL. Success We can access Bookinfo application configured with Istio and Envoy sidecar using private IP via tunnel Step 10 Remove Bookinfo sample application: kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 11 Uninstall Istio istioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f - kubectl delete namespace istio-system kubectl label namespace default istio-injection- 7.2 Deploy Istio using Istio Operator \u00b6 Step 1: Deploy the Istio operator: istioctl operator init Note This command runs the operator by creating the following resources in the istio-operator namespace: The operator custom resource definition The operator controller deployment A service to access operator metrics Necessary Istio operator RBAC rules Step 2: To install the Istio demo configuration profile using the operator, run the following command: kubectl apply -f - <<EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: example-istiocontrolplane spec: profile: demo EOF Note The Istio operator controller begins the process of installing Istio within 90 seconds of the creation of the IstioOperator resource. The Istio installation completes within 120 seconds. Step 3 Verify Operator resource status on Kubernetes Cluster: kubectl get istiooperators -n istio-system Output: NAMESPACE NAME REVISION STATUS AGE istio-system example-istiocontrolplane HEALTHY 25m kubectl describe istiooperators example-istiocontrolplane -n istio-system Spec: Profile: demo Status: Component Status: Base: Status: HEALTHY Egress Gateways: Status: HEALTHY Ingress Gateways: Status: HEALTHY Pilot: Status: HEALTHY Status: HEALTHY Events: <none> kubectl get pods -n istio-system Output: istio-egressgateway-9dc6cbc49-5wkqt 1/1 Running 0 2m54s istio-ingressgateway-7975cdb749-xjc5g 1/1 Running 0 2m54s istiod-77b4d7b55d-j6kb5 1/1 Running 0 3m9s Info Istio control-plane include following components: istiod - contains components such as Citadel and Pilot istio-ingressgateway Istio Ingress Gateway istio-ingressgateway Istio Egress Gateway Verify installation with istioctl CLI: istioctl verify-install Step 5 Enable automatic Envoy sidecar injection. Currently there are 2 method to inject Envoy sidecar inside Istio Mesh: Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment. Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update. Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later: kubectl label namespace default istio-injection=enabled Step 8 Deploy Bookinfo sample application on Istio with Auto Sidecar Injection kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Step 8 Verify deployed application: The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it. kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-zqjbg 2/2 Running 0 13m productpage-v1-6b746f74dc-dj959 2/2 Running 0 12m ratings-v1-b6994bb9-29n4w 2/2 Running 0 12m reviews-v1-545db77b95-p9pgf 2/2 Running 0 12m reviews-v2-7bf8c9648f-4ltkg 2/2 Running 0 12m reviews-v3-84779c7bbc-vhsmw 2/2 Running 0 12m Summary We've leaned to deploy Istio Control plane using istioctl cli and using Istio Operator. We also deployed regular k8s application on the namespace marked with auto injection! Step 9 Cleanup Uninstall Istio Operator kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 10 Uninstall Istio Operator kubectl delete istiooperators.install.istio.io -n istio-system example-istiocontrolplane istioctl operator remove kubectl delete ns istio-system --grace-period=0 --force 8 Commit Readme doc to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy_ycit020_a4/helm and notepad-infrastructure/helm folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"HelmFile Configuration to deploy NotePad and Suppporting Applications\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 9 Cleanup \u00b6 We going to cleanup GCP Service foundation layer with GKE Cluster to avoid excessive cost. cd ~/$MY_REPO/notepad-infrastructure terraform destroy -var-file terraform.tfvars","title":"Assignmen4 Sol"},{"location":"020_Assignment_4_sol_Helm_Foundation/#prerequisite","text":"","title":"Prerequisite"},{"location":"020_Assignment_4_sol_Helm_Foundation/#locate-assignment-4","text":"Step 1 Clone ycit020 repo with Kubernetes manifests, which going to use for our work: cd ~/ycit020 # Alternatively: cd ~ & git clone https://github.com/Cloud-Architects-Program/ycit020 git pull cd ~/ycit020/Assignment4/ ls Result You can see Kubernetes manifests and terraform configs Step 1 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 (Optional) If you terraform config is not working you can copy working config from Assignment4 folder to your notepad-infrastructure folder. Step 4 Create structure for Helm Deployments for supporting applications cd ~/$MY_REPO/notepad-infrastructure mkdir helm cat <<EOF> helm/README.md # Helm Files and values to deploy supporting applications EOF Step 5 Copy Assignment 4 deploy_a4 folder to your repo: cd ~/$MY_REPO/ cp -r ~/ycit020/Assignment4/deploy_a4 deploy_ycit020_a4 ls deploy_ycit020_a4 Result You should see k8s-manifest folder Step 6 Create structure for NotePad Helm Chart locations cd ~/$MY_REPO/deploy_ycit020_a4 mkdir helm cat <<EOF> helm/README.md # Helm Chart, HelmFiles and values to deploy `NotePad` application. EOF","title":"Locate Assignment 4"},{"location":"020_Assignment_4_sol_Helm_Foundation/#reserve-static-ip-addresses","text":"In the next few assignments we might deploy many applications and so we will require expose them using Ingress. When you create an Ingress object, you get a stable external IP address that clients can use to access your Services and in turn, your running containers. The IP address is stable in the sense that it lasts for the lifetime of the Ingress object. If you delete your Ingress and create a new Ingress from the same manifest file, you are not guaranteed to get the same external IP address. We would like to have for each student a a permanent IP address that stays the same across deleting your Ingress and creating a new one For that you must reserve a Regional static External IP address and provide it teacher, so we can setup A Record on the DNS for your behalf. Reference: Reserving a static external IP address Terraform resource google_compute_address Step 1: Create folder for static ip terraform configuration: cd ~/$MY_REPO/ mkdir ip-infrastructure Important The reason we creating a new folder for static_ip creation is because we don't want to destroy or delete this IP throughout our training. Step 1: Create Terraform configuration: Set you current PROJECT_ID value here: export PROJECT_ID=<YOUR_PROJECT_ID> Declare Provider: cd ~/$MY_REPO/ip-infrastructure cat << EOF>> provider.tf terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~> 3.70.0\" } } } EOF Configure global address resource: cat << EOF>> static_ip.tf provider \"google\" { project = var.gcp_project_id } resource \"google_compute_address\" \"regional_external_ip\" { provider = google name = \"static-ingres-ip\" address_type = \"EXTERNAL\" region = \"us-central1\" } EOF Configure variables: cat <<EOF> variables.tf variable \"gcp_project_id\" { type = string description = \"The GCP Seeding project ID\" default = \"\" } EOF Configure tfvars : gcp_project_id = \"$PROJECT_ID\" Configure outputs: cat <<EOF >> outputs.tf output \"addresses\" { description = \"Global IPv4 address for proxy load balancing to the nearest Ingress controller\" value = google_compute_address.regional_external_ip.address } output \"name\" { description = \"Static IP Name\" value = google_compute_address.regional_external_ip.name } EOF Step 2: Apply Terraform configuration: Initialize: terraform init Plan and Deploy Infrastructure: terraform plan -var-file terraform.tfvars terraform apply -var-file terraform.tfvars Set variable: cd ~/$MY_REPO/ip-infrastructure export STATIC_IP_ADDRESS=$(terraform output | grep 'addresses' |awk '{ print $3}') export STATIC_IP_NAME=$(terraform output | grep 'name' |awk '{ print $3}') Create readme: cat <<EOF> README.md Generated Static IP for Ingress: * Name - $STATIC_IP_NAME # Used Ingress Manifest * Address - $STATIC_IP_ADDRESS # Used to Configure DNS A Record EOF Step 6 Commit deploy folder using the following Git commands: cd ~/$MY_REPO git status git add . git commit -m \"adding documentation for ycit020 assignment 4\"","title":"Reserve Static IP addresses"},{"location":"020_Assignment_4_sol_Helm_Foundation/#commit-to-repository","text":"Step 1 Commit ip-infrastructure and helm folders using the following Git commands: cd ~/$MY_REPO git status git add . git commit -m \"Assignement 4\" Step 2 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"Commit to Repository"},{"location":"020_Assignment_4_sol_Helm_Foundation/#1-install-and-configure-helm-client","text":"","title":"1 Install and Configure Helm Client"},{"location":"020_Assignment_4_sol_Helm_Foundation/#11-install-helm-3","text":"GCP Cloud Shell comes with many common tools pre-installed including helm . Step 1: Verify and validate the version of Helm that is installed: helm --version Output: version.BuildInfo{Version:\"v3.5.0\"} Result Helm 3 is installed Note Until November 2020, two different major versions of Helm were actively maintained. The current stable major version of Helm is version 3. Note Helm follows a versioning convention known as Semantic Versioning (SemVer). In Semantic Versioning, the version number conveys meaning about what you can expect in the release. Because Helm follows this specification, users can expect certain things out of releases simply by carefully reading the version number. At its core, a semantic version has three numerical components and an optional stability marker (for alphas, betas, and release candidates). Here are some examples: v1.0.0 v3.3.2 SemVer represents format of X.Y.Z , where X is a major version, Y is a minor version and Z is a patch release: The major release number tends to be incremented infrequently. It indicates that major changes have been made to Helm, and that some of those changes may break compatibility with previous versions. The difference between Helm 2 and Helm 3 is substantial, and there is work necessary to migrate between the versions. The minor release number indicates feature additions. The difference between 3.2.0 and 3.3.0 might be that a few small new features were added. However, there are no breaking changes between versions. (With one caveat: a security fix might necessitate a breaking change, but we announce boldly when that is the case.) The patch release number indicates that only backward compatible bug fixes have been made between this release and the last one. It is always recommended to stay at the latest patch release. (OPTIONAL) Step 2: If you want to use specific version of helm or want to install helm in you local machine (On macOS and Linux,) use following link to install Helm. The usual sequence of commands for installing this way is as follows: $ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 $ chmod 700 get_helm.sh $ ./get_helm.sh The preceding commands fetch the latest version of the get_helm.sh script, and then use that to find and install the latest version of Helm 3. Alternatively, you can download latest binary of you OS choice here .","title":"1.1 Install Helm 3"},{"location":"020_Assignment_4_sol_Helm_Foundation/#13-deploy-gke-cluster","text":"We going to reuse Terraform configuration to deploy our GKE Cluster. And we assume that terraform.tfvars has been properly configured already with required values. Step 1: Locate Terraform Configuration directory. cd ~/$MY_REPO/notepad-infrastructure Step 2: Initialize Terraform Providers terraform init Step 3: Increase GKE Node Pool VM size edit terraform.tfvars Update gke_pool_machine_type from e2-small to e2-highcpu-4 , to support larger workloads. Step 4: Review TF Plan: terraform plan -var-file terraform.tfvars Step 5: Create GKE Cluster and Node Pool: terraform apply -var-file terraform.tfvars !!! result GKE Clusters has been created","title":"1.3 Deploy GKE cluster"},{"location":"020_Assignment_4_sol_Helm_Foundation/#14-configure-helm","text":"Helm interacts directly with the Kubernetes API server. For that reason, Helm needs to be able to connect to a Kubernetes cluster. Helm attempts to do this automatically by reading the same configuration files used by kubectl . Helm will try to find this information by reading the environment variable $KUBECONFIG. Step 1 Authenticate to the cluster. export STUDENT_NAME= gcloud container clusters get-credentials gke-$STUDENT_NAME-notepad-dev --region us-central1 Step 2 Test that kubectl client connected to GKE cluster: kubectl get pods Output: No resources found in default namespace. Step 3 Test that helm client connected to GKE cluster: helm list Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION Result As expected there are no Charts has been Installed on our cluster yet Summary Helm 3 has been installed and configured to work with our cluster. Let's deploy some charts!","title":"1.4 Configure Helm"},{"location":"020_Assignment_4_sol_Helm_Foundation/#2-basic-helm-chart-installation","text":"","title":"2 Basic Helm Chart Installation"},{"location":"020_Assignment_4_sol_Helm_Foundation/#21-searching-chart","text":"A Helm chart is an package that can be installed into your Kubernetes cluster. During chart development, you will often just work with a chart that is stored on your local system and later pushed to GitHub. But when it comes to sharing charts, Helm describes a standard format for indexing and sharing information about Helm charts . A Helm chart repository is simply a set of files, reachable over the network, that conforms to the Helm specification for indexing packages. There are huge number of chart repositories on the internet. The easiest way to find the popular repositories is to use your web browser to navigate to the Artifact Hub . There you will find thousands of Helm charts, each hosted on an appropriate repository. Deprecation In the past all charts were located and maintained by Helm Kubernetes Community Git repositories, known as https://github.com/kubernetes/charts, and had 2 types: Stable Incubator All the charts were rendered in Web UI via Helm Hub page. While the central Git Repository to maintain charts was great idea, with fast growing Helm popularity, it become hard to impossible to manage and maintain it by small group of maintainers, as they had to aprove hundreds of PR per day and frustrating for chart contributors as they had to wait several weeks their PR to be reviewed and approved. As a result GitHub project for Helm stable and incubator charts as well as [Helm Hub] has been deprecated and archived. All the charts are now maintained by independent contributors in their subsequent repo's. (e.g vault, is under hashicorp/vault-helm repo) and the central place to find all active and official Charts can be foind in Artifact Hub . Important Helm 2 came with a Helm repository installed by default. The stable chart repository was at one time the official source of production-ready Helm charts. As we discussed above stable chart repository has been now deprecated. In Helm 3, there is no default repository. Users are encouraged to use the Artifact Hub to find what they are looking for and then add their preferred repositories. Step 1: Helm provides native way to search charts from CLI in artifacthub.io cd ~/$MY_REPO/notepad-infrastructure/helm helm search hub drupal Output: https://artifacthub.io/packages/helm/bitnami/drupal 10.2.30 9.2.3 One of the most versatile open source content m... https://artifacthub.io/packages/helm/cetic/drupal 0.1.0 1.16.0 Drupal is a free and open-source web content ma.. Result Search is a good way to find existing Helm packages Step 2: Use the link to navigate to Artifact Hub Drupal chart: https://artifacthub.io/packages/helm/bitnami/drupal Note Drupal is OSS content management systems that can be installed this days on K8s. Result: The Artifact Page opened with information about the chart, it's parameters and how to use the chart. Step 3: Review Github source Code of the chart: https://github.com/bitnami/charts/tree/master/bitnami/drupal Result: Github page with Drupal Helm Chart itself, where you can browse the templates , values.yaml , Chart.yaml to understand how this chart is actually working and if you have any issues with the chart this would be the right location to open the issue or send a PR with new feature if you decide to contribute.","title":"2.1 Searching Chart"},{"location":"020_Assignment_4_sol_Helm_Foundation/#22-adding-a-chart-repository","text":"Once you found a chart, it's logical to install it. However first step you need to do is to add a Chart Repository. Step 1: Adding a Helm chart is done with the helm repo add command: helm repo add bitnami https://charts.bitnami.com/bitnami Note Bitnami is company well known to package application for Any Platforms and Cloud environments. With popularity of Helm, Bitnami developers were among the core contributors who designed the Helm repository system. They have contributed to the establishment of Helm\u2019s best practices for chart development and have written many of the most widely used charts. Bitnami is now part of VMware, provides IT organizations with an enterprise offering that is secure, compliant, continuously maintained and customizable to your organizational policies. Step 2: Now, we can verify that the Bitnami repository exists by running a helm repo list command: helm repo list Output: NAME URL bitnami https://charts.bitnami.com/bitnami Result This command shows us all of the repositories installed for Helm. Right now, we see only the Bitnami repository that we just added.","title":"2.2 Adding a Chart Repository"},{"location":"020_Assignment_4_sol_Helm_Foundation/#23-helm-chart-installation","text":"Step 1: At very minimum, installing a chart in Helm requires just 2 pieces of information: the name of the installation and the chart you want to install: helm install mywebsite bitnami/drupal Output: NAME: mywebsite LAST DEPLOYED: Sun Aug 8 08:25:29 2021 NAMESPACE: prometheus STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ******************************************************************* *** PLEASE BE PATIENT: Drupal may take a few minutes to install *** ******************************************************************* 1. Get the Drupal URL: NOTE: It may take a few minutes for the LoadBalancer IP to be available. Watch the status with: 'kubectl get svc --namespace prometheus -w mywebsite-drupal' export SERVICE_IP=$(kubectl get svc --namespace prometheus mywebsite-drupal --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\") echo \"Drupal URL: http://$SERVICE_IP/\" 2. Get your Drupal login credentials by running: echo Username: user echo Password: $(kubectl get secret --namespace prometheus mywebsite-drupal -o jsonpath=\"{.data.drupal-password}\" | base64 --decode) Result When using Helm, you will see that output for each installation. A good chart would provide helpful notes on how to connect to the deployed solution. Tip You can get notes information any time after helm installation using helm get notes mywebsite command. Step 2: Follow your Drupal Helm Chart notes Instruction to access Website 1. Get the Drupal URL: NOTE: It may take a few minutes for the LoadBalancer IP to be available. Watch the status with: 'kubectl get svc --namespace test -w mywebsite-drupal' export SERVICE_IP=$(kubectl get svc --namespace test mywebsite-drupal --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\") echo \"Drupal URL: http://$SERVICE_IP/\" Success We can access My blog website and login with provider user and password. You can start your own blog now, that runs on Kubernetes Step 3: List deployed Kubernetes resources: kubectl get all --namespace test kubectl get pvc --namespace test Summary Our Drupal website consist of MariaDB statefulset , Drupal deployment , 2 pvc s and services . Mywebsite Drupal service is type LoadBalancer and that's how we able to access it. Step 4: List installed chart: helm list Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION mywebsite default 1 2021-08-08 08:25:29.625862 -0400 EDT deployed drupal-10.2.30 9.2.3 Note Like other commands, helm list is namespace aware. By default, Helm uses the namespace your Kubernetes configuration file sets as the default. Usually this is the namespace named default . Step 5: Deploy same chart in namespace test : kubectl create ns test helm install --namespace test mywebsite bitnami/drupal --create-namespace Tip By adding --create-namespace , indicates to Helm that we acknowledge that there may not be a namespace with that name already, and we just want one to be created. List deployed chart in namespace test : helm list --namespace test Summary In Helm 2, instance names were cluster-wide. You could only have an instance named mywebsite once per cluster. In Helm 3, naming has been changed. Now instance names are scoped to Kubernetes namespaces. We could install 2 instances named mywebsite as long as they each lived in a different namespace. Step 5: Cleanup Helm Drupal deployments using helm uninstall command: helm uninstall mywebsite helm uninstall mywebsite -n test kubectl delete ns test","title":"2.3 Helm Chart Installation"},{"location":"020_Assignment_4_sol_Helm_Foundation/#3-advanced-helm-chart-installation","text":"","title":"3 Advanced Helm Chart Installation"},{"location":"020_Assignment_4_sol_Helm_Foundation/#321-deploy-nginx-ingress-chart-with-custom-configuration","text":"Let's deploy another Helm application on our Kubernetes cluster: Ingress Nginx - is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer. In the previous classes we used Google implementation of Ingress - GKE Ingress for HTTP(S) Load Balancing . While this solutions provides managed Ingress experience and advanced features like Cloud Armor, DDoS protection and Identity aware proxy. Ingress Nginx Controller is popular solution and has a lot of features and integrations. If you want to deploy Kubernetes Application on different cloud providers or On-prem the same way, Nginx Ingress Controller becomes a default option. Our task is to configure Ingress Nginx using type:LoadBalancer with Regional Static IP configured in the section ### Reserve Static IP addresses . Additionally we want to enable metrics service that will fetch Prometheus monitoring metrics from ingress and disable admissionWebhooks configuration. Step 1: Let's search ingress-nginx in artifacthub.io helm search hub ingress-nginx Output: URL CHART VERSION APP VERSION DESCRIPTION https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx 4.0.0 1.0.0-beta.1 Ingress controller for Kubernetes using NGINX https://artifacthub.io/packages/helm/api/ingress-nginx 3.29.1 0.45.0 Ingress controller for Kubernetes using NGINX Result We going to select the first ingress-nginx chart that is maintained by Kubernetes Community Step 2: Review Hub Page details about this chart and locate information how to add repository: https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx Result: The Artifact Page opened with information about the chart, it's parameters, and how to use the chart itself Step 3: Add ingress-nginx Repo: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update Step 4: Oftentimes, searching is a useful way to find not only what charts can be installed, but what versions are available: helm search repo ingress-nginx --versions | head Output: NAME CHART VERSION APP VERSION DESCRIPTION ingress-nginx/ingress-nginx 3.35.0 0.48.1 Ingress controller for Kubernetes using NGINX a... ingress-nginx/ingress-nginx 3.34.0 0.47.0 Ingress controller for Kubernetes using NGINX a... ingress-nginx/ingress-nginx 3.33.0 0.47.0 Ingress controller for Kubernetes using NGINX a... .... Note By default, Helm tries to install the latest stable release of a chart, but you can override this behavior and install a specific version of a chart. Thus it is often useful to see not just the summary info for a chart, but exactly which versions exist for a chart. Every new version of the chart can be bring fixes and new changes, so for production use it's better to go with tested version and pin installation version. Summary We going with latest listed official version of the chart 3.35.0 of ingress-nginx Chart.","title":"3.2.1 Deploy NGINX Ingress Chart with Custom Configuration"},{"location":"020_Assignment_4_sol_Helm_Foundation/#322-download-and-inspect-chart-locally","text":"Step 1: Pull ingress-nginx Helm Chart of specific version to Local filesystem: cd ~/$MY_REPO/notepad-infrastructure/helm helm pull ingress-nginx/ingress-nginx --version 3.35.0 tar -xvzf ingress-nginx-3.35.0.tgz Step 2: See the tree structure of the chart sudo apt-get install tree tree -L 2 ingress-nginx Output: ingress-nginx \u251c\u2500\u2500 CHANGELOG.md \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 OWNERS \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ci \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 admission-webhooks \u2502 \u251c\u2500\u2500 clusterrole.yaml \u2502 \u251c\u2500\u2500 clusterrolebinding.yaml \u2502 \u251c\u2500\u2500 controller-configmap-addheaders.yaml \u2502 \u251c\u2500\u2500 controller-configmap-proxyheaders.yaml \u2502 \u251c\u2500\u2500 controller-configmap-tcp.yaml \u2502 \u251c\u2500\u2500 controller-configmap-udp.yaml \u2502 \u251c\u2500\u2500 controller-configmap.yaml \u2502 \u251c\u2500\u2500 controller-daemonset.yaml \u2502 \u251c\u2500\u2500 controller-deployment.yaml \u2502 \u251c\u2500\u2500 controller-hpa.yaml \u2502 \u251c\u2500\u2500 controller-ingressclass.yaml \u2502 \u251c\u2500\u2500 controller-keda.yaml \u2502 \u251c\u2500\u2500 controller-poddisruptionbudget.yaml \u2502 \u251c\u2500\u2500 controller-prometheusrules.yaml \u2502 \u251c\u2500\u2500 controller-psp.yaml \u2502 \u251c\u2500\u2500 controller-role.yaml \u2502 \u251c\u2500\u2500 controller-rolebinding.yaml \u2502 \u251c\u2500\u2500 controller-service-internal.yaml \u2502 \u251c\u2500\u2500 controller-service-metrics.yaml \u2502 \u251c\u2500\u2500 controller-service-webhook.yaml \u2502 \u251c\u2500\u2500 controller-service.yaml \u2502 \u251c\u2500\u2500 controller-serviceaccount.yaml \u2502 \u251c\u2500\u2500 controller-servicemonitor.yaml \u2502 \u251c\u2500\u2500 default-backend-deployment.yaml \u2502 \u251c\u2500\u2500 default-backend-hpa.yaml \u2502 \u251c\u2500\u2500 default-backend-poddisruptionbudget.yaml \u2502 \u251c\u2500\u2500 default-backend-psp.yaml \u2502 \u251c\u2500\u2500 default-backend-role.yaml \u2502 \u251c\u2500\u2500 default-backend-rolebinding.yaml \u2502 \u251c\u2500\u2500 default-backend-service.yaml \u2502 \u251c\u2500\u2500 default-backend-serviceaccount.yaml \u2502 \u2514\u2500\u2500 dh-param-secret.yaml \u2514\u2500\u2500 values.yaml Result Typical structure of the helm chart, where: The Chart.yaml file contains metadata and some functionality controls for the chart templates folder contains all yaml manifests in Go templates and used to generate Kubernetes manifests values.yaml contains default values applied during the rendering. The NOTES.txt file is a special template. When a chart is installed, the NOTES.txt template is rendered and displayed rather than being installed into a cluster.","title":"3.2.2 Download and inspect Chart locally"},{"location":"020_Assignment_4_sol_Helm_Foundation/#322-helm-template","text":"As a reminder our goal it customize Nginx deployment to configure Ingress Nginx using type:LoadBalancer with Regional Static IP configured in the section ### Reserve Static IP addresses . Additionally we want to enable metrics service that will fetch Prometheus monitoring metrics from ingress and disable admissionWebhooks configuration. Step 1: Review values.yaml cd ~/$MY_REPO/notepad-infrastructure/helm/ingress-nginx edit values.yaml Result It is pretty confusing at this point to understand what this chart is going to deploy, it would of been great to render to YAML format first. Step 2: Execute helm template command and store output in render.yaml helm template ingress-nginx/ingress-nginx --version 3.35.0 > render.yaml Note During helm template, Helm never contacts a remote Kubernetes server, hence the chart only has access to default Kubernetes kinds. The template command always acts like an installation. Review rendered values: edit render.yaml grep \"Source\" render.yaml Result helm template designed to isolate the template rendering process of Helm from the installation. The template command performs following phases of Helm Lifecycle: loads the chart determines the values renders the templates formats to YAML Step 3: Let's first create a configuration that will disable admissionWebhooks . Review looking in values.yaml for admissionWebhooks configuration: grep -A10 admissionWebhooks values.yaml Output: admissionWebhooks: annotations: {} enabled: true failurePolicy: Fail # timeoutSeconds: 10 port: 8443 certificate: \"/usr/local/certificates/cert\" key: \"/usr/local/certificates/key\" namespaceSelector: {} objectSelector: {} Result admissionWebhooks enabled: true Step 3: Let's create custom_values.yaml file with admissionWebhooks disabled cat << EOF>> custom_values.yaml controller: admissionWebhooks: enabled: false EOF Execute helm template and store output in new file render2.yaml : helm template ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 > render2.yaml Now comparing this 2 render files you can see the what change in configuration has resulted in the rendered file: diff render.yaml render2.yaml This is the YAML files that will be generated after this change: grep \"Source\" render2.yaml Output: # Source: ingress-nginx/templates/controller-serviceaccount.yaml # Source: ingress-nginx/templates/controller-configmap.yaml # Source: ingress-nginx/templates/clusterrole.yaml # Source: ingress-nginx/templates/clusterrolebinding.yaml # Source: ingress-nginx/templates/controller-role.yaml # Source: ingress-nginx/templates/controller-rolebinding.yaml # Source: ingress-nginx/templates/controller-service.yaml # Source: ingress-nginx/templates/controller-deployment.yaml Step 4: Now, let's add a custom Configuration to controller-service.yaml to use Static loadBalancerIP , we've configured in step ### Reserve Static IP addresses . First we need to locate the correct service parameters: grep -A20 \"service:\" values.yaml Output Shows that we have 4 different services in values file Let's narrow our search: grep -A20 \"service:\" values.yaml | grep \"List of IP address\" Output: ## List of IP addresses at which the controller services are available ## List of IP addresses at which the stats-exporter service is available ## List of IP addresses at which the default backend service is available Output: service: enabled: true annotations: {} labels: {} # clusterIP: \"\" ## List of IP addresses at which the controller services are available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] # loadBalancerIP: \"\" loadBalancerSourceRanges: [] Result It seems controller is first service: in values.yaml and currently is it has loadBalancerIP section commented Step 5: Let's add loadBalancerIP IP configuration to custom_values.yaml Set variable: cd ~/$MY_REPO/ip-infrastructure export STATIC_IP_ADDRESS=$(terraform output | grep 'addresses' |awk '{ print $3}') echo $STATIC_IP_ADDRESS Add custom values: cd ~/$MY_REPO/notepad-infrastructure/helm/ingress-nginx cat << EOF>> custom_values.yaml service: loadBalancerIP: $STATIC_IP_ADDRESS EOF Our final custom_values.yaml should look as following: controller: admissionWebhooks: enabled: false service: loadBalancerIP: 35.X.X.X metrics: enabled: true Execute helm template and store output in new file render3.yaml : helm template ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 > render3.yaml Show the difference in rendered file: diff render2.yaml render3.yaml Output: > loadBalancerIP: 35.X.X.X Ensure that this change is in fact belongs to controller-service.yaml : grep -C16 \"loadBalancerIP:\" render3.yaml Result We configured correct service that will Ingress Controller service with type: Loadbalancer and static IP with previously generated with terraform. Summary helm template is great tool to render you charts to YAML. Tip You can use Helm as packaging you charts only. And then use helm template to generate actual YAMLs and apply them with kubectl apply","title":"3.2.2 Helm Template"},{"location":"020_Assignment_4_sol_Helm_Foundation/#323-dry-runs","text":"Before applying configuration to Kubernetes Cluster it is good idea to Dry-Run it for Errors. This is especially important if you doing Upgrades. The dry-run feature provides Helm users a way to debug the output of a chart before it is sent on to Kubernetes. With all of the templates rendered, you can inspect exactly what would have been submitted to your cluster. And with the release data, you can verify that the release would have been created as you expected. Here is some of the dry-run working principals: --dry-run mixes non-YAML information with the rendered templates. This means the data has to be cleaned up before being sent to tools like kubectl. A --dry-run on upgrade can produce different YAML output than a --dry-run on install, and this can be confusing. It contacts the Kubernetes API server for validation, which means Helm has to have Kubernetes credentials even if it is just used to --dry-run a release. It also inserts information into the template engine that is cluster-specific. Because of this, the output of some rendering processes may be cluster-specific. Difference between dry-run and template is that the dry-run contacts Kubernetes API. It is good idea to use dry-run prior deployment and upgrade as it creates mutated output, while template doesn't contacts API and can to pure rendering. Step 1: Execute our deployment with dry-run command: helm install ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --dry-run Output: NAME: ingress-nginx LAST DEPLOYED: Tue Aug 10 07:22:56 2021 NAMESPACE: default STATUS: pending-install REVISION: 1 TEST SUITE: None HOOKS: MANIFEST: --- # Source: ingress-nginx/templates/controller-serviceaccount.yaml ........ NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace default get services -o wide -w ingress-nginx-controller' An example Ingress that makes use of the controller: ... If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: <base64 encoded cert> tls.key: <base64 encoded key> type: kubernetes.io/tls At the top of the output, it will print some information about the release in our case it tells what phase of the release it is in (pending-install), and the revision number. Next, after the informational block, all of the rendered templates are dumped to standard output. Finally, at the bottom of the dry-run output, Helm prints the user-oriented release notes: Note --dry-run dumps the output validates, but doesn't deploy actual chart. Step 2: Finally let's deploy Nginx Ingress Charts with our parameters in ingress-nginx namespace kubectl create ns ingress-nginx helm install ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx Step 3: Verify Helm Deployment: helm list Follow Installation note and verify Ingress-Controller Service: kubectl --namespace ingress-nginx get services -o wide ingress-nginx-controller Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 172.10.0.213 35.192.101.76 80:30930/TCP,443:30360/TCP 3m9s","title":"3.2.3 Dry-Runs"},{"location":"020_Assignment_4_sol_Helm_Foundation/#324-deploy-drupal-using-nginx-ingress-controller","text":"Let's test our newly configured ingress-nginx-controller and expose drupal application using ingress instead of LoadBalancer . Check if STATIC_IP_ADDRESS variable is set: echo $STATIC_IP_ADDRESS If not set it with you Static IP address value. cd ~/$MY_REPO/notepad-infrastructure/helm/ kubectl delete pvc data-mywebsite-mariadb-0 cat << EOF>> drupal_ing_values.yaml ingress: annotations: {kubernetes.io/ingress.class: \"nginx\"} enabled: true hostname: $STATIC_IP_ADDRESS.nip.io path: / pathType: ImplementationSpecific EOF Note Here we using nip.io domain that provides simple wildcard DNS for any IP Address, however if you provided you STATIC_IP to teacher, they will generate following DNS, and you can replace hostname: with value $student-name.cloud-montreal.ca helm install mywebsite bitnami/drupal --values drupal_ing_values.yaml 1. Get the Drupal URL: You should be able to access your new Drupal installation through http://35.X.X.X.nip.io/ Success Drupal site is now accessible via Ingress. Our Nginx Ingress Controller has been setup correctly! Uninstall Drupal: helm uninstall mywebsite kubectl delete pvc data-mywebsite-mariadb-0","title":"3.2.4 Deploy Drupal using Nginx Ingress Controller"},{"location":"020_Assignment_4_sol_Helm_Foundation/#325-using-set-values-and-upgrading-charts","text":"In addition to --value option, there is a second flag that can be used to add individual parameters to an install or upgrade . The --set flag takes one or more values directly. They do not need to be stored in a YAML file. Step 1: Let's update our ingress-nginx release with new parameter --set controller.image.pullPolicy=Always , but first we want to render template with and without parameter to see what the change will be applied: helm template ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx > render4.yaml helm template ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=Always > render5.yaml diff render4.yaml render5.yaml Output: < imagePullPolicy: IfNotPresent --- > imagePullPolicy: Always Step 2: Let's apply update to our ingress-nginx release. For that we going to use helm upgrade command: helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=Always Verify that upgrade was successful: helm list --namespace ingress-nginx Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ingress-nginx ingress-nginx 2 2021-08-10 10:48:35.265506 -0400 EDT deployed ingress-nginx-3.35.0 0.48.1 Result We can see that Revision change to 2 Note When we talk about upgrading in Helm, we talk about upgrading an installation, not a chart. An installation is a particular instance of a chart in your cluster. When you run helm install , it creates the installation. To modify that installation, use helm upgrade . This is an important distinction to make in the present context because upgrading an installation can consist of two different kinds of changes: * You can upgrade the version of the chart * You can upgrade the configuration of the installation In this case we upgrading the configuration of the installation. Extra If you interested upgrade chart version of ingress-nginx to the next available release or 3.34.0 , give it a try.","title":"3.2.5  Using  --set values and Upgrading Charts"},{"location":"020_Assignment_4_sol_Helm_Foundation/#325-listing-releases-history-rollbacks","text":"Step 1: Let's see if we can upgrade our release with wrong value pullPolicy=NoSuchPolicy that doesn't exist on Kubernetes. We will first run in dry-run mode helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=NoSuchPolicy --dry-run Result Release \"ingress-nginx\" has been upgraded. Happy Helming! Let's now apply same config without --dry-run mode: helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx --set controller.image.pullPolicy=NoSuchPolicy Output: Error: UPGRADE FAILED: cannot patch \"ingress-nginx-controller\" with kind Deployment: Deployment.apps \"ingress-nginx-controller\" is invalid: spec.template.spec.containers[0].imagePullPolicy: Unsupported value: \"NoSuchPolicy\": supported values: \"Always\", \"IfNotPresent\", \"Never\" Failed As the error message indicates, a pull policy cannot be set to NoSuchPolicy. This error came from the Kubernetes API server, which means Helm submitted the manifest, and Kubernetes rejected it. So our release should be in a failed state. Verify with helm list to confirm failed state: helm list Output: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ingress-nginx ingress-nginx 3 2021-08-10 11:03:05.224405 -0400 EDT failed ingress-nginx-3.35.0 0.48.1 Step 2: List all releases with helm history : helm history ingress-nginx -n ingress-nginx REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Tue Aug 10 08:47:32 2021 superseded ingress-nginx-3.35.0 0.48.1 Install complete 2 Tue Aug 10 10:48:35 2021 deployed ingress-nginx-3.35.0 0.48.1 Upgrade complete 3 Tue Aug 10 11:03:05 2021 failed ingress-nginx-3.35.0 0.48.1 Upgrade \"ingress-nginx\" failed: cannot patch \"ingress-nginx-controller\" with kind Deployment: Deployment.apps \"ingress-nginx-controller\" is invalid: spec.template.spec.containers[0].imagePullPolicy: Unsupported value: \"NoSuchPolicy\": supported values: \"Always\", \"IfNotPresent\", \"Never\" Info During the life cycle of a release, it can pass through several different statuses. Here they are, approximately in the order you would likely see them: pending-install Before sending the manifests to Kubernetes, Helm claims the installation by creating a release (marked version 1) whose status is set to pending-install. deployed As soon as Kubernetes accepts the manifest from Helm, Helm updates the release record, marking it as deployed. pending-upgrade When a Helm upgrade is begun, a new release is created for an installation (e.g., v2), and its status is set to pending-upgrade. superseded When an upgrade is run, the last deployed release is updated, marked as superseded, and the newly upgraded release is changed from pending-upgrade to deployed. pending-rollback If a rollback is created, a new release (e.g., v3) is created, and its status is set to pending-rollback until Kubernetes accepts the release manifest. Then it is marked deployed and the last release is marked superseded. uninstalling When a helm uninstall is executed, the most recent release is read and then its status is changed to uninstalling. uninstalled If history is preserved during deletion, then when the helm uninstall is complete, the last release\u2019s status is changed to uninstalled. failed Finally, if during any operation, Kubernetes rejects a manifest submitted by Helm, Helm will mark that release failed. Step 3: Rollback release with helm rollback . From the error, we know that the release failed because we supplied an invalid image pull policy. So of course we could correct this by simply running another helm upgrade. But imagine a case where the cause of error was not readily available. Rather than leave the application in a failed state while diagnosing the problem, it would be nice to simply revert back to the release that worked before. helm rollback ingress-nginx 2 -n ingress-nginx helm history ingress-nginx -n ingress-nginx helm list 4 Tue Aug 10 11:16:24 2021 deployed ingress-nginx-3.35.0 0.48.1 Rollback to 2 --- ingress-nginx ingress-nginx 4 2021-08-10 11:16:24.424371 -0400 EDT deployed ingress-nginx-3.35.0 0.48.1 Rollback was a success! Happy Helming! Success This command tells Helm to fetch the ingress-nginx version 2 release, and resubmit that manifest to Kubernetes. A rollback does not restore to a previous snapshot of the cluster. Helm does not track enough information to do that. What it does is resubmit the previous configuration, and Kubernetes attempts to reset the resources to match.","title":"3.2.5  Listing Releases, History, Rollbacks"},{"location":"020_Assignment_4_sol_Helm_Foundation/#325-upgrade-release-with-helm-diff-plugin","text":"While Helm provides a lot of functionality out of the box. Community contributes a lot of great functionality via helm plugins. Plugins allow you to add extra functionality to Helm and integrate seamlessly with the CLI, making them a popular choice for users with unique workflow requirements. There are a number of third-party plugins available online for common use cases, such as secrets management. In addition, plugins are incredibly easy to build on your own for unique, one-off tasks. Helm plugins are external tools that are accessible directly from the Helm CLI. They allow you to add custom subcommands to Helm without making any modifications to Helm\u2019s Go source code. Many third-party plugins are made open source and publicly available on GitHub. Many of these plugins use the \u201chelm-plugin\u201d tag/topic to make them easy to find. Refer to the documentation for Helm plugins on GitHub Let's Upgrade our running Ingress Nginx chart with metrics service that will enable Prometheus to scrape metrics from our Nginx. Step 1: Update custom_values.yaml with following information: cat << EOF>> custom_values.yaml metrics: enabled: true EOF Step 2: So far we've used helm template to render and compare manifests. However most of the time you might not be able to do it during upgrades. However you want to have a clear understanding what will be upgraded if you change something. That's where Helm Diff Plugin can be handy. Helm Diff plugin giving your a preview of what a helm upgrade would change. It basically generates a diff between the latest deployed version of a release and a helm upgrade --dry-run Install Helm Diff Plugin: helm plugin install https://github.com/databus23/helm-diff helm diff upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx Output: - imagePullPolicy: Always + imagePullPolicy: IfNotPresent + protocol: TCP + - name: metrics + containerPort: 1025 + # Source: ingress-nginx/templates/controller-service-metrics.yaml + apiVersion: v1 + kind: Service ... Result Amazing! This looks like terraform plan but for helm :) Important Another discovery from above printout is that since we forgot to add --set imagePullPolicy command, our value will be reverted with upgrade. This is really important to understand as you configuration maybe lost if you use --set Step 3: Let's upgrade our ingress-nginx release: helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 3.35.0 --namespace ingress-nginx helm list -n ingress-nginx Success Our ingress-nginx is ready to run. Going forward we going to always deploy this chart. Summary So far we've learned: How to customize helm chart deployment using --set and --values using values.yaml Using values.yaml preferable mehtod in order to achieve reproducible deployments How to upgrade and rollback releases Helm template and dry-run Helm Diff Plugin","title":"3.2.5  Upgrade Release with Helm Diff Plugin"},{"location":"020_Assignment_4_sol_Helm_Foundation/#326-install-minio-with-helm","text":"Let's deploy another Helm application on our Kubernetes cluster: Minio - high-performance, S3 compatible object storage. helm install myminio bitnami/minio Follow your Minio Helm Chart notes Instruction to access Website: To access the MinIO&reg; web UI: - Get the MinIO&reg; URL: echo \"MinIO&reg; web URL: http://127.0.0.1:9000/minio\" kubectl port-forward --namespace default svc/myminio 8080:9000 In Gcloud console execute kubectl port-forward : kubectl port-forward --namespace default svc/myminio 8080:9000 Result We can access Minio UI using private IP via tunnel and we have to use autogenerated ACCESS_KEY and SECRET_KEY. While it looks easy to start with helm and deploy any charts, most of the time you want to have a full control of the deployment process. You want to specify, tested version, safe values or configurations that make sense for you or your organization.","title":"3.2.6 Install Minio with Helm"},{"location":"020_Assignment_4_sol_Helm_Foundation/#327-customize-minio-installation-task","text":"Task N1: Customize Minio Installation with following values: * Deploy `Minio` of specific version: `7.1.7` * Expose Minio using `Ingress` resource: * ingress.class: \"nginx\" * path: \"/\" * pathType: \"ImplementationSpecific\" * * Specify custom `ACCESS_KEY` for `Minio` frontend: `myaccesskey` * Specify custom `SECRET_KEY` for `Minio` frontend: `mysecretkey` Step 1: Use the following Minio Chart: helm search hub minio URL CHART VERSION APP VERSION DESCRIPTION https://artifacthub.io/packages/helm/bitnami/minio 7.1.7 2021.6.17 Bitnami Object Storage based on MinIO&reg ..... Result We going to choose bitnami package again as it's top of the list and seems maintained the most. Step 2: Since we already added bitnami repository locally, let's verify if we can list minio in it: helm search repo minio Result We can now go ahead and install minio Step 4: Pull Minio Helm Chart of specific version to Local filesystem to work with values file. cd ~/$MY_REPO/notepad-infrastructure/helm helm pull bitnami/minio --version 7.1.7 cd minio cat <<EOF> custom_values.yaml TODO: Finish the remaining steps EOF TODO: Complete custom_values.yaml and test deployment of Minio . Test Minio by creating bucket and uploading object to it. Step 5 Commit deploy_ycit020_a4/helm and notepad-infrastructure/helm folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Helm values for Minio\" Step 6 Push commit to the Cloud Source Repositories: git push origin master Solution: Step 5: Let's review values.yaml : cd minio grep -A60 \"service:\" values.yaml | grep -v \"#\" Output: service: type: ClusterIP port: 9000 nodePort: \"\" loadBalancerIP: \"\" loadBalancerSourceRanges: [] externalTrafficPolicy: Cluster annotations: {} ingress: enabled: false certManager: false apiVersion: \"\" hostname: foo.example.com path: / pathType: ImplementationSpecific servicePort: minio Result By default Minio frontend will have ClusterIP type, let's try it with LoadBalancer Step 6: Let's review values.yaml and search Minio Access specific information: grep -A20 \"mode:\" values.yaml | grep -v \"#\" Output: mode: standalone accessKey: password: \"\" forcePassword: false secretKey: password: \"\" forcePassword: false Result By default Minio will generate accessKey.password and secretKey.password, let's create our own. Step 7: Create custom_values.yaml which will enable Ingress: cat <<EOF> custom_values.yaml accessKey: password: myaccesskey forcePassword: false secretKey: password: mysecretkey forcePassword: false ingress: annotations: {kubernetes.io/ingress.class: \"nginx\"} enabled: true hostname: 35.192.101.76.nip.io path: / pathType: ImplementationSpecific servicePort: minio EOF Step 8: Install Minio Chart with Custom Configuration helm install myminio bitnami/minio --values custom_values.yaml --version 7.1.6","title":"3.2.7 Customize Minio Installation Task"},{"location":"020_Assignment_4_sol_Helm_Foundation/#4-creating-notepad-helm-charts","text":"So far with learned how to deploy existing charts from the Artifact Hub. However sometimes you or your company needs to build software that require's to be distributed and shared externally, as well as have lifecycle and release management. Helm Charts becomes are valuable option for that, specifically if you application is based of containers! Imagine that our solution is Go-based NotePad has first customer that wants to deploy it on their system and they requested delivery via Helm Charts that available on GCP based Helm Repository. To achieve such task we going to create 2 Charts: * `gowebapp-mysql` chart * `gowebapp` chart And store them in Google Artifact Registry that provides Helm 3 OCI Repository.","title":"4 Creating NotePad Helm Charts"},{"location":"020_Assignment_4_sol_Helm_Foundation/#41-design-gowebapp-mysql-chart","text":"","title":"4.1 Design gowebapp-mysql chart"},{"location":"020_Assignment_4_sol_Helm_Foundation/#411-create-gowebapp-mysql-chart","text":"As scary as it sounds creating a new basic helm chart is 5 minute thing! We going to learn 2 quick methods to create helm charts, that will help you to save time and get started with helm quickly. Helm includes the helm create command to make it easy for you to create a chart of your own, and it\u2019s a great way to get started. The create command creates a chart for you, with all the required chart structure and files. These files are documented to help you understand what is needed, and the templates it provides showcase multiple Kubernetes manifests working together to deploy an application. In addition, you can install and test this chart right out of the box. Step 1 Create a new chart: cd ~/$MY_REPO/deploy_ycit020_a4/helm helm create gowebapp-mysql cd gowebapp-mysql tree Output: \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 hpa.yaml \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u251c\u2500\u2500 service.yaml \u2502 \u251c\u2500\u2500 serviceaccount.yaml \u2502 \u2514\u2500\u2500 tests \u2502 \u2514\u2500\u2500 test-connection.yaml \u2514\u2500\u2500 values.yaml Result This command creates a new Nginx chart, with a name of your choice, following best practices for a chart layout. Since Kubernetes clusters can have different methods to expose an application, this chart makes the way Nginx is exposed to network traffic configurable so it can be exposed in a wide variety of clusters. The chart has following structure: The Chart.yaml file contains metadata and some functionality controls for the chart The charts folder currently empty may container charts dependency of the top level chart. For example we could of make our gowebapp-mysql as dependency chart for gowebapp templates folder contains all yaml manifests in Go templates and used to generate Kubernetes manifests values.yaml contains default values applied during the rendering. The NOTES.txt file is a special template. When a chart is installed, the NOTES.txt template is rendered and displayed rather than being installed into a cluster. _helpers.tpl - helper templates for your other templates (e.g creating same labers across all charts). Files with _ are not rendered to Kubernetes object definitions, but are available everywhere within other chart templates for use. Here we going to show a quick way to create a Helm chart without adding any templating. Step 2 Delete templates folder and create empty one: rm -rf templates mkdir templates cd templates Step 3 Copy existing gowebapp-mysql manifests to templates folder: cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-mysql-pvc.yaml . # PVC cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/secret-mysql.yaml . # Secret cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-mysql-service.yaml . # Service cp -r ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-mysql-deployment.yaml . # Deployment Step 4 Install Helm Chart locally: Create dev namespace: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp-mysql kubectl create ns dev kubectl config set-context --current --namespace=dev Render the manifest locally and compare to original manifests: helm template gowebapp-mysql . > render.yaml Install the chart: helm install gowebapp-mysql . Output: NAME: gowebapp-mysql LAST DEPLOYED: Tue Aug 10 15:25:44 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None helm list kubectl get all kubectl get pvc Success We've deployed our own helm chart locally. While we haven't used templating of the chart at all, we have a helm chart that can be installed and upgraded using helm release features.","title":"4.1.1  Create gowebapp-mysql chart"},{"location":"020_Assignment_4_sol_Helm_Foundation/#42-design-gowebapp-chart","text":"","title":"4.2  Design gowebapp chart."},{"location":"020_Assignment_4_sol_Helm_Foundation/#421-create-gowebapp-chart","text":"We going to use another method to deploy our second chart gowebapp . In this case we going to use nginx template provided by help team. Task N2: Create gowebapp Helm chart: * Configure `Deployment` template in `values.yaml` * Configure `Service` template in `values.yaml` * Disable `Service account` template in `values.yaml` * Configure `Ingress` template in `values.yaml`: * ingress.class: \"nginx\" * path: \"/\" * pathType: \"ImplementationSpecific\" * host: $STATIC_IP_ADDRESS.nip.io * Templatize the ConfigMap Resource * Ensure the chart is deployable * Ensure `gowebapp` in browser Step 1 Create a new chart: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ helm create gowebapp cd gowebapp tree Output: \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 hpa.yaml \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u251c\u2500\u2500 service.yaml \u2502 \u251c\u2500\u2500 serviceaccount.yaml \u2502 \u2514\u2500\u2500 tests \u2502 \u2514\u2500\u2500 test-connection.yaml \u2514\u2500\u2500 values.yaml edit values.yaml","title":"4.2.1  Create gowebapp chart"},{"location":"020_Assignment_4_sol_Helm_Foundation/#422-template-the-deployment","text":"Step 1 Update the replicaCount value to 2 in the values.yaml file: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Step 2 Update the repository and tag section to point to your gowebapp docker image in the values.yaml file: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Step 3 Update the resources section in the values.yaml file to include the resource requests and limits the gowebapp application needs: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml See the reference gowebapp-deployment.yaml : cat ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-deployment.yaml Step 4 Update the livness and readiness sections to match what you have in the gowebapp deployment.yaml See the reference gowebapp-deployment.yaml : cat ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-deployment.yaml Update livness and readiness sections for template deployment.yaml : cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/deployment.yaml Step 5 Notice that the deployment.yaml file does not have an environment variable section for secrets , so let's add one. For this chart we will assume that this section is optional based on whether or not a secrets section exist in the Values.yaml file. Step 5-a Include the following in the deployment template: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/deployment.yaml And add following code snippet in the appropriate location: {{- if .Values.secrets }} - env: - name: {{.Values.secrets.name}} valueFrom: secretKeyRef: name: {{.Values.secrets.secretReference.name}} key: {{.Values.secrets.secretReference.key}} {{- end}} Step 5-b Include a section in the values.yaml file: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Add following snippet: secrets: enabled: true name: DB_PASSWORD secretReference: name: mysql key: password Step 6 For this lab, we will include the volumes and volumeMounts sections without templating, so just copy the required sections to the appropriate location in the deployment.yaml template. cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/deployment.yaml See the reference gowebapp-deployment.yaml : cat ~/$MY_REPO/deploy_ycit020_a4/k8s-manifests/gowebapp-deployment.yaml Step 7 Render the Chart locally and compare Deployment to original gowebapp-deployment.yaml manifests: helm template gowebapp-mysql . > render.yaml","title":"4.2.2 Template the Deployment"},{"location":"020_Assignment_4_sol_Helm_Foundation/#423-template-the-service","text":"Step 1: The service.yaml template doesn't have an annotation section, so modify the template to add an annotation section, that looks following: cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/templates/service.yaml {{- with .Values.annotations }} annotations: {{- toYaml . | nindent 4 }} {{- end }} TODO: Next, modify the values.yaml file to allow chart users to add annotations for the service. Make sure to use the right section in the values.yaml file, based on how you modified your /templates/service.yaml file. Reference documentation: * https://helm.sh/docs/chart_template_guide/control_structures/#modifying-scope-using-with Step 2: Under the service: section in the values.yaml file, update the service port: to 9000. Step 3 In the values.yaml file, update the service type to NodePort Step 4 Render the Chart locally and compare Service to original gowebapp-service.yaml manifests: helm template gowebapp-mysql . > render.yaml","title":"4.2.3 Template the Service"},{"location":"020_Assignment_4_sol_Helm_Foundation/#424-disable-the-service-account","text":"Step 1 Update the values.yaml file to disable the service account creation for the gowebapp deployment.","title":"4.2.4 Disable the Service account"},{"location":"020_Assignment_4_sol_Helm_Foundation/#425-template-the-ingress-resource","text":"Step 1 enable the ingress the values.yaml file and configur according requirements: : * Expose `gowebapp` using `Ingress` resource: * ingress.class: \"nginx\" * path: \"/\" * pathType: \"ImplementationSpecific\" * host: $STATIC_IP_ADDRESS.nip.io cd ~/$MY_REPO/deploy_ycit020_a4/helm/ edit gowebapp/values.yaml Step 2 Render the Chart locally and verify if any issues: helm template gowebapp-mysql . > render.yaml","title":"4.2.5 Template the Ingress Resource"},{"location":"020_Assignment_4_sol_Helm_Foundation/#425-templatize-the-configmap-resource","text":"Step 1 Create and templatize configmap resource for our gowebapp that provides connection to Mysql: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp/templates/ cat <<EOF>> configmap.yaml kind: ConfigMap apiVersion: v1 metadata: name: {{ .Values.configMap.name }} data: webapp-config-json: |- {{ .Files.Get \"config.json\" | indent 4 }} EOF Store the config.json inside the chart repository: cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp cat <<EOF>> config.json { \"Database\": { \"Type\": \"MySQL\", \"Bolt\": { \"Path\": \"gowebapp.db\" }, \"MongoDB\": { \"URL\": \"127.0.0.1\", \"Database\": \"gowebapp\" }, \"MySQL\": { \"Username\": \"root\", \"Password\": \"rootpasswd\", \"Name\": \"gowebapp\", \"Hostname\": \"gowebapp-mysql\", \"Port\": 3306, \"Parameter\": \"?parseTime=true\" } }, \"Email\": { \"Username\": \"\", \"Password\": \"\", \"Hostname\": \"\", \"Port\": 25, \"From\": \"\" }, \"Recaptcha\": { \"Enabled\": false, \"Secret\": \"\", \"SiteKey\": \"\" }, \"Server\": { \"Hostname\": \"\", \"UseHTTP\": true, \"UseHTTPS\": false, \"HTTPPort\": 80, \"HTTPSPort\": 443, \"CertFile\": \"tls/server.crt\", \"KeyFile\": \"tls/server.key\" }, \"Session\": { \"SecretKey\": \"@r4B?EThaSEh_drudR7P_hub=s#s2Pah\", \"Name\": \"gosess\", \"Options\": { \"Path\": \"/\", \"Domain\": \"\", \"MaxAge\": 28800, \"Secure\": false, \"HttpOnly\": true } }, \"Template\": { \"Root\": \"base\", \"Children\": [ \"partial/menu\", \"partial/footer\" ] }, \"View\": { \"BaseURI\": \"/\", \"Extension\": \"tmpl\", \"Folder\": \"template\", \"Name\": \"blank\", \"Caching\": true } } EOF And finally add following snippet inside values.yaml : edit gowebapp/values.yaml configMap: name: gowebapp Step 2 Render the Chart locally and verify if any issues: helm template gowebapp-mysql . > render.yaml","title":"4.2.5 Templatize the ConfigMap Resource"},{"location":"020_Assignment_4_sol_Helm_Foundation/#426-deploy-gowebapp","text":"Before deployment make sure test chart with dry-run and template . cd ~/$MY_REPO/deploy_ycit020_a4/helm/gowebapp helm install gowebapp . Output: NAME: gowebapp LAST DEPLOYED: Tue Aug 10 15:25:44 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None helm ls kubectl get all kubectl get pvc kubectl get ing Access gowebapp with Ingress","title":"4.2.6 Deploy gowebapp"},{"location":"020_Assignment_4_sol_Helm_Foundation/#427-submit-assignement","text":"Step 1 Commit chart configuration in deploy_ycit020_a4/helm folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"Gowebapp Helm Chart\" Step 2 Push commit to the Cloud Source Repositories: git push origin master Step 3 Uninstall chart gowebapp and gowebapp-mysql chart helm install gowebapp helm uninstall gowebapp-mysql Step 4 Resize GKE Cluster to 0 nodes, to avoid charges: cd ~/$MY_REPO/notepad-infrastructure edit terraform.tfvars And set gke_pool_node_count = \"0\" Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars Step 4: Shutdown all Nodes in GKE Cluster Node Pool: terraform apply -var-file terraform.tfvars !!! result GKE Clusters has been scale down to 0 nodes.","title":"4.2.7 Submit assignement"},{"location":"020_Assignment_4_sol_Helm_Foundation/#part-2-wip","text":"","title":"Part 2 WIP"},{"location":"020_Assignment_4_sol_Helm_Foundation/#5-chart-repositories","text":"Imagine that our NotePad applicaiton proved to be succesfull and our company wants to sell the software to end customers and enable continious delivery of the applicaiton to the customer, and address software feature requests and bugs in timely manner. In order to distribute our Helm Charts we need to de","title":"5 Chart Repositories"},{"location":"020_Assignment_4_sol_Helm_Foundation/#6-helmfile","text":"","title":"6 HelmFile"},{"location":"020_Assignment_4_sol_Helm_Foundation/#7-install-istio-on-gke","text":"Currently Istio community developing following options of Istio deployment on Kubernetes Clusters: Install with Istioctl installation via istioctl command line tool used to showcase Istio functionality. Using the CLI, we generate a YAML file with all Istio resources and then deploy it to the Kubernetes cluster Istio Operator Install Takes installation of Istio to the next level as it's managing not only Istio installation but overall lifecicle of the Istio deployment including Upgrades and configurations. Install with Helm (alpha) Allows to farther simplify Istio deployment and it's customization.","title":"7 Install Istio on GKE"},{"location":"020_Assignment_4_sol_Helm_Foundation/#70-prerequisite","text":"Scaleup cluster back to 3 nodes and Update VPC firewall rules to enable auto-injection and the istioctl version and istioctl ps commands. References: Opening ports on a private cluster Istio deployment on GKE Private Clusters Step 1: Locate Terraform Configuration directory. cd ~/$MY_REPO/notepad-infrastructure Step 2: Configure GKE Cluster to Scale back to 1 node per zone in a region. edit terraform.tfvars And set gke_pool_node_count = \"1\" Step 3: Create new firewall rule for source range (master-ipv4-cidr) of the cluster, that will open required ports for Istio installation: cat <<EOF> istio_firewall.tf resource \"google_compute_firewall\" \"istio_specific\" { name = format(\"allow-istio-in-privategke-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link source_ranges = [\"172.16.0.0/28\"] allow { protocol = \"tcp\" ports = [\"10250\", \"443\", \"15017\", \"15014\", \"8080\"] } } EOF Step 4: Review TF Plan: terraform plan -var-file terraform.tfvars Step 5: Scale up GKE Cluster Node Pool and update firewall rules for required range: terraform apply -var-file terraform.tfvars Result GKE Clusters has been scalled to 3 nodes and has firewall rules opened Step 6: Verify firewall rule: gcloud compute firewall-rules list --filter=\"name~allow-istio-in-privategke*\" Output: NAME NETWORK DIRECTION PRIORITY ALLOW DENY DISABLED allow-istio-in-privategke-$student-notepad-dev vpc-$student-notepad-dev INGRESS 1000 tcp:10250,tcp:443,tcp:15017,tcp:15014,tcp:8080 False","title":"7.0 Prerequisite"},{"location":"020_Assignment_4_sol_Helm_Foundation/#71-deploy-istio-using-custom-resources","text":"This installation guide uses the istioctl command line tool to provide rich customization of the Istio control plane and of the sidecars for the Istio data plane. Download and extract the latest release: curl -L https://istio.io/downloadIstio | sh - cd istio-1.11.0 export PATH=$PWD/bin:$PATH Success The above command will fetch Istio packages and untar them in the same folder. tree -L 1 . \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 bin \u251c\u2500\u2500 manifest.yaml \u251c\u2500\u2500 manifests \u251c\u2500\u2500 samples \u2514\u2500\u2500 tools Note Istio installation directory contains: bin directory contains istioctl client binary manifests Installation Profiles, configurations and Helm Charts samples directory contains sample applications deployment tools directory contains auto-completion tooling and etc. Step 2 Deploy Istio Custom Resource Definitions (CRDs) Istio extends Kubernetes using Custom Resource Definitions (CRDs). CRDs allow registration of new/non-default Kubernetes resources. When Istio CRDs are deployed, Istio\u2019s objects are registered as Kubernetes objects, providing a highly integrated experience with Kubernetes as a deployment platform and thus allowing Kubernetes to store configuration of Istio features such as routing, security and telemetry and etc. We going to install using using demo configuration profile. The demo configuration profile allows to experiment with most of Istio features with modest resource requirements. Since it enables high levels of tracing and access logging, it is not suitable for production use cases. istioctl install --set profile=demo -y Output: \u2714 Istio core installed \u2714 Istiod installed \u2714 Egress gateways installed \u2714 Ingress gateways installed \u2714 Installation complete Thank you for installing Istio 1.11. Please take a few minutes to tell us about your install/upgrade experience! Note Wait a few seconds for the CRDs to be committed in the Kubernetes API-server. Step 3 Verify Istio CRDs successfully applied to Kubernetes Cluster. kubectl get crds | grep istio authorizationpolicies.security.istio.io 2021-08-16T19:38:16Z destinationrules.networking.istio.io 2021-08-16T19:38:16Z envoyfilters.networking.istio.io 2021-08-16T19:38:16Z gateways.networking.istio.io 2021-08-16T19:38:17Z istiooperators.install.istio.io 2021-08-16T19:38:17Z peerauthentications.security.istio.io 2021-08-16T19:38:17Z requestauthentications.security.istio.io 2021-08-16T19:38:17Z serviceentries.networking.istio.io 2021-08-16T19:38:17Z sidecars.networking.istio.io 2021-08-16T19:38:17Z telemetries.telemetry.istio.io 2021-08-16T19:38:18Z virtualservices.networking.istio.io 2021-08-16T19:38:18Z workloadentries.networking.istio.io 2021-08-16T19:38:18Z workloadgroups.networking.istio.io 2021-08-16T19:38:19Z Info Above CRDs will be avaialable as a new Kubernetes Resources and stored in Kubernetes ETCD database. The Kubernetes API will represent these new resources as endpoints that can be used as other native Kubernetes object (such as Pod, Services) levereging kubectl, RBAC and other features and admission controllers of Kubernetes. Step 4 Count total number of Installed CRDs: kubectl get crds | grep istio | wc -l Note CRDs count will vary based on Istio version and profile deployed. Step 5 Verify that Istio control plane has been installed successfully. kubectl get pods -n istio-system Output: istio-egressgateway-9dc6cbc49-5wkqt 1/1 Running 0 2m54s istio-ingressgateway-7975cdb749-xjc5g 1/1 Running 0 2m54s istiod-77b4d7b55d-j6kb5 1/1 Running 0 3m9s Info Istio control-plane include following components: istiod - contains components such as Citadel and Pilot istio-ingressgateway Istio Ingress Gateway istio-ingressgateway Istio Egress Gateway Step 6 Verify installation with istioctl CLI: istioctl version istioctl verify-install Output: Checked 13 custom resource definitions Checked 3 Istio Deployments \u2714 Istio is installed and verified successfully Step 5 Deploy Bookinfo sample application: kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-cjj5b 1/1 Running 0 12s productpage-v1-6b746f74dc-sxdxz 1/1 Running 0 10s ratings-v1-b6994bb9-jd985 1/1 Running 0 11s reviews-v1-545db77b95-5kjr4 1/1 Running 0 11s reviews-v2-7bf8c9648f-5v2s9 1/1 Running 0 11s reviews-v3-84779c7bbc-5khlt 1/1 Running 0 11s Note This is a typical Kubernetes deployment, and has nothing specific to Istio. Each Pod has 1 container. Step 6 Remove Bookinfo sample application: kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 7 Enable automatic Envoy sidecar injection. Currently there are 2 method to inject Envoy sidecar inside Istio Mesh: Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment. Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update. Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later: kubectl label namespace default istio-injection=enabled Step 8 Deploy Bookinfo sample application on Istio with Auto Sidecar Injection kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Step 8 Verify deployed application: The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it. kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-zqjbg 2/2 Running 0 13m productpage-v1-6b746f74dc-dj959 2/2 Running 0 12m ratings-v1-b6994bb9-29n4w 2/2 Running 0 12m reviews-v1-545db77b95-p9pgf 2/2 Running 0 12m reviews-v2-7bf8c9648f-4ltkg 2/2 Running 0 12m reviews-v3-84779c7bbc-vhsmw 2/2 Running 0 12m Note Each Pod has now 2 containers. One application container and another is istio-proxy sidecar container. Step 9 Access UI productpage via console: kubectl get services In Gcloud console execute kubectl port-forward : kubectl port-forward --namespace default svc/productpage 8080:9080 Click Web-Preview button in Gcloud, and select preview on port 8080 Click Normal user URL. Success We can access Bookinfo application configured with Istio and Envoy sidecar using private IP via tunnel Step 10 Remove Bookinfo sample application: kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 11 Uninstall Istio istioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f - kubectl delete namespace istio-system kubectl label namespace default istio-injection-","title":"7.1 Deploy Istio using Custom Resources"},{"location":"020_Assignment_4_sol_Helm_Foundation/#72-deploy-istio-using-istio-operator","text":"Step 1: Deploy the Istio operator: istioctl operator init Note This command runs the operator by creating the following resources in the istio-operator namespace: The operator custom resource definition The operator controller deployment A service to access operator metrics Necessary Istio operator RBAC rules Step 2: To install the Istio demo configuration profile using the operator, run the following command: kubectl apply -f - <<EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: example-istiocontrolplane spec: profile: demo EOF Note The Istio operator controller begins the process of installing Istio within 90 seconds of the creation of the IstioOperator resource. The Istio installation completes within 120 seconds. Step 3 Verify Operator resource status on Kubernetes Cluster: kubectl get istiooperators -n istio-system Output: NAMESPACE NAME REVISION STATUS AGE istio-system example-istiocontrolplane HEALTHY 25m kubectl describe istiooperators example-istiocontrolplane -n istio-system Spec: Profile: demo Status: Component Status: Base: Status: HEALTHY Egress Gateways: Status: HEALTHY Ingress Gateways: Status: HEALTHY Pilot: Status: HEALTHY Status: HEALTHY Events: <none> kubectl get pods -n istio-system Output: istio-egressgateway-9dc6cbc49-5wkqt 1/1 Running 0 2m54s istio-ingressgateway-7975cdb749-xjc5g 1/1 Running 0 2m54s istiod-77b4d7b55d-j6kb5 1/1 Running 0 3m9s Info Istio control-plane include following components: istiod - contains components such as Citadel and Pilot istio-ingressgateway Istio Ingress Gateway istio-ingressgateway Istio Egress Gateway Verify installation with istioctl CLI: istioctl verify-install Step 5 Enable automatic Envoy sidecar injection. Currently there are 2 method to inject Envoy sidecar inside Istio Mesh: Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment. Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update. Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later: kubectl label namespace default istio-injection=enabled Step 8 Deploy Bookinfo sample application on Istio with Auto Sidecar Injection kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Step 8 Verify deployed application: The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it. kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-zqjbg 2/2 Running 0 13m productpage-v1-6b746f74dc-dj959 2/2 Running 0 12m ratings-v1-b6994bb9-29n4w 2/2 Running 0 12m reviews-v1-545db77b95-p9pgf 2/2 Running 0 12m reviews-v2-7bf8c9648f-4ltkg 2/2 Running 0 12m reviews-v3-84779c7bbc-vhsmw 2/2 Running 0 12m Summary We've leaned to deploy Istio Control plane using istioctl cli and using Istio Operator. We also deployed regular k8s application on the namespace marked with auto injection! Step 9 Cleanup Uninstall Istio Operator kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 10 Uninstall Istio Operator kubectl delete istiooperators.install.istio.io -n istio-system example-istiocontrolplane istioctl operator remove kubectl delete ns istio-system --grace-period=0 --force","title":"7.2 Deploy Istio using Istio Operator"},{"location":"020_Assignment_4_sol_Helm_Foundation/#8-commit-readme-doc-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy_ycit020_a4/helm and notepad-infrastructure/helm folder using the following Git commands: cd ~/$MY_REPO git add . git commit -m \"HelmFile Configuration to deploy NotePad and Suppporting Applications\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"8 Commit Readme doc to repository and share it with Instructor/Teacher"},{"location":"020_Assignment_4_sol_Helm_Foundation/#9-cleanup","text":"We going to cleanup GCP Service foundation layer with GKE Cluster to avoid excessive cost. cd ~/$MY_REPO/notepad-infrastructure terraform destroy -var-file terraform.tfvars","title":"9 Cleanup"},{"location":"020_Assignment_5_sol_EFK/","text":"Lab 5 - Logging with EFK Stack In a complicated distributed system such as Kubernetes, we will have different logs for different components, and extracting insights from logs can be a daunting task. The EFK stack (ElasticSearch, FluentD, Kibana) can help make this task easier. In this Lab, we are going to deploy EFK stack. Deploy robot-shop app that emits logs to stdout in JSON format. We will then deploy Cloud Native FluentD logging agent and configure Input Plugin /var/log/containers folder (location used by docker daemon on a Kubernetes node to store stdout from running containers) and tail Input Plugin . And configure FluentD Output Plugin host and logstash_prefix to send to logs to ElastackSearch, under logstash-* prefix. Objective: Install Elasticsearch and Kibanna Deploy online boutique Application Install and Configure FluentD Configure ElasticSearch with FluentD 1 Install Elasticsearch and Kibana \u00b6 1.1 Elasticsearch Installation \u00b6 Create a Kubernetes Cluster in your lab environment. Make sure cluster has nodes with 4 cores each. Create a new namespace for EFK stack: kubectl create ns efk Install Elasticsearch using Helm: helm repo add elastic https://helm.elastic.co helm install elasticsearch elastic/elasticsearch -n efk Check the status of deployment: kubectl get pods --namespace=efk -l app=elasticsearch-master -w 1.2 Kibana installation \u00b6 Deploy Kibana with Helm: helm install kibana elastic/kibana -n efk Set up port forwarding for temporary access to Kibanna: kubectl -n efk port-forward deployment/kibana-kibana 5601 Access Kibana from your browser at http://localhost:5601 or from cloud shell Web Preview (Make sure the port number matches) For permanant access to Kibana, modify the service type form ClusterIP to LoadBalancer : kubectl -n efk patch svc kibana-kibana -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' 1.3 Fluentd installation \u00b6 Deploy Fluentd with Helm: helm repo add fluent https://fluent.github.io/helm-charts helm repo update helm install fluentd fluent/fluentd -n efk 2 View Logs from Kibana \u00b6 Locate Kibana Public IP kubectl get svc -n efk | grep kibana Launch the Kibana web interface http://<Public_IP>:5601 You should see your Kibana interface Click on Discover in the left-hand navigation menu. Create an index pattern. Use fluentd* as the index pattern name. More documentation can be found here Next, configure which field Kibana will use to filter log data by time. In the dropdown, select the @timestamp field, and hit Create index pattern . Now, Click on Discover again, you should see logs from your Kubernetes cluster. Try different filters, see how you can navigate logs and find valuable information about the cluster. 3 Deploy a sample application \u00b6 Deploy microservices application online boutique . Create Namespace onlineboutique kubectl create ns onlineboutique Deploy Microservice application kubectl apply -f https://raw.githubusercontent.com/Cloud-Architects-Program/microservices-demo/main/release/kubernetes-manifests.yaml -n onlineboutique Verify Deployment kubectl get all -n onlineboutique 4 Configure Fluentd for specific logs \u00b6 In many cases, you only need your applications' logs being sent to your logging stack. To achieve this, Fluentd should be configured to only intake specific logs so that no resources are wasted. Review sources configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A20 01_sources.conf 01_sources.conf: |- <source> @type tail @id in_tail_container_logs @label @KUBERNETES path /var/log/containers/*.log pos_file /var/log/fluentd-containers.log.pos tag kubernetes.* read_from_head true <parse> @type multi_format <pattern> format json time_key time time_type string time_format \"%Y-%m-%dT%H:%M:%S.%NZ\" keep_time_key false </pattern> <pattern> format regexp expression /^(?<time>.+) (?<stream>stdout|stderr)( (.))? (?<log>.*)$/ Note id : A unique identifier to reference this source. This can be used for further filtering and routing of structured log data type : Inbuilt directive understood by fluentd. In this case, \u201ctail\u201d instructs fluentd to gather data by tailing logs from a given location. Another example is \u201chttp\u201d which instructs fluentd to collect data by using GET on http endpoint. path : Specific to type \u201ctail\u201d. Instructs fluentd to collect all logs under /var/log/containers directory. This is the location used by docker daemon on a Kubernetes node to store stdout from running containers pos_file : Used as a checkpoint. In case the fluentd process restarts, it uses the position from this file to resume log data collection tag : A custom string for matching source to destination/filters. fluentd matches source/destination tags to route log data Step 2 Review Filter configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A20 03_dispatch.conf 02_filters.conf: |- <label @KUBERNETES> <match kubernetes.var.log.containers.fluentd**> @type relabel @label @FLUENT_LOG </match> <filter kubernetes.**> @type kubernetes_metadata @id filter_kube_metadata skip_labels false skip_container_metadata false skip_namespace_metadata true skip_master_url true </filter> <match **> @type relabel @label @DISPATCH </match> Step 3 Review Dispatch configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A20 03_dispatch.conf 03_dispatch.conf: |- <label @DISPATCH> <filter **> @type prometheus <metric> name fluentd_input_status_num_records_total type counter desc The total number of incoming records <labels> tag ${tag} hostname ${hostname} </labels> </metric> </filter> <match **> @type relabel @label @OUTPUT </match> </label> Step 4 Review Output Plugin configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A10 04_outputs.conf 04_outputs.conf: |- <label @OUTPUT> <match **> @type elasticsearch host \"elasticsearch-master\" port 9200 path \"\" user elastic password changeme </match> </label> Note match : tag indicates a destination. It is followed by a regular expression for matching the source. In this case, we want to capture all logs and send them to Elasticsearch, so simply use ** type : Supported output plugin identifier. In this case, we are using ElasticSearch which is a built-in plugin of fluentd. host/port : ElasticSearch host/port. Credentials can be configured as well, but not shown here. To configure Fluentd to only send one namespace's logs to elasticsearch, modify 03_dispatch.conf so that only log files that matches onlineboutique namespace is labeled to be sent: 03_dispatch.conf: |- <label @DISPATCH> <filter **> @type prometheus <metric> name fluentd_input_status_num_records_total type counter desc The total number of incoming records <labels> tag ${tag} hostname ${hostname} </labels> </metric> </filter> <match kubernetes.var.log.containers.**_onlineboutique_**> @type relabel @label @OUTPUT </match> </label> <match kubernetes.var.log.containers.**_onlineboutique_**> will filter out logs with _onlineboutique_ . This configuration will only relabel the logs that matches the configuration as @OUTPUT . As specified in 04_outputs.conf , only logs labelled as @OUTPUT will be sent to elasticsearch. Note that this is not the only way to configure fluentd to send one namespace's logs.","title":"020 Assignment 5 sol EFK"},{"location":"020_Assignment_5_sol_EFK/#1-install-elasticsearch-and-kibana","text":"","title":"1 Install Elasticsearch and Kibana"},{"location":"020_Assignment_5_sol_EFK/#11-elasticsearch-installation","text":"Create a Kubernetes Cluster in your lab environment. Make sure cluster has nodes with 4 cores each. Create a new namespace for EFK stack: kubectl create ns efk Install Elasticsearch using Helm: helm repo add elastic https://helm.elastic.co helm install elasticsearch elastic/elasticsearch -n efk Check the status of deployment: kubectl get pods --namespace=efk -l app=elasticsearch-master -w","title":"1.1 Elasticsearch Installation"},{"location":"020_Assignment_5_sol_EFK/#12-kibana-installation","text":"Deploy Kibana with Helm: helm install kibana elastic/kibana -n efk Set up port forwarding for temporary access to Kibanna: kubectl -n efk port-forward deployment/kibana-kibana 5601 Access Kibana from your browser at http://localhost:5601 or from cloud shell Web Preview (Make sure the port number matches) For permanant access to Kibana, modify the service type form ClusterIP to LoadBalancer : kubectl -n efk patch svc kibana-kibana -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'","title":"1.2 Kibana installation"},{"location":"020_Assignment_5_sol_EFK/#13-fluentd-installation","text":"Deploy Fluentd with Helm: helm repo add fluent https://fluent.github.io/helm-charts helm repo update helm install fluentd fluent/fluentd -n efk","title":"1.3 Fluentd installation"},{"location":"020_Assignment_5_sol_EFK/#2-view-logs-from-kibana","text":"Locate Kibana Public IP kubectl get svc -n efk | grep kibana Launch the Kibana web interface http://<Public_IP>:5601 You should see your Kibana interface Click on Discover in the left-hand navigation menu. Create an index pattern. Use fluentd* as the index pattern name. More documentation can be found here Next, configure which field Kibana will use to filter log data by time. In the dropdown, select the @timestamp field, and hit Create index pattern . Now, Click on Discover again, you should see logs from your Kubernetes cluster. Try different filters, see how you can navigate logs and find valuable information about the cluster.","title":"2 View Logs from Kibana"},{"location":"020_Assignment_5_sol_EFK/#3-deploy-a-sample-application","text":"Deploy microservices application online boutique . Create Namespace onlineboutique kubectl create ns onlineboutique Deploy Microservice application kubectl apply -f https://raw.githubusercontent.com/Cloud-Architects-Program/microservices-demo/main/release/kubernetes-manifests.yaml -n onlineboutique Verify Deployment kubectl get all -n onlineboutique","title":"3 Deploy a sample application"},{"location":"020_Assignment_5_sol_EFK/#4-configure-fluentd-for-specific-logs","text":"In many cases, you only need your applications' logs being sent to your logging stack. To achieve this, Fluentd should be configured to only intake specific logs so that no resources are wasted. Review sources configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A20 01_sources.conf 01_sources.conf: |- <source> @type tail @id in_tail_container_logs @label @KUBERNETES path /var/log/containers/*.log pos_file /var/log/fluentd-containers.log.pos tag kubernetes.* read_from_head true <parse> @type multi_format <pattern> format json time_key time time_type string time_format \"%Y-%m-%dT%H:%M:%S.%NZ\" keep_time_key false </pattern> <pattern> format regexp expression /^(?<time>.+) (?<stream>stdout|stderr)( (.))? (?<log>.*)$/ Note id : A unique identifier to reference this source. This can be used for further filtering and routing of structured log data type : Inbuilt directive understood by fluentd. In this case, \u201ctail\u201d instructs fluentd to gather data by tailing logs from a given location. Another example is \u201chttp\u201d which instructs fluentd to collect data by using GET on http endpoint. path : Specific to type \u201ctail\u201d. Instructs fluentd to collect all logs under /var/log/containers directory. This is the location used by docker daemon on a Kubernetes node to store stdout from running containers pos_file : Used as a checkpoint. In case the fluentd process restarts, it uses the position from this file to resume log data collection tag : A custom string for matching source to destination/filters. fluentd matches source/destination tags to route log data Step 2 Review Filter configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A20 03_dispatch.conf 02_filters.conf: |- <label @KUBERNETES> <match kubernetes.var.log.containers.fluentd**> @type relabel @label @FLUENT_LOG </match> <filter kubernetes.**> @type kubernetes_metadata @id filter_kube_metadata skip_labels false skip_container_metadata false skip_namespace_metadata true skip_master_url true </filter> <match **> @type relabel @label @DISPATCH </match> Step 3 Review Dispatch configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A20 03_dispatch.conf 03_dispatch.conf: |- <label @DISPATCH> <filter **> @type prometheus <metric> name fluentd_input_status_num_records_total type counter desc The total number of incoming records <labels> tag ${tag} hostname ${hostname} </labels> </metric> </filter> <match **> @type relabel @label @OUTPUT </match> </label> Step 4 Review Output Plugin configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A10 04_outputs.conf 04_outputs.conf: |- <label @OUTPUT> <match **> @type elasticsearch host \"elasticsearch-master\" port 9200 path \"\" user elastic password changeme </match> </label> Note match : tag indicates a destination. It is followed by a regular expression for matching the source. In this case, we want to capture all logs and send them to Elasticsearch, so simply use ** type : Supported output plugin identifier. In this case, we are using ElasticSearch which is a built-in plugin of fluentd. host/port : ElasticSearch host/port. Credentials can be configured as well, but not shown here. To configure Fluentd to only send one namespace's logs to elasticsearch, modify 03_dispatch.conf so that only log files that matches onlineboutique namespace is labeled to be sent: 03_dispatch.conf: |- <label @DISPATCH> <filter **> @type prometheus <metric> name fluentd_input_status_num_records_total type counter desc The total number of incoming records <labels> tag ${tag} hostname ${hostname} </labels> </metric> </filter> <match kubernetes.var.log.containers.**_onlineboutique_**> @type relabel @label @OUTPUT </match> </label> <match kubernetes.var.log.containers.**_onlineboutique_**> will filter out logs with _onlineboutique_ . This configuration will only relabel the logs that matches the configuration as @OUTPUT . As specified in 04_outputs.conf , only logs labelled as @OUTPUT will be sent to elasticsearch. Note that this is not the only way to configure fluentd to send one namespace's logs.","title":"4 Configure Fluentd for specific logs"},{"location":"020_Lab_3_Terraform_Fundamentals/","text":"Lab 2 Terraform Fundamentals Objective: Automating through code the configuration and provisioning of resources 1 What is Terraform? \u00b6 Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing, popular service providers and custom in-house solutions. Configuration files describe to Terraform the components needed to run a single application or your entire data center. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform can determine what changed and create incremental execution plans that can be applied. The infrastructure Terraform can manage includes both low-level components such as compute instances, storage, and networking, and high-level components such as DNS entries and SaaS features. 1.2 Key features \u00b6 Infrastructure as code Infrastructure is described using a high-level configuration syntax. This allows a blueprint of your data center to be versioned and treated as you would any other code. Additionally, infrastructure can be shared and re-used. Execution plans Terraform has a planning step in which it generates an execution plan. The execution plan shows what Terraform will do when you execute the apply command. This lets you avoid any surprises when Terraform manipulates infrastructure. Resource graph Terraform builds a graph of all your resources and parallelizes the creation and modification of any non-dependent resources. Because of this, Terraform builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure. Change automation Complex changesets can be applied to your infrastructure with minimal human interaction. With the previously mentioned execution plan and resource graph, you know exactly what Terraform will change and in what order, which helps you avoid many possible human errors.","title":"Lab 3 Terraform Fundamentals"},{"location":"020_Lab_3_Terraform_Fundamentals/#1-what-is-terraform","text":"Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing, popular service providers and custom in-house solutions. Configuration files describe to Terraform the components needed to run a single application or your entire data center. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform can determine what changed and create incremental execution plans that can be applied. The infrastructure Terraform can manage includes both low-level components such as compute instances, storage, and networking, and high-level components such as DNS entries and SaaS features.","title":"1 What is Terraform?"},{"location":"020_Lab_3_Terraform_Fundamentals/#12-key-features","text":"Infrastructure as code Infrastructure is described using a high-level configuration syntax. This allows a blueprint of your data center to be versioned and treated as you would any other code. Additionally, infrastructure can be shared and re-used. Execution plans Terraform has a planning step in which it generates an execution plan. The execution plan shows what Terraform will do when you execute the apply command. This lets you avoid any surprises when Terraform manipulates infrastructure. Resource graph Terraform builds a graph of all your resources and parallelizes the creation and modification of any non-dependent resources. Because of this, Terraform builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure. Change automation Complex changesets can be applied to your infrastructure with minimal human interaction. With the previously mentioned execution plan and resource graph, you know exactly what Terraform will change and in what order, which helps you avoid many possible human errors.","title":"1.2 Key features"},{"location":"020_Lab_5_EFK/","text":"Lab 5 - Logging with EFK Stack In a complicated distributed system such as Kubernetes, we will have different logs for different components, and extracting insights from logs can be a daunting task. The EFK stack (ElasticSearch, FluentD, Kibana) can help make this task easier. In this Lab, we are going to deploy EFK stack. Deploy online-boutique app that emits logs to stdout in JSON format. We will then deploy Cloud Native FluentD logging agent and configure: source directive (input) that will use logs from /var/log/containers folder (location used by docker daemon on a Kubernetes node to store stdout from running containers). This events will be tailed read from text file using in_tail Input Plugin . And parsed using multi_format plugin. filter directive that will use kubernetes metadata plugin to add metadata to the log and parse all kubernetes logs in specific format. match directive (output) that will use out_elasticsearch Output Plugin and send all logs to ElasticSearch under fluentd-* prefix. Objective: Install Elasticsearch and Kibanna Deploy online boutique Application Install and Configure FluentD Configure ElastackSearch with FluentD 0 Create Regional GKE Cluster \u00b6 0 Create Regional GKE Cluster on GCP \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-efk-lab \\ --region us-central1 \\ --enable-ip-alias \\ --enable-network-policy \\ --num-nodes 2 \\ --machine-type \"e2-standard-4\" \\ --release-channel stable gcloud container clusters get-credentials k8s-efk-lab --region us-central1 1 Install Elasticsearch, Kibana, Fluentd Helm Charts \u00b6 1.1 Elasticsearch Installation \u00b6 Create a new namespace for EFK stack: kubectl create ns efk Install Elasticsearch using Helm: helm repo add elastic https://helm.elastic.co helm install elasticsearch elastic/elasticsearch -n efk Check the status of deployment: kubectl get pods --namespace=efk -l app=elasticsearch-master -w 1.2 Kibana installation \u00b6 Step 1: Create custom Kibana configuration, that will allow to expose Kibana Dashboard: export STUDENT=ayrat ## change to your name cat << EOF>> kibana_values.yaml service: type: LoadBalancer EOF cat kibana_values.yaml Make sure values are correct Step 3: Deploy Kibana Ingress Charts with custom parameters in efk namespace: helm install kibana elastic/kibana -n efk --values kibana_values.yaml helm list -n efk Access Kibana from your browser using LoadBalancer IP: kubectl get svc -n efk 1.3 Fluentd installation \u00b6 Step 1: Create custom Fluentd configuration, below snippet is updating existing configmap with some parameters: cat << EOF>> fluentd_values.yaml fileConfigs: # here we read the logs from Docker's containers and parse them 01_sources.conf: |- ## logs from podman <source> @type tail @id in_tail_container_logs @label @KUBERNETES path /var/log/containers/*.log pos_file /var/log/fluentd-containers.log.pos tag kubernetes.* read_from_head true <parse> @type multi_format <pattern> format json time_key time time_type string time_format \"%Y-%m-%dT%H:%M:%S.%NZ\" keep_time_key true </pattern> <pattern> format regexp expression /^(?<time>.+) (?<stream>stdout|stderr)( (.))? (?<log>.*)$/ time_format '%Y-%m-%dT%H:%M:%S.%NZ' keep_time_key true </pattern> </parse> emit_unmatched_lines true </source> # we use kubernetes metadata plugin to add metadata to the log 02_filters.conf: |- <label @KUBERNETES> <match kubernetes.var.log.containers.fluentd**> @type relabel @label @FLUENT_LOG </match> # <match kubernetes.var.log.containers.**_kube-system_**> # @type null # @id ignore_kube_system_logs # </match> <filter kubernetes.**> @type kubernetes_metadata @id filter_kube_metadata skip_labels false skip_container_metadata false skip_namespace_metadata true skip_master_url true </filter> <match **> @type relabel @label @DISPATCH </match> </label> # We filtering what logs will be send 03_dispatch.conf: |- <label @DISPATCH> <filter **> @type prometheus <metric> name fluentd_input_status_num_records_total type counter desc The total number of incoming records <labels> tag ${tag} hostname ${hostname} </labels> </metric> </filter> <match **> @type relabel @label @OUTPUT </match> </label> # we send the logs to Elasticsearch 04_outputs.conf: |- <label @OUTPUT> <match **> @type elasticsearch host \"elasticsearch-master\" port 9200 path \"\" user elastic password changeme </match> </label> EOF Step 2: Deploy Fluentd Chart with custom parameters in efk namespace: Deploy Fluentd with Helm: helm repo add fluent https://fluent.github.io/helm-charts helm repo update helm install fluentd fluent/fluentd -n efk --values fluentd_values.yaml 2 Configure Kibana and Create Index: \u00b6 Kibana requires an index pattern to access the Elasticsearch data that you want to explore. An index pattern selects the data to use and allows you to define properties of the fields. In our case we configured fluentd index, that should be populated at Elasticsearch Step 1: Locate Kibana URL: kubectl get svc -n efk Step 2: Launch the Kibana web interface: loadbalancer_ip Result You should see your Kibana interface Step 3: Update Kibana Configuration to support time Meta fields: In UI Go to: Management - Stack Management: Step 4: Then Kibana - Advanced Setting: Step 5: Find field: Meta fields , and add time field as following: Step 7: Create an index pattern. In UI Click: Discover (under analytics) Click Create Index Pattern button. Note alternative path: Management - Stack Management -> Kibana Index Patterns Step 8: In Create index pattern window. Type inside Index pattern name : fluentd* as the index pattern name. More documentation can be found here Result Your index pattern matches 1 source Click Next Step > Step 9: Configure which field Kibana will use to filter log data by time. In the dropdown, select the @time field, and hit Create index pattern . Summary Our ElasticSearch and Kibana fully configured. You should be able now see some cluster logs in Kibana. Step 10: View Kibana Logs from our GKE cluster: In Query search field paste: `kubernetes.namespace_name: kube-system` In Range field paste: Select last 1 hour Click Refresh Result you can see logs from kube-system namespace Note You can save you Query for future use 3 Deploy onlineboutique application \u00b6 Deploy microservices application onlineboutique : Create Namespace onlineboutique kubectl create ns onlineboutique Deploy Microservice application cd ~ git clone https://github.com/GoogleCloudPlatform/microservices-demo.git cd microservices-demo kubectl apply -f ./release/kubernetes-manifests.yaml -n onlineboutique Verify Deployment: kubectl get all -n onlineboutique 4 Observe onlineboutique logs \u00b6 Step 1: View Kibana Logs from our GKE cluster: In Query search field paste: `kubernetes.namespace_name: onlineboutique` In Range field paste: Select last 15 minutes Click Refresh 5 Review Fluentd configuration \u00b6 In some cases, you want to filter logs from only your applications to be seen by your team. To achieve this, Fluentd should be configured to only intake specific logs so that no resources are wasted. Review sources configuration: kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A20 01_sources.conf 01_sources.conf: |- <source> @type tail @id in_tail_container_logs @label @KUBERNETES path /var/log/containers/*.log pos_file /var/log/fluentd-containers.log.pos tag kubernetes.* read_from_head true <parse> @type multi_format <pattern> format json time_key time time_type string time_format \"%Y-%m-%dT%H:%M:%S.%NZ\" keep_time_key false </pattern> <pattern> format regexp expression /^(?<time>.+) (?<stream>stdout|stderr)( (.))? (?<log>.*)$/ Note id : A unique identifier to reference this source. This can be used for further filtering and routing of structured log data type : Inbuilt directive understood by fluentd. In this case, \u201ctail\u201d instructs fluentd to gather data by tailing logs from a given location. Another example is \u201chttp\u201d which instructs fluentd to collect data by using GET on http endpoint. path : Specific to type \u201ctail\u201d. Instructs fluentd to collect all logs under /var/log/containers directory. This is the location used by docker daemon on a Kubernetes node to store stdout from running containers pos_file : Used as a checkpoint. In case the fluentd process restarts, it uses the position from this file to resume log data collection tag : A custom string for matching source to destination/filters. fluentd matches source/destination tags to route log data Step 2 Review Filter configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A20 03_dispatch.conf 02_filters.conf: |- <label @KUBERNETES> <match kubernetes.var.log.containers.fluentd**> @type relabel @label @FLUENT_LOG </match> <filter kubernetes.**> @type kubernetes_metadata @id filter_kube_metadata skip_labels false skip_container_metadata false skip_namespace_metadata true skip_master_url true </filter> <match **> @type relabel @label @DISPATCH </match> Step 3 Review Dispatch configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A20 03_dispatch.conf 03_dispatch.conf: |- <label @DISPATCH> <filter **> @type prometheus <metric> name fluentd_input_status_num_records_total type counter desc The total number of incoming records <labels> tag ${tag} hostname ${hostname} </labels> </metric> </filter> <match **> @type relabel @label @OUTPUT </match> </label> Step 4 Review Output Plugin configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A10 04_outputs.conf 04_outputs.conf: |- <label @OUTPUT> <match **> @type elasticsearch host \"elasticsearch-master\" port 9200 path \"\" user elastic password changeme </match> </label> Note match : tag indicates a destination. It is followed by a regular expression for matching the source. In this case, we want to capture all logs and send them to Elasticsearch, so simply use ** type : Supported output plugin identifier. In this case, we are using ElasticSearch which is a built-in plugin of fluentd. host/port : ElasticSearch host/port. Credentials can be configured as well, but not shown here. 6 Configure Fluentd to specific logs \u00b6 Let's configure Fluentd to only send onlineboutique namespace logs to elasticsearch: Step 1: Modify helm fluentd_values.yaml values file for 03_dispatch.conf config so that only log files that matches onlineboutique namespace is labeled to be sent: edit fluentd_values.yaml Replace line from: <match **> # Send all logs To: <match kubernetes.var.log.containers.**_onlineboutique_**> So it looks as following: 03_dispatch.conf: |- <label @DISPATCH> <filter **> @type prometheus <metric> name fluentd_input_status_num_records_total type counter desc The total number of incoming records <labels> tag ${tag} hostname ${hostname} </labels> </metric> </filter> <match kubernetes.var.log.containers.**_onlineboutique_**> @type relabel @label @OUTPUT </match> </label> <match kubernetes.var.log.containers.**_onlineboutique_**> will filter out logs with _onlineboutique_ . This configuration will only relabel the logs that matches the configuration as @OUTPUT . As specified in 04_outputs.conf , only logs labelled as @OUTPUT will be sent to elasticsearch. Note that this is not the only way to configure fluentd to send one namespace's logs. Step 2: Install helm diff plugin helm plugin install https://github.com/databus23/helm-diff Step 3: Verify diff helm diff upgrade fluentd fluent/fluentd -n efk --values fluentd-values.yaml Step 4: Update fluentd configmap via Helm Upgrade helm upgrade fluentd fluent/fluentd -n efk --values fluentd-values.yaml Step 5: Observe that fluentd stop sending logs for other namespaces than _onlineboutique_ In Query search field paste: `kubernetes.namespace_name: kube-system` In Range field paste: Select last 15 minute Click Refresh Result No more kube-system logs send to the ElasticSearch and hence Kibana can display them. 7 Cleanup \u00b6 Uninstall Helm Charts: helm uninstall fluentd -n efk helm uninstall kibana -n efk helm uninstall elasticsearch -n efk Delete GKE cluster: gcloud container clusters delete k8s-efk-lab --region us-central1","title":"Lab 5 EFK Stack"},{"location":"020_Lab_5_EFK/#0-create-regional-gke-cluster","text":"","title":"0 Create Regional GKE Cluster"},{"location":"020_Lab_5_EFK/#0-create-regional-gke-cluster-on-gcp","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-efk-lab \\ --region us-central1 \\ --enable-ip-alias \\ --enable-network-policy \\ --num-nodes 2 \\ --machine-type \"e2-standard-4\" \\ --release-channel stable gcloud container clusters get-credentials k8s-efk-lab --region us-central1","title":"0 Create Regional GKE Cluster on GCP"},{"location":"020_Lab_5_EFK/#1-install-elasticsearch-kibana-fluentd-helm-charts","text":"","title":"1 Install Elasticsearch, Kibana, Fluentd Helm Charts"},{"location":"020_Lab_5_EFK/#11-elasticsearch-installation","text":"Create a new namespace for EFK stack: kubectl create ns efk Install Elasticsearch using Helm: helm repo add elastic https://helm.elastic.co helm install elasticsearch elastic/elasticsearch -n efk Check the status of deployment: kubectl get pods --namespace=efk -l app=elasticsearch-master -w","title":"1.1 Elasticsearch Installation"},{"location":"020_Lab_5_EFK/#12-kibana-installation","text":"Step 1: Create custom Kibana configuration, that will allow to expose Kibana Dashboard: export STUDENT=ayrat ## change to your name cat << EOF>> kibana_values.yaml service: type: LoadBalancer EOF cat kibana_values.yaml Make sure values are correct Step 3: Deploy Kibana Ingress Charts with custom parameters in efk namespace: helm install kibana elastic/kibana -n efk --values kibana_values.yaml helm list -n efk Access Kibana from your browser using LoadBalancer IP: kubectl get svc -n efk","title":"1.2 Kibana installation"},{"location":"020_Lab_5_EFK/#13-fluentd-installation","text":"Step 1: Create custom Fluentd configuration, below snippet is updating existing configmap with some parameters: cat << EOF>> fluentd_values.yaml fileConfigs: # here we read the logs from Docker's containers and parse them 01_sources.conf: |- ## logs from podman <source> @type tail @id in_tail_container_logs @label @KUBERNETES path /var/log/containers/*.log pos_file /var/log/fluentd-containers.log.pos tag kubernetes.* read_from_head true <parse> @type multi_format <pattern> format json time_key time time_type string time_format \"%Y-%m-%dT%H:%M:%S.%NZ\" keep_time_key true </pattern> <pattern> format regexp expression /^(?<time>.+) (?<stream>stdout|stderr)( (.))? (?<log>.*)$/ time_format '%Y-%m-%dT%H:%M:%S.%NZ' keep_time_key true </pattern> </parse> emit_unmatched_lines true </source> # we use kubernetes metadata plugin to add metadata to the log 02_filters.conf: |- <label @KUBERNETES> <match kubernetes.var.log.containers.fluentd**> @type relabel @label @FLUENT_LOG </match> # <match kubernetes.var.log.containers.**_kube-system_**> # @type null # @id ignore_kube_system_logs # </match> <filter kubernetes.**> @type kubernetes_metadata @id filter_kube_metadata skip_labels false skip_container_metadata false skip_namespace_metadata true skip_master_url true </filter> <match **> @type relabel @label @DISPATCH </match> </label> # We filtering what logs will be send 03_dispatch.conf: |- <label @DISPATCH> <filter **> @type prometheus <metric> name fluentd_input_status_num_records_total type counter desc The total number of incoming records <labels> tag ${tag} hostname ${hostname} </labels> </metric> </filter> <match **> @type relabel @label @OUTPUT </match> </label> # we send the logs to Elasticsearch 04_outputs.conf: |- <label @OUTPUT> <match **> @type elasticsearch host \"elasticsearch-master\" port 9200 path \"\" user elastic password changeme </match> </label> EOF Step 2: Deploy Fluentd Chart with custom parameters in efk namespace: Deploy Fluentd with Helm: helm repo add fluent https://fluent.github.io/helm-charts helm repo update helm install fluentd fluent/fluentd -n efk --values fluentd_values.yaml","title":"1.3 Fluentd installation"},{"location":"020_Lab_5_EFK/#2-configure-kibana-and-create-index","text":"Kibana requires an index pattern to access the Elasticsearch data that you want to explore. An index pattern selects the data to use and allows you to define properties of the fields. In our case we configured fluentd index, that should be populated at Elasticsearch Step 1: Locate Kibana URL: kubectl get svc -n efk Step 2: Launch the Kibana web interface: loadbalancer_ip Result You should see your Kibana interface Step 3: Update Kibana Configuration to support time Meta fields: In UI Go to: Management - Stack Management: Step 4: Then Kibana - Advanced Setting: Step 5: Find field: Meta fields , and add time field as following: Step 7: Create an index pattern. In UI Click: Discover (under analytics) Click Create Index Pattern button. Note alternative path: Management - Stack Management -> Kibana Index Patterns Step 8: In Create index pattern window. Type inside Index pattern name : fluentd* as the index pattern name. More documentation can be found here Result Your index pattern matches 1 source Click Next Step > Step 9: Configure which field Kibana will use to filter log data by time. In the dropdown, select the @time field, and hit Create index pattern . Summary Our ElasticSearch and Kibana fully configured. You should be able now see some cluster logs in Kibana. Step 10: View Kibana Logs from our GKE cluster: In Query search field paste: `kubernetes.namespace_name: kube-system` In Range field paste: Select last 1 hour Click Refresh Result you can see logs from kube-system namespace Note You can save you Query for future use","title":"2 Configure Kibana and Create Index:"},{"location":"020_Lab_5_EFK/#3-deploy-onlineboutique-application","text":"Deploy microservices application onlineboutique : Create Namespace onlineboutique kubectl create ns onlineboutique Deploy Microservice application cd ~ git clone https://github.com/GoogleCloudPlatform/microservices-demo.git cd microservices-demo kubectl apply -f ./release/kubernetes-manifests.yaml -n onlineboutique Verify Deployment: kubectl get all -n onlineboutique","title":"3 Deploy onlineboutique application"},{"location":"020_Lab_5_EFK/#4-observe-onlineboutique-logs","text":"Step 1: View Kibana Logs from our GKE cluster: In Query search field paste: `kubernetes.namespace_name: onlineboutique` In Range field paste: Select last 15 minutes Click Refresh","title":"4 Observe onlineboutique logs"},{"location":"020_Lab_5_EFK/#5-review-fluentd-configuration","text":"In some cases, you want to filter logs from only your applications to be seen by your team. To achieve this, Fluentd should be configured to only intake specific logs so that no resources are wasted. Review sources configuration: kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A20 01_sources.conf 01_sources.conf: |- <source> @type tail @id in_tail_container_logs @label @KUBERNETES path /var/log/containers/*.log pos_file /var/log/fluentd-containers.log.pos tag kubernetes.* read_from_head true <parse> @type multi_format <pattern> format json time_key time time_type string time_format \"%Y-%m-%dT%H:%M:%S.%NZ\" keep_time_key false </pattern> <pattern> format regexp expression /^(?<time>.+) (?<stream>stdout|stderr)( (.))? (?<log>.*)$/ Note id : A unique identifier to reference this source. This can be used for further filtering and routing of structured log data type : Inbuilt directive understood by fluentd. In this case, \u201ctail\u201d instructs fluentd to gather data by tailing logs from a given location. Another example is \u201chttp\u201d which instructs fluentd to collect data by using GET on http endpoint. path : Specific to type \u201ctail\u201d. Instructs fluentd to collect all logs under /var/log/containers directory. This is the location used by docker daemon on a Kubernetes node to store stdout from running containers pos_file : Used as a checkpoint. In case the fluentd process restarts, it uses the position from this file to resume log data collection tag : A custom string for matching source to destination/filters. fluentd matches source/destination tags to route log data Step 2 Review Filter configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A20 03_dispatch.conf 02_filters.conf: |- <label @KUBERNETES> <match kubernetes.var.log.containers.fluentd**> @type relabel @label @FLUENT_LOG </match> <filter kubernetes.**> @type kubernetes_metadata @id filter_kube_metadata skip_labels false skip_container_metadata false skip_namespace_metadata true skip_master_url true </filter> <match **> @type relabel @label @DISPATCH </match> Step 3 Review Dispatch configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A20 03_dispatch.conf 03_dispatch.conf: |- <label @DISPATCH> <filter **> @type prometheus <metric> name fluentd_input_status_num_records_total type counter desc The total number of incoming records <labels> tag ${tag} hostname ${hostname} </labels> </metric> </filter> <match **> @type relabel @label @OUTPUT </match> </label> Step 4 Review Output Plugin configuration kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" | grep -A10 04_outputs.conf 04_outputs.conf: |- <label @OUTPUT> <match **> @type elasticsearch host \"elasticsearch-master\" port 9200 path \"\" user elastic password changeme </match> </label> Note match : tag indicates a destination. It is followed by a regular expression for matching the source. In this case, we want to capture all logs and send them to Elasticsearch, so simply use ** type : Supported output plugin identifier. In this case, we are using ElasticSearch which is a built-in plugin of fluentd. host/port : ElasticSearch host/port. Credentials can be configured as well, but not shown here.","title":"5 Review Fluentd configuration"},{"location":"020_Lab_5_EFK/#6-configure-fluentd-to-specific-logs","text":"Let's configure Fluentd to only send onlineboutique namespace logs to elasticsearch: Step 1: Modify helm fluentd_values.yaml values file for 03_dispatch.conf config so that only log files that matches onlineboutique namespace is labeled to be sent: edit fluentd_values.yaml Replace line from: <match **> # Send all logs To: <match kubernetes.var.log.containers.**_onlineboutique_**> So it looks as following: 03_dispatch.conf: |- <label @DISPATCH> <filter **> @type prometheus <metric> name fluentd_input_status_num_records_total type counter desc The total number of incoming records <labels> tag ${tag} hostname ${hostname} </labels> </metric> </filter> <match kubernetes.var.log.containers.**_onlineboutique_**> @type relabel @label @OUTPUT </match> </label> <match kubernetes.var.log.containers.**_onlineboutique_**> will filter out logs with _onlineboutique_ . This configuration will only relabel the logs that matches the configuration as @OUTPUT . As specified in 04_outputs.conf , only logs labelled as @OUTPUT will be sent to elasticsearch. Note that this is not the only way to configure fluentd to send one namespace's logs. Step 2: Install helm diff plugin helm plugin install https://github.com/databus23/helm-diff Step 3: Verify diff helm diff upgrade fluentd fluent/fluentd -n efk --values fluentd-values.yaml Step 4: Update fluentd configmap via Helm Upgrade helm upgrade fluentd fluent/fluentd -n efk --values fluentd-values.yaml Step 5: Observe that fluentd stop sending logs for other namespaces than _onlineboutique_ In Query search field paste: `kubernetes.namespace_name: kube-system` In Range field paste: Select last 15 minute Click Refresh Result No more kube-system logs send to the ElasticSearch and hence Kibana can display them.","title":"6 Configure Fluentd to specific logs"},{"location":"020_Lab_5_EFK/#7-cleanup","text":"Uninstall Helm Charts: helm uninstall fluentd -n efk helm uninstall kibana -n efk helm uninstall elasticsearch -n efk Delete GKE cluster: gcloud container clusters delete k8s-efk-lab --region us-central1","title":"7 Cleanup"},{"location":"020_ass2/","text":"1 GKE Deployment \u00b6 Objective: Review process of creation Custom VPC Review process of VPC subnet planning Review creation of Cloud Nat Review process of creation Private GKE Automating Process using Terraform Prepare Lab Environment \u00b6","title":"1 GKE Deployment"},{"location":"020_ass2/#1-gke-deployment","text":"Objective: Review process of creation Custom VPC Review process of VPC subnet planning Review creation of Cloud Nat Review process of creation Private GKE Automating Process using Terraform","title":"1 GKE Deployment"},{"location":"020_ass2/#prepare-lab-environment","text":"","title":"Prepare Lab Environment"},{"location":"020_ass2_sol/","text":"1 Containerize Applications \u00b6 Objective: Review process of creation Custom VPC Review process of VPC subnet planning Review creation of Cloud Nat Review process of creation Private GKE Automating Process using Terraform Prepare Lab Environment \u00b6","title":"1 Containerize Applications"},{"location":"020_ass2_sol/#1-containerize-applications","text":"Objective: Review process of creation Custom VPC Review process of VPC subnet planning Review creation of Cloud Nat Review process of creation Private GKE Automating Process using Terraform","title":"1 Containerize Applications"},{"location":"020_ass2_sol/#prepare-lab-environment","text":"","title":"Prepare Lab Environment"},{"location":"Helm_foundation_dima_section/","text":"4 Building a NotePad Chart \u00b6 Step 1 Create a new chart helm create gowebapp 4.1 Template the Deployment \u00b6 Step 1 Update the replicaCount value to 2 in the Values.yaml file. Step 2 Update the repository and tag section to point to your goweb image. Step 3 Update the livness and readiness sections to match what you have in the gowebapp deployment Step 4 Include an entry in the Values.yaml file for containerPort and set it to 80 Step 5 Update the resorues section in the Values.yaml file to include the resource requests and limits the gowebapp application needs. Step 6 Notice that the deployment.yaml file does not have an environment variable section for secrets, so let's add one. For this chart we will assume that this section is optional based on whether or not a secrets section exist in the Values.yaml file. Step 6-a Include the following in the deployment template: {{- if .Values.secrets }} - env: - name: {{.Values.secrets.name}} valueFrom: secretKeyRef: name: {{.Values.secrets.secretReference.name}} key: {{.Values.secrets.secretReference.key}} {{- end}} Step 6-b Inculde a section in the values file: secrets: enabled: true name: DB_PASSWORD secretReference: name: mysql key: password Step 7 For this lab, we will include the \"volumes\" and \"volumeMounts\" sections without templating, so just copy these sections to the appropriate location in the deployment template. 4.1 Template the Service \u00b6 Step 1 The Service Template doesn't have an annotation section, so modify the template to include an \"annotation\" section. Check the deployment file for an example on how to accomplish this task. #TODO: Use \"with\", \"toYaml\" and \"nindent\" to include a section for annotations #TODO: Modify the Values.yaml file to allow chart users to add annotations for the service. Make sure to use the right section in the values file, based on how you modified your template file. Step 2 Under the \"service\" section in the Values.yaml file, update the service port to 9000. Step 3 In the Values.yaml file, update the service type to \"NodePort\" 4.3 Service Account Template \u00b6 Step 1 Update the Values.yaml file to not create a service account for the gowebapp deployment.","title":"Helm foundation dima section"},{"location":"Helm_foundation_dima_section/#4-building-a-notepad-chart","text":"Step 1 Create a new chart helm create gowebapp","title":"4 Building a NotePad Chart"},{"location":"Helm_foundation_dima_section/#41-template-the-deployment","text":"Step 1 Update the replicaCount value to 2 in the Values.yaml file. Step 2 Update the repository and tag section to point to your goweb image. Step 3 Update the livness and readiness sections to match what you have in the gowebapp deployment Step 4 Include an entry in the Values.yaml file for containerPort and set it to 80 Step 5 Update the resorues section in the Values.yaml file to include the resource requests and limits the gowebapp application needs. Step 6 Notice that the deployment.yaml file does not have an environment variable section for secrets, so let's add one. For this chart we will assume that this section is optional based on whether or not a secrets section exist in the Values.yaml file. Step 6-a Include the following in the deployment template: {{- if .Values.secrets }} - env: - name: {{.Values.secrets.name}} valueFrom: secretKeyRef: name: {{.Values.secrets.secretReference.name}} key: {{.Values.secrets.secretReference.key}} {{- end}} Step 6-b Inculde a section in the values file: secrets: enabled: true name: DB_PASSWORD secretReference: name: mysql key: password Step 7 For this lab, we will include the \"volumes\" and \"volumeMounts\" sections without templating, so just copy these sections to the appropriate location in the deployment template.","title":"4.1 Template the Deployment"},{"location":"Helm_foundation_dima_section/#41-template-the-service","text":"Step 1 The Service Template doesn't have an annotation section, so modify the template to include an \"annotation\" section. Check the deployment file for an example on how to accomplish this task. #TODO: Use \"with\", \"toYaml\" and \"nindent\" to include a section for annotations #TODO: Modify the Values.yaml file to allow chart users to add annotations for the service. Make sure to use the right section in the values file, based on how you modified your template file. Step 2 Under the \"service\" section in the Values.yaml file, update the service port to 9000. Step 3 In the Values.yaml file, update the service type to \"NodePort\"","title":"4.1 Template the Service"},{"location":"Helm_foundation_dima_section/#43-service-account-template","text":"Step 1 Update the Values.yaml file to not create a service account for the gowebapp deployment.","title":"4.3 Service Account Template"},{"location":"Istio%20Demo/","text":"Lab 4 Learning Istio 7 Install Istio on GKE \u00b6 Currently Istio community developing following options of Istio deployment on Kubernetes Clusters: Install with Istioctl installation via istioctl command line tool used to showcase Istio functionality. Using the CLI, we generate a YAML file with all Istio resources and then deploy it to the Kubernetes cluster Istio Operator Install Takes installation of Istio to the next level as it's managing not only Istio installation but overall lifecicle of the Istio deployment including Upgrades and configurations. Install with Helm (alpha) Allows to farther simplify Istio deployment and it's customization. 7.0 Prerequisite \u00b6 Scaleup cluster back to 3 nodes and Update VPC firewall rules to enable auto-injection and the istioctl version and istioctl ps commands. References: Opening ports on a private cluster Istio deployment on GKE Private Clusters Step 1: Locate Terraform Configuration directory. cd ~/$MY_REPO/notepad-infrastructure edit terraform.tfvars And set gke_pool_node_count = \"0\" Step 2: Create new firewall rule for source range (master-ipv4-cidr) of the cluster, that will open required ports for Istio installation: cat <<EOF> istio_firewall.tf resource \"google_compute_firewall\" \"istio_specific\" { name = format(\"allow-istio-in-privategke-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link source_ranges = [\"172.16.0.0/28\"] allow { protocol = \"tcp\" ports = [\"10250\", \"443\", \"15017\", \"15014\", \"8080\"] } } EOF Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars Step 4: Scale up GKE Cluster Node Pool and update firewall rules for required range: terraform apply -var-file terraform.tfvars Result GKE Clusters has been scalled to 3 nodes and has firewall rules opened Step 5: Verify firewall rule: gcloud compute firewall-rules list --filter=\"name~allow-istio-in-privategke*\" Output: NAME NETWORK DIRECTION PRIORITY ALLOW DENY DISABLED allow-istio-in-privategke-$student-notepad-dev vpc-$student-notepad-dev INGRESS 1000 tcp:10250,tcp:443,tcp:15017,tcp:15014,tcp:8080 False 7.1 Deploy Istio using Custom Resources \u00b6 This installation guide uses the istioctl command line tool to provide rich customization of the Istio control plane and of the sidecars for the Istio data plane. Download and extract the latest release: curl -L https://istio.io/downloadIstio | sh - cd istio-1.11.0 export PATH=$PWD/bin:$PATH Success The above command will fetch Istio packages and untar them in the same folder. tree -L 1 . \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 bin \u251c\u2500\u2500 manifest.yaml \u251c\u2500\u2500 manifests \u251c\u2500\u2500 samples \u2514\u2500\u2500 tools Note Istio installation directory contains: bin directory contains istioctl client binary manifests Installation Profiles, configurations and Helm Charts samples directory contains sample applications deployment tools directory contains auto-completion tooling and etc. Step 2 Deploy Istio Custom Resource Definitions (CRDs) Istio extends Kubernetes using Custom Resource Definitions (CRDs). CRDs allow registration of new/non-default Kubernetes resources. When Istio CRDs are deployed, Istio\u2019s objects are registered as Kubernetes objects, providing a highly integrated experience with Kubernetes as a deployment platform and thus allowing Kubernetes to store configuration of Istio features such as routing, security and telemetry and etc. We going to install using using demo configuration profile. The demo configuration profile allows to experiment with most of Istio features with modest resource requirements. Since it enables high levels of tracing and access logging, it is not suitable for production use cases. istioctl install --set profile=demo -y Output: \u2714 Istio core installed \u2714 Istiod installed \u2714 Egress gateways installed \u2714 Ingress gateways installed \u2714 Installation complete Thank you for installing Istio 1.11. Please take a few minutes to tell us about your install/upgrade experience! Note Wait a few seconds for the CRDs to be committed in the Kubernetes API-server. Step 3 Verify Istio CRDs successfully applied to Kubernetes Cluster. kubectl get crds | grep istio authorizationpolicies.security.istio.io 2021-08-16T19:38:16Z destinationrules.networking.istio.io 2021-08-16T19:38:16Z envoyfilters.networking.istio.io 2021-08-16T19:38:16Z gateways.networking.istio.io 2021-08-16T19:38:17Z istiooperators.install.istio.io 2021-08-16T19:38:17Z peerauthentications.security.istio.io 2021-08-16T19:38:17Z requestauthentications.security.istio.io 2021-08-16T19:38:17Z serviceentries.networking.istio.io 2021-08-16T19:38:17Z sidecars.networking.istio.io 2021-08-16T19:38:17Z telemetries.telemetry.istio.io 2021-08-16T19:38:18Z virtualservices.networking.istio.io 2021-08-16T19:38:18Z workloadentries.networking.istio.io 2021-08-16T19:38:18Z workloadgroups.networking.istio.io 2021-08-16T19:38:19Z Info Above CRDs will be avaialable as a new Kubernetes Resources and stored in Kubernetes ETCD database. The Kubernetes API will represent these new resources as endpoints that can be used as other native Kubernetes object (such as Pod, Services) levereging kubectl, RBAC and other features and admission controllers of Kubernetes. Step 4 Count total number of Installed CRDs: kubectl get crds | grep istio | wc -l Note CRDs count will vary based on Istio version and profile deployed. Step 5 Verify that Istio control plane has been installed successfully. kubectl get pods -n istio-system Output: istio-egressgateway-9dc6cbc49-5wkqt 1/1 Running 0 2m54s istio-ingressgateway-7975cdb749-xjc5g 1/1 Running 0 2m54s istiod-77b4d7b55d-j6kb5 1/1 Running 0 3m9s Info Istio control-plane include following components: istiod - contains components such as Citadel and Pilot istio-ingressgateway Istio Ingress Gateway istio-ingressgateway Istio Egress Gateway Step 6 Verify installation with istioctl CLI: istioctl version istioctl verify-install Output: Checked 13 custom resource definitions Checked 3 Istio Deployments \u2714 Istio is installed and verified successfully Step 5 Deploy Bookinfo sample application: kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-cjj5b 1/1 Running 0 12s productpage-v1-6b746f74dc-sxdxz 1/1 Running 0 10s ratings-v1-b6994bb9-jd985 1/1 Running 0 11s reviews-v1-545db77b95-5kjr4 1/1 Running 0 11s reviews-v2-7bf8c9648f-5v2s9 1/1 Running 0 11s reviews-v3-84779c7bbc-5khlt 1/1 Running 0 11s Note This is a typical Kubernetes deployment, and has nothing specific to Istio. Each Pod has 1 container. Step 6 Remove Bookinfo sample application: kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step X: Manual Sidecar injection: cat <<EOF > go-web-app-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: webk8sbirthday-v2 labels: run: webk8sbirthday-v2 spec: replicas: 1 selector: matchLabels: run: webk8sbirthday-v2 template: metadata: labels: run: webk8sbirthday-v2 spec: containers: - name: k8s-demo image: archyufa/webk8sbirthday:v2 ports: - name: port containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: webk8sbirthday-v2 spec: type: ClusterIP ports: - port: 80 targetPort: 8080 protocol: TCP name: http selector: run: webk8sbirthday-v2 EOF kubectl apply -f go-web-app-deployment.yaml kubectl port-forward --namespace default svc/webk8sbirthday-v2 8080:80 kubectl delete -f go-web-app-deployment.yaml Step 7 Enable manual Envoy sidecar injection. Using istioctl kube-inject manually inject Envoy containers in your application pods before deploying them. istioctl kube-inject -f go-web-app-deployment.yaml | kubectl apply -f - Alternatively, injection can be done using local copies of the configuration. kubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath='{.data.config}' > inject-config.yaml kubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath='{.data.values}' > inject-values.yaml kubectl -n istio-system get configmap istio -o=jsonpath='{.data.mesh}' > mesh-config.yaml Run kube-inject over the input file and deploy. Info Optionally manual injection can also load configuration from local custom files. See example below: $ istioctl kube-inject --injectConfigFile inject-config.yaml --meshConfigFile mesh-config.yaml --filename go-web-app-deployment.yaml --valuesFile inject-values.yaml --output go-web-app-deployment-injected.yaml grep \"image:\" -A1 go-web-app-deployment-injected.yaml Step 7 Enable automatic Envoy sidecar injection. Currently there are 2 method to inject Envoy sidecar inside Istio Mesh: Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment. Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update. Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later: kubectl label namespace default istio-injection=enabled Step 8 Deploy Bookinfo sample application on Istio with Auto Sidecar Injection kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Step 8 Verify deployed application: The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it. kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-zqjbg 2/2 Running 0 13m productpage-v1-6b746f74dc-dj959 2/2 Running 0 12m ratings-v1-b6994bb9-29n4w 2/2 Running 0 12m reviews-v1-545db77b95-p9pgf 2/2 Running 0 12m reviews-v2-7bf8c9648f-4ltkg 2/2 Running 0 12m reviews-v3-84779c7bbc-vhsmw 2/2 Running 0 12m Note Each Pod has now 2 containers. One application container and another is istio-proxy sidecar container. Step 9 Access UI productpage via console: kubectl get services In Gcloud console execute kubectl port-forward : kubectl port-forward --namespace default svc/productpage 8080:9080 Click Web-Preview button in Gcloud, and select preview on port 8080 Click Normal user URL. Success We can access Bookinfo application configured with Istio and Envoy sidecar using private IP via tunnel Step 10 Remove Bookinfo sample application: kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 11 Uninstall Istio istioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f - kubectl delete namespace istio-system kubectl label namespace default istio-injection- 7.2 Deploy Istio using Istio Operator \u00b6 Step 1: Deploy the Istio operator: istioctl operator init Note This command runs the operator by creating the following resources in the istio-operator namespace: The operator custom resource definition The operator controller deployment A service to access operator metrics Necessary Istio operator RBAC rules Step 2: To install the Istio demo configuration profile using the operator, run the following command: kubectl apply -f - <<EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: example-istiocontrolplane spec: profile: demo EOF Note The Istio operator controller begins the process of installing Istio within 90 seconds of the creation of the IstioOperator resource. The Istio installation completes within 120 seconds. Step 3 Verify Operator resource status on Kubernetes Cluster: kubectl get istiooperators -n istio-system Output: NAMESPACE NAME REVISION STATUS AGE istio-system example-istiocontrolplane HEALTHY 25m kubectl describe istiooperators example-istiocontrolplane -n istio-system Spec: Profile: demo Status: Component Status: Base: Status: HEALTHY Egress Gateways: Status: HEALTHY Ingress Gateways: Status: HEALTHY Pilot: Status: HEALTHY Status: HEALTHY Events: <none> kubectl get pods -n istio-system Output: istio-egressgateway-9dc6cbc49-5wkqt 1/1 Running 0 2m54s istio-ingressgateway-7975cdb749-xjc5g 1/1 Running 0 2m54s istiod-77b4d7b55d-j6kb5 1/1 Running 0 3m9s Info Istio control-plane include following components: istiod - contains components such as Citadel and Pilot istio-ingressgateway Istio Ingress Gateway istio-ingressgateway Istio Egress Gateway Verify installation with istioctl CLI: istioctl verify-install Step 5 Enable automatic Envoy sidecar injection. Currently there are 2 method to inject Envoy sidecar inside Istio Mesh: Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment. Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update. Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later: kubectl label namespace default istio-injection=enabled Step 8 Deploy Bookinfo sample application on Istio with Auto Sidecar Injection kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Step 8 Verify deployed application: The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it. kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-zqjbg 2/2 Running 0 13m productpage-v1-6b746f74dc-dj959 2/2 Running 0 12m ratings-v1-b6994bb9-29n4w 2/2 Running 0 12m reviews-v1-545db77b95-p9pgf 2/2 Running 0 12m reviews-v2-7bf8c9648f-4ltkg 2/2 Running 0 12m reviews-v3-84779c7bbc-vhsmw 2/2 Running 0 12m Summary We've leaned to deploy Istio Control plane using istioctl cli and using Istio Operator. We also deployed regular k8s application on the namespace marked with auto injection! Step 9 Cleanup and Uninstall Istio Operator kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl delete istiooperators.install.istio.io -n istio-system example-istiocontrolplane Note Wait until Istio is uninstalled - this may take some time. Then you can remove the Istio operator for the old revision by running the following command: istioctl operator remove","title":"Istio Demo"},{"location":"Istio%20Demo/#7-install-istio-on-gke","text":"Currently Istio community developing following options of Istio deployment on Kubernetes Clusters: Install with Istioctl installation via istioctl command line tool used to showcase Istio functionality. Using the CLI, we generate a YAML file with all Istio resources and then deploy it to the Kubernetes cluster Istio Operator Install Takes installation of Istio to the next level as it's managing not only Istio installation but overall lifecicle of the Istio deployment including Upgrades and configurations. Install with Helm (alpha) Allows to farther simplify Istio deployment and it's customization.","title":"7 Install Istio on GKE"},{"location":"Istio%20Demo/#70-prerequisite","text":"Scaleup cluster back to 3 nodes and Update VPC firewall rules to enable auto-injection and the istioctl version and istioctl ps commands. References: Opening ports on a private cluster Istio deployment on GKE Private Clusters Step 1: Locate Terraform Configuration directory. cd ~/$MY_REPO/notepad-infrastructure edit terraform.tfvars And set gke_pool_node_count = \"0\" Step 2: Create new firewall rule for source range (master-ipv4-cidr) of the cluster, that will open required ports for Istio installation: cat <<EOF> istio_firewall.tf resource \"google_compute_firewall\" \"istio_specific\" { name = format(\"allow-istio-in-privategke-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link source_ranges = [\"172.16.0.0/28\"] allow { protocol = \"tcp\" ports = [\"10250\", \"443\", \"15017\", \"15014\", \"8080\"] } } EOF Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars Step 4: Scale up GKE Cluster Node Pool and update firewall rules for required range: terraform apply -var-file terraform.tfvars Result GKE Clusters has been scalled to 3 nodes and has firewall rules opened Step 5: Verify firewall rule: gcloud compute firewall-rules list --filter=\"name~allow-istio-in-privategke*\" Output: NAME NETWORK DIRECTION PRIORITY ALLOW DENY DISABLED allow-istio-in-privategke-$student-notepad-dev vpc-$student-notepad-dev INGRESS 1000 tcp:10250,tcp:443,tcp:15017,tcp:15014,tcp:8080 False","title":"7.0 Prerequisite"},{"location":"Istio%20Demo/#71-deploy-istio-using-custom-resources","text":"This installation guide uses the istioctl command line tool to provide rich customization of the Istio control plane and of the sidecars for the Istio data plane. Download and extract the latest release: curl -L https://istio.io/downloadIstio | sh - cd istio-1.11.0 export PATH=$PWD/bin:$PATH Success The above command will fetch Istio packages and untar them in the same folder. tree -L 1 . \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 bin \u251c\u2500\u2500 manifest.yaml \u251c\u2500\u2500 manifests \u251c\u2500\u2500 samples \u2514\u2500\u2500 tools Note Istio installation directory contains: bin directory contains istioctl client binary manifests Installation Profiles, configurations and Helm Charts samples directory contains sample applications deployment tools directory contains auto-completion tooling and etc. Step 2 Deploy Istio Custom Resource Definitions (CRDs) Istio extends Kubernetes using Custom Resource Definitions (CRDs). CRDs allow registration of new/non-default Kubernetes resources. When Istio CRDs are deployed, Istio\u2019s objects are registered as Kubernetes objects, providing a highly integrated experience with Kubernetes as a deployment platform and thus allowing Kubernetes to store configuration of Istio features such as routing, security and telemetry and etc. We going to install using using demo configuration profile. The demo configuration profile allows to experiment with most of Istio features with modest resource requirements. Since it enables high levels of tracing and access logging, it is not suitable for production use cases. istioctl install --set profile=demo -y Output: \u2714 Istio core installed \u2714 Istiod installed \u2714 Egress gateways installed \u2714 Ingress gateways installed \u2714 Installation complete Thank you for installing Istio 1.11. Please take a few minutes to tell us about your install/upgrade experience! Note Wait a few seconds for the CRDs to be committed in the Kubernetes API-server. Step 3 Verify Istio CRDs successfully applied to Kubernetes Cluster. kubectl get crds | grep istio authorizationpolicies.security.istio.io 2021-08-16T19:38:16Z destinationrules.networking.istio.io 2021-08-16T19:38:16Z envoyfilters.networking.istio.io 2021-08-16T19:38:16Z gateways.networking.istio.io 2021-08-16T19:38:17Z istiooperators.install.istio.io 2021-08-16T19:38:17Z peerauthentications.security.istio.io 2021-08-16T19:38:17Z requestauthentications.security.istio.io 2021-08-16T19:38:17Z serviceentries.networking.istio.io 2021-08-16T19:38:17Z sidecars.networking.istio.io 2021-08-16T19:38:17Z telemetries.telemetry.istio.io 2021-08-16T19:38:18Z virtualservices.networking.istio.io 2021-08-16T19:38:18Z workloadentries.networking.istio.io 2021-08-16T19:38:18Z workloadgroups.networking.istio.io 2021-08-16T19:38:19Z Info Above CRDs will be avaialable as a new Kubernetes Resources and stored in Kubernetes ETCD database. The Kubernetes API will represent these new resources as endpoints that can be used as other native Kubernetes object (such as Pod, Services) levereging kubectl, RBAC and other features and admission controllers of Kubernetes. Step 4 Count total number of Installed CRDs: kubectl get crds | grep istio | wc -l Note CRDs count will vary based on Istio version and profile deployed. Step 5 Verify that Istio control plane has been installed successfully. kubectl get pods -n istio-system Output: istio-egressgateway-9dc6cbc49-5wkqt 1/1 Running 0 2m54s istio-ingressgateway-7975cdb749-xjc5g 1/1 Running 0 2m54s istiod-77b4d7b55d-j6kb5 1/1 Running 0 3m9s Info Istio control-plane include following components: istiod - contains components such as Citadel and Pilot istio-ingressgateway Istio Ingress Gateway istio-ingressgateway Istio Egress Gateway Step 6 Verify installation with istioctl CLI: istioctl version istioctl verify-install Output: Checked 13 custom resource definitions Checked 3 Istio Deployments \u2714 Istio is installed and verified successfully Step 5 Deploy Bookinfo sample application: kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-cjj5b 1/1 Running 0 12s productpage-v1-6b746f74dc-sxdxz 1/1 Running 0 10s ratings-v1-b6994bb9-jd985 1/1 Running 0 11s reviews-v1-545db77b95-5kjr4 1/1 Running 0 11s reviews-v2-7bf8c9648f-5v2s9 1/1 Running 0 11s reviews-v3-84779c7bbc-5khlt 1/1 Running 0 11s Note This is a typical Kubernetes deployment, and has nothing specific to Istio. Each Pod has 1 container. Step 6 Remove Bookinfo sample application: kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step X: Manual Sidecar injection: cat <<EOF > go-web-app-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: webk8sbirthday-v2 labels: run: webk8sbirthday-v2 spec: replicas: 1 selector: matchLabels: run: webk8sbirthday-v2 template: metadata: labels: run: webk8sbirthday-v2 spec: containers: - name: k8s-demo image: archyufa/webk8sbirthday:v2 ports: - name: port containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: webk8sbirthday-v2 spec: type: ClusterIP ports: - port: 80 targetPort: 8080 protocol: TCP name: http selector: run: webk8sbirthday-v2 EOF kubectl apply -f go-web-app-deployment.yaml kubectl port-forward --namespace default svc/webk8sbirthday-v2 8080:80 kubectl delete -f go-web-app-deployment.yaml Step 7 Enable manual Envoy sidecar injection. Using istioctl kube-inject manually inject Envoy containers in your application pods before deploying them. istioctl kube-inject -f go-web-app-deployment.yaml | kubectl apply -f - Alternatively, injection can be done using local copies of the configuration. kubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath='{.data.config}' > inject-config.yaml kubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath='{.data.values}' > inject-values.yaml kubectl -n istio-system get configmap istio -o=jsonpath='{.data.mesh}' > mesh-config.yaml Run kube-inject over the input file and deploy. Info Optionally manual injection can also load configuration from local custom files. See example below: $ istioctl kube-inject --injectConfigFile inject-config.yaml --meshConfigFile mesh-config.yaml --filename go-web-app-deployment.yaml --valuesFile inject-values.yaml --output go-web-app-deployment-injected.yaml grep \"image:\" -A1 go-web-app-deployment-injected.yaml Step 7 Enable automatic Envoy sidecar injection. Currently there are 2 method to inject Envoy sidecar inside Istio Mesh: Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment. Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update. Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later: kubectl label namespace default istio-injection=enabled Step 8 Deploy Bookinfo sample application on Istio with Auto Sidecar Injection kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Step 8 Verify deployed application: The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it. kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-zqjbg 2/2 Running 0 13m productpage-v1-6b746f74dc-dj959 2/2 Running 0 12m ratings-v1-b6994bb9-29n4w 2/2 Running 0 12m reviews-v1-545db77b95-p9pgf 2/2 Running 0 12m reviews-v2-7bf8c9648f-4ltkg 2/2 Running 0 12m reviews-v3-84779c7bbc-vhsmw 2/2 Running 0 12m Note Each Pod has now 2 containers. One application container and another is istio-proxy sidecar container. Step 9 Access UI productpage via console: kubectl get services In Gcloud console execute kubectl port-forward : kubectl port-forward --namespace default svc/productpage 8080:9080 Click Web-Preview button in Gcloud, and select preview on port 8080 Click Normal user URL. Success We can access Bookinfo application configured with Istio and Envoy sidecar using private IP via tunnel Step 10 Remove Bookinfo sample application: kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml Step 11 Uninstall Istio istioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f - kubectl delete namespace istio-system kubectl label namespace default istio-injection-","title":"7.1 Deploy Istio using Custom Resources"},{"location":"Istio%20Demo/#72-deploy-istio-using-istio-operator","text":"Step 1: Deploy the Istio operator: istioctl operator init Note This command runs the operator by creating the following resources in the istio-operator namespace: The operator custom resource definition The operator controller deployment A service to access operator metrics Necessary Istio operator RBAC rules Step 2: To install the Istio demo configuration profile using the operator, run the following command: kubectl apply -f - <<EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: example-istiocontrolplane spec: profile: demo EOF Note The Istio operator controller begins the process of installing Istio within 90 seconds of the creation of the IstioOperator resource. The Istio installation completes within 120 seconds. Step 3 Verify Operator resource status on Kubernetes Cluster: kubectl get istiooperators -n istio-system Output: NAMESPACE NAME REVISION STATUS AGE istio-system example-istiocontrolplane HEALTHY 25m kubectl describe istiooperators example-istiocontrolplane -n istio-system Spec: Profile: demo Status: Component Status: Base: Status: HEALTHY Egress Gateways: Status: HEALTHY Ingress Gateways: Status: HEALTHY Pilot: Status: HEALTHY Status: HEALTHY Events: <none> kubectl get pods -n istio-system Output: istio-egressgateway-9dc6cbc49-5wkqt 1/1 Running 0 2m54s istio-ingressgateway-7975cdb749-xjc5g 1/1 Running 0 2m54s istiod-77b4d7b55d-j6kb5 1/1 Running 0 3m9s Info Istio control-plane include following components: istiod - contains components such as Citadel and Pilot istio-ingressgateway Istio Ingress Gateway istio-ingressgateway Istio Egress Gateway Verify installation with istioctl CLI: istioctl verify-install Step 5 Enable automatic Envoy sidecar injection. Currently there are 2 method to inject Envoy sidecar inside Istio Mesh: Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment. Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update. Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later: kubectl label namespace default istio-injection=enabled Step 8 Deploy Bookinfo sample application on Istio with Auto Sidecar Injection kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Step 8 Verify deployed application: The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it. kubectl get pods Output: NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-zqjbg 2/2 Running 0 13m productpage-v1-6b746f74dc-dj959 2/2 Running 0 12m ratings-v1-b6994bb9-29n4w 2/2 Running 0 12m reviews-v1-545db77b95-p9pgf 2/2 Running 0 12m reviews-v2-7bf8c9648f-4ltkg 2/2 Running 0 12m reviews-v3-84779c7bbc-vhsmw 2/2 Running 0 12m Summary We've leaned to deploy Istio Control plane using istioctl cli and using Istio Operator. We also deployed regular k8s application on the namespace marked with auto injection! Step 9 Cleanup and Uninstall Istio Operator kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl delete istiooperators.install.istio.io -n istio-system example-istiocontrolplane Note Wait until Istio is uninstalled - this may take some time. Then you can remove the Istio operator for the old revision by running the following command: istioctl operator remove","title":"7.2 Deploy Istio using Istio Operator"},{"location":"Traffic_control/","text":"Istio Traffic Control - Canary Deployments \u00b6 Objective: Install Istio Deploy an application and enable automatic sidecar injection Deploy a canary service and use Istio to route some traffic to the new service Prerequisite (Please Skip if already done) \u00b6 Scale-up cluster back to 3 nodes and Update VPC firewall rules to enable auto-injection and the istioctl version and istioctl ps commands. References: Opening ports on a private cluster Istio deployment on GKE Private Clusters Step 1: Locate Terraform Configuration directory. cd ~/$MY_REPO/notepad-infrastructure Step 2: Configure GKE Cluster to Scale back to 1 node per zone in a region. edit terraform.tfvars And set gke_pool_node_count = \"1\" (Skip if already installed Istio on this cluster) Step 3: Create new firewall rule for source range (master-ipv4-cidr) of the cluster, that will open required ports for Istio installation: cat <<EOF> istio_firewall.tf resource \"google_compute_firewall\" \"istio_specific\" { name = format(\"allow-istio-in-privategke-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link source_ranges = [\"172.16.0.0/28\"] allow { protocol = \"tcp\" ports = [\"10250\", \"443\", \"15017\", \"15014\", \"8080\"] } } EOF Step 4: Review TF Plan: terraform plan -var-file terraform.tfvars Step 5: Scale up GKE Cluster Node Pool and update firewall rules for required range: terraform apply -var-file terraform.tfvars Result GKE Clusters has been scalled to 3 nodes and has firewall rules opened Step 6: Verify firewall rule: gcloud compute firewall-rules list --filter=\"name~allow-istio-in-privategke*\" Output: NAME NETWORK DIRECTION PRIORITY ALLOW DENY DISABLED allow-istio-in-privategke-$student-notepad-dev vpc-$student-notepad-dev INGRESS 1000 tcp:10250,tcp:443,tcp:15017,tcp:15014,tcp:8080 False 1 Deploy Istio using Custom Resources \u00b6 This installation guide uses the istioctl command line tool to provide rich customization of the Istio control plane and of the sidecars for the Istio data plane. Download and extract the latest release: curl -L https://istio.io/downloadIstio | sh - cd istio-1.11.0 export PATH=$PWD/bin:$PATH Success The above command will fetch Istio packages and untar them in the same folder. Note Istio installation directory contains: bin directory contains istioctl client binary manifests Installation Profiles, configurations and Helm Charts samples directory contains sample applications deployment tools directory contains auto-completion tooling and etc. Step 2 Deploy Istio Custom Resource Definitions (CRDs) Istio extends Kubernetes using Custom Resource Definitions (CRDs). CRDs allow registration of new/non-default Kubernetes resources. When Istio CRDs are deployed, Istio\u2019s objects are registered as Kubernetes objects, providing a highly integrated experience with Kubernetes as a deployment platform and thus allowing Kubernetes to store configuration of Istio features such as routing, security and telemetry and etc. We going to install using using demo configuration profile. The demo configuration profile allows to experiment with most of Istio features with modest resource requirements. Since it enables high levels of tracing and access logging, it is not suitable for production use cases. export STATIC_IP=<set your Static IP> istioctl install --set profile=demo --set values.gateways.istio-ingressgateway.loadBalancerIP=$STATIC_IP Output: \u2714 Istio core installed \u2714 Istiod installed \u2714 Egress gateways installed \u2714 Ingress gateways installed \u2714 Installation complete Thank you for installing Istio 1.11. Please take a few minutes to tell us about your install/upgrade experience! Note Wait a few seconds for the CRDs to be committed in the Kubernetes API-server. Step 3 Verify Istio CRDs successfully applied to Kubernetes Cluster. kubectl get crds | grep istio 2 Deploy an Micorservices Application \u00b6 Step 1 Get the source code for a sample application. For this lab, we will use the Online Boutique sample app. cd ~ git clone https://github.com/GoogleCloudPlatform/microservices-demo.git cd microservices-demo Step 2 Create a namespace online-boutique for example kubectl create namespace online-boutique Step 3 Enable automatic sidecar injection for the online-boutique namespace kubectl label namespace online-boutique istio-injection=enabled --overwrite kubectl config set-context --current --namespace=online-boutique Step 4 Deploy the application by creating the kubernetes manifests kubectl apply -f ./release/kubernetes-manifests.yaml -n online-boutique Step 5 Watch the pods and verify that all pods were started successfully with the sidecar injected watch kubectl get pods -n online-boutique Output: NAME READY STATUS RESTARTS AGE adservice-5844cffbd4-h7lkk 2/2 Running 0 84s cartservice-fdc659ddc-fzmn9 2/2 Running 0 86s checkoutservice-64db75877d-hk25l 2/2 Running 0 88s currencyservice-9b7cdb45b-9xzhl 2/2 Running 0 85s emailservice-64d98b6f9d-v44rm 2/2 Running 0 89s frontend-76ff9556-p62g9 2/2 Running 0 87s loadgenerator-589648f87f-4dfkp 2/2 Running 0 85s paymentservice-65bdf6757d-zd4w4 2/2 Running 0 87s productcatalogservice-5cd47f8cc8-c6hdn 2/2 Running 0 86s recommendationservice-b75687c5b-clcbt 2/2 Running 0 88s redis-cart-74594bd569-9gpjt 2/2 Running 0 84s shippingservice-778554994-ttwf2 2/2 Running 0 85s Result Pods Started with sidecar auto-injected 3 Allow Traffic to Online Boutique Application \u00b6 In order for traffic to flow to the frontend of the application, we need to create an ingress gateway and a virtual service attached to it. Step 1 Apply the istio manifests to configure the Istio Gateway : cat <<EOF> frontend-gateway.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: frontend-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" EOF kubectl apply -f frontend-gateway.yaml Info Gateway resource important fields: name - name of gateway spec.selector.istio - which gateway to use (can be several) spec.servers - defines which hosts will use this gateway proxy spec.servers.port.number - ports to expose spec.servers.port.host - host(s) for this port Note At this point our gateway is listening for request that match any host on port 80 and it's accessible on the public internet. Check the created gateway: kubectl get gateway kubectl get gateway -o yaml Check the Envoy listeners been created: export INGRESS_POD=$(kubectl get pods -n istio-system | grep 'ingress' |awk '{ print $1}') istioctl proxy-config listener $INGRESS_POD -n istio-system Success HTTP port correctly exposed correctly Verify what Ingress Gateway spec.selector.istio selects for use: kubectl get pod --selector=\"istio=ingressgateway\" --all-namespaces Result istio-ingressgateway-xxx pod in istio-system (our edge envoy pod) will proxy traffic further to Kubernetes services. Try to connect to the Istio Ingress IP: curl -v $STATIC_IP Result It's returning 404 error. Summary We've create gateway resource that is listening for request that match any host on port 80 and it's accessible on the public internet, however it's returning 404 error because once the gateway receives the request it doesn't know where it needs to send it. Step 2 Apply the istio manifests to configure VirtualService to allow external traffic into the mesh and route it to the frontend service When traffic comes into the gateway , it is required to get it to a specific service within the service mesh and to do that, we\u2019ll use the VirtualService resource. In Istio, a VirtualService defines the rules that control how requests for a service are routed within an Istio service mesh. For instance, a VirtualService can route requests to different versions of a service , Requests can be routed based on the request source and destination, HTTP paths and header field and weights associated with individual service versions. As well as use advanced routing properties such as retries, request timeouts, circuit braking, fault injection and etc. export STUDENT=ayrat cat <<EOF> frontend-vs.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend-ingress spec: hosts: - \"$STUDENT.cloud-montreal.ca\" gateways: - frontend-gateway http: - route: - destination: host: frontend port: number: 80 EOF kubectl apply -f frontend-vs.yaml Acquire frontend app URL: kubectl get virtualservices Access frontend app URL from your browser: !!! success We can access frontend web app via Istio Ingress Gateway frontend.yaml: defines another virtual service to be used by our load generator to make sure traffic directed to the frontend from within the mesh stays internal, and doesn't go through the gateway. Whitelist-egress-googleapis.yaml: Used to whitelist various google APIs used by services within the mesh. Step 3 Defines another Virtualservice to be used by our load generator to make sure traffic directed to the frontend from within the mesh stays internal, and doesn't go through the gateway. cat <<EOF> frontend-load-vs.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend spec: hosts: - \"frontend.default.svc.cluster.local\" http: - route: - destination: host: frontend port: number: 80 EOF kubectl apply -f frontend-load-vs.yaml Step 4 Create ServiceEntry custom resource to allow access to external APIs googleapis.com and GCE metadata service metadata.google.internal that app is using: Because all outbound traffic from an Istio-enabled pod is redirected to its sidecar proxy by default, accessibility of URLs outside of the cluster depends on the configuration of the proxy. By default, Istio configures the Envoy proxy to passthrough requests for unknown services. Although this provides a convenient way to get started with Istio, configuring stricter control is usually preferable in production scenarios. Istio has an option, global.outboundTrafficPolicy.mode , that configures the sidecar handling of external services, that is, those services that are not defined in Istio\u2019s internal service registry. If this option is set to ALLOW_ANY , the Istio proxy lets calls to unknown services pass through. If the option is set to REGISTRY_ONLY , then the Istio proxy blocks any host without an HTTP service or service entry defined within the mesh. cat <<EOF> allow-egress-googleapis.yaml apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: allow-egress-googleapis spec: hosts: - \"accounts.google.com\" # Used to get token - \"*.googleapis.com\" ports: - number: 80 protocol: HTTP name: http - number: 443 protocol: HTTPS name: https --- apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: allow-egress-google-metadata spec: hosts: - metadata.google.internal addresses: - 169.254.169.254 # GCE metadata server ports: - number: 80 name: http protocol: HTTP - number: 443 name: https protocol: HTTPS EOF kubectl apply -f allow-egress-googleapis.yaml kubectl get serviceentries Result ServiceEntry resource inserts an entry into Istio\u2019s service registry which makes explicit that clients within the mesh are allowed to call a metadata.google.internal , accounts.google.com , *.googleapis.com regardless the namespaces 4 Canary Deployments with Istio \u00b6 We going to deploy a new version of recommendationservice with canary label. Once we\u2019ve deployed the new version, we can start releasing it gradually by routing 20% of all incoming requests to the latest version, while the rest of the requests (98%) still goes to the existing version. Step 1 Patch the Recommendation service deployment to give it a production label kubectl patch deployment -n online-boutique recommendationservice --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/metadata/labels/version\", \"value\": \"production\"}]' Step 2 Check the labels on the deployment export RECOMMMENDATIONSERVICE=$(kubectl get pods | grep 'recommendationservice' |awk '{ print $1}') kubectl get pod -n online-boutique $RECOMMMENDATIONSERVICE --show-labels Step 3 Create a DestinationRule that defines two subsets production and canary cat <<EOF> destinationrule-recommendation.yaml apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: recommendationservice namespace: online-boutique spec: host: recommendationservice subsets: - name: production labels: version: production - name: canary labels: version: canary EOF kubectl apply -f destinationrule-recommendation.yaml kubectl get destinationrules Result We've defined DestinationRule with 2 subsets. Step 4 Make a change to the Recommendation service and by modifying it's max_responses parameter. cd ~/microservices-demo ls cat src/recommendationservice/Dockerfile sed -i 's/max_responses = 5/max_responses = 2/g' \\ src/recommendationservice/recommendation_server.py Step 5 Build a new version of the image PROJECT_ID=$(gcloud config list --format \"value(core.project)\") docker build -t \\ gcr.io/${PROJECT_ID}/microservices_demo/recommendationservice:canary \\ src/recommendationservice Step 6 Push the image to GCR docker push gcr.io/${PROJECT_ID}/microservices_demo/recommendationservice:canary Step 7 Deploy the canary version of the Recommendation service: cat <<EOF> recommendation-deployment-vcanary.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: recommendationservice version: canary name: recommendationservice-canary namespace: online-boutique spec: replicas: 1 selector: matchLabels: app: recommendationservice version: canary template: metadata: labels: app: recommendationservice version: canary spec: containers: - env: - name: PORT value: \"8080\" - name: PRODUCT_CATALOG_SERVICE_ADDR value: productcatalogservice:3550 - name: ENABLE_PROFILER value: \"0\" image: gcr.io/$PROJECT_ID/microservices_demo/recommendationservice:canary imagePullPolicy: Always livenessProbe: exec: command: - /bin/grpc_health_probe - -addr=:8080 failureThreshold: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 name: server ports: - containerPort: 8080 protocol: TCP readinessProbe: exec: command: - /bin/grpc_health_probe - -addr=:8080 failureThreshold: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 resources: limits: cpu: 200m memory: 450Mi requests: cpu: 100m memory: 220Mi EOF kubectl apply -f recommendation-deployment-vcanary.yaml kubectl get deploy | grep recommendationservice Result 2 versions of recommendationservice has been deployed Step 8 Create a virtual service to direct 20% of the traffic to the canary version cat <<EOF> vs-recommendation.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendationservice namespace: online-boutique spec: hosts: - recommendationservice http: - route: - destination: host: recommendationservice port: number: 8080 subset: production weight: 80 - destination: host: recommendationservice port: number: 8080 subset: canary weight: 20 EOF kubectl apply -f vs-recommendation.yaml kubectl get VirtualService Step 9 Refresh the page for one of the products where recommendations are shown multiple times, and notice how most of the time you get 5 recommendations while if 20% of the time you get only 2 recommendations. 5 Cleanup \u00b6 Step 1 Uninstall Istio istioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f - kubectl delete namespace istio-system kubectl delete namespace online-boutique Step 2 Resize GKE Cluster to 0 nodes, to avoid charges: cd ~/$MY_REPO/notepad-infrastructure edit terraform.tfvars And set gke_pool_node_count = \"0\" Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars Step 4: Shutdown all Nodes in GKE Cluster Node Pool: terraform apply -var-file terraform.tfvars !!! result GKE Clusters has been scale down to 0 nodes.","title":"Lab 4 Traffic Management"},{"location":"Traffic_control/#istio-traffic-control-canary-deployments","text":"Objective: Install Istio Deploy an application and enable automatic sidecar injection Deploy a canary service and use Istio to route some traffic to the new service","title":"Istio Traffic Control - Canary Deployments"},{"location":"Traffic_control/#prerequisite-please-skip-if-already-done","text":"Scale-up cluster back to 3 nodes and Update VPC firewall rules to enable auto-injection and the istioctl version and istioctl ps commands. References: Opening ports on a private cluster Istio deployment on GKE Private Clusters Step 1: Locate Terraform Configuration directory. cd ~/$MY_REPO/notepad-infrastructure Step 2: Configure GKE Cluster to Scale back to 1 node per zone in a region. edit terraform.tfvars And set gke_pool_node_count = \"1\" (Skip if already installed Istio on this cluster) Step 3: Create new firewall rule for source range (master-ipv4-cidr) of the cluster, that will open required ports for Istio installation: cat <<EOF> istio_firewall.tf resource \"google_compute_firewall\" \"istio_specific\" { name = format(\"allow-istio-in-privategke-%s-%s-%s\", var.org, var.product, var.environment) network = google_compute_network.vpc_network.self_link source_ranges = [\"172.16.0.0/28\"] allow { protocol = \"tcp\" ports = [\"10250\", \"443\", \"15017\", \"15014\", \"8080\"] } } EOF Step 4: Review TF Plan: terraform plan -var-file terraform.tfvars Step 5: Scale up GKE Cluster Node Pool and update firewall rules for required range: terraform apply -var-file terraform.tfvars Result GKE Clusters has been scalled to 3 nodes and has firewall rules opened Step 6: Verify firewall rule: gcloud compute firewall-rules list --filter=\"name~allow-istio-in-privategke*\" Output: NAME NETWORK DIRECTION PRIORITY ALLOW DENY DISABLED allow-istio-in-privategke-$student-notepad-dev vpc-$student-notepad-dev INGRESS 1000 tcp:10250,tcp:443,tcp:15017,tcp:15014,tcp:8080 False","title":"Prerequisite (Please Skip if already done)"},{"location":"Traffic_control/#1-deploy-istio-using-custom-resources","text":"This installation guide uses the istioctl command line tool to provide rich customization of the Istio control plane and of the sidecars for the Istio data plane. Download and extract the latest release: curl -L https://istio.io/downloadIstio | sh - cd istio-1.11.0 export PATH=$PWD/bin:$PATH Success The above command will fetch Istio packages and untar them in the same folder. Note Istio installation directory contains: bin directory contains istioctl client binary manifests Installation Profiles, configurations and Helm Charts samples directory contains sample applications deployment tools directory contains auto-completion tooling and etc. Step 2 Deploy Istio Custom Resource Definitions (CRDs) Istio extends Kubernetes using Custom Resource Definitions (CRDs). CRDs allow registration of new/non-default Kubernetes resources. When Istio CRDs are deployed, Istio\u2019s objects are registered as Kubernetes objects, providing a highly integrated experience with Kubernetes as a deployment platform and thus allowing Kubernetes to store configuration of Istio features such as routing, security and telemetry and etc. We going to install using using demo configuration profile. The demo configuration profile allows to experiment with most of Istio features with modest resource requirements. Since it enables high levels of tracing and access logging, it is not suitable for production use cases. export STATIC_IP=<set your Static IP> istioctl install --set profile=demo --set values.gateways.istio-ingressgateway.loadBalancerIP=$STATIC_IP Output: \u2714 Istio core installed \u2714 Istiod installed \u2714 Egress gateways installed \u2714 Ingress gateways installed \u2714 Installation complete Thank you for installing Istio 1.11. Please take a few minutes to tell us about your install/upgrade experience! Note Wait a few seconds for the CRDs to be committed in the Kubernetes API-server. Step 3 Verify Istio CRDs successfully applied to Kubernetes Cluster. kubectl get crds | grep istio","title":"1  Deploy Istio using Custom Resources"},{"location":"Traffic_control/#2-deploy-an-micorservices-application","text":"Step 1 Get the source code for a sample application. For this lab, we will use the Online Boutique sample app. cd ~ git clone https://github.com/GoogleCloudPlatform/microservices-demo.git cd microservices-demo Step 2 Create a namespace online-boutique for example kubectl create namespace online-boutique Step 3 Enable automatic sidecar injection for the online-boutique namespace kubectl label namespace online-boutique istio-injection=enabled --overwrite kubectl config set-context --current --namespace=online-boutique Step 4 Deploy the application by creating the kubernetes manifests kubectl apply -f ./release/kubernetes-manifests.yaml -n online-boutique Step 5 Watch the pods and verify that all pods were started successfully with the sidecar injected watch kubectl get pods -n online-boutique Output: NAME READY STATUS RESTARTS AGE adservice-5844cffbd4-h7lkk 2/2 Running 0 84s cartservice-fdc659ddc-fzmn9 2/2 Running 0 86s checkoutservice-64db75877d-hk25l 2/2 Running 0 88s currencyservice-9b7cdb45b-9xzhl 2/2 Running 0 85s emailservice-64d98b6f9d-v44rm 2/2 Running 0 89s frontend-76ff9556-p62g9 2/2 Running 0 87s loadgenerator-589648f87f-4dfkp 2/2 Running 0 85s paymentservice-65bdf6757d-zd4w4 2/2 Running 0 87s productcatalogservice-5cd47f8cc8-c6hdn 2/2 Running 0 86s recommendationservice-b75687c5b-clcbt 2/2 Running 0 88s redis-cart-74594bd569-9gpjt 2/2 Running 0 84s shippingservice-778554994-ttwf2 2/2 Running 0 85s Result Pods Started with sidecar auto-injected","title":"2 Deploy an Micorservices Application"},{"location":"Traffic_control/#3-allow-traffic-to-online-boutique-application","text":"In order for traffic to flow to the frontend of the application, we need to create an ingress gateway and a virtual service attached to it. Step 1 Apply the istio manifests to configure the Istio Gateway : cat <<EOF> frontend-gateway.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: frontend-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" EOF kubectl apply -f frontend-gateway.yaml Info Gateway resource important fields: name - name of gateway spec.selector.istio - which gateway to use (can be several) spec.servers - defines which hosts will use this gateway proxy spec.servers.port.number - ports to expose spec.servers.port.host - host(s) for this port Note At this point our gateway is listening for request that match any host on port 80 and it's accessible on the public internet. Check the created gateway: kubectl get gateway kubectl get gateway -o yaml Check the Envoy listeners been created: export INGRESS_POD=$(kubectl get pods -n istio-system | grep 'ingress' |awk '{ print $1}') istioctl proxy-config listener $INGRESS_POD -n istio-system Success HTTP port correctly exposed correctly Verify what Ingress Gateway spec.selector.istio selects for use: kubectl get pod --selector=\"istio=ingressgateway\" --all-namespaces Result istio-ingressgateway-xxx pod in istio-system (our edge envoy pod) will proxy traffic further to Kubernetes services. Try to connect to the Istio Ingress IP: curl -v $STATIC_IP Result It's returning 404 error. Summary We've create gateway resource that is listening for request that match any host on port 80 and it's accessible on the public internet, however it's returning 404 error because once the gateway receives the request it doesn't know where it needs to send it. Step 2 Apply the istio manifests to configure VirtualService to allow external traffic into the mesh and route it to the frontend service When traffic comes into the gateway , it is required to get it to a specific service within the service mesh and to do that, we\u2019ll use the VirtualService resource. In Istio, a VirtualService defines the rules that control how requests for a service are routed within an Istio service mesh. For instance, a VirtualService can route requests to different versions of a service , Requests can be routed based on the request source and destination, HTTP paths and header field and weights associated with individual service versions. As well as use advanced routing properties such as retries, request timeouts, circuit braking, fault injection and etc. export STUDENT=ayrat cat <<EOF> frontend-vs.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend-ingress spec: hosts: - \"$STUDENT.cloud-montreal.ca\" gateways: - frontend-gateway http: - route: - destination: host: frontend port: number: 80 EOF kubectl apply -f frontend-vs.yaml Acquire frontend app URL: kubectl get virtualservices Access frontend app URL from your browser: !!! success We can access frontend web app via Istio Ingress Gateway frontend.yaml: defines another virtual service to be used by our load generator to make sure traffic directed to the frontend from within the mesh stays internal, and doesn't go through the gateway. Whitelist-egress-googleapis.yaml: Used to whitelist various google APIs used by services within the mesh. Step 3 Defines another Virtualservice to be used by our load generator to make sure traffic directed to the frontend from within the mesh stays internal, and doesn't go through the gateway. cat <<EOF> frontend-load-vs.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend spec: hosts: - \"frontend.default.svc.cluster.local\" http: - route: - destination: host: frontend port: number: 80 EOF kubectl apply -f frontend-load-vs.yaml Step 4 Create ServiceEntry custom resource to allow access to external APIs googleapis.com and GCE metadata service metadata.google.internal that app is using: Because all outbound traffic from an Istio-enabled pod is redirected to its sidecar proxy by default, accessibility of URLs outside of the cluster depends on the configuration of the proxy. By default, Istio configures the Envoy proxy to passthrough requests for unknown services. Although this provides a convenient way to get started with Istio, configuring stricter control is usually preferable in production scenarios. Istio has an option, global.outboundTrafficPolicy.mode , that configures the sidecar handling of external services, that is, those services that are not defined in Istio\u2019s internal service registry. If this option is set to ALLOW_ANY , the Istio proxy lets calls to unknown services pass through. If the option is set to REGISTRY_ONLY , then the Istio proxy blocks any host without an HTTP service or service entry defined within the mesh. cat <<EOF> allow-egress-googleapis.yaml apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: allow-egress-googleapis spec: hosts: - \"accounts.google.com\" # Used to get token - \"*.googleapis.com\" ports: - number: 80 protocol: HTTP name: http - number: 443 protocol: HTTPS name: https --- apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: allow-egress-google-metadata spec: hosts: - metadata.google.internal addresses: - 169.254.169.254 # GCE metadata server ports: - number: 80 name: http protocol: HTTP - number: 443 name: https protocol: HTTPS EOF kubectl apply -f allow-egress-googleapis.yaml kubectl get serviceentries Result ServiceEntry resource inserts an entry into Istio\u2019s service registry which makes explicit that clients within the mesh are allowed to call a metadata.google.internal , accounts.google.com , *.googleapis.com regardless the namespaces","title":"3 Allow Traffic to Online Boutique Application"},{"location":"Traffic_control/#4-canary-deployments-with-istio","text":"We going to deploy a new version of recommendationservice with canary label. Once we\u2019ve deployed the new version, we can start releasing it gradually by routing 20% of all incoming requests to the latest version, while the rest of the requests (98%) still goes to the existing version. Step 1 Patch the Recommendation service deployment to give it a production label kubectl patch deployment -n online-boutique recommendationservice --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/metadata/labels/version\", \"value\": \"production\"}]' Step 2 Check the labels on the deployment export RECOMMMENDATIONSERVICE=$(kubectl get pods | grep 'recommendationservice' |awk '{ print $1}') kubectl get pod -n online-boutique $RECOMMMENDATIONSERVICE --show-labels Step 3 Create a DestinationRule that defines two subsets production and canary cat <<EOF> destinationrule-recommendation.yaml apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: recommendationservice namespace: online-boutique spec: host: recommendationservice subsets: - name: production labels: version: production - name: canary labels: version: canary EOF kubectl apply -f destinationrule-recommendation.yaml kubectl get destinationrules Result We've defined DestinationRule with 2 subsets. Step 4 Make a change to the Recommendation service and by modifying it's max_responses parameter. cd ~/microservices-demo ls cat src/recommendationservice/Dockerfile sed -i 's/max_responses = 5/max_responses = 2/g' \\ src/recommendationservice/recommendation_server.py Step 5 Build a new version of the image PROJECT_ID=$(gcloud config list --format \"value(core.project)\") docker build -t \\ gcr.io/${PROJECT_ID}/microservices_demo/recommendationservice:canary \\ src/recommendationservice Step 6 Push the image to GCR docker push gcr.io/${PROJECT_ID}/microservices_demo/recommendationservice:canary Step 7 Deploy the canary version of the Recommendation service: cat <<EOF> recommendation-deployment-vcanary.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: recommendationservice version: canary name: recommendationservice-canary namespace: online-boutique spec: replicas: 1 selector: matchLabels: app: recommendationservice version: canary template: metadata: labels: app: recommendationservice version: canary spec: containers: - env: - name: PORT value: \"8080\" - name: PRODUCT_CATALOG_SERVICE_ADDR value: productcatalogservice:3550 - name: ENABLE_PROFILER value: \"0\" image: gcr.io/$PROJECT_ID/microservices_demo/recommendationservice:canary imagePullPolicy: Always livenessProbe: exec: command: - /bin/grpc_health_probe - -addr=:8080 failureThreshold: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 name: server ports: - containerPort: 8080 protocol: TCP readinessProbe: exec: command: - /bin/grpc_health_probe - -addr=:8080 failureThreshold: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 resources: limits: cpu: 200m memory: 450Mi requests: cpu: 100m memory: 220Mi EOF kubectl apply -f recommendation-deployment-vcanary.yaml kubectl get deploy | grep recommendationservice Result 2 versions of recommendationservice has been deployed Step 8 Create a virtual service to direct 20% of the traffic to the canary version cat <<EOF> vs-recommendation.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendationservice namespace: online-boutique spec: hosts: - recommendationservice http: - route: - destination: host: recommendationservice port: number: 8080 subset: production weight: 80 - destination: host: recommendationservice port: number: 8080 subset: canary weight: 20 EOF kubectl apply -f vs-recommendation.yaml kubectl get VirtualService Step 9 Refresh the page for one of the products where recommendations are shown multiple times, and notice how most of the time you get 5 recommendations while if 20% of the time you get only 2 recommendations.","title":"4 Canary Deployments with Istio"},{"location":"Traffic_control/#5-cleanup","text":"Step 1 Uninstall Istio istioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f - kubectl delete namespace istio-system kubectl delete namespace online-boutique Step 2 Resize GKE Cluster to 0 nodes, to avoid charges: cd ~/$MY_REPO/notepad-infrastructure edit terraform.tfvars And set gke_pool_node_count = \"0\" Step 3: Review TF Plan: terraform plan -var-file terraform.tfvars Step 4: Shutdown all Nodes in GKE Cluster Node Pool: terraform apply -var-file terraform.tfvars !!! result GKE Clusters has been scale down to 0 nodes.","title":"5 Cleanup"},{"location":"helmfile/","text":"Install krew \u00b6 ( set -x; cd \"$(mktemp -d)\" && OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" && ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz\" && tar zxvf krew.tar.gz && KREW=./krew-\"${OS}_${ARCH}\" && \"$KREW\" install krew ) export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\" Install kubens \u00b6 kubectl krew install ctx kubectl krew install ns HelmFile: \u00b6 Install Helm \u00b6 Install helmfile from https://github.com/roboll/helmfile#installation \u00b6 wget https://github.com/roboll/helmfile/releases/download/v0.140.0/helmfile_linux_amd64 chmod +x helmfile_linux_amd64 sudo mv helmfile_linux_amd64 /usr/local/bin/helmfile cat <<EOF>> values.yaml alertmanager: enabled: true EOF cat <<EOF>> helmfile-prometheus.yaml repositories: - name: prometheus url: https://prometheus-community.github.io/helm-charts releases: - name: prometheus namespace: prometheus chart: prometheus/prometheus version: 14.5.0 values: - values.yaml EOF \u00b6 Defining a simple Helmfile \u00b6 \u00b6 helmfile --file helmfile-prometheus.yaml apply --wait Test: kubens prometheus helm list","title":"Helmfile"},{"location":"helmfile/#install-krew","text":"( set -x; cd \"$(mktemp -d)\" && OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" && ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz\" && tar zxvf krew.tar.gz && KREW=./krew-\"${OS}_${ARCH}\" && \"$KREW\" install krew ) export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"","title":"Install krew"},{"location":"helmfile/#install-kubens","text":"kubectl krew install ctx kubectl krew install ns","title":"Install kubens"},{"location":"helmfile/#helmfile","text":"","title":"HelmFile:"},{"location":"helmfile/#install-helm","text":"","title":"Install Helm"},{"location":"helmfile/#install-helmfile-from-httpsgithubcomrobollhelmfileinstallation","text":"wget https://github.com/roboll/helmfile/releases/download/v0.140.0/helmfile_linux_amd64 chmod +x helmfile_linux_amd64 sudo mv helmfile_linux_amd64 /usr/local/bin/helmfile cat <<EOF>> values.yaml alertmanager: enabled: true EOF cat <<EOF>> helmfile-prometheus.yaml repositories: - name: prometheus url: https://prometheus-community.github.io/helm-charts releases: - name: prometheus namespace: prometheus chart: prometheus/prometheus version: 14.5.0 values: - values.yaml EOF","title":"Install helmfile from https://github.com/roboll/helmfile#installation"},{"location":"helmfile/#_1","text":"","title":""},{"location":"helmfile/#defining-a-simple-helmfile","text":"","title":"Defining a simple Helmfile"},{"location":"helmfile/#_2","text":"helmfile --file helmfile-prometheus.yaml apply --wait Test: kubens prometheus helm list","title":""},{"location":"ycit019_Lab_10_Networking/","text":"K8s Networking \u00b6 Objective: Kubernetes Network Policy Run applications and expose them via service via Ingress resource. 0 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-networking \\ --zone us-central1-c \\ --enable-network-policy \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-networking --zone us-central1-c 1. Network Policy \u00b6 1.1 Basic Policy Demo \u00b6 This guide will deploy pods in a Kubernetes Namespaces. Let\u2019s create the Namespace object for this guide. Step 1 Create policy-demo Namespace: kubectl create ns policy-demo Step 2 Than create a policy-demo nginx deployment in policy-demo namespace: kubectl create deployment --namespace=policy-demo nginx --replicas=2 --image=nginx Then create a policy-demo service: kubectl expose --namespace=policy-demo deployment nginx --port=80 Step 3 Ensure nginx service is accessible. In order to do that create busybox pod and try to access the nginx service. kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh Output: Waiting for pod `policy-demo/access-472357175-y0m47` to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q nginx -O - Success You should see a response from nginx . Great! Our service is accessible. You can exit the Pod now. Result Pods in a given namespace can be accessed by anyone. Step 4 Now let\u2019s turn on isolation in our policy-demo Namespace. Calico will then prevent connections to pods in this Namespace. In order for Calico to prevent connection to pods in a namespace, we first need to enable network isolation in the namespace by creating following NetworkPolicy : kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny namespace: policy-demo spec: podSelector: matchLabels: {} EOF Note Current Lab is using Calico v3.0. Older versions of Calico (2.1 and prior) used namespace annotation to deny all traffic. Step 5 Verify that all access to the nginx Service is blocked. We can see the effect by trying to access the Service again. In order to do that, run a busybox deployment and try to access the nginx service. kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh Waiting for pod policy-demo/access-472357175-y0m47 to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q --timeout=5 nginx -O - Result download timed out The request should time out after 5 seconds. By enabling isolation on the Namespace, we\u2019ve prevented access to the Service. Step 6 Allow Access using a NetworkPolicy Now, let\u2019s enable access to the nginx Service using a NetworkPolicy. This will allow incoming connections from our access Pod, but not from anywhere else. Create a network policy access-nginx with the following contents: kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: access-nginx namespace: policy-demo spec: podSelector: matchLabels: app: nginx ingress: - from: - podSelector: matchLabels: run: access EOF Notice The NetworkPolicy allows traffic from Pods with the label run: access to Pods with the label app: nginx . The labels are automatically added by kubectl and are based on the name of the resource. Step 7: We should now be able to access the Service from the access Pod. In order to check that create busybox deployment and try to access the nginx Service. kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh Waiting for pod policy-demo/access-472357175-y0m47 to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q --timeout=5 nginx -O - Step 8 Without closing a prompt to a container, open a new terminal and run kubectl get pods --show-labels -n policy-demo Output: NAME READY STATUS RESTARTS AGE LABELS access 1/1 Running 0 5s run=access nginx-6799fc88d8-5r64l 1/1 Running 0 17m app=nginx,pod-template-hash=6799fc88d8 nginx-6799fc88d8-dbtgk 1/1 Running 0 17m app=nginx,pod-template-hash=6799fc88d8 Result We can see the Labels run=access and app=nginx , that has been defined in Network Policy. Step 9 However, we still cannot access the Service from a Pod without the label run: access: Once again run busybox deployment and try to access the nginx service. kubectl run --namespace=policy-demo cant-access --rm -ti --image busybox /bin/sh Waiting for pod policy-demo/cant-access-472357175-y0m47 to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q --timeout=5 nginx -O - wget: download timed out / # Step 9 You can clean up the demo by deleting the demo Namespace: kubectl delete ns policy-demo 1.2 (Demo) Stars Policy \u00b6 1.2.1 Deploy 3 tier-app \u00b6 Let's Deploy 3 tier-app: UI, frontend and backend service, as well as a client service. And configures network policy on each service. Step 1 Deploy Stars Namespace and management-ui apps inside it: kubectl create -f - <<EOF --- kind: Namespace apiVersion: v1 metadata: name: stars --- apiVersion: v1 kind: Namespace metadata: name: management-ui labels: role: management-ui --- apiVersion: v1 kind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 9001 targetPort: 9001 selector: role: management-ui --- apiVersion: apps/v1 kind: Deployment metadata: name: management-ui labels: role: management-ui namespace: management-ui spec: selector: matchLabels: role: management-ui replicas: 1 template: metadata: labels: role: management-ui spec: containers: - name: management-ui image: calico/star-collect:v0.1.0 imagePullPolicy: Always ports: - containerPort: 9001 EOF Step 2 Deploy backend application inside of the stars Namespace: Backend: kubectl create -f - <<EOF apiVersion: v1 kind: Service metadata: name: backend namespace: stars spec: ports: - port: 6379 targetPort: 6379 selector: role: backend --- apiVersion: apps/v1 kind: Deployment metadata: name: backend labels: role: backend namespace: stars spec: selector: matchLabels: role: backend replicas: 1 template: metadata: labels: role: backend spec: containers: - name: backend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=6379 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 6379 EOF Step 3 Deploy frontend application inside of the stars Namespace: kubectl create -f - <<EOF apiVersion: v1 kind: Service metadata: name: frontend namespace: stars spec: ports: - port: 80 targetPort: 80 selector: role: frontend --- apiVersion: apps/v1 kind: Deployment metadata: name: frontend labels: role: frontend namespace: stars spec: selector: matchLabels: role: frontend replicas: 1 template: metadata: labels: role: frontend spec: containers: - name: frontend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=80 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 80 EOF Step 4 Finally deploy client application inside of the client Namespace: kubectl create -f - <<EOF kind: Namespace apiVersion: v1 metadata: name: client labels: role: client --- apiVersion: apps/v1 kind: Deployment metadata: name: client labels: role: client namespace: client spec: selector: matchLabels: role: client replicas: 1 template: metadata: labels: role: client spec: containers: - name: client image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status ports: - containerPort: 9000 --- apiVersion: v1 kind: Service metadata: name: client namespace: client spec: ports: - port: 9000 targetPort: 9000 selector: role: client EOF Step 5 Wait for all the pods to enter Running state. kubectl get pods --all-namespaces --watch Note It may take several minutes to download the necessary Docker images for this demo. Step 6 Locate LoadBalancer IP: kubectl get svc -n management-ui Step 9 Open UI in you browser http://EXTERNAL-IP:9001 in a browser. Result Once all the pods are started, they should have full connectivity. You can see this by visiting the UI. Each service is represented by a single node in the graph. backend -> Node \"B\" frontend -> Node \"F\" client -> Node \"C\" 1.2.2 Enable isolation \u00b6 Step 1 Enable isolation Running the following commands will prevent all Ingress access to the frontend, backend, and client Services located in namespaces starts and client kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny namespace: stars spec: podSelector: matchLabels: {} ingress: [] --- kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny namespace: client spec: podSelector: matchLabels: {} policyTypes: - Ingress EOF Result Now that we've enabled isolation, the UI can no longer access the pods, and so they will no longer show up in the UI. Note You need to Refresh web-browser to see result. Step 2 Allow apps from stars and client namespace access UI service via NetworkPolicy rules: kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui --- kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: client name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui EOF Result refresh the UI - it should now show the Services, but they should not be able to access each other any more. Step 4 Create the NetworkPolicy to allow traffic from the frontend to the backend. kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 EOF Result Refresh the UI. You should see the following: The frontend can now access the backend (on TCP port 80 only). The backend cannot access the frontend at all. The client cannot access the frontend, nor can it access the backend. Step 5 Expose the frontend service to the client namespace. kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80 EOF Success We isolated our app using NetworkPolicy ingress rules so that: The client can now access the frontend, but not the backend. Neither the frontend nor the backend can initiate connections to the client. The frontend can still access the backend. 1.2.3 Cleanup \u00b6 Step 1 You can clean up the demo by deleting the demo Namespaces: kubectl delete ns client stars management-ui 2. Ingress \u00b6 In GKE, Ingress is implemented using Cloud Load Balancing. In other words, when yiu create an Ingress resource in your cluster, GKE creates an HTTP(S) LB for you and configure it. In this lab, we will create a fanout Ingress with two backends for two verisons of the hello-app application. Note If you want to experiment with a different type of Ingress, you can install Nginx Controller using Helm. You can also follow these [instructions] (https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md). 2.1 Deploy an Application \u00b6 Step 1 Deploy the web application version 1, and its service cat <<EOF > web-service-v1.yaml apiVersion: apps/v1 kind: Deployment metadata: name: web-v1 namespace: default spec: selector: matchLabels: run: web version: v1 template: metadata: labels: run: web version: v1 spec: containers: - image: gcr.io/google-samples/hello-app:1.0 imagePullPolicy: IfNotPresent name: web ports: - containerPort: 8080 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: web-v1 namespace: default spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: run: web version: v1 type: NodePort EOF Step 2 Apply the resources to the cluster kubectl apply -f web-service-v1.yaml Step 3 Deploy the web application version 2, and its service cat <<EOF > web-service-v2.yaml apiVersion: apps/v1 kind: Deployment metadata: name: web-v2 namespace: default spec: selector: matchLabels: run: web version: v2 template: metadata: labels: run: web version: v2 spec: containers: - image: gcr.io/google-samples/hello-app:2.0 imagePullPolicy: IfNotPresent name: web ports: - containerPort: 8080 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: web-v2 namespace: default spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: run: web version: v2 type: NodePort EOF Step 4 Apply the resources to the cluster kubectl apply -f web-service-v2.yaml Step 5 Take a look at the pods created and view their IPs kubectl get pod -o wide Step 6 Take a look at the endpoints created and notice how they match the IP for the Pods in the matching service. kubectl get ep 2.2 Creating Single Service Ingress rule \u00b6 Let's expose our web-v1 service using Ingress! Step 1 To start, let's create an Ingress resource that directs traffic to v1 of the web Service cat <<EOF > single-svc-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: single-svc-ingress spec: defaultBackend: service: name: web-v1 port: number: 8080 EOF This manifest defines a Single Service Ingress rule , which makes sure all HTTP requests that will hit the external IP for the Cloud LB created for the Ingress, will be proxied to the web-v1 service on port 8080 . Step 2 Create the Ingress resource kubectl apply -f single-svc-ingress.yaml Step 3 Verify Ingress Resource kubectl get ing Output NAME CLASS HOSTS ADDRESS PORTS AGE single-svc-ingress <none> * 34.117.137.144 80 110s Notice that the Ingress controllers and load balancers may take a minute or two to allocate an IP address. Bingo! Step 4 descirbe the Ingress object created and notice the Google Cloud LB resources created. kubectl describe ing single-svc-ingress Notice that you might need to give it some time to get a similar output to what I have below. Output: Name: single-svc-ingress Namespace: default Address: 34.117.137.144 Default backend: web-v1:8080 (10.32.0.10:8080) Rules: Host Path Backends ---- ---- -------- * * web-v1:8080 (10.32.0.10:8080) Annotations: ingress.kubernetes.io/backends: {\"k8s-be-30059--a2b52ac0331abe29\":\"HEALTHY\"} ingress.kubernetes.io/forwarding-rule: k8s2-fr-8rwy2qu4-default-single-svc-ingress-74hvjigm ingress.kubernetes.io/target-proxy: k8s2-tp-8rwy2qu4-default-single-svc-ingress-74hvjigm ingress.kubernetes.io/url-map: k8s2-um-8rwy2qu4-default-single-svc-ingress-74hvjigm Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 10m loadbalancer-controller UrlMap \"k8s2-um-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created Normal Sync 10m loadbalancer-controller TargetProxy \"k8s2-tp-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created Normal Sync 10m loadbalancer-controller ForwardingRule \"k8s2-fr-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created Normal IPChanged 10m loadbalancer-controller IP is now 34.117.137.144 Normal Sync 4m27s (x6 over 11m) loadbalancer-controller Scheduled for sync Step 5 Access deployed app via Ingress by navigating to the public IP of the ingress, which we got in step 3. Step 6 Finally execute the following command to cleanup the ingress. kubectl delete -f single-svc-ingress.yaml 2.3 Simple fanout \u00b6 Let's explore more sophisticated Ingress rules. This time we going to deploy Simple fanout type that allows for exposing multiple services on same host, but via different paths. This type is very handy when you running in CloudProvider and want to cut cost on creating LoadBalancers for each of you application. Or when running on prem. and it's required to expose multiple services via same host. Step 1 Let's start by a simple fanout with one service. Notice that we are not using a hostname here as for this lab, we don't want to go through setting up DNS. If you have a DNS setup, you can try with an actual hostname . Just make sure to update you DNS records with the Ingress external IP. cat <<EOF > fanout-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: fanout-ingress spec: rules: - http: paths: - path: /v1 pathType: ImplementationSpecific backend: service: name: web-v1 port: number: 8080 EOF Step 2 Create the Ingress resource kubectl apply -f fanout-ingress.yaml Step 3 Verify Ingress Resource kubectl get ing fanout-ingress Step 4 Verify that you can navigate to http://INGRESS_PUBLIC_IP/v1 and get the following Hello, world! Version: 1.0.0 Step 5 Verify that ingress is returning default backend - 404 page in a browser. Open the browser with public_ip: http://INGRESS_PUBLIC_IP/test Step 6 On your own modify the Ingress resource to add a second path so when we navigate to http://INGRESS_PUBLIC_IP/v2 users will get routed to web-v2 Step 7 Delete the demo Finally execute the following command to cleanup the application and ingress. kubectl delete -f fanout-ingress.yaml kubectl apply -f web-service-v1.yaml kubectl apply -f web-service-v2.yaml 2.4 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-networking","title":"Lab 10 Kubernetes Networking"},{"location":"ycit019_Lab_10_Networking/#k8s-networking","text":"Objective: Kubernetes Network Policy Run applications and expose them via service via Ingress resource.","title":"K8s Networking"},{"location":"ycit019_Lab_10_Networking/#0-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-networking \\ --zone us-central1-c \\ --enable-network-policy \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-networking --zone us-central1-c","title":"0 Create GKE Cluster"},{"location":"ycit019_Lab_10_Networking/#1-network-policy","text":"","title":"1. Network Policy"},{"location":"ycit019_Lab_10_Networking/#11-basic-policy-demo","text":"This guide will deploy pods in a Kubernetes Namespaces. Let\u2019s create the Namespace object for this guide. Step 1 Create policy-demo Namespace: kubectl create ns policy-demo Step 2 Than create a policy-demo nginx deployment in policy-demo namespace: kubectl create deployment --namespace=policy-demo nginx --replicas=2 --image=nginx Then create a policy-demo service: kubectl expose --namespace=policy-demo deployment nginx --port=80 Step 3 Ensure nginx service is accessible. In order to do that create busybox pod and try to access the nginx service. kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh Output: Waiting for pod `policy-demo/access-472357175-y0m47` to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q nginx -O - Success You should see a response from nginx . Great! Our service is accessible. You can exit the Pod now. Result Pods in a given namespace can be accessed by anyone. Step 4 Now let\u2019s turn on isolation in our policy-demo Namespace. Calico will then prevent connections to pods in this Namespace. In order for Calico to prevent connection to pods in a namespace, we first need to enable network isolation in the namespace by creating following NetworkPolicy : kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny namespace: policy-demo spec: podSelector: matchLabels: {} EOF Note Current Lab is using Calico v3.0. Older versions of Calico (2.1 and prior) used namespace annotation to deny all traffic. Step 5 Verify that all access to the nginx Service is blocked. We can see the effect by trying to access the Service again. In order to do that, run a busybox deployment and try to access the nginx service. kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh Waiting for pod policy-demo/access-472357175-y0m47 to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q --timeout=5 nginx -O - Result download timed out The request should time out after 5 seconds. By enabling isolation on the Namespace, we\u2019ve prevented access to the Service. Step 6 Allow Access using a NetworkPolicy Now, let\u2019s enable access to the nginx Service using a NetworkPolicy. This will allow incoming connections from our access Pod, but not from anywhere else. Create a network policy access-nginx with the following contents: kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: access-nginx namespace: policy-demo spec: podSelector: matchLabels: app: nginx ingress: - from: - podSelector: matchLabels: run: access EOF Notice The NetworkPolicy allows traffic from Pods with the label run: access to Pods with the label app: nginx . The labels are automatically added by kubectl and are based on the name of the resource. Step 7: We should now be able to access the Service from the access Pod. In order to check that create busybox deployment and try to access the nginx Service. kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh Waiting for pod policy-demo/access-472357175-y0m47 to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q --timeout=5 nginx -O - Step 8 Without closing a prompt to a container, open a new terminal and run kubectl get pods --show-labels -n policy-demo Output: NAME READY STATUS RESTARTS AGE LABELS access 1/1 Running 0 5s run=access nginx-6799fc88d8-5r64l 1/1 Running 0 17m app=nginx,pod-template-hash=6799fc88d8 nginx-6799fc88d8-dbtgk 1/1 Running 0 17m app=nginx,pod-template-hash=6799fc88d8 Result We can see the Labels run=access and app=nginx , that has been defined in Network Policy. Step 9 However, we still cannot access the Service from a Pod without the label run: access: Once again run busybox deployment and try to access the nginx service. kubectl run --namespace=policy-demo cant-access --rm -ti --image busybox /bin/sh Waiting for pod policy-demo/cant-access-472357175-y0m47 to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q --timeout=5 nginx -O - wget: download timed out / # Step 9 You can clean up the demo by deleting the demo Namespace: kubectl delete ns policy-demo","title":"1.1 Basic Policy Demo"},{"location":"ycit019_Lab_10_Networking/#12-demo-stars-policy","text":"","title":"1.2 (Demo) Stars Policy"},{"location":"ycit019_Lab_10_Networking/#121-deploy-3-tier-app","text":"Let's Deploy 3 tier-app: UI, frontend and backend service, as well as a client service. And configures network policy on each service. Step 1 Deploy Stars Namespace and management-ui apps inside it: kubectl create -f - <<EOF --- kind: Namespace apiVersion: v1 metadata: name: stars --- apiVersion: v1 kind: Namespace metadata: name: management-ui labels: role: management-ui --- apiVersion: v1 kind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 9001 targetPort: 9001 selector: role: management-ui --- apiVersion: apps/v1 kind: Deployment metadata: name: management-ui labels: role: management-ui namespace: management-ui spec: selector: matchLabels: role: management-ui replicas: 1 template: metadata: labels: role: management-ui spec: containers: - name: management-ui image: calico/star-collect:v0.1.0 imagePullPolicy: Always ports: - containerPort: 9001 EOF Step 2 Deploy backend application inside of the stars Namespace: Backend: kubectl create -f - <<EOF apiVersion: v1 kind: Service metadata: name: backend namespace: stars spec: ports: - port: 6379 targetPort: 6379 selector: role: backend --- apiVersion: apps/v1 kind: Deployment metadata: name: backend labels: role: backend namespace: stars spec: selector: matchLabels: role: backend replicas: 1 template: metadata: labels: role: backend spec: containers: - name: backend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=6379 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 6379 EOF Step 3 Deploy frontend application inside of the stars Namespace: kubectl create -f - <<EOF apiVersion: v1 kind: Service metadata: name: frontend namespace: stars spec: ports: - port: 80 targetPort: 80 selector: role: frontend --- apiVersion: apps/v1 kind: Deployment metadata: name: frontend labels: role: frontend namespace: stars spec: selector: matchLabels: role: frontend replicas: 1 template: metadata: labels: role: frontend spec: containers: - name: frontend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=80 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 80 EOF Step 4 Finally deploy client application inside of the client Namespace: kubectl create -f - <<EOF kind: Namespace apiVersion: v1 metadata: name: client labels: role: client --- apiVersion: apps/v1 kind: Deployment metadata: name: client labels: role: client namespace: client spec: selector: matchLabels: role: client replicas: 1 template: metadata: labels: role: client spec: containers: - name: client image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status ports: - containerPort: 9000 --- apiVersion: v1 kind: Service metadata: name: client namespace: client spec: ports: - port: 9000 targetPort: 9000 selector: role: client EOF Step 5 Wait for all the pods to enter Running state. kubectl get pods --all-namespaces --watch Note It may take several minutes to download the necessary Docker images for this demo. Step 6 Locate LoadBalancer IP: kubectl get svc -n management-ui Step 9 Open UI in you browser http://EXTERNAL-IP:9001 in a browser. Result Once all the pods are started, they should have full connectivity. You can see this by visiting the UI. Each service is represented by a single node in the graph. backend -> Node \"B\" frontend -> Node \"F\" client -> Node \"C\"","title":"1.2.1 Deploy 3 tier-app"},{"location":"ycit019_Lab_10_Networking/#122-enable-isolation","text":"Step 1 Enable isolation Running the following commands will prevent all Ingress access to the frontend, backend, and client Services located in namespaces starts and client kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny namespace: stars spec: podSelector: matchLabels: {} ingress: [] --- kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny namespace: client spec: podSelector: matchLabels: {} policyTypes: - Ingress EOF Result Now that we've enabled isolation, the UI can no longer access the pods, and so they will no longer show up in the UI. Note You need to Refresh web-browser to see result. Step 2 Allow apps from stars and client namespace access UI service via NetworkPolicy rules: kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui --- kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: client name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui EOF Result refresh the UI - it should now show the Services, but they should not be able to access each other any more. Step 4 Create the NetworkPolicy to allow traffic from the frontend to the backend. kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 EOF Result Refresh the UI. You should see the following: The frontend can now access the backend (on TCP port 80 only). The backend cannot access the frontend at all. The client cannot access the frontend, nor can it access the backend. Step 5 Expose the frontend service to the client namespace. kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80 EOF Success We isolated our app using NetworkPolicy ingress rules so that: The client can now access the frontend, but not the backend. Neither the frontend nor the backend can initiate connections to the client. The frontend can still access the backend.","title":"1.2.2 Enable isolation"},{"location":"ycit019_Lab_10_Networking/#123-cleanup","text":"Step 1 You can clean up the demo by deleting the demo Namespaces: kubectl delete ns client stars management-ui","title":"1.2.3 Cleanup"},{"location":"ycit019_Lab_10_Networking/#2-ingress","text":"In GKE, Ingress is implemented using Cloud Load Balancing. In other words, when yiu create an Ingress resource in your cluster, GKE creates an HTTP(S) LB for you and configure it. In this lab, we will create a fanout Ingress with two backends for two verisons of the hello-app application. Note If you want to experiment with a different type of Ingress, you can install Nginx Controller using Helm. You can also follow these [instructions] (https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md).","title":"2. Ingress"},{"location":"ycit019_Lab_10_Networking/#21-deploy-an-application","text":"Step 1 Deploy the web application version 1, and its service cat <<EOF > web-service-v1.yaml apiVersion: apps/v1 kind: Deployment metadata: name: web-v1 namespace: default spec: selector: matchLabels: run: web version: v1 template: metadata: labels: run: web version: v1 spec: containers: - image: gcr.io/google-samples/hello-app:1.0 imagePullPolicy: IfNotPresent name: web ports: - containerPort: 8080 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: web-v1 namespace: default spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: run: web version: v1 type: NodePort EOF Step 2 Apply the resources to the cluster kubectl apply -f web-service-v1.yaml Step 3 Deploy the web application version 2, and its service cat <<EOF > web-service-v2.yaml apiVersion: apps/v1 kind: Deployment metadata: name: web-v2 namespace: default spec: selector: matchLabels: run: web version: v2 template: metadata: labels: run: web version: v2 spec: containers: - image: gcr.io/google-samples/hello-app:2.0 imagePullPolicy: IfNotPresent name: web ports: - containerPort: 8080 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: web-v2 namespace: default spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: run: web version: v2 type: NodePort EOF Step 4 Apply the resources to the cluster kubectl apply -f web-service-v2.yaml Step 5 Take a look at the pods created and view their IPs kubectl get pod -o wide Step 6 Take a look at the endpoints created and notice how they match the IP for the Pods in the matching service. kubectl get ep","title":"2.1 Deploy an Application"},{"location":"ycit019_Lab_10_Networking/#22-creating-single-service-ingress-rule","text":"Let's expose our web-v1 service using Ingress! Step 1 To start, let's create an Ingress resource that directs traffic to v1 of the web Service cat <<EOF > single-svc-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: single-svc-ingress spec: defaultBackend: service: name: web-v1 port: number: 8080 EOF This manifest defines a Single Service Ingress rule , which makes sure all HTTP requests that will hit the external IP for the Cloud LB created for the Ingress, will be proxied to the web-v1 service on port 8080 . Step 2 Create the Ingress resource kubectl apply -f single-svc-ingress.yaml Step 3 Verify Ingress Resource kubectl get ing Output NAME CLASS HOSTS ADDRESS PORTS AGE single-svc-ingress <none> * 34.117.137.144 80 110s Notice that the Ingress controllers and load balancers may take a minute or two to allocate an IP address. Bingo! Step 4 descirbe the Ingress object created and notice the Google Cloud LB resources created. kubectl describe ing single-svc-ingress Notice that you might need to give it some time to get a similar output to what I have below. Output: Name: single-svc-ingress Namespace: default Address: 34.117.137.144 Default backend: web-v1:8080 (10.32.0.10:8080) Rules: Host Path Backends ---- ---- -------- * * web-v1:8080 (10.32.0.10:8080) Annotations: ingress.kubernetes.io/backends: {\"k8s-be-30059--a2b52ac0331abe29\":\"HEALTHY\"} ingress.kubernetes.io/forwarding-rule: k8s2-fr-8rwy2qu4-default-single-svc-ingress-74hvjigm ingress.kubernetes.io/target-proxy: k8s2-tp-8rwy2qu4-default-single-svc-ingress-74hvjigm ingress.kubernetes.io/url-map: k8s2-um-8rwy2qu4-default-single-svc-ingress-74hvjigm Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 10m loadbalancer-controller UrlMap \"k8s2-um-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created Normal Sync 10m loadbalancer-controller TargetProxy \"k8s2-tp-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created Normal Sync 10m loadbalancer-controller ForwardingRule \"k8s2-fr-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created Normal IPChanged 10m loadbalancer-controller IP is now 34.117.137.144 Normal Sync 4m27s (x6 over 11m) loadbalancer-controller Scheduled for sync Step 5 Access deployed app via Ingress by navigating to the public IP of the ingress, which we got in step 3. Step 6 Finally execute the following command to cleanup the ingress. kubectl delete -f single-svc-ingress.yaml","title":"2.2 Creating Single Service Ingress rule"},{"location":"ycit019_Lab_10_Networking/#23-simple-fanout","text":"Let's explore more sophisticated Ingress rules. This time we going to deploy Simple fanout type that allows for exposing multiple services on same host, but via different paths. This type is very handy when you running in CloudProvider and want to cut cost on creating LoadBalancers for each of you application. Or when running on prem. and it's required to expose multiple services via same host. Step 1 Let's start by a simple fanout with one service. Notice that we are not using a hostname here as for this lab, we don't want to go through setting up DNS. If you have a DNS setup, you can try with an actual hostname . Just make sure to update you DNS records with the Ingress external IP. cat <<EOF > fanout-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: fanout-ingress spec: rules: - http: paths: - path: /v1 pathType: ImplementationSpecific backend: service: name: web-v1 port: number: 8080 EOF Step 2 Create the Ingress resource kubectl apply -f fanout-ingress.yaml Step 3 Verify Ingress Resource kubectl get ing fanout-ingress Step 4 Verify that you can navigate to http://INGRESS_PUBLIC_IP/v1 and get the following Hello, world! Version: 1.0.0 Step 5 Verify that ingress is returning default backend - 404 page in a browser. Open the browser with public_ip: http://INGRESS_PUBLIC_IP/test Step 6 On your own modify the Ingress resource to add a second path so when we navigate to http://INGRESS_PUBLIC_IP/v2 users will get routed to web-v2 Step 7 Delete the demo Finally execute the following command to cleanup the application and ingress. kubectl delete -f fanout-ingress.yaml kubectl apply -f web-service-v1.yaml kubectl apply -f web-service-v2.yaml","title":"2.3 Simple fanout"},{"location":"ycit019_Lab_10_Networking/#24-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-networking","title":"2.4 Cleaning Up"},{"location":"ycit019_Lab_11_Storage/","text":"Kubernetes Storage Concepts \u00b6 Objective: Create a PersistentVolume (PV) referencing a disk in your environment. Learn how to dynamically provision volumes. Create a Single MySQL Deployment based on the Volume Claim Deploy a Replicated MySQL (Master/Slaves) with a StatefulSet controller. 0 Create Regional GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-storage \\ --region us-central1 \\ --enable-network-policy \\ --num-nodes 2 \\ --machine-type \"e2-standard-2\" \\ --node-locations \"us-central1-b\",\"us-central1-c\" Note We created a Regional cluster with Nodes deployed in \"us-central1-b\" and \"us-central1-c\" zones. Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-storage us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-storage --region us-central1 --project jfrog2021 1 Dynamically Provision Volume \u00b6 Our Lab already has provisioned Default Storageclass created by Cluster Administrator. Step 1 Verify what storage class is used in our lab: kubectl get sc Output: NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE premium-rwo pd.csi.storage.gke.io Delete WaitForFirstConsumer true 5m2s standard (default) kubernetes.io/gce-pd Delete Immediate true 5m2s standard-rwo pd.csi.storage.gke.io Delete WaitForFirstConsumer true 5m2s Info The PROVISIONER field determines what volume plugin is used for provisioning PVs. standard provisions standard GCP PDs (In-tree volume plugin) standard-rwo provisions balanced GCP persistent disk (CSI based) premium-rwo provisions GCP SSD PDs (CSI based) Info The RECLAIMPOLICY field tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained , Recycled , or Deleted Delete reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk Retain reclaim policy allows for manual reclamation of the resource. When the persistent volume is released (this happens when you delete the claim that\u2019s bound to it), Kubernetes retains the volume. The cluster administrator must manually reclaim the volume. This is the default policy for manually created persistent volumes aka Static Provisioners Recycle - This option is deprecated and shouldn\u2019t be used as it may not be supported by the underlying volume plugin. This policy typically causes all files on the volume to be deleted and makes the persistent volume available again without the need to delete and recreate it. If no reclaimPolicy is specified when a StorageClass object is created, it will default to Delete. Info The VOLUMEBINDINGMODE field controls when volume binding and dynamic provisioning should occur. When unset, \"Immediate\" mode is used by default. Immediate The Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created. For storage backends that are topology-constrained and not globally accessible from all Nodes in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod's scheduling requirements. This may result in unschedulable Pods. WaitForFirstConsumer The volume is provisioned and bound to the claim when the first pod that uses this claim is created. This mode is used for topology-constrained volume types. The following plugins support WaitForFirstConsumer with dynamic provisioning: AWSElasticBlockStore GCEPersistentDisk AzureDisk kubectl describe sc Output: Name: premium-rwo IsDefaultClass: No Annotations: components.gke.io/component-name=pdcsi,components.gke.io/component-version=0.9.6,components.gke.io/layer=addon Provisioner: pd.csi.storage.gke.io Parameters: type=pd-ssd AllowVolumeExpansion: True MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: WaitForFirstConsumer Events: <none> Name: standard IsDefaultClass: Yes Annotations: storageclass.kubernetes.io/is-default-class=true Provisioner: kubernetes.io/gce-pd Parameters: type=pd-standard AllowVolumeExpansion: True MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> Name: standard-rwo IsDefaultClass: No Annotations: components.gke.io/layer=addon,storageclass.kubernetes.io/is-default-class=false Provisioner: pd.csi.storage.gke.io Parameters: type=pd-balanced AllowVolumeExpansion: True MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: WaitForFirstConsumer Events: <none> Summary The StorageClass resource specifies which provisioner should be used for provisioning the persistent volume when a persistent volume claim requests this storage class. The parameters defined in the storage class definition are passed to the provisioner and are specific to each provisioner plugin. Step 2 Let's create a new StorageClass for Regional PDs: cat > regionalpd-sc.yaml << EOF kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: regionalpd-storageclass provisioner: pd.csi.storage.gke.io parameters: type: pd-standard replication-type: regional-pd allowedTopologies: - matchLabelExpressions: - key: failure-domain.beta.kubernetes.io/zone values: - us-central1-b - us-central1-c EOF kubectl apply -f regionalpd-sc.yaml kubectl describe sc regionalpd-storageclass Result We've created a new StorageClass that uses GCP PD csi provisioner to create Regional Disks in GCP. Step 3 Create a Persistent Volume Claim (PVC) pvc-demo-ssd.yaml file that will Dynamically creates 30G GCP PD Persistent Volume (PV), using SSD persistent disk Provisioner. cat > pvc-demo-ssd.yaml << EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: hello-web-disk spec: storageClassName: premium-rwo accessModes: - ReadWriteOnce resources: requests: storage: 30G EOF Step 3 Create a PVC: kubectl create -f pvc-demo-ssd.yaml Step 4 Verify STATUS of PVC kubectl get pvc Output: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongodb-pvc Pending standard-rwo 5s List PVs: kubectl get pv List GCP Disks: gcloud compute disks list Result What we see is that: PVC is in Pending PV is not created GCP PD is not created Question : why PVC is in Pending State ? Step 5 Let's review VolumeBindingMode of premium-rwo Storage Class: kubectl describe sc premium-rwo | grep VolumeBindingMode Info This StorageClass using VolumeBindingMode - WaitForFirstConsumer that creates PV only, when the first pod that uses this claim is created. Result Ok so if we want PV created we actually need to create a Pod first. This mode is especially important in the Cloud, as Pods can be created in different zones and so PV needs to be created in the correct zone as well. Step 6 Create a pod-volume-demo.yaml manifest that will create a Pod and mount Persistent Volume from hello-web-disk PVC . cat > pod-volume-demo.yaml << EOF kind: Pod apiVersion: v1 metadata: name: pvc-demo-pod spec: containers: - name: frontend image: nginx volumeMounts: - mountPath: \"/var/www/html\" name: pvc-demo-volume volumes: - name: pvc-demo-volume persistentVolumeClaim: claimName: hello-web-disk EOF Step 3 Create a Pod kubectl create -f pod-volume-demo.yaml Step 4 Verify STATUS of PVC now kubectl get pvc Output: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE hello-web-disk Bound pvc-4c006173-284e-4786-b752-028bdae768e9 28Gi RWO premium-rwo 15m Result PVC STATUS shows as Claim hello-web-disk as Bound , and that Claim has been attached to VOLUME pvc-4c006173-284e-4786-b752-028bdae768e9 with CAPACITY 28Gi and ACCESS MODES RWO via STORAGECLASS premium-rwo using SSD. List PVs: kubectl get pv Result PV STATUS shows as Bound to the CLAIM default/hello-web-disk, with RECLAIM POLICY Delete, meaning that SSD Disk will be deleted after PVC is deleted from Kubernetes. Output: NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-4c006173-284e-4786-b752-028bdae768e9 28Gi RWO Delete Bound default/hello-web-disk premium-rwo 14m List GCP Disks: gcloud compute disks list Output: pvc-4c006173-284e-4786-b752-028bdae768e9 us-central1-c zone 28 pd-ssd READ Result We can see that CSI Provisioner created SSD disk on GCP infrastructure. Step 5 Verify STATUS of Pod kubectl get pod Output: NAME READY STATUS RESTARTS AGE pvc-demo-pod 1/1 Running 0 21m Step 6 Delete PVC kubectl delete pod pvc-demo-pod kubeclt delete pvc hello-web-disk Step 6 Verify resources has been released: kubectl get pv,pvc,pods 2 Deploy Single MySQL Database with Volume \u00b6 You can run a stateful application by creating a Kubernetes Deployment and connecting it to an existing PersistentVolume using a PersistentVolumeClaim. Step 1 Below Manifest file going to creates 3 Kubernetes resources: PersistentVolumeClaim that looks for a 2G volume. This claim will be satisfied by dynamic provisioner general and appropriate PV going to be created Deployment that runs MySQL and references the PersistentVolumeClaim that is mounted in /var/lib/mysql. Service that depoyed as ClusterIP:None that lets the Service DNS name resolve directly to the Pod\u2019s IP kubectl create -f - <<EOF --- apiVersion: v1 kind: Service metadata: name: mysql spec: ports: - port: 3306 selector: app: mysql clusterIP: None --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pv-claim spec: storageClassName: standard accessModes: - ReadWriteOnce resources: requests: storage: 2Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: mysql spec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6.37 name: mysql env: # Use secret in real use case - name: MYSQL_ROOT_PASSWORD value: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim EOF Note The password is defined inside of the Manifest as environment, which is not insecure. See Kubernetes Secrets for a secure solution. Result Single node MySQL database has been deployed with a Volume Step 3 Display information about the Deployment: kubectl describe deployment mysql Output: Name: mysql Namespace: default CreationTimestamp: Tue, 01 Nov 2016 11:18:45 -0700 Labels: app=mysql Annotations: deployment.kubernetes.io/revision=1 Selector: app=mysql Replicas: 1 desired | 1 updated | 1 total | 0 available | 1 unavailable StrategyType: Recreate MinReadySeconds: 0 Pod Template: Labels: app=mysql Containers: mysql: Image: mysql:5.6 Port: 3306/TCP Environment: MYSQL_ROOT_PASSWORD: password Mounts: /var/lib/mysql from mysql-persistent-storage (rw) Volumes: mysql-persistent-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: mysql-pv-claim ReadOnly: false Conditions: Type Status Reason ---- ------ ------ Available False MinimumReplicasUnavailable Progressing True ReplicaSetUpdated OldReplicaSets: <none> NewReplicaSet: mysql-63082529 (1/1 replicas created) Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 33s 33s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set mysql-63082529 to 1 Step 4 List the pods created by the Deployment: kubectl get pods -l app=mysql Output: NAME READY STATUS RESTARTS AGE mysql-63082529-2z3ki 1/1 Running 0 3m Step 5 Inspect the PersistentVolumeClaim: kubectl describe pvc mysql-pv-claim Output: Name: mysql-pv-claim Namespace: default StorageClass: Status: Bound Volume: mysql-pv Labels: <none> Annotations: pv.kubernetes.io/bind-completed=yes pv.kubernetes.io/bound-by-controller=yes Capacity: 20Gi Access Modes: RWO Events: <none> Step 5 Inspect created PersistentVolume: kubectl get pv kubectl describe pv gcloud compute disks list Step 6 Access the MySQL instance The Service option clusterIP: None lets the Service DNS name resolve directly to the Pod's IP address. This is optimal when you have only one Pod behind a Service and you don't intend to increase the number of Pods. Run a MySQL client to connect to the server: kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h mysql -ppassword This command creates a new Pod in the cluster running a MySQL client and connects it to the server through the Service. If it connects, you know your stateful MySQL database is up and running. Waiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | +--------------------+ 3 rows in set (0.00 sec) mysql> exit Step 7 Update the MySQL instance The image or any other part of the Deployment can be updated as usual with the kubectl apply command. Important Don't scale the app. This setup is for single-instance apps only. The underlying PersistentVolume can only be mounted to one Pod. For clustered stateful apps, see the StatefulSet documentation . Use strategy: type: Recreate in the Deployment configuration YAML file. This instructs Kubernetes to not use rolling updates. Rolling updates will not work, as you cannot have more than one Pod running at a time. The Recreate strategy will stop the first pod before creating a new one with the updated configuration. Step 8 Delete the MySQL instance Delete the deployed objects by name: kubectl delete deployment,svc mysql kubectl delete pvc mysql-pv-claim Since we used a dynamic provisioner, it automatically deletes the PersistentVolume when it sees that you deleted the PersistentVolumeClaim. Step 9 Check that GCP Volume has been deleted: gcloud compute disks list Note If PersistentVolume was manually provisioned, it is requrire to manually delete it, as well as release the underlying resource. 3 Deploying highly available PostgreSQL with GKE \u00b6 3.1 Deploying PostgreSQL \u00b6 Step 1 Create regional persistent disk StorageClass cat <<EOF | kubectl create -f - kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: regionalpd-storageclass provisioner: kubernetes.io/gce-pd parameters: type: pd-standard replication-type: regional-pd allowedTopologies: - matchLabelExpressions: - key: failure-domain.beta.kubernetes.io/zone values: - us-central1-b - us-central1-c EOF Step 2 Create PersistentVolumeClaim based on a regional persistent disk StorageClass cat <<EOF | kubectl create -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: postgresql-pv spec: storageClassName: regionalpd-storageclass accessModes: - ReadWriteOnce resources: requests: storage: 300Gi EOF Step 3 Create a PostgreSQL deployment: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: postgres spec: strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate replicas: 1 selector: matchLabels: app: postgres template: metadata: labels: app: postgres spec: containers: - name: postgres image: postgres:10 resources: limits: cpu: \"1\" memory: \"3Gi\" requests: cpu: \"1\" memory: \"2Gi\" ports: - containerPort: 5432 env: - name: POSTGRES_PASSWORD value: password - name: PGDATA value: /var/lib/postgresql/data/pgdata volumeMounts: - mountPath: /var/lib/postgresql/data name: postgredb volumes: - name: postgredb persistentVolumeClaim: claimName: postgresql-pv EOF Step 4 Create PostgreSQL service: cat <<EOF | kubectl create -f - apiVersion: v1 kind: Service metadata: name: postgres spec: ports: - port: 5432 selector: app: postgres clusterIP: None EOF Step 5 Check Regional PVs has been Provisioned based on PVC request kubectl get pvc,pv Step 6 Check that Postgres is up and Running kubectl get deploy,pods 3.2 Creating a test dataset \u00b6 Step 1 Connect to your PostgreSQL instance: POD=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $1}'` kubectl exec -it $POD -- psql -U postgres Step 2 Create a database and a table, and then insert some test rows: create database gke_test_regional; \\c gke_test_regional; CREATE TABLE test( data VARCHAR (255) NULL ); insert into test values ('Learning GKE is fun'), ('Databases on GKE are easy'); Step 3 Verify that the test rows were inserted, select all rows: select * from test; Step 4 Exit the PostgreSQL shell: \\q 3.3 Simulating database instance failover \u00b6 Step 0 Identify the node that is currently hosting PostgreSQL kubectl get pods -l app=postgres -o wide Note Take a note on which of the nodes Pod is Running Step 1 Prepare that node to be CORDONED in other words Disabled for scheduling: CORDONED_NODE=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $7}'` echo ${CORDONED_NODE} gcloud compute instances list --filter=\"name=${CORDONED_NODE}\" Step 2 Disable scheduling of any new pods on this node: kubectl cordon ${CORDONED_NODE} kubectl get nodes Result The node is cordoned, so scheduling is disabled on the node that the database instance resides on. Step 3 Delete the existing PostgreSQL pod POD=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $1}'` kubectl delete pod ${POD} Step 4 Verify that a new pod is created on the other node. kubectl get pods -l app=postgres -o wide Important It might take a while for the new pod to be ready (usually around 30 seconds). Step 5 Verify the node's zone NODE=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $7}'` echo ${NODE} gcloud compute instances list --filter=\"name=${NODE}\" Result Notice that the pod is deployed in a different zone from where the node was created at the beginning of this procedure. Step 6 Connect to the database instance POD=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $1}'` kubectl exec -it $POD -- psql -U postgres Step 7 Verify that the test dataset exists \\c gke_test_regional; select * from test; \\q Step 8 Re-enable scheduling for the node for which scheduling was disabled: kubectl uncordon $CORDONED_NODE Step 9 Check that the node is ready again: kubectl get nodes Step 10 Cleanup Postgres Deployment and PVC kubectl delete pvc postgresql-pv kubectl delete deploy postgres 4 Deploy StatefulSet \u00b6 Scale up our GKE cluster to 4 nodes: gcloud container clusters resize k8s-storage --node-pool=default-pool --num-nodes=2 --region us-central1 4.1 Deploy Replicated MySQL (Master/Slaves) Cluster using StatefulSet. \u00b6 Our Replicated MySQL deployment going to consists of: 1 ConfigMap 2 Services 1 StatefulSet Step 1 Create the ConfigMap (just copy paste below): cat <<EOF | kubectl create -f - apiVersion: v1 kind: ConfigMap metadata: name: mysql labels: app: mysql data: primary.cnf: | # Apply this config only on the primary. [mysqld] log-bin replica.cnf: | # Apply this config only on replicas. [mysqld] super-read-only EOF Result This ConfigMap provides overrides that let you independently control configuration on the MySQL master and slaves. In this case: master going to be able to serve replication logs to slaves slaves to reject any writes that don't come via replication. There's nothing special about the ConfigMap itself that causes different portions to apply to different Pods. Each Pod decides which portion to look at as it's initializing, based on information provided by the StatefulSet controller. Step 2 Create 2 Services (just copy paste below): cat <<EOF | kubectl create -f - # Headless service for stable DNS entries of StatefulSet members. # Headless service for stable DNS entries of StatefulSet members. apiVersion: v1 kind: Service metadata: name: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- # Client service for connecting to any MySQL instance for reads. # For writes, you must instead connect to the primary: mysql-0.mysql. apiVersion: v1 kind: Service metadata: name: mysql-read labels: app: mysql spec: ports: - name: mysql port: 3306 selector: app: mysql EOF Info The Headless Service provides a home for the DNS entries that the StatefulSet controller creates for each Pod that's part of the set. Because the Headless Service is named mysql, the Pods are accessible by resolving .mysql from within any other Pod in the same Kubernetes cluster and namespace. The Client Service, called mysql-read, is a normal Service with its own cluster IP that distributes connections across all MySQL Pods that report being Ready. The set of potential endpoints includes the MySQL master and all slaves. Note Only read queries can use the load-balanced Client Service. Because there is only one MySQL master, clients should connect directly to the MySQL master Pod (through its DNS entry within the Headless Service) to execute writes. Step 3 Create StatefulSet mysql-statefulset.yaml manifest: apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql spec: selector: matchLabels: app: mysql serviceName: mysql replicas: 3 template: metadata: labels: app: mysql spec: initContainers: - name: init-mysql image: mysql:5.7 command: - bash - \"-c\" - | set -ex # Generate mysql server-id from pod ordinal index. [[ `hostname` =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} echo [mysqld] > /mnt/conf.d/server-id.cnf # Add an offset to avoid reserved server-id=0 value. echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf # Copy appropriate conf.d files from config-map to emptyDir. if [[ $ordinal -eq 0 ]]; then cp /mnt/config-map/primary.cnf /mnt/conf.d/ else cp /mnt/config-map/replica.cnf /mnt/conf.d/ fi volumeMounts: - name: conf mountPath: /mnt/conf.d - name: config-map mountPath: /mnt/config-map - name: clone-mysql image: gcr.io/google-samples/xtrabackup:1.0 command: - bash - \"-c\" - | set -ex # Skip the clone if data already exists. [[ -d /var/lib/mysql/mysql ]] && exit 0 # Skip the clone on primary (ordinal index 0). [[ `hostname` =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} [[ $ordinal -eq 0 ]] && exit 0 # Clone data from previous peer. ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql # Prepare the backup. xtrabackup --prepare --target-dir=/var/lib/mysql volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ALLOW_EMPTY_PASSWORD value: \"1\" ports: - name: mysql containerPort: 3306 volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 500m memory: 1Gi livenessProbe: exec: command: [\"mysqladmin\", \"ping\"] initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 readinessProbe: exec: # Check we can execute queries over TCP (skip-networking is off). command: [\"mysql\", \"-h\", \"127.0.0.1\", \"-e\", \"SELECT 1\"] initialDelaySeconds: 5 periodSeconds: 2 timeoutSeconds: 1 - name: xtrabackup image: gcr.io/google-samples/xtrabackup:1.0 ports: - name: xtrabackup containerPort: 3307 command: - bash - \"-c\" - | set -ex cd /var/lib/mysql # Determine binlog position of cloned data, if any. if [[ -f xtrabackup_slave_info && \"x$(<xtrabackup_slave_info)\" != \"x\" ]]; then # XtraBackup already generated a partial \"CHANGE MASTER TO\" query # because we're cloning from an existing replica. (Need to remove the tailing semicolon!) cat xtrabackup_slave_info | sed -E 's/;$//g' > change_master_to.sql.in # Ignore xtrabackup_binlog_info in this case (it's useless). rm -f xtrabackup_slave_info xtrabackup_binlog_info elif [[ -f xtrabackup_binlog_info ]]; then # We're cloning directly from primary. Parse binlog position. [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1 rm -f xtrabackup_binlog_info xtrabackup_slave_info echo \"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\ MASTER_LOG_POS=${BASH_REMATCH[2]}\" > change_master_to.sql.in fi # Check if we need to complete a clone by starting replication. if [[ -f change_master_to.sql.in ]]; then echo \"Waiting for mysqld to be ready (accepting connections)\" until mysql -h 127.0.0.1 -e \"SELECT 1\"; do sleep 1; done echo \"Initializing replication from clone position\" mysql -h 127.0.0.1 \\ -e \"$(<change_master_to.sql.in), \\ MASTER_HOST='mysql-0.mysql', \\ MASTER_USER='root', \\ MASTER_PASSWORD='', \\ MASTER_CONNECT_RETRY=10; \\ START SLAVE;\" || exit 1 # In case of container restart, attempt this at-most-once. mv change_master_to.sql.in change_master_to.sql.orig fi # Start a server to send backups when requested by peers. exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \\ \"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root\" volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 100m memory: 100Mi volumes: - name: conf emptyDir: {} - name: config-map configMap: name: mysql volumeClaimTemplates: - metadata: name: data spec: accessModes: [\"ReadWriteOnce\"] storageClassName: \"standard-rwo\" resources: requests: storage: 10Gi Deploy StatefulSet : kubectl apply -f https://k8s.io/examples/application/mysql/mysql-statefulset.yaml Step 4 Monitor Deployment Process/Sequence watch kubectl get statefulset,pvc,pv,pods -l app=mysql Press Ctrl+C to cancel the watch when all pods, pvc and statefulset provisioned. kubectl get pods Output: NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 2m6s mysql-1 2/2 Running 0 77s Result The StatefulSet controller started Pods one at a time, in order by their ordinal index . It waits until each Pod reports being Ready before starting the next one. In addition, the controller assigned each Pod a unique , stable name of the form <statefulset-name>-<ordinal-index> . In this case, that results in Pods named mysql-0 , mysql-1 , and mysql-2 . The Pod template in the above StatefulSet manifest takes advantage of these properties to perform orderly startup of MySQL replication. Generating configuration Before starting any of the containers in the Pod spec, the Pod first runs any [Init Containers] in the order defined. The first Init Container, named init-mysql , generates special MySQL config files based on the ordinal index. The script determines its own ordinal index by extracting it from the end of the Pod name, which is returned by the hostname command. Then it saves the ordinal (with a numeric offset to avoid reserved values) into a file called server-id.cnf in the MySQL conf.d directory. This translates the unique, stable identity provided by the StatefulSet controller into the domain of MySQL server IDs, which require the same properties. The script in the init-mysql container also applies either master.cnf or slave.cnf from the ConfigMap by copying the contents into conf.d . Because the example topology consists of a single MySQL master and any number of slaves, the script simply assigns ordinal 0 to be the master, and everyone else to be slaves. Combined with the StatefulSet controller's deployment order guarantee ensures the MySQL master is Ready before creating slaves, so they can begin replicating. Cloning existing data In general, when a new Pod joins the set as a slave, it must assume the MySQL master might already have data on it. It also must assume that the replication logs might not go all the way back to the beginning of time. These conservative assumptions are the key to allow a running StatefulSet to scale up and down over time, rather than being fixed at its initial size. The second Init Container, named clone-mysql , performs a clone operation on a slave Pod the first time it starts up on an empty PersistentVolume. That means it copies all existing data from another running Pod, so its local state is consistent enough to begin replicating from the master. MySQL itself does not provide a mechanism to do this, so the example uses a popular open-source tool called Percona XtraBackup. During the clone, the source MySQL server might suffer reduced performance. To minimize impact on the MySQL master, the script instructs each Pod to clone from the Pod whose ordinal index is one lower. This works because the StatefulSet controller always ensures Pod N is Ready before starting Pod N+1 . Starting replication After the Init Containers complete successfully, the regular containers run. The MySQL Pods consist of a mysql container that runs the actual mysqld server, and an xtrabackup container that acts as a sidecar . The xtrabackup sidecar looks at the cloned data files and determines if it's necessary to initialize MySQL replication on the slave. If so, it waits for mysqld to be ready and then executes the CHANGE MASTER TO and START SLAVE commands with replication parameters extracted from the XtraBackup clone files. Once a slave begins replication, it remembers its MySQL master and reconnects automatically if the server restarts or the connection dies. Also, because slaves look for the master at its stable DNS name ( mysql-0.mysql ), they automatically find the master even if it gets a new Pod IP due to being rescheduled. Lastly, after starting replication, the xtrabackup container listens for connections from other Pods requesting a data clone. This server remains up indefinitely in case the StatefulSet scales up, or in case the next Pod loses its PersistentVolumeClaim and needs to redo the clone. 4.2 Test the MySQL cluster app and running \u00b6 Step 1 Create Database, Table and message on Master MySQL database Send test queries to the MySQL master (hostname mysql-0.mysql ) by running a temporary container with the mysql:5.7 image and running the mysql client binary. kubectl run mysql-client --image = mysql:5.7 -i --rm --restart = Never -- \\ mysql -h mysql-0.mysql <<EOF CREATE DATABASE test; CREATE TABLE test.messages (message VARCHAR(250)); INSERT INTO test.messages VALUES ('hello'); EOF Step 2 Verify that recorded data has been replicated to the slaves: Use the hostname mysql-read to send test queries to any server that reports being Ready: kubectl run mysql-client --image = mysql:5.7 -i -t --rm --restart = Never -- \\ mysql -h mysql-read -e \"SELECT * FROM test.messages\" You should get output like this: Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false +---------+ | message | +---------+ | hello | +---------+ pod \"mysql-client\" deleted Step 3 Demonstrate that the mysql-read Service distributes connections across servers, you can run SELECT @@hostname in a loop: kubectl run mysql-client-loop --image = mysql:5.7 -i -t --rm --restart = Never -- \\ bash -ic \"while sleep 1; do mysql -h mysql-read -e 'SELECT @@hostname,NOW()'; done\" You should see the reported @@hostname change randomly, because a different endpoint might be selected upon each connection attempt: +-------------+---------------------+ | @@hostname | NOW() | +-------------+---------------------+ | 100 | 2006-01-02 15:04:05 | +-------------+---------------------+ +-------------+---------------------+ | @@hostname | NOW() | +-------------+---------------------+ | 102 | 2006-01-02 15:04:06 | +-------------+---------------------+ +-------------+---------------------+ | @@hostname | NOW() | +-------------+---------------------+ | 101 | 2006-01-02 15:04:07 | +-------------+---------------------+ You can press Ctrl+C when you want to stop the loop, but it's useful to keep it running in another window so you can see the effects of the following steps. 4.3 Delete Pods \u00b6 The StatefulSet recreates Pods if they're deleted, similar to what a ReplicaSet does for stateless Pods. Step 1 Try to fail Mysql cluster by deleting mysql-1 pod: kubectl delete pod mysql-1 The StatefulSet controller notices that no mysql-1 Pod exists anymore, and creates a new one with the same name and linked to the same PersistentVolumeClaim. Step 5 Monitor Deployment Process/Sequence watch kubectl get statefulset,pvc,pv,pods -l app=mysql Result You should see server ID 102 disappear from the loop output for a while and then return on its own. 4.4 Scaling the number of slaves \u00b6 With MySQL replication, you can scale your read query capacity by adding slaves. With StatefulSet, you can do this with a single command: Step 1 Scale up statefulset: kubectl scale statefulset mysql --replicas = 5 Step 2 Watch the new Pods come up by running: kubectl get pods -l app = mysql --watch Step 3 Watch the new Pods come up by running: kubectl run mysql-client --image = mysql:5.7 -i -t --rm --restart = Never -- \\ mysql -h mysql-3.mysql -e \"SELECT * FROM test.messages\" Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false +---------+ | message | +---------+ | hello | +---------+ pod \"mysql-client\" deleted Step 4 Scaling back down is also seamless: kubectl scale statefulset mysql --replicas = 3 Note, however, that while scaling up creates new PersistentVolumeClaims automatically, scaling down does not automatically delete these PVCs. This gives you the choice to keep those initialized PVCs around to make scaling back up quicker, or to extract data before deleting them. You can see this by running: kubectl get pvc -l app = mysql Which shows that all 3 PVCs still exist, despite having scaled the StatefulSet down to 1: NAME STATUS VOLUME CAPACITY ACCESSMODES AGE data-mysql-0 Bound pvc-8acbf5dc-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-1 Bound pvc-8ad39820-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-2 Bound pvc-8ad69a6d-b103-11e6-93fa-42010a800002 10Gi RWO 20m If you don't intend to reuse the extra PVCs, you can delete them: kubectl delete pvc data-mysql-0 kubectl delete pvc data-mysql-1 kubectl delete pvc data-mysql-2 kubectl delete pvc data-mysql-3 kubectl delete pvc data-mysql-4 Step 5 Cancel the SELECT @@server_id loop by pressing Ctrl+C in its terminal, or running the following from another terminal: kubectl delete pod mysql-client-loop --now Step 6 Delete the StatefulSet. This also begins terminating the Pods. kubectl delete statefulset mysql 5 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-storage","title":"Lab 11 Kubernetes Storage"},{"location":"ycit019_Lab_11_Storage/#kubernetes-storage-concepts","text":"Objective: Create a PersistentVolume (PV) referencing a disk in your environment. Learn how to dynamically provision volumes. Create a Single MySQL Deployment based on the Volume Claim Deploy a Replicated MySQL (Master/Slaves) with a StatefulSet controller.","title":"Kubernetes Storage Concepts"},{"location":"ycit019_Lab_11_Storage/#0-create-regional-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-storage \\ --region us-central1 \\ --enable-network-policy \\ --num-nodes 2 \\ --machine-type \"e2-standard-2\" \\ --node-locations \"us-central1-b\",\"us-central1-c\" Note We created a Regional cluster with Nodes deployed in \"us-central1-b\" and \"us-central1-c\" zones. Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-storage us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-storage --region us-central1 --project jfrog2021","title":"0 Create Regional GKE Cluster"},{"location":"ycit019_Lab_11_Storage/#1-dynamically-provision-volume","text":"Our Lab already has provisioned Default Storageclass created by Cluster Administrator. Step 1 Verify what storage class is used in our lab: kubectl get sc Output: NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE premium-rwo pd.csi.storage.gke.io Delete WaitForFirstConsumer true 5m2s standard (default) kubernetes.io/gce-pd Delete Immediate true 5m2s standard-rwo pd.csi.storage.gke.io Delete WaitForFirstConsumer true 5m2s Info The PROVISIONER field determines what volume plugin is used for provisioning PVs. standard provisions standard GCP PDs (In-tree volume plugin) standard-rwo provisions balanced GCP persistent disk (CSI based) premium-rwo provisions GCP SSD PDs (CSI based) Info The RECLAIMPOLICY field tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained , Recycled , or Deleted Delete reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk Retain reclaim policy allows for manual reclamation of the resource. When the persistent volume is released (this happens when you delete the claim that\u2019s bound to it), Kubernetes retains the volume. The cluster administrator must manually reclaim the volume. This is the default policy for manually created persistent volumes aka Static Provisioners Recycle - This option is deprecated and shouldn\u2019t be used as it may not be supported by the underlying volume plugin. This policy typically causes all files on the volume to be deleted and makes the persistent volume available again without the need to delete and recreate it. If no reclaimPolicy is specified when a StorageClass object is created, it will default to Delete. Info The VOLUMEBINDINGMODE field controls when volume binding and dynamic provisioning should occur. When unset, \"Immediate\" mode is used by default. Immediate The Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created. For storage backends that are topology-constrained and not globally accessible from all Nodes in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod's scheduling requirements. This may result in unschedulable Pods. WaitForFirstConsumer The volume is provisioned and bound to the claim when the first pod that uses this claim is created. This mode is used for topology-constrained volume types. The following plugins support WaitForFirstConsumer with dynamic provisioning: AWSElasticBlockStore GCEPersistentDisk AzureDisk kubectl describe sc Output: Name: premium-rwo IsDefaultClass: No Annotations: components.gke.io/component-name=pdcsi,components.gke.io/component-version=0.9.6,components.gke.io/layer=addon Provisioner: pd.csi.storage.gke.io Parameters: type=pd-ssd AllowVolumeExpansion: True MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: WaitForFirstConsumer Events: <none> Name: standard IsDefaultClass: Yes Annotations: storageclass.kubernetes.io/is-default-class=true Provisioner: kubernetes.io/gce-pd Parameters: type=pd-standard AllowVolumeExpansion: True MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> Name: standard-rwo IsDefaultClass: No Annotations: components.gke.io/layer=addon,storageclass.kubernetes.io/is-default-class=false Provisioner: pd.csi.storage.gke.io Parameters: type=pd-balanced AllowVolumeExpansion: True MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: WaitForFirstConsumer Events: <none> Summary The StorageClass resource specifies which provisioner should be used for provisioning the persistent volume when a persistent volume claim requests this storage class. The parameters defined in the storage class definition are passed to the provisioner and are specific to each provisioner plugin. Step 2 Let's create a new StorageClass for Regional PDs: cat > regionalpd-sc.yaml << EOF kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: regionalpd-storageclass provisioner: pd.csi.storage.gke.io parameters: type: pd-standard replication-type: regional-pd allowedTopologies: - matchLabelExpressions: - key: failure-domain.beta.kubernetes.io/zone values: - us-central1-b - us-central1-c EOF kubectl apply -f regionalpd-sc.yaml kubectl describe sc regionalpd-storageclass Result We've created a new StorageClass that uses GCP PD csi provisioner to create Regional Disks in GCP. Step 3 Create a Persistent Volume Claim (PVC) pvc-demo-ssd.yaml file that will Dynamically creates 30G GCP PD Persistent Volume (PV), using SSD persistent disk Provisioner. cat > pvc-demo-ssd.yaml << EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: hello-web-disk spec: storageClassName: premium-rwo accessModes: - ReadWriteOnce resources: requests: storage: 30G EOF Step 3 Create a PVC: kubectl create -f pvc-demo-ssd.yaml Step 4 Verify STATUS of PVC kubectl get pvc Output: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongodb-pvc Pending standard-rwo 5s List PVs: kubectl get pv List GCP Disks: gcloud compute disks list Result What we see is that: PVC is in Pending PV is not created GCP PD is not created Question : why PVC is in Pending State ? Step 5 Let's review VolumeBindingMode of premium-rwo Storage Class: kubectl describe sc premium-rwo | grep VolumeBindingMode Info This StorageClass using VolumeBindingMode - WaitForFirstConsumer that creates PV only, when the first pod that uses this claim is created. Result Ok so if we want PV created we actually need to create a Pod first. This mode is especially important in the Cloud, as Pods can be created in different zones and so PV needs to be created in the correct zone as well. Step 6 Create a pod-volume-demo.yaml manifest that will create a Pod and mount Persistent Volume from hello-web-disk PVC . cat > pod-volume-demo.yaml << EOF kind: Pod apiVersion: v1 metadata: name: pvc-demo-pod spec: containers: - name: frontend image: nginx volumeMounts: - mountPath: \"/var/www/html\" name: pvc-demo-volume volumes: - name: pvc-demo-volume persistentVolumeClaim: claimName: hello-web-disk EOF Step 3 Create a Pod kubectl create -f pod-volume-demo.yaml Step 4 Verify STATUS of PVC now kubectl get pvc Output: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE hello-web-disk Bound pvc-4c006173-284e-4786-b752-028bdae768e9 28Gi RWO premium-rwo 15m Result PVC STATUS shows as Claim hello-web-disk as Bound , and that Claim has been attached to VOLUME pvc-4c006173-284e-4786-b752-028bdae768e9 with CAPACITY 28Gi and ACCESS MODES RWO via STORAGECLASS premium-rwo using SSD. List PVs: kubectl get pv Result PV STATUS shows as Bound to the CLAIM default/hello-web-disk, with RECLAIM POLICY Delete, meaning that SSD Disk will be deleted after PVC is deleted from Kubernetes. Output: NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-4c006173-284e-4786-b752-028bdae768e9 28Gi RWO Delete Bound default/hello-web-disk premium-rwo 14m List GCP Disks: gcloud compute disks list Output: pvc-4c006173-284e-4786-b752-028bdae768e9 us-central1-c zone 28 pd-ssd READ Result We can see that CSI Provisioner created SSD disk on GCP infrastructure. Step 5 Verify STATUS of Pod kubectl get pod Output: NAME READY STATUS RESTARTS AGE pvc-demo-pod 1/1 Running 0 21m Step 6 Delete PVC kubectl delete pod pvc-demo-pod kubeclt delete pvc hello-web-disk Step 6 Verify resources has been released: kubectl get pv,pvc,pods","title":"1 Dynamically Provision Volume"},{"location":"ycit019_Lab_11_Storage/#2-deploy-single-mysql-database-with-volume","text":"You can run a stateful application by creating a Kubernetes Deployment and connecting it to an existing PersistentVolume using a PersistentVolumeClaim. Step 1 Below Manifest file going to creates 3 Kubernetes resources: PersistentVolumeClaim that looks for a 2G volume. This claim will be satisfied by dynamic provisioner general and appropriate PV going to be created Deployment that runs MySQL and references the PersistentVolumeClaim that is mounted in /var/lib/mysql. Service that depoyed as ClusterIP:None that lets the Service DNS name resolve directly to the Pod\u2019s IP kubectl create -f - <<EOF --- apiVersion: v1 kind: Service metadata: name: mysql spec: ports: - port: 3306 selector: app: mysql clusterIP: None --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pv-claim spec: storageClassName: standard accessModes: - ReadWriteOnce resources: requests: storage: 2Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: mysql spec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6.37 name: mysql env: # Use secret in real use case - name: MYSQL_ROOT_PASSWORD value: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim EOF Note The password is defined inside of the Manifest as environment, which is not insecure. See Kubernetes Secrets for a secure solution. Result Single node MySQL database has been deployed with a Volume Step 3 Display information about the Deployment: kubectl describe deployment mysql Output: Name: mysql Namespace: default CreationTimestamp: Tue, 01 Nov 2016 11:18:45 -0700 Labels: app=mysql Annotations: deployment.kubernetes.io/revision=1 Selector: app=mysql Replicas: 1 desired | 1 updated | 1 total | 0 available | 1 unavailable StrategyType: Recreate MinReadySeconds: 0 Pod Template: Labels: app=mysql Containers: mysql: Image: mysql:5.6 Port: 3306/TCP Environment: MYSQL_ROOT_PASSWORD: password Mounts: /var/lib/mysql from mysql-persistent-storage (rw) Volumes: mysql-persistent-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: mysql-pv-claim ReadOnly: false Conditions: Type Status Reason ---- ------ ------ Available False MinimumReplicasUnavailable Progressing True ReplicaSetUpdated OldReplicaSets: <none> NewReplicaSet: mysql-63082529 (1/1 replicas created) Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 33s 33s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set mysql-63082529 to 1 Step 4 List the pods created by the Deployment: kubectl get pods -l app=mysql Output: NAME READY STATUS RESTARTS AGE mysql-63082529-2z3ki 1/1 Running 0 3m Step 5 Inspect the PersistentVolumeClaim: kubectl describe pvc mysql-pv-claim Output: Name: mysql-pv-claim Namespace: default StorageClass: Status: Bound Volume: mysql-pv Labels: <none> Annotations: pv.kubernetes.io/bind-completed=yes pv.kubernetes.io/bound-by-controller=yes Capacity: 20Gi Access Modes: RWO Events: <none> Step 5 Inspect created PersistentVolume: kubectl get pv kubectl describe pv gcloud compute disks list Step 6 Access the MySQL instance The Service option clusterIP: None lets the Service DNS name resolve directly to the Pod's IP address. This is optimal when you have only one Pod behind a Service and you don't intend to increase the number of Pods. Run a MySQL client to connect to the server: kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h mysql -ppassword This command creates a new Pod in the cluster running a MySQL client and connects it to the server through the Service. If it connects, you know your stateful MySQL database is up and running. Waiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | +--------------------+ 3 rows in set (0.00 sec) mysql> exit Step 7 Update the MySQL instance The image or any other part of the Deployment can be updated as usual with the kubectl apply command. Important Don't scale the app. This setup is for single-instance apps only. The underlying PersistentVolume can only be mounted to one Pod. For clustered stateful apps, see the StatefulSet documentation . Use strategy: type: Recreate in the Deployment configuration YAML file. This instructs Kubernetes to not use rolling updates. Rolling updates will not work, as you cannot have more than one Pod running at a time. The Recreate strategy will stop the first pod before creating a new one with the updated configuration. Step 8 Delete the MySQL instance Delete the deployed objects by name: kubectl delete deployment,svc mysql kubectl delete pvc mysql-pv-claim Since we used a dynamic provisioner, it automatically deletes the PersistentVolume when it sees that you deleted the PersistentVolumeClaim. Step 9 Check that GCP Volume has been deleted: gcloud compute disks list Note If PersistentVolume was manually provisioned, it is requrire to manually delete it, as well as release the underlying resource.","title":"2 Deploy Single MySQL Database with Volume"},{"location":"ycit019_Lab_11_Storage/#3-deploying-highly-available-postgresql-with-gke","text":"","title":"3 Deploying highly available PostgreSQL with GKE"},{"location":"ycit019_Lab_11_Storage/#31-deploying-postgresql","text":"Step 1 Create regional persistent disk StorageClass cat <<EOF | kubectl create -f - kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: regionalpd-storageclass provisioner: kubernetes.io/gce-pd parameters: type: pd-standard replication-type: regional-pd allowedTopologies: - matchLabelExpressions: - key: failure-domain.beta.kubernetes.io/zone values: - us-central1-b - us-central1-c EOF Step 2 Create PersistentVolumeClaim based on a regional persistent disk StorageClass cat <<EOF | kubectl create -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: postgresql-pv spec: storageClassName: regionalpd-storageclass accessModes: - ReadWriteOnce resources: requests: storage: 300Gi EOF Step 3 Create a PostgreSQL deployment: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: postgres spec: strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate replicas: 1 selector: matchLabels: app: postgres template: metadata: labels: app: postgres spec: containers: - name: postgres image: postgres:10 resources: limits: cpu: \"1\" memory: \"3Gi\" requests: cpu: \"1\" memory: \"2Gi\" ports: - containerPort: 5432 env: - name: POSTGRES_PASSWORD value: password - name: PGDATA value: /var/lib/postgresql/data/pgdata volumeMounts: - mountPath: /var/lib/postgresql/data name: postgredb volumes: - name: postgredb persistentVolumeClaim: claimName: postgresql-pv EOF Step 4 Create PostgreSQL service: cat <<EOF | kubectl create -f - apiVersion: v1 kind: Service metadata: name: postgres spec: ports: - port: 5432 selector: app: postgres clusterIP: None EOF Step 5 Check Regional PVs has been Provisioned based on PVC request kubectl get pvc,pv Step 6 Check that Postgres is up and Running kubectl get deploy,pods","title":"3.1 Deploying PostgreSQL"},{"location":"ycit019_Lab_11_Storage/#32-creating-a-test-dataset","text":"Step 1 Connect to your PostgreSQL instance: POD=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $1}'` kubectl exec -it $POD -- psql -U postgres Step 2 Create a database and a table, and then insert some test rows: create database gke_test_regional; \\c gke_test_regional; CREATE TABLE test( data VARCHAR (255) NULL ); insert into test values ('Learning GKE is fun'), ('Databases on GKE are easy'); Step 3 Verify that the test rows were inserted, select all rows: select * from test; Step 4 Exit the PostgreSQL shell: \\q","title":"3.2 Creating a test dataset"},{"location":"ycit019_Lab_11_Storage/#33-simulating-database-instance-failover","text":"Step 0 Identify the node that is currently hosting PostgreSQL kubectl get pods -l app=postgres -o wide Note Take a note on which of the nodes Pod is Running Step 1 Prepare that node to be CORDONED in other words Disabled for scheduling: CORDONED_NODE=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $7}'` echo ${CORDONED_NODE} gcloud compute instances list --filter=\"name=${CORDONED_NODE}\" Step 2 Disable scheduling of any new pods on this node: kubectl cordon ${CORDONED_NODE} kubectl get nodes Result The node is cordoned, so scheduling is disabled on the node that the database instance resides on. Step 3 Delete the existing PostgreSQL pod POD=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $1}'` kubectl delete pod ${POD} Step 4 Verify that a new pod is created on the other node. kubectl get pods -l app=postgres -o wide Important It might take a while for the new pod to be ready (usually around 30 seconds). Step 5 Verify the node's zone NODE=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $7}'` echo ${NODE} gcloud compute instances list --filter=\"name=${NODE}\" Result Notice that the pod is deployed in a different zone from where the node was created at the beginning of this procedure. Step 6 Connect to the database instance POD=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $1}'` kubectl exec -it $POD -- psql -U postgres Step 7 Verify that the test dataset exists \\c gke_test_regional; select * from test; \\q Step 8 Re-enable scheduling for the node for which scheduling was disabled: kubectl uncordon $CORDONED_NODE Step 9 Check that the node is ready again: kubectl get nodes Step 10 Cleanup Postgres Deployment and PVC kubectl delete pvc postgresql-pv kubectl delete deploy postgres","title":"3.3 Simulating database instance failover"},{"location":"ycit019_Lab_11_Storage/#4-deploy-statefulset","text":"Scale up our GKE cluster to 4 nodes: gcloud container clusters resize k8s-storage --node-pool=default-pool --num-nodes=2 --region us-central1","title":"4 Deploy StatefulSet"},{"location":"ycit019_Lab_11_Storage/#41-deploy-replicated-mysql-masterslaves-cluster-using-statefulset","text":"Our Replicated MySQL deployment going to consists of: 1 ConfigMap 2 Services 1 StatefulSet Step 1 Create the ConfigMap (just copy paste below): cat <<EOF | kubectl create -f - apiVersion: v1 kind: ConfigMap metadata: name: mysql labels: app: mysql data: primary.cnf: | # Apply this config only on the primary. [mysqld] log-bin replica.cnf: | # Apply this config only on replicas. [mysqld] super-read-only EOF Result This ConfigMap provides overrides that let you independently control configuration on the MySQL master and slaves. In this case: master going to be able to serve replication logs to slaves slaves to reject any writes that don't come via replication. There's nothing special about the ConfigMap itself that causes different portions to apply to different Pods. Each Pod decides which portion to look at as it's initializing, based on information provided by the StatefulSet controller. Step 2 Create 2 Services (just copy paste below): cat <<EOF | kubectl create -f - # Headless service for stable DNS entries of StatefulSet members. # Headless service for stable DNS entries of StatefulSet members. apiVersion: v1 kind: Service metadata: name: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- # Client service for connecting to any MySQL instance for reads. # For writes, you must instead connect to the primary: mysql-0.mysql. apiVersion: v1 kind: Service metadata: name: mysql-read labels: app: mysql spec: ports: - name: mysql port: 3306 selector: app: mysql EOF Info The Headless Service provides a home for the DNS entries that the StatefulSet controller creates for each Pod that's part of the set. Because the Headless Service is named mysql, the Pods are accessible by resolving .mysql from within any other Pod in the same Kubernetes cluster and namespace. The Client Service, called mysql-read, is a normal Service with its own cluster IP that distributes connections across all MySQL Pods that report being Ready. The set of potential endpoints includes the MySQL master and all slaves. Note Only read queries can use the load-balanced Client Service. Because there is only one MySQL master, clients should connect directly to the MySQL master Pod (through its DNS entry within the Headless Service) to execute writes. Step 3 Create StatefulSet mysql-statefulset.yaml manifest: apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql spec: selector: matchLabels: app: mysql serviceName: mysql replicas: 3 template: metadata: labels: app: mysql spec: initContainers: - name: init-mysql image: mysql:5.7 command: - bash - \"-c\" - | set -ex # Generate mysql server-id from pod ordinal index. [[ `hostname` =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} echo [mysqld] > /mnt/conf.d/server-id.cnf # Add an offset to avoid reserved server-id=0 value. echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf # Copy appropriate conf.d files from config-map to emptyDir. if [[ $ordinal -eq 0 ]]; then cp /mnt/config-map/primary.cnf /mnt/conf.d/ else cp /mnt/config-map/replica.cnf /mnt/conf.d/ fi volumeMounts: - name: conf mountPath: /mnt/conf.d - name: config-map mountPath: /mnt/config-map - name: clone-mysql image: gcr.io/google-samples/xtrabackup:1.0 command: - bash - \"-c\" - | set -ex # Skip the clone if data already exists. [[ -d /var/lib/mysql/mysql ]] && exit 0 # Skip the clone on primary (ordinal index 0). [[ `hostname` =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} [[ $ordinal -eq 0 ]] && exit 0 # Clone data from previous peer. ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql # Prepare the backup. xtrabackup --prepare --target-dir=/var/lib/mysql volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ALLOW_EMPTY_PASSWORD value: \"1\" ports: - name: mysql containerPort: 3306 volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 500m memory: 1Gi livenessProbe: exec: command: [\"mysqladmin\", \"ping\"] initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 readinessProbe: exec: # Check we can execute queries over TCP (skip-networking is off). command: [\"mysql\", \"-h\", \"127.0.0.1\", \"-e\", \"SELECT 1\"] initialDelaySeconds: 5 periodSeconds: 2 timeoutSeconds: 1 - name: xtrabackup image: gcr.io/google-samples/xtrabackup:1.0 ports: - name: xtrabackup containerPort: 3307 command: - bash - \"-c\" - | set -ex cd /var/lib/mysql # Determine binlog position of cloned data, if any. if [[ -f xtrabackup_slave_info && \"x$(<xtrabackup_slave_info)\" != \"x\" ]]; then # XtraBackup already generated a partial \"CHANGE MASTER TO\" query # because we're cloning from an existing replica. (Need to remove the tailing semicolon!) cat xtrabackup_slave_info | sed -E 's/;$//g' > change_master_to.sql.in # Ignore xtrabackup_binlog_info in this case (it's useless). rm -f xtrabackup_slave_info xtrabackup_binlog_info elif [[ -f xtrabackup_binlog_info ]]; then # We're cloning directly from primary. Parse binlog position. [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1 rm -f xtrabackup_binlog_info xtrabackup_slave_info echo \"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\ MASTER_LOG_POS=${BASH_REMATCH[2]}\" > change_master_to.sql.in fi # Check if we need to complete a clone by starting replication. if [[ -f change_master_to.sql.in ]]; then echo \"Waiting for mysqld to be ready (accepting connections)\" until mysql -h 127.0.0.1 -e \"SELECT 1\"; do sleep 1; done echo \"Initializing replication from clone position\" mysql -h 127.0.0.1 \\ -e \"$(<change_master_to.sql.in), \\ MASTER_HOST='mysql-0.mysql', \\ MASTER_USER='root', \\ MASTER_PASSWORD='', \\ MASTER_CONNECT_RETRY=10; \\ START SLAVE;\" || exit 1 # In case of container restart, attempt this at-most-once. mv change_master_to.sql.in change_master_to.sql.orig fi # Start a server to send backups when requested by peers. exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \\ \"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root\" volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 100m memory: 100Mi volumes: - name: conf emptyDir: {} - name: config-map configMap: name: mysql volumeClaimTemplates: - metadata: name: data spec: accessModes: [\"ReadWriteOnce\"] storageClassName: \"standard-rwo\" resources: requests: storage: 10Gi Deploy StatefulSet : kubectl apply -f https://k8s.io/examples/application/mysql/mysql-statefulset.yaml Step 4 Monitor Deployment Process/Sequence watch kubectl get statefulset,pvc,pv,pods -l app=mysql Press Ctrl+C to cancel the watch when all pods, pvc and statefulset provisioned. kubectl get pods Output: NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 2m6s mysql-1 2/2 Running 0 77s Result The StatefulSet controller started Pods one at a time, in order by their ordinal index . It waits until each Pod reports being Ready before starting the next one. In addition, the controller assigned each Pod a unique , stable name of the form <statefulset-name>-<ordinal-index> . In this case, that results in Pods named mysql-0 , mysql-1 , and mysql-2 . The Pod template in the above StatefulSet manifest takes advantage of these properties to perform orderly startup of MySQL replication. Generating configuration Before starting any of the containers in the Pod spec, the Pod first runs any [Init Containers] in the order defined. The first Init Container, named init-mysql , generates special MySQL config files based on the ordinal index. The script determines its own ordinal index by extracting it from the end of the Pod name, which is returned by the hostname command. Then it saves the ordinal (with a numeric offset to avoid reserved values) into a file called server-id.cnf in the MySQL conf.d directory. This translates the unique, stable identity provided by the StatefulSet controller into the domain of MySQL server IDs, which require the same properties. The script in the init-mysql container also applies either master.cnf or slave.cnf from the ConfigMap by copying the contents into conf.d . Because the example topology consists of a single MySQL master and any number of slaves, the script simply assigns ordinal 0 to be the master, and everyone else to be slaves. Combined with the StatefulSet controller's deployment order guarantee ensures the MySQL master is Ready before creating slaves, so they can begin replicating. Cloning existing data In general, when a new Pod joins the set as a slave, it must assume the MySQL master might already have data on it. It also must assume that the replication logs might not go all the way back to the beginning of time. These conservative assumptions are the key to allow a running StatefulSet to scale up and down over time, rather than being fixed at its initial size. The second Init Container, named clone-mysql , performs a clone operation on a slave Pod the first time it starts up on an empty PersistentVolume. That means it copies all existing data from another running Pod, so its local state is consistent enough to begin replicating from the master. MySQL itself does not provide a mechanism to do this, so the example uses a popular open-source tool called Percona XtraBackup. During the clone, the source MySQL server might suffer reduced performance. To minimize impact on the MySQL master, the script instructs each Pod to clone from the Pod whose ordinal index is one lower. This works because the StatefulSet controller always ensures Pod N is Ready before starting Pod N+1 . Starting replication After the Init Containers complete successfully, the regular containers run. The MySQL Pods consist of a mysql container that runs the actual mysqld server, and an xtrabackup container that acts as a sidecar . The xtrabackup sidecar looks at the cloned data files and determines if it's necessary to initialize MySQL replication on the slave. If so, it waits for mysqld to be ready and then executes the CHANGE MASTER TO and START SLAVE commands with replication parameters extracted from the XtraBackup clone files. Once a slave begins replication, it remembers its MySQL master and reconnects automatically if the server restarts or the connection dies. Also, because slaves look for the master at its stable DNS name ( mysql-0.mysql ), they automatically find the master even if it gets a new Pod IP due to being rescheduled. Lastly, after starting replication, the xtrabackup container listens for connections from other Pods requesting a data clone. This server remains up indefinitely in case the StatefulSet scales up, or in case the next Pod loses its PersistentVolumeClaim and needs to redo the clone.","title":"4.1 Deploy Replicated MySQL (Master/Slaves) Cluster using StatefulSet."},{"location":"ycit019_Lab_11_Storage/#42-test-the-mysql-cluster-app-and-running","text":"Step 1 Create Database, Table and message on Master MySQL database Send test queries to the MySQL master (hostname mysql-0.mysql ) by running a temporary container with the mysql:5.7 image and running the mysql client binary. kubectl run mysql-client --image = mysql:5.7 -i --rm --restart = Never -- \\ mysql -h mysql-0.mysql <<EOF CREATE DATABASE test; CREATE TABLE test.messages (message VARCHAR(250)); INSERT INTO test.messages VALUES ('hello'); EOF Step 2 Verify that recorded data has been replicated to the slaves: Use the hostname mysql-read to send test queries to any server that reports being Ready: kubectl run mysql-client --image = mysql:5.7 -i -t --rm --restart = Never -- \\ mysql -h mysql-read -e \"SELECT * FROM test.messages\" You should get output like this: Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false +---------+ | message | +---------+ | hello | +---------+ pod \"mysql-client\" deleted Step 3 Demonstrate that the mysql-read Service distributes connections across servers, you can run SELECT @@hostname in a loop: kubectl run mysql-client-loop --image = mysql:5.7 -i -t --rm --restart = Never -- \\ bash -ic \"while sleep 1; do mysql -h mysql-read -e 'SELECT @@hostname,NOW()'; done\" You should see the reported @@hostname change randomly, because a different endpoint might be selected upon each connection attempt: +-------------+---------------------+ | @@hostname | NOW() | +-------------+---------------------+ | 100 | 2006-01-02 15:04:05 | +-------------+---------------------+ +-------------+---------------------+ | @@hostname | NOW() | +-------------+---------------------+ | 102 | 2006-01-02 15:04:06 | +-------------+---------------------+ +-------------+---------------------+ | @@hostname | NOW() | +-------------+---------------------+ | 101 | 2006-01-02 15:04:07 | +-------------+---------------------+ You can press Ctrl+C when you want to stop the loop, but it's useful to keep it running in another window so you can see the effects of the following steps.","title":"4.2 Test the MySQL cluster app and running"},{"location":"ycit019_Lab_11_Storage/#43-delete-pods","text":"The StatefulSet recreates Pods if they're deleted, similar to what a ReplicaSet does for stateless Pods. Step 1 Try to fail Mysql cluster by deleting mysql-1 pod: kubectl delete pod mysql-1 The StatefulSet controller notices that no mysql-1 Pod exists anymore, and creates a new one with the same name and linked to the same PersistentVolumeClaim. Step 5 Monitor Deployment Process/Sequence watch kubectl get statefulset,pvc,pv,pods -l app=mysql Result You should see server ID 102 disappear from the loop output for a while and then return on its own.","title":"4.3 Delete Pods"},{"location":"ycit019_Lab_11_Storage/#44-scaling-the-number-of-slaves","text":"With MySQL replication, you can scale your read query capacity by adding slaves. With StatefulSet, you can do this with a single command: Step 1 Scale up statefulset: kubectl scale statefulset mysql --replicas = 5 Step 2 Watch the new Pods come up by running: kubectl get pods -l app = mysql --watch Step 3 Watch the new Pods come up by running: kubectl run mysql-client --image = mysql:5.7 -i -t --rm --restart = Never -- \\ mysql -h mysql-3.mysql -e \"SELECT * FROM test.messages\" Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false +---------+ | message | +---------+ | hello | +---------+ pod \"mysql-client\" deleted Step 4 Scaling back down is also seamless: kubectl scale statefulset mysql --replicas = 3 Note, however, that while scaling up creates new PersistentVolumeClaims automatically, scaling down does not automatically delete these PVCs. This gives you the choice to keep those initialized PVCs around to make scaling back up quicker, or to extract data before deleting them. You can see this by running: kubectl get pvc -l app = mysql Which shows that all 3 PVCs still exist, despite having scaled the StatefulSet down to 1: NAME STATUS VOLUME CAPACITY ACCESSMODES AGE data-mysql-0 Bound pvc-8acbf5dc-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-1 Bound pvc-8ad39820-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-2 Bound pvc-8ad69a6d-b103-11e6-93fa-42010a800002 10Gi RWO 20m If you don't intend to reuse the extra PVCs, you can delete them: kubectl delete pvc data-mysql-0 kubectl delete pvc data-mysql-1 kubectl delete pvc data-mysql-2 kubectl delete pvc data-mysql-3 kubectl delete pvc data-mysql-4 Step 5 Cancel the SELECT @@server_id loop by pressing Ctrl+C in its terminal, or running the following from another terminal: kubectl delete pod mysql-client-loop --now Step 6 Delete the StatefulSet. This also begins terminating the Pods. kubectl delete statefulset mysql","title":"4.4 Scaling the number of slaves"},{"location":"ycit019_Lab_11_Storage/#5-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-storage","title":"5 Cleaning Up"},{"location":"ycit019_Lab_2_Docker_basics/","text":"Lab 2 Docker basics Objective: Practice to run Docker containers 1 Docker basics \u00b6 1.1 Show running containers \u00b6 Step 1 Run docker ps to show running containers: docker ps Step 2 The output shows that there are no running containers at the moment. Use the command docker ps -a to list all containers including the ones has been stopped: docker ps -a Output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e6db2a24a8e hello-world \"/hello\" 15 minutes ago Exited (0) 15 min dreamy_nobel Review the collumns CONTAINER ID , STATUS , COMMAND , PORTS , NAMES . In the previous section we started one container and the command docker ps -a shows it as Exited . Note You can name your own containers with --name when you use docker run. If you do not provide a name, Docker will generate a random one like the one you have. Question Why Docker names are random? How docker containers named? Step 3 Let\u2019s run the command docker images to show all the images on your local system: docker images As you see, there is only one image that was downloaded from the Docker Hub. 1.2 Specify a container main process \u00b6 Step 1 Let\u2019s run our own \"hello world\" container. For that we will use the official Ubuntu image : docker run ubuntu /bin/echo 'Hello world' Output: Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:latest Hello world As you see, Docker downloaded the image ubuntu because it was not on the local machine. Step 2 Let\u2019s run the command docker images again: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Step 3 If you run the same \"hello world\" container again, Docker will use a local copy of the image: docker run ubuntu /bin/echo 'Hello world' Output: Hello world Question Compare Ubuntu Docker image with ISO image or with Cloud VM image. Why the size is so different ? Summary Pulling docker images from Docker Hub takes sometime. This time depends on: How large is the image? How fast is the network to Internet ? However, it is still much faster than booting traditional OS with Ubuntu on VM. If image already pulled on local host it takes fraction of a second to start a container. Running application in docker containers considered as a best practice for running CI/CD pipelines as it considerably faster than using VMs and reduce time for deploying a test environments. 1.3 Specify an image version \u00b6 Step 1 As you see, Docker has downloaded the ubuntu:latest image. You can see Ubuntu version by running the following command: docker run ubuntu /bin/cat /etc/issue.net Output: Ubuntu 16.04 LTS Let\u2019s say you need a previous Ubuntu LTS release. In this case, you can specify the version you need: docker run ubuntu:14.04 /bin/cat /etc/issue.net Output: Unable to find image 'ubuntu:14.04' locally 14.04: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:14.04 Ubuntu 14.04.4 LTS Step 2 The docker images command should show that we have 3 Ubuntu images downloaded locally: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB ubuntu 14.04 0ccb13bf1954 11 days ago 188 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Tip Running CI/CD pipeline with Docker using latest tag considered as a Bad Practice. Instead consider using: Versioning SHA tagging. 1.4 Run an interactive container \u00b6 Step 1 Let\u2019s use the ubuntu image to run an interactive bash session and inspect what is running inside our docker image. To achive that we going to use -i and -t flags. The -i is shorthand for --interactive , which instructs Docker to keep stdin open so that we can send commands to the sprocess. The -t flag is short for --tty and allocates a pseudo-TTY or terminal inside of the session. docker run -it ubuntu /bin/bash root@17d8bdeda98e:/# Result We get a bash shell prompt inside of the container. Note Bash prompt is not availabe for all docker images. Step 2 Let's print the system information of the latest Ubuntu image: root@17d8bdeda98e:/# uname -a Linux 17d8bdeda98e 3.19.0-31-generic ... Step 3 Let's verify what Ubuntu version is run by latest image of ubuntu: root@17d8bdeda98e:/# lsb_release -a bash: lsb_release: command not found Failure Why the standard Ubuntu command that checks version of OS is not working as expeced ? Step 4 Let's verify Ubuntu version using alternative way by checking /etc/lsb-release file. root@8cbcbd0fe8d2:/# cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.3 LTS\" Step 5 Let's compare the number of executable binaries availabe inside of the docker image versus Cloud VM that we running our class environment. First, run ls command on /bin and /usr/bin directories inside of the running ubuntu container as well as dpkg --list command that shows total number of installed packages: root@8cbcbd0fe8d2:/# ls /bin | wc -l 86 root@8cbcbd0fe8d2:/# ls /usr/bin | wc -l 233 root@eb11cd0b4106:/# dpkg --list | wc -l 101 Step 6 Use the exit command or press Ctrl-D to exit the interactive bash session back to Cloud VM. root@eb11cd0b4106:/# exit Step 7 Now run ls command on /bin and /usr/bin directories on Cloud VM that we using as our class environment: cca-user@userx-docker-vm:~$ ls /bin | wc -l 171 cca-user@userx-docker-vm:~$ ls /usr/bin | wc -l 660 cca-user@userx-docker-vm:~$ dpkg --list | wc -l 463 Result Official Docker container has much less binaries and packages installed vs Ubuntu Cloud Image. Summary Some of the use cases running docker containers in interactive mode are: Troubleshooting containerized applications Deploying and running containerized application on the existing production systems without affecting it. We've also learned that an official Docker \"minimal\" ubuntu image, does not include lsb_release command, as well as many other commands and packages that can be found in Official Ubuntu ISO image . The docker images are ment to contain only required core system commands and functions to make Images as light as possible. That say you can still install required packages using apt-get install , however this may increase size of docker image considerably. Hint While Docker Ubuntu image we used so far or Docker Centos image are very familiar to users and can be good starting point for learning docker containers. Using them in production or development considered as a Bad Practice. This is due those images still considered as heavy and potentially contain a lot more valnurabilities compare to specialized images. To reduce image pull time from docker hub and follow the best secuirity practices consider using specialized images that works well with you underlining code (Node image for NodeJS applications and etc.). Examples of specialized images are: Alpine Linux Node Atomic In fact, not so long ago all the official Docker Images in Docker-Hub has been moved to use Alpine Image . Step 8 Finally let\u2019s check that when the shell process has finished, the container stops: docker ps 1.5 Run a container in a background \u00b6 Now we know how to connect to running container and execute commands in it. However in most cases you just want run a container in a background so it can do a specific action. Step 1 Run a container in a background using the -d command line argument: docker run -d ubuntu /bin/sh -c \"while true; do date; echo hello world; sleep 1; done\" Result Command should return the container ID. Step 2 Let\u2019s use the docker ps command to see running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac231579e57f ubuntu \"/bin/sh -c 'while tr\" 1 minute ago Up 11 minute evil_golick Note Container id is going to be different in your case Hint Instead of using full container-id when building commands, it is possible simply type first few characters of container-id, to make things nice and easy. Step 3 Let\u2019s use container-id to show the container standard output: docker logs <container-id> Thu Jan 26 00:23:45 UTC 2017 hello world Thu Jan 26 00:23:46 UTC 2017 hello world Thu Jan 26 00:23:47 UTC 2017 hello world ... As you can see, in the docker ps command output, the auto generated container name is evil_golick (your container can have a different name). Step 4 Now, instead of using docker contaier-id use container name to show the container standard output: docker logs <name> Thu Jan 26 00:23:51 UTC 2017 hello world Thu Jan 26 00:23:52 UTC 2017 hello world Thu Jan 26 00:23:53 UTC 2017 hello world ... Step 5 Finally, let\u2019s stop our container: docker stop <name> Step 6 Check, that there are no running containers: docker ps Summary docker logs is a very usefull command to troubleshoot containers, and going to be used very often both for Docker and Kubernertes troubleshooting. 1.6 Accessing Containers from the Internet \u00b6 Step 1 Let\u2019s run a simple web application. We will use the existing image training/webapp, which contains a Python Flask application: docker run -d -P training/webapp python app.py ... Status: Downloaded newer image for training/webapp:latest 6e88f42d3d853762edcbfe1fe73fdc5c48865275bc6df759b83b0939d5bd2456 In the command above we specified the main process (python app.py), the -d command line argument, which tells Docker to run the container in the background. The -P command line argument tells Docker to map any required network ports inside our container to our host. This allows us to access the web application in the container. Step 2 Use the docker ps command to list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e88f42d3d85 training/webapp \"python app.py\" 3 minutes ago Up 3 minutes 0.0.0.0:32768->5000/tcp determined_torvalds The PORTS column contains the mapped ports. In our case, Docker has exposed port 5000 (the default Python Flask port) on port 32768 (can be different in your case). Step 3 The docker port command shows the exposed port. We will use the container name (determined_torvalds in the example above, it can be different in your case): docker port <name> 5000 0.0.0.0:32768 Step 4 Let\u2019s check that we can access the web application exposed port: curl http://localhost:<port>/ Result Hello world! Step 5 Let\u2019s stop our web application for now: docker stop <name> Step 6 We want to manually specify the local port to expose (-p argument). Let\u2019s use the standard HTTP port 80. We also want to specify the container name (--name argument): docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 7 Let\u2019s check that the port 80 is exposed: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp curl http://localhost/ Result `Hello world!`` Step 8 You can also observe Hello world! webapp from you laptop, for that you need to use you public VM IP that can be gather from VMs list: Your_VM_Public_IP Than paste VM Public IP address in you browser. Result Our web-app can be accessed from Internet! 1.7 Restart a container \u00b6 Step 1 Let\u2019s stop the container with web application: docker stop webapp The main process inside of the container will receive SIGTERM, and after a grace period, SIGKILL. Step 2 You can start the container later using the docker start command: docker start webapp Step 3 Check that the web application works: curl http://localhost/ Hello world! Step 4 You also can restart the running container using the docker restart command. docker restart webapp Step 4 Run docker ps command and check STATUS field: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS 6e400179070f training/webapp \"python app.py\" 25 minutes ago Up 3 seconds 1.8 Ensuring Container Uptime \u00b6 Docker considers any containers to exit with a non-zero exit code to have crashed. By default a crashed container will remain stopped. Step 1 Start the container that outputs a message and then exits with code 1 to simulate a crash. docker run -d --name restart-default scrapbook/docker-restart-example docker ps -a | grep restart-default CONTAINER ID IMAGE CREATED STATUS NAMES c854289d2f39 scrapbook/docker-restart-example 5 seconds ago Exited 3 sec ago restart-default $ docker logs restart-default Sun Sep 17 20:34:55 UTC 2017 Booting up... Result Container crushed and exited. However, there are several ways to ensure that you container up and running even if it\u2019s restarts. Step 2 The option --restart=on-failure : allows you to say how many times Docker should try again: docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example docker logs restart-3 Thu Apr 20 14:01:27 UTC 2017 Booting up... Thu Apr 20 14:01:28 UTC 2017 Booting up... Thu Apr 20 14:01:29 UTC 2017 Booting up... Thu Apr 20 14:01:31 UTC 2017 Booting up... Step 3 Finally, Docker can always restart a failed container. In this case, Docker will keep trying until the container is explicitly told to stop. docker run -d --name restart-always --restart=always scrapbook/docker-restart-example docker logs restart-always Step 4 After sometime stop running docker container, as it will be keep failing and starting again: docker stop restart-always 1.9 Inspect a container \u00b6 Step 1 You can use the docker inspect command to see the configuration and status information for the specified container: docker inspect webapp [ { \"Id\": \"249476631f7d...\", \"Created\": \"2016-08-02T23:42:56.932135327Z\", \"Path\": \"python\", \"Args\": [ \"app.py\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 16055, \"ExitCode\": 0, \"Error\": \"\", ... Step 2 You can specify a filter (-f command line argument) to show only specific elements. For example: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp 172.17.0.2 The command returns the IP address of the container. 1.10 Interacting with containers \u00b6 In some cases using docker log is not enough to undertand issues and you want to login inside of running VM. Also sometimes you package you applicaiton and in order to run it you need to login inside of container and execute and leave it running in background. Below provded few ways to interacting with containers that can help to achive descrined use cases. 1.10.1 Detach from Interactive container \u00b6 In Module, 1.4 Run an interactive container we run an Ubuntu container with -it flag and able directly login inside of the container to interact with it, however after we exited contianer using Ctrl-D or exit command container stopped. However you can exit from Interactive mode without stoping a container. Let's demonstrate how this works: Step 1 Start Ubunu container in interactive mode: docker run -it ubuntu /bin/bash Step 2 Run watch date command inside running container in order to exit date command every 2 seconds. root@1d688a9f4ed4:/# watch date Step 3 Detach from a container and leave it running using the CTRL-p CTRL-q key sequence. Step 4 Verify that Ubuntu container is still running: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS NAMES 1d688a9f4ed4 ubuntu \"/bin/bash\" 1 minutes ago Up 1 minutes admiring_lovelace Result Great you were able to detach from Docker container without stopping it, while it is executing a process in it. What about attaching back to container ? Important CTRL-p CTRL-q sequence key only works if docker contaienr started with -it command! 1.11.2 Attach to a container \u00b6 Now let's get back and attach to our running Ubuntu image. For that docker provides docker attach command. docker attach <container name> Every 2.0s: date Mon Sep 18 00:08:57 2017 Summary docker attach attaches your contairs terminal\u2019s standard input, output, and error (or any combination of the 3) to a running container. This allows you to view its ongoing output or to control it interactively, as though the commands were running directly in your terminal. 1.11.3 Execute a process in a container \u00b6 Step 1 Let verify if webapp container is still running docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp If not running start it with following command: $ docker run -d -p 80:5000 --name webapp training/webapp python app.py other wise skip to next step . Step 2 Use the docker exec command to execute a command in the running container. For example: docker exec webapp ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 26 0.0 0.0 15572 2104 ? Rs 00:12 0:00 ps aux The same command with the -it command line argument can be used to run an interactive session in the container: docker exec -it webapp bash root@249476631f7d:/opt/webapp# ps auxw ps auxw USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 32 0.0 0.0 18144 3064 ? Ss 00:14 0:00 bash root 47 0.0 0.0 15572 2076 ? R+ 00:16 0:00 ps auxw Step 2 Use the exit command or press Ctrl-D to exit the interactive bash session: root@249476631f7d:/opt/webapp# exit Summary docker exec is one of the most usefull docker commands used for troubleshooting containers. 1.12 Copy files to/from container \u00b6 The docker cp command allows you to copy files from the container to the local machine or from the local file system to the container. This command works for a running or stopped container. Step 1 Let\u2019s copy the container\u2019s app.py file to the local machine: docker cp webapp:/opt/webapp/app.py . Step 2 Edit the local app.py file. For example, change the line return 'Hello '+provider+'!' to return 'Hello '+provider+'!!!'. Copy the modified file back and restart the container: docker cp app.py webapp:/opt/webapp/ docker restart webapp Step 3 Check that the modified web application works:: curl http://localhost/ Result `Hello world!!!`` 1.12 Remove containers \u00b6 Now let's clean up the environment and at the same time learn how delete containers. Step 1 First list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 81c4c66baaf9 training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp Step 2 Than try to delete running container using docker rm <container_id> docker rm $container_id Error response from daemon: You cannot remove a running container 81c4c66baaf9. Stop the container before attempting removal or force remove. Failure Docker containers needs to be first stopped or deleted using --force flag. docker rm $container_id -f Alternatively, you can run stop and rm in sequence: docker stop 81c4c66baaf9 docker rm 81c4c66baaf9 Summary We've learned a lot of docker commands which are very handy to know both when using Docker and Kubernetes. In the next Module we going to deep dive in to details of how networking and storage works in Docker.","title":"Lab 2 Docker Basics"},{"location":"ycit019_Lab_2_Docker_basics/#1-docker-basics","text":"","title":"1 Docker basics"},{"location":"ycit019_Lab_2_Docker_basics/#11-show-running-containers","text":"Step 1 Run docker ps to show running containers: docker ps Step 2 The output shows that there are no running containers at the moment. Use the command docker ps -a to list all containers including the ones has been stopped: docker ps -a Output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e6db2a24a8e hello-world \"/hello\" 15 minutes ago Exited (0) 15 min dreamy_nobel Review the collumns CONTAINER ID , STATUS , COMMAND , PORTS , NAMES . In the previous section we started one container and the command docker ps -a shows it as Exited . Note You can name your own containers with --name when you use docker run. If you do not provide a name, Docker will generate a random one like the one you have. Question Why Docker names are random? How docker containers named? Step 3 Let\u2019s run the command docker images to show all the images on your local system: docker images As you see, there is only one image that was downloaded from the Docker Hub.","title":"1.1 Show running containers"},{"location":"ycit019_Lab_2_Docker_basics/#12-specify-a-container-main-process","text":"Step 1 Let\u2019s run our own \"hello world\" container. For that we will use the official Ubuntu image : docker run ubuntu /bin/echo 'Hello world' Output: Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:latest Hello world As you see, Docker downloaded the image ubuntu because it was not on the local machine. Step 2 Let\u2019s run the command docker images again: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Step 3 If you run the same \"hello world\" container again, Docker will use a local copy of the image: docker run ubuntu /bin/echo 'Hello world' Output: Hello world Question Compare Ubuntu Docker image with ISO image or with Cloud VM image. Why the size is so different ? Summary Pulling docker images from Docker Hub takes sometime. This time depends on: How large is the image? How fast is the network to Internet ? However, it is still much faster than booting traditional OS with Ubuntu on VM. If image already pulled on local host it takes fraction of a second to start a container. Running application in docker containers considered as a best practice for running CI/CD pipelines as it considerably faster than using VMs and reduce time for deploying a test environments.","title":"1.2 Specify a container main process"},{"location":"ycit019_Lab_2_Docker_basics/#13-specify-an-image-version","text":"Step 1 As you see, Docker has downloaded the ubuntu:latest image. You can see Ubuntu version by running the following command: docker run ubuntu /bin/cat /etc/issue.net Output: Ubuntu 16.04 LTS Let\u2019s say you need a previous Ubuntu LTS release. In this case, you can specify the version you need: docker run ubuntu:14.04 /bin/cat /etc/issue.net Output: Unable to find image 'ubuntu:14.04' locally 14.04: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:14.04 Ubuntu 14.04.4 LTS Step 2 The docker images command should show that we have 3 Ubuntu images downloaded locally: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB ubuntu 14.04 0ccb13bf1954 11 days ago 188 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Tip Running CI/CD pipeline with Docker using latest tag considered as a Bad Practice. Instead consider using: Versioning SHA tagging.","title":"1.3 Specify an image version"},{"location":"ycit019_Lab_2_Docker_basics/#14-run-an-interactive-container","text":"Step 1 Let\u2019s use the ubuntu image to run an interactive bash session and inspect what is running inside our docker image. To achive that we going to use -i and -t flags. The -i is shorthand for --interactive , which instructs Docker to keep stdin open so that we can send commands to the sprocess. The -t flag is short for --tty and allocates a pseudo-TTY or terminal inside of the session. docker run -it ubuntu /bin/bash root@17d8bdeda98e:/# Result We get a bash shell prompt inside of the container. Note Bash prompt is not availabe for all docker images. Step 2 Let's print the system information of the latest Ubuntu image: root@17d8bdeda98e:/# uname -a Linux 17d8bdeda98e 3.19.0-31-generic ... Step 3 Let's verify what Ubuntu version is run by latest image of ubuntu: root@17d8bdeda98e:/# lsb_release -a bash: lsb_release: command not found Failure Why the standard Ubuntu command that checks version of OS is not working as expeced ? Step 4 Let's verify Ubuntu version using alternative way by checking /etc/lsb-release file. root@8cbcbd0fe8d2:/# cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.3 LTS\" Step 5 Let's compare the number of executable binaries availabe inside of the docker image versus Cloud VM that we running our class environment. First, run ls command on /bin and /usr/bin directories inside of the running ubuntu container as well as dpkg --list command that shows total number of installed packages: root@8cbcbd0fe8d2:/# ls /bin | wc -l 86 root@8cbcbd0fe8d2:/# ls /usr/bin | wc -l 233 root@eb11cd0b4106:/# dpkg --list | wc -l 101 Step 6 Use the exit command or press Ctrl-D to exit the interactive bash session back to Cloud VM. root@eb11cd0b4106:/# exit Step 7 Now run ls command on /bin and /usr/bin directories on Cloud VM that we using as our class environment: cca-user@userx-docker-vm:~$ ls /bin | wc -l 171 cca-user@userx-docker-vm:~$ ls /usr/bin | wc -l 660 cca-user@userx-docker-vm:~$ dpkg --list | wc -l 463 Result Official Docker container has much less binaries and packages installed vs Ubuntu Cloud Image. Summary Some of the use cases running docker containers in interactive mode are: Troubleshooting containerized applications Deploying and running containerized application on the existing production systems without affecting it. We've also learned that an official Docker \"minimal\" ubuntu image, does not include lsb_release command, as well as many other commands and packages that can be found in Official Ubuntu ISO image . The docker images are ment to contain only required core system commands and functions to make Images as light as possible. That say you can still install required packages using apt-get install , however this may increase size of docker image considerably. Hint While Docker Ubuntu image we used so far or Docker Centos image are very familiar to users and can be good starting point for learning docker containers. Using them in production or development considered as a Bad Practice. This is due those images still considered as heavy and potentially contain a lot more valnurabilities compare to specialized images. To reduce image pull time from docker hub and follow the best secuirity practices consider using specialized images that works well with you underlining code (Node image for NodeJS applications and etc.). Examples of specialized images are: Alpine Linux Node Atomic In fact, not so long ago all the official Docker Images in Docker-Hub has been moved to use Alpine Image . Step 8 Finally let\u2019s check that when the shell process has finished, the container stops: docker ps","title":"1.4 Run an interactive container"},{"location":"ycit019_Lab_2_Docker_basics/#15-run-a-container-in-a-background","text":"Now we know how to connect to running container and execute commands in it. However in most cases you just want run a container in a background so it can do a specific action. Step 1 Run a container in a background using the -d command line argument: docker run -d ubuntu /bin/sh -c \"while true; do date; echo hello world; sleep 1; done\" Result Command should return the container ID. Step 2 Let\u2019s use the docker ps command to see running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac231579e57f ubuntu \"/bin/sh -c 'while tr\" 1 minute ago Up 11 minute evil_golick Note Container id is going to be different in your case Hint Instead of using full container-id when building commands, it is possible simply type first few characters of container-id, to make things nice and easy. Step 3 Let\u2019s use container-id to show the container standard output: docker logs <container-id> Thu Jan 26 00:23:45 UTC 2017 hello world Thu Jan 26 00:23:46 UTC 2017 hello world Thu Jan 26 00:23:47 UTC 2017 hello world ... As you can see, in the docker ps command output, the auto generated container name is evil_golick (your container can have a different name). Step 4 Now, instead of using docker contaier-id use container name to show the container standard output: docker logs <name> Thu Jan 26 00:23:51 UTC 2017 hello world Thu Jan 26 00:23:52 UTC 2017 hello world Thu Jan 26 00:23:53 UTC 2017 hello world ... Step 5 Finally, let\u2019s stop our container: docker stop <name> Step 6 Check, that there are no running containers: docker ps Summary docker logs is a very usefull command to troubleshoot containers, and going to be used very often both for Docker and Kubernertes troubleshooting.","title":"1.5 Run a container in a background"},{"location":"ycit019_Lab_2_Docker_basics/#16-accessing-containers-from-the-internet","text":"Step 1 Let\u2019s run a simple web application. We will use the existing image training/webapp, which contains a Python Flask application: docker run -d -P training/webapp python app.py ... Status: Downloaded newer image for training/webapp:latest 6e88f42d3d853762edcbfe1fe73fdc5c48865275bc6df759b83b0939d5bd2456 In the command above we specified the main process (python app.py), the -d command line argument, which tells Docker to run the container in the background. The -P command line argument tells Docker to map any required network ports inside our container to our host. This allows us to access the web application in the container. Step 2 Use the docker ps command to list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e88f42d3d85 training/webapp \"python app.py\" 3 minutes ago Up 3 minutes 0.0.0.0:32768->5000/tcp determined_torvalds The PORTS column contains the mapped ports. In our case, Docker has exposed port 5000 (the default Python Flask port) on port 32768 (can be different in your case). Step 3 The docker port command shows the exposed port. We will use the container name (determined_torvalds in the example above, it can be different in your case): docker port <name> 5000 0.0.0.0:32768 Step 4 Let\u2019s check that we can access the web application exposed port: curl http://localhost:<port>/ Result Hello world! Step 5 Let\u2019s stop our web application for now: docker stop <name> Step 6 We want to manually specify the local port to expose (-p argument). Let\u2019s use the standard HTTP port 80. We also want to specify the container name (--name argument): docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 7 Let\u2019s check that the port 80 is exposed: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp curl http://localhost/ Result `Hello world!`` Step 8 You can also observe Hello world! webapp from you laptop, for that you need to use you public VM IP that can be gather from VMs list: Your_VM_Public_IP Than paste VM Public IP address in you browser. Result Our web-app can be accessed from Internet!","title":"1.6 Accessing Containers from the Internet"},{"location":"ycit019_Lab_2_Docker_basics/#17-restart-a-container","text":"Step 1 Let\u2019s stop the container with web application: docker stop webapp The main process inside of the container will receive SIGTERM, and after a grace period, SIGKILL. Step 2 You can start the container later using the docker start command: docker start webapp Step 3 Check that the web application works: curl http://localhost/ Hello world! Step 4 You also can restart the running container using the docker restart command. docker restart webapp Step 4 Run docker ps command and check STATUS field: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS 6e400179070f training/webapp \"python app.py\" 25 minutes ago Up 3 seconds","title":"1.7 Restart a container"},{"location":"ycit019_Lab_2_Docker_basics/#18-ensuring-container-uptime","text":"Docker considers any containers to exit with a non-zero exit code to have crashed. By default a crashed container will remain stopped. Step 1 Start the container that outputs a message and then exits with code 1 to simulate a crash. docker run -d --name restart-default scrapbook/docker-restart-example docker ps -a | grep restart-default CONTAINER ID IMAGE CREATED STATUS NAMES c854289d2f39 scrapbook/docker-restart-example 5 seconds ago Exited 3 sec ago restart-default $ docker logs restart-default Sun Sep 17 20:34:55 UTC 2017 Booting up... Result Container crushed and exited. However, there are several ways to ensure that you container up and running even if it\u2019s restarts. Step 2 The option --restart=on-failure : allows you to say how many times Docker should try again: docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example docker logs restart-3 Thu Apr 20 14:01:27 UTC 2017 Booting up... Thu Apr 20 14:01:28 UTC 2017 Booting up... Thu Apr 20 14:01:29 UTC 2017 Booting up... Thu Apr 20 14:01:31 UTC 2017 Booting up... Step 3 Finally, Docker can always restart a failed container. In this case, Docker will keep trying until the container is explicitly told to stop. docker run -d --name restart-always --restart=always scrapbook/docker-restart-example docker logs restart-always Step 4 After sometime stop running docker container, as it will be keep failing and starting again: docker stop restart-always","title":"1.8 Ensuring Container Uptime"},{"location":"ycit019_Lab_2_Docker_basics/#19-inspect-a-container","text":"Step 1 You can use the docker inspect command to see the configuration and status information for the specified container: docker inspect webapp [ { \"Id\": \"249476631f7d...\", \"Created\": \"2016-08-02T23:42:56.932135327Z\", \"Path\": \"python\", \"Args\": [ \"app.py\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 16055, \"ExitCode\": 0, \"Error\": \"\", ... Step 2 You can specify a filter (-f command line argument) to show only specific elements. For example: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp 172.17.0.2 The command returns the IP address of the container.","title":"1.9 Inspect a container"},{"location":"ycit019_Lab_2_Docker_basics/#110-interacting-with-containers","text":"In some cases using docker log is not enough to undertand issues and you want to login inside of running VM. Also sometimes you package you applicaiton and in order to run it you need to login inside of container and execute and leave it running in background. Below provded few ways to interacting with containers that can help to achive descrined use cases.","title":"1.10 Interacting with containers"},{"location":"ycit019_Lab_2_Docker_basics/#1101-detach-from-interactive-container","text":"In Module, 1.4 Run an interactive container we run an Ubuntu container with -it flag and able directly login inside of the container to interact with it, however after we exited contianer using Ctrl-D or exit command container stopped. However you can exit from Interactive mode without stoping a container. Let's demonstrate how this works: Step 1 Start Ubunu container in interactive mode: docker run -it ubuntu /bin/bash Step 2 Run watch date command inside running container in order to exit date command every 2 seconds. root@1d688a9f4ed4:/# watch date Step 3 Detach from a container and leave it running using the CTRL-p CTRL-q key sequence. Step 4 Verify that Ubuntu container is still running: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS NAMES 1d688a9f4ed4 ubuntu \"/bin/bash\" 1 minutes ago Up 1 minutes admiring_lovelace Result Great you were able to detach from Docker container without stopping it, while it is executing a process in it. What about attaching back to container ? Important CTRL-p CTRL-q sequence key only works if docker contaienr started with -it command!","title":"1.10.1 Detach from Interactive container"},{"location":"ycit019_Lab_2_Docker_basics/#1112-attach-to-a-container","text":"Now let's get back and attach to our running Ubuntu image. For that docker provides docker attach command. docker attach <container name> Every 2.0s: date Mon Sep 18 00:08:57 2017 Summary docker attach attaches your contairs terminal\u2019s standard input, output, and error (or any combination of the 3) to a running container. This allows you to view its ongoing output or to control it interactively, as though the commands were running directly in your terminal.","title":"1.11.2 Attach to a container"},{"location":"ycit019_Lab_2_Docker_basics/#1113-execute-a-process-in-a-container","text":"Step 1 Let verify if webapp container is still running docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp If not running start it with following command: $ docker run -d -p 80:5000 --name webapp training/webapp python app.py other wise skip to next step . Step 2 Use the docker exec command to execute a command in the running container. For example: docker exec webapp ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 26 0.0 0.0 15572 2104 ? Rs 00:12 0:00 ps aux The same command with the -it command line argument can be used to run an interactive session in the container: docker exec -it webapp bash root@249476631f7d:/opt/webapp# ps auxw ps auxw USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 32 0.0 0.0 18144 3064 ? Ss 00:14 0:00 bash root 47 0.0 0.0 15572 2076 ? R+ 00:16 0:00 ps auxw Step 2 Use the exit command or press Ctrl-D to exit the interactive bash session: root@249476631f7d:/opt/webapp# exit Summary docker exec is one of the most usefull docker commands used for troubleshooting containers.","title":"1.11.3 Execute a process in a container"},{"location":"ycit019_Lab_2_Docker_basics/#112-copy-files-tofrom-container","text":"The docker cp command allows you to copy files from the container to the local machine or from the local file system to the container. This command works for a running or stopped container. Step 1 Let\u2019s copy the container\u2019s app.py file to the local machine: docker cp webapp:/opt/webapp/app.py . Step 2 Edit the local app.py file. For example, change the line return 'Hello '+provider+'!' to return 'Hello '+provider+'!!!'. Copy the modified file back and restart the container: docker cp app.py webapp:/opt/webapp/ docker restart webapp Step 3 Check that the modified web application works:: curl http://localhost/ Result `Hello world!!!``","title":"1.12 Copy files to/from container"},{"location":"ycit019_Lab_2_Docker_basics/#112-remove-containers","text":"Now let's clean up the environment and at the same time learn how delete containers. Step 1 First list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 81c4c66baaf9 training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp Step 2 Than try to delete running container using docker rm <container_id> docker rm $container_id Error response from daemon: You cannot remove a running container 81c4c66baaf9. Stop the container before attempting removal or force remove. Failure Docker containers needs to be first stopped or deleted using --force flag. docker rm $container_id -f Alternatively, you can run stop and rm in sequence: docker stop 81c4c66baaf9 docker rm 81c4c66baaf9 Summary We've learned a lot of docker commands which are very handy to know both when using Docker and Kubernetes. In the next Module we going to deep dive in to details of how networking and storage works in Docker.","title":"1.12 Remove containers"},{"location":"ycit019_Lab_3_Advanced_Docker/","text":"Lab 3 Docker Networking, Persistence, Monitoring and Logging Objective: Networks Docker basics User-defined private Networks Persistence Data Volumes 1 Docker Networking \u00b6 1.1 Docker Networking Basics \u00b6 Step 1: The Docker Network Command The docker network command is the main command for configuring and managing container networks. Run the docker network command from the first terminal. docker network Usage: docker network COMMAND Manage networks Options: --help Print usage Commands: connect Connect a container to a network create Create a network disconnect Disconnect a container from a network inspect Display detailed information on one or more networks ls List networks prune Remove all unused networks rm Remove one or more networks Run 'docker network COMMAND --help' for more information on a command. The command output shows how to use the command as well as all of the docker network sub-commands. As you can see from the output, the docker network command allows you to create new networks, list existing networks, inspect networks, and remove networks. It also allows you to connect and disconnect containers from networks. Step 2 Run a docker network ls command to view existing container networks on the current Docker host. docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local The output above shows the container networks that are created as part of a standard installation of Docker. New networks that you create will also show up in the output of the docker network ls command. You can see that each network gets a unique ID and NAME . Each network is also associated with a single driver. Notice that the \"bridge\" network and the \"host\" network have the same name as their respective drivers. Step 3: The docker network inspect command is used to view network configuration details. These details include; name, ID, driver, IPAM driver, subnet info, connected containers, and more. Use docker network inspect <network> to view configuration details of the container networks on your Docker host. The command below shows the details of the network called bridge . docker network inspect bridge [ { \"Name\": \"bridge\", \"Id\": \"3430ad6f20bf1486df2e5f64ddc93cc4ff95d81f59b6baea8a510ad500df2e57\", \"Created\": \"2017-04-03T16:49:58.6536278Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Containers\": {}, \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" }, \"Labels\": {} } ] Note The syntax of the docker network inspect command is docker network inspect <network> , where <network> can be either network name or network ID. In the example above we are showing the configuration details for the network called \"bridge\". Do not confuse this with the \"bridge\" driver. Step 4 Now, list Docker supported network driver plugins. For that run docker info command, that shows a lot of interesting information about a Docker installation. Run the docker info command and locate the list of network plugins. docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ee-3 Storage Driver: aufs <Snip> Plugins: Volume: local Network: bridge host macvlan null overlay Swarm: inactive Runtimes: runc <Snip> The output above shows the bridge , host , macvlan , null , and overlay drivers. Summary We've quickly reviewed available docker networking commands as well as found what drivers current docker setup supports. 1.2 Default bridge network \u00b6 Every clean installation of Docker comes with a pre-built network called Default bridge network . Let's explore in more details how it works. Step 1 Verify this with the docker network ls . docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local Result The output above shows that the bridge network is associated with the bridge driver. It's important to note that the network and the driver are connected, but they are not the same. In this example the network and the driver have the same name - but they are not the same thing! The output above also shows that the bridge network is scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the bridge driver - the bridge driver provides single-host networking. All networks created with the bridge driver are based on a Linux bridge (a.k.a. a virtual switch). Step 5 Start webapp in Default bridge network docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 6 Check that the webapp and db containers are running: Command: docker ps 1.3 User-defined Private Networks \u00b6 So far we\u2019ve learned how Docker networking works with Docker default bridge network . With the introduction of user-defined networking in Docker 1.9, it is now possible to create multiple Docker bridges to allow network segregation within the same host or multi-host networking to allow communicate Docker containers between hosts. The commands are available through the Docker Engine CLI are: docker network create docker network connect docker network ls docker network rm docker network disconnect docker network inspect Let's demonstrate how to create a custom bridge network. Step 1 By default, Docker runs containers in the bridge network. You may want to isolate one or more containers in a separate network. Let\u2019s create a new network: docker network create my-network \\ -d bridge \\ --subnet 172.19.0.0/16 The -d bridge command line argument specifies the bridge network driver and the --subnet command line argument specifies the network segment in CIDR format. If you do not specify a subnet when creating a network, then Docker assigns a subnet automatically, so it is a good idea to specify a subnet to avoid potential conflicts with the existing networks. Below are some other options that are available with the bridge Driver: com.docker.network.bridge.enable_ip_masquerade: This instructs the Docker host to hide or masquerade all containers in this network behind the Docker host's interfaces if the container attempts to route off the local host . com.docker.network.bridge.name: This is the name you wish to give to the bridge. com.docker.network.bridge.enable_icc: This turns on or off Inter-Container Connectivity (ICC) mode for the bridge. com.docker.network.bridge.host_binding_ipv4: This defines the host interface that should be used for port binding. com.docker.network.driver.mtu: This sets MTU for containers attached to this bridge. Step 2 To check that the new network is created, execute docker network ls: docker network ls NETWORK ID NAME DRIVER SCOPE d428e49e4869 bridge bridge local 0d1f78528cc5 host host local 56ef0481820d my-network bridge local 4a07cef84617 none null local Step 3 Let\u2019s inspect the new network: docker network inspect my-network [ { \"Name\": \"my-network\", \"Id\": \"56ef0481820d...\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"172.19.0.0/16\" } ] }, \"Internal\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ] Step 4 As expected, there are no containers connected to the my-network. Let\u2019s recreate the db container in the my-network: docker rm -f db docker run -d --network=my-network --name db training/postgres Step 5 Inspect the my-network again: docker network inspect my-network Output: \"Containers\": { \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... As you see, the db container is connected to the my-network and has 172.19.0.2 address. Step 6 Let\u2019s start an interactive session in the db container and ping the IP address of the webapp again: Note Quick reminder how to locate webapp ip: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp docker exec -it db bash Once inside of container run: root@c3afff20019a:/# ping -c 1 172.17.0.3 PING 172.17.0.3 (172.17.0.3) 56(84) bytes of data. --- 172.17.0.3 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms As expected, the webapp container is no longer accessible from the db container, because they are connected to different networks. Summary Using Multi-host networking provides network isolation within a Docker host via network namepsaces. This is can be used if you want to deploy different applications on same host for isolation or resource duplicate prevention. Step 7 Let\u2019s connect the webapp container to the my-network: docker network connect my-network webapp Step 8 Check that the webapp container now is connected to the my-network: docker network inspect my-network Output: ... \"Containers\": { \"62ed4a627356...\": { \"Name\": \"webapp\", \"EndpointID\": \"ae95b0103bbc...\", \"MacAddress\": \"02:42:ac:12:00:03\", \"IPv4Address\": \"172.19.0.3/16\", \"IPv6Address\": \"\" }, \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... The output shows that two containers are connected to the my-network and the webapp container has 172.19.0.3 address in that network. Step 9 Check that the webapp container is accessible from the db container using its new IP address: docker exec -it db bash root@c3afff20019a:/# ping -c 1 172.19.0.3 PING 172.19.0.3 (172.19.0.3) 56(84) bytes of data. 64 bytes from 172.19.0.3: icmp_seq=1 ttl=64 time=0.136 ms --- 172.19.0.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms Success As expected containers can communicate with each other. Step 10 You can now remove the existing container. You should stop the container before removing it. Alternatively you can use the -f command line argument: docker rm -f webapp docker rm -f db docker network rm my-network Hint Use below command to delete running containers in bulk : docker rm -f $(docker ps -q) Summary It is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses 1.4 Access containers from outside \u00b6 External Access to the Containers can be configured via publishing mechanism. Docker provides 2 options to publish ports: -P flag publishes all exposed ports -p flag allows you to specify specific ports and interfaces to use when mapping ports. The -p flag can take several different forms with the syntax looking like this: Specify the host port and container port: \u2013p <host port>:<container port> Specify the host interface, host port, and container port: \u2013p <host IP interface>:<host port>:<container port> Specify the host interface, have Docker choose a random host port, and specify the container port: \u2013p <host IP interface>::<container port> Specify only a container port and have Docker use a random host port: \u2013p <container port> Let's test exposing containers. For that let's start a new NGINX container and map port 8080 on the Docker host to port 80 inside of the container. This means that traffic that hits the Docker host on port 8080 will be passed on to port 80 inside the container. Note If you start a new container from the official NGINX image without specifying a command to run, the container will run a basic web server on port 80. Step 1 Start a new container based off the official NGINX image by running docker run --name web1 -d -p 8080:80 nginx . docker run --name web1 -d -p 8080:80 nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 6d827a3ef358: Pull complete b556b18c7952: Pull complete 03558b976e24: Pull complete 9abee7e1ef9d: Pull complete Digest: sha256:52f84ace6ea43f2f58937e5f9fc562e99ad6876e82b99d171916c1ece587c188 Status: Downloaded newer image for nginx:latest 4e0da45b0f169f18b0e1ee9bf779500cb0f756402c0a0821d55565f162741b3e Step 2 Review the container status and port mappings by running docker ps . docker ps CONTAINER ID IMAGE COMMAND PORTS NAMES 4e0da45b0f16 nginx \"nginx -g 'daemon ...\" 443/tcp, 0.0.0.0:8080->80/tcp web1 Result The top line shows the new web1 container running NGINX. Take note of the command the container is running as well as the port mapping - 0.0.0.0:8080->80/tcp maps port 8080 on all host interfaces to port 80 inside the web1 container. This port mapping is what effectively makes the containers web service accessible from external sources (via the Docker hosts IP address on port 8080). Step 3 Test connectivity to the NGINX web server, by pasting <Public_IP:8080> of VM to the browser. Note In order to locate Public IP see the list of VMs. Alternatively from inside of VM run curl 127.0.0.1:8080 command. curl 127.0.0.1:8080 <!DOCTYPE html> <html> <Snip> <head> <title>Welcome to nginx!</title> <Snip> <p><em>Thank you for using nginx.</em></p> </body> </html> Success Both CLI and UI method works! If you try and curl the IP address on a different port number it will fail. Summary Docker provides easy way to expose containers outside of the Docker Node. This can ber used for connecting containers between each other: Between networks on the same host Between networks on different host Accessing containers from outside (e.g web site) However, port mapping is implemented via port address translation (PAT) unlike in Kubernetes which we learn soon, exposes applications via service IPs and communicates via POD IPs using (NAT) Step 4 Cleanup environment docker rm -f $(docker ps -q) 2 Persistant Volumes \u00b6 2.1 Storage driver \u00b6 We've discussed several Storage drivers (graphdrivers) during the class. Let's find out what graphdriver is running in our Lab environment. docker info | grep Storage WARNING: No swap limit support Storage Driver: aufs Result Our Classroom is running aufs storage driver. Not a suprise as we running our Lab on Ubuntu VM. Summary Systems runnng Ubuntu or Debian ,going to run aufs graphdriver by default and will most likely meet the majority of your needs. In future overlay2 may replace aufs stay tunned! 2.2 Persisting Data Using Volumes \u00b6 Docker Volumes are created and assigned when containers are started. Data Volumes allow you to map a host directory to a container for sharing data. This mapping is bi-directional. It allows data stored on the host to be accessed from within the container. It also means data saved by the process inside the container is persisted on the host. 2.2.1 Create and manage volumes \u00b6 Step 1 Create a volume: docker volume create --name my-vol Step 2 List volumes: docker volume ls Output: local my-vol Step 3 Inspect a volume: docker volume inspect my-vol [ { \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\", \"Name\": \"my-vol\", \"Options\": {}, \"Scope\": \"local\" } ] Step 3 Add some data to the Mountpoint of the volume: sudo touch /var/lib/docker/volumes/my-vol/_data/test_vol sudo ls /var/lib/docker/volumes/my-vol/_data/ Step 4 Create a container busybox alpine image and attach created my-vol volume in to it: docker run -it -v my-vol:/world busybox / # ls /world test_vol / # Result Volume is mounted and test_vol file is under /world folder as expected Step 5 Try to delete the volume: docker volume rm my-vol Error response from daemon: unable to remove volume: remove my-vol: volume is in use - [6ef3055b516b306847150af8fcea796c02cd90578967802ac29c39d3a2c90102] Failure Deleting container that is attached is not permited. However you can delete with -f option Step 5 Busybox container stopped, howerver it is not deleted. Let's locate stopped busybox container and delete it: docker ps -a | grep busybox docker rm $docker_id Step 6 You can now delete my-vol Note Volume is still avaiable if needed to be reattached any time docker volume ls docker volume rm my-vol docker volume ls Summary Volumes can be craeted and managed separately from containers. 2.2.2 Start a container with a volume \u00b6 If you start a container with a volume that does not yet exist, Docker creates the volume for you. Step 1 Add a data volume to a container: docker run -d -P --name webapp -v /webapp training/webapp python app.py Result Command started a new container and created a new volume inside the container at /webapp. Step 2 Locate the volume on the host using the docker inspect command: docker inspect webapp | grep -A9 Mounts ``` **Output:** ``` \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\", \"Source\": \"/var/lib/docker/volumes/39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d/_data\", \"Destination\": \"/webapp\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" Step 3 List container docker volume ls Output: DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Step 5 Alternatively, you can specify a host directory you want to use as a data volume: mkdir db docker run -d --name db -v ~/db:/db training/postgres Step 2 Start an interactive session in the db container and create a new file in the /db directory: docker exec -it db bash Type inside docker containers console: root@9a7a4fbcc929:/# cd /db root@9a7a4fbcc929:/db# touch hello_from_db_container root@9a7a4fbcc929:/db# exit Step 4 Check that the local db directory contains the new file: ls db hello_from_db_container Step 5 Check that the data volume is persistent. Remove the db container: docker rm -f db Step 6 Create the db container again: docker run -d --name db -v ~/db:/db training/postgres Step 7 Check that its /db directory contains the hello_from_db_container file: docker exec -it db bash Run commands inside container: root@47a60c01590e:/# ls /db hello_from_db_container root@47a60c01590e:/# exit 2.2.3 Use a read-only volume \u00b6 Step 1 Mounting Volumes gives the container full read and write access to the directory. You can specify read-only permissions on the directory by adding the permissions :ro to the mount. If the container attempts to modify data within the directory it will error. docker run -d --name db1 -v ~/db:/db:ro training/postgres docker exec -it db1 bash cd db touch test Result touch: cannot touch 'test': Read-only file system $ exit Step 2 Clean up containers and volumes: docker rm -f $(docker ps -q) docker volume ls Output DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d docker volume rm 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Summary We've learned how to manage volumes with containers Hint If you Docker host has several Storage plugins configured (e.g. ceph, gluster) you can specify via --opt type=btrfs, nfs or --driver=glusterfs during docker volume creation.","title":"Lab 3 Advanced Docker"},{"location":"ycit019_Lab_3_Advanced_Docker/#1-docker-networking","text":"","title":"1 Docker Networking"},{"location":"ycit019_Lab_3_Advanced_Docker/#11-docker-networking-basics","text":"Step 1: The Docker Network Command The docker network command is the main command for configuring and managing container networks. Run the docker network command from the first terminal. docker network Usage: docker network COMMAND Manage networks Options: --help Print usage Commands: connect Connect a container to a network create Create a network disconnect Disconnect a container from a network inspect Display detailed information on one or more networks ls List networks prune Remove all unused networks rm Remove one or more networks Run 'docker network COMMAND --help' for more information on a command. The command output shows how to use the command as well as all of the docker network sub-commands. As you can see from the output, the docker network command allows you to create new networks, list existing networks, inspect networks, and remove networks. It also allows you to connect and disconnect containers from networks. Step 2 Run a docker network ls command to view existing container networks on the current Docker host. docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local The output above shows the container networks that are created as part of a standard installation of Docker. New networks that you create will also show up in the output of the docker network ls command. You can see that each network gets a unique ID and NAME . Each network is also associated with a single driver. Notice that the \"bridge\" network and the \"host\" network have the same name as their respective drivers. Step 3: The docker network inspect command is used to view network configuration details. These details include; name, ID, driver, IPAM driver, subnet info, connected containers, and more. Use docker network inspect <network> to view configuration details of the container networks on your Docker host. The command below shows the details of the network called bridge . docker network inspect bridge [ { \"Name\": \"bridge\", \"Id\": \"3430ad6f20bf1486df2e5f64ddc93cc4ff95d81f59b6baea8a510ad500df2e57\", \"Created\": \"2017-04-03T16:49:58.6536278Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Containers\": {}, \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" }, \"Labels\": {} } ] Note The syntax of the docker network inspect command is docker network inspect <network> , where <network> can be either network name or network ID. In the example above we are showing the configuration details for the network called \"bridge\". Do not confuse this with the \"bridge\" driver. Step 4 Now, list Docker supported network driver plugins. For that run docker info command, that shows a lot of interesting information about a Docker installation. Run the docker info command and locate the list of network plugins. docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ee-3 Storage Driver: aufs <Snip> Plugins: Volume: local Network: bridge host macvlan null overlay Swarm: inactive Runtimes: runc <Snip> The output above shows the bridge , host , macvlan , null , and overlay drivers. Summary We've quickly reviewed available docker networking commands as well as found what drivers current docker setup supports.","title":"1.1 Docker Networking Basics"},{"location":"ycit019_Lab_3_Advanced_Docker/#12-default-bridge-network","text":"Every clean installation of Docker comes with a pre-built network called Default bridge network . Let's explore in more details how it works. Step 1 Verify this with the docker network ls . docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local Result The output above shows that the bridge network is associated with the bridge driver. It's important to note that the network and the driver are connected, but they are not the same. In this example the network and the driver have the same name - but they are not the same thing! The output above also shows that the bridge network is scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the bridge driver - the bridge driver provides single-host networking. All networks created with the bridge driver are based on a Linux bridge (a.k.a. a virtual switch). Step 5 Start webapp in Default bridge network docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 6 Check that the webapp and db containers are running: Command: docker ps","title":"1.2 Default bridge network"},{"location":"ycit019_Lab_3_Advanced_Docker/#13-user-defined-private-networks","text":"So far we\u2019ve learned how Docker networking works with Docker default bridge network . With the introduction of user-defined networking in Docker 1.9, it is now possible to create multiple Docker bridges to allow network segregation within the same host or multi-host networking to allow communicate Docker containers between hosts. The commands are available through the Docker Engine CLI are: docker network create docker network connect docker network ls docker network rm docker network disconnect docker network inspect Let's demonstrate how to create a custom bridge network. Step 1 By default, Docker runs containers in the bridge network. You may want to isolate one or more containers in a separate network. Let\u2019s create a new network: docker network create my-network \\ -d bridge \\ --subnet 172.19.0.0/16 The -d bridge command line argument specifies the bridge network driver and the --subnet command line argument specifies the network segment in CIDR format. If you do not specify a subnet when creating a network, then Docker assigns a subnet automatically, so it is a good idea to specify a subnet to avoid potential conflicts with the existing networks. Below are some other options that are available with the bridge Driver: com.docker.network.bridge.enable_ip_masquerade: This instructs the Docker host to hide or masquerade all containers in this network behind the Docker host's interfaces if the container attempts to route off the local host . com.docker.network.bridge.name: This is the name you wish to give to the bridge. com.docker.network.bridge.enable_icc: This turns on or off Inter-Container Connectivity (ICC) mode for the bridge. com.docker.network.bridge.host_binding_ipv4: This defines the host interface that should be used for port binding. com.docker.network.driver.mtu: This sets MTU for containers attached to this bridge. Step 2 To check that the new network is created, execute docker network ls: docker network ls NETWORK ID NAME DRIVER SCOPE d428e49e4869 bridge bridge local 0d1f78528cc5 host host local 56ef0481820d my-network bridge local 4a07cef84617 none null local Step 3 Let\u2019s inspect the new network: docker network inspect my-network [ { \"Name\": \"my-network\", \"Id\": \"56ef0481820d...\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"172.19.0.0/16\" } ] }, \"Internal\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ] Step 4 As expected, there are no containers connected to the my-network. Let\u2019s recreate the db container in the my-network: docker rm -f db docker run -d --network=my-network --name db training/postgres Step 5 Inspect the my-network again: docker network inspect my-network Output: \"Containers\": { \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... As you see, the db container is connected to the my-network and has 172.19.0.2 address. Step 6 Let\u2019s start an interactive session in the db container and ping the IP address of the webapp again: Note Quick reminder how to locate webapp ip: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp docker exec -it db bash Once inside of container run: root@c3afff20019a:/# ping -c 1 172.17.0.3 PING 172.17.0.3 (172.17.0.3) 56(84) bytes of data. --- 172.17.0.3 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms As expected, the webapp container is no longer accessible from the db container, because they are connected to different networks. Summary Using Multi-host networking provides network isolation within a Docker host via network namepsaces. This is can be used if you want to deploy different applications on same host for isolation or resource duplicate prevention. Step 7 Let\u2019s connect the webapp container to the my-network: docker network connect my-network webapp Step 8 Check that the webapp container now is connected to the my-network: docker network inspect my-network Output: ... \"Containers\": { \"62ed4a627356...\": { \"Name\": \"webapp\", \"EndpointID\": \"ae95b0103bbc...\", \"MacAddress\": \"02:42:ac:12:00:03\", \"IPv4Address\": \"172.19.0.3/16\", \"IPv6Address\": \"\" }, \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... The output shows that two containers are connected to the my-network and the webapp container has 172.19.0.3 address in that network. Step 9 Check that the webapp container is accessible from the db container using its new IP address: docker exec -it db bash root@c3afff20019a:/# ping -c 1 172.19.0.3 PING 172.19.0.3 (172.19.0.3) 56(84) bytes of data. 64 bytes from 172.19.0.3: icmp_seq=1 ttl=64 time=0.136 ms --- 172.19.0.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms Success As expected containers can communicate with each other. Step 10 You can now remove the existing container. You should stop the container before removing it. Alternatively you can use the -f command line argument: docker rm -f webapp docker rm -f db docker network rm my-network Hint Use below command to delete running containers in bulk : docker rm -f $(docker ps -q) Summary It is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses","title":"1.3 User-defined Private Networks"},{"location":"ycit019_Lab_3_Advanced_Docker/#14-access-containers-from-outside","text":"External Access to the Containers can be configured via publishing mechanism. Docker provides 2 options to publish ports: -P flag publishes all exposed ports -p flag allows you to specify specific ports and interfaces to use when mapping ports. The -p flag can take several different forms with the syntax looking like this: Specify the host port and container port: \u2013p <host port>:<container port> Specify the host interface, host port, and container port: \u2013p <host IP interface>:<host port>:<container port> Specify the host interface, have Docker choose a random host port, and specify the container port: \u2013p <host IP interface>::<container port> Specify only a container port and have Docker use a random host port: \u2013p <container port> Let's test exposing containers. For that let's start a new NGINX container and map port 8080 on the Docker host to port 80 inside of the container. This means that traffic that hits the Docker host on port 8080 will be passed on to port 80 inside the container. Note If you start a new container from the official NGINX image without specifying a command to run, the container will run a basic web server on port 80. Step 1 Start a new container based off the official NGINX image by running docker run --name web1 -d -p 8080:80 nginx . docker run --name web1 -d -p 8080:80 nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 6d827a3ef358: Pull complete b556b18c7952: Pull complete 03558b976e24: Pull complete 9abee7e1ef9d: Pull complete Digest: sha256:52f84ace6ea43f2f58937e5f9fc562e99ad6876e82b99d171916c1ece587c188 Status: Downloaded newer image for nginx:latest 4e0da45b0f169f18b0e1ee9bf779500cb0f756402c0a0821d55565f162741b3e Step 2 Review the container status and port mappings by running docker ps . docker ps CONTAINER ID IMAGE COMMAND PORTS NAMES 4e0da45b0f16 nginx \"nginx -g 'daemon ...\" 443/tcp, 0.0.0.0:8080->80/tcp web1 Result The top line shows the new web1 container running NGINX. Take note of the command the container is running as well as the port mapping - 0.0.0.0:8080->80/tcp maps port 8080 on all host interfaces to port 80 inside the web1 container. This port mapping is what effectively makes the containers web service accessible from external sources (via the Docker hosts IP address on port 8080). Step 3 Test connectivity to the NGINX web server, by pasting <Public_IP:8080> of VM to the browser. Note In order to locate Public IP see the list of VMs. Alternatively from inside of VM run curl 127.0.0.1:8080 command. curl 127.0.0.1:8080 <!DOCTYPE html> <html> <Snip> <head> <title>Welcome to nginx!</title> <Snip> <p><em>Thank you for using nginx.</em></p> </body> </html> Success Both CLI and UI method works! If you try and curl the IP address on a different port number it will fail. Summary Docker provides easy way to expose containers outside of the Docker Node. This can ber used for connecting containers between each other: Between networks on the same host Between networks on different host Accessing containers from outside (e.g web site) However, port mapping is implemented via port address translation (PAT) unlike in Kubernetes which we learn soon, exposes applications via service IPs and communicates via POD IPs using (NAT) Step 4 Cleanup environment docker rm -f $(docker ps -q)","title":"1.4 Access containers from outside"},{"location":"ycit019_Lab_3_Advanced_Docker/#2-persistant-volumes","text":"","title":"2 Persistant Volumes"},{"location":"ycit019_Lab_3_Advanced_Docker/#21-storage-driver","text":"We've discussed several Storage drivers (graphdrivers) during the class. Let's find out what graphdriver is running in our Lab environment. docker info | grep Storage WARNING: No swap limit support Storage Driver: aufs Result Our Classroom is running aufs storage driver. Not a suprise as we running our Lab on Ubuntu VM. Summary Systems runnng Ubuntu or Debian ,going to run aufs graphdriver by default and will most likely meet the majority of your needs. In future overlay2 may replace aufs stay tunned!","title":"2.1 Storage driver"},{"location":"ycit019_Lab_3_Advanced_Docker/#22-persisting-data-using-volumes","text":"Docker Volumes are created and assigned when containers are started. Data Volumes allow you to map a host directory to a container for sharing data. This mapping is bi-directional. It allows data stored on the host to be accessed from within the container. It also means data saved by the process inside the container is persisted on the host.","title":"2.2 Persisting Data Using Volumes"},{"location":"ycit019_Lab_3_Advanced_Docker/#221-create-and-manage-volumes","text":"Step 1 Create a volume: docker volume create --name my-vol Step 2 List volumes: docker volume ls Output: local my-vol Step 3 Inspect a volume: docker volume inspect my-vol [ { \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\", \"Name\": \"my-vol\", \"Options\": {}, \"Scope\": \"local\" } ] Step 3 Add some data to the Mountpoint of the volume: sudo touch /var/lib/docker/volumes/my-vol/_data/test_vol sudo ls /var/lib/docker/volumes/my-vol/_data/ Step 4 Create a container busybox alpine image and attach created my-vol volume in to it: docker run -it -v my-vol:/world busybox / # ls /world test_vol / # Result Volume is mounted and test_vol file is under /world folder as expected Step 5 Try to delete the volume: docker volume rm my-vol Error response from daemon: unable to remove volume: remove my-vol: volume is in use - [6ef3055b516b306847150af8fcea796c02cd90578967802ac29c39d3a2c90102] Failure Deleting container that is attached is not permited. However you can delete with -f option Step 5 Busybox container stopped, howerver it is not deleted. Let's locate stopped busybox container and delete it: docker ps -a | grep busybox docker rm $docker_id Step 6 You can now delete my-vol Note Volume is still avaiable if needed to be reattached any time docker volume ls docker volume rm my-vol docker volume ls Summary Volumes can be craeted and managed separately from containers.","title":"2.2.1  Create and manage volumes"},{"location":"ycit019_Lab_3_Advanced_Docker/#222-start-a-container-with-a-volume","text":"If you start a container with a volume that does not yet exist, Docker creates the volume for you. Step 1 Add a data volume to a container: docker run -d -P --name webapp -v /webapp training/webapp python app.py Result Command started a new container and created a new volume inside the container at /webapp. Step 2 Locate the volume on the host using the docker inspect command: docker inspect webapp | grep -A9 Mounts ``` **Output:** ``` \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\", \"Source\": \"/var/lib/docker/volumes/39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d/_data\", \"Destination\": \"/webapp\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" Step 3 List container docker volume ls Output: DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Step 5 Alternatively, you can specify a host directory you want to use as a data volume: mkdir db docker run -d --name db -v ~/db:/db training/postgres Step 2 Start an interactive session in the db container and create a new file in the /db directory: docker exec -it db bash Type inside docker containers console: root@9a7a4fbcc929:/# cd /db root@9a7a4fbcc929:/db# touch hello_from_db_container root@9a7a4fbcc929:/db# exit Step 4 Check that the local db directory contains the new file: ls db hello_from_db_container Step 5 Check that the data volume is persistent. Remove the db container: docker rm -f db Step 6 Create the db container again: docker run -d --name db -v ~/db:/db training/postgres Step 7 Check that its /db directory contains the hello_from_db_container file: docker exec -it db bash Run commands inside container: root@47a60c01590e:/# ls /db hello_from_db_container root@47a60c01590e:/# exit","title":"2.2.2 Start a container with a volume"},{"location":"ycit019_Lab_3_Advanced_Docker/#223-use-a-read-only-volume","text":"Step 1 Mounting Volumes gives the container full read and write access to the directory. You can specify read-only permissions on the directory by adding the permissions :ro to the mount. If the container attempts to modify data within the directory it will error. docker run -d --name db1 -v ~/db:/db:ro training/postgres docker exec -it db1 bash cd db touch test Result touch: cannot touch 'test': Read-only file system $ exit Step 2 Clean up containers and volumes: docker rm -f $(docker ps -q) docker volume ls Output DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d docker volume rm 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Summary We've learned how to manage volumes with containers Hint If you Docker host has several Storage plugins configured (e.g. ceph, gluster) you can specify via --opt type=btrfs, nfs or --driver=glusterfs during docker volume creation.","title":"2.2.3 Use a read-only volume"},{"location":"ycit019_Lab_4_Docker_Images/","text":"Lab 4 Managing Docker Images Objective: Learn to build docker images using Dockerfiles. Store images in Docker Hub Learn alternative registry solutions (GCR) Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1 Distributing Docker images with Container Registry \u00b6 In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: https://github.com/Cloud-Architects-Program/ycit019/tree/main/Module4/flask-app 1.1 Create DOCKERFILE \u00b6 Step 1 Clone git repo on you laptop: git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\" , \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\" , \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\" , \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\" , \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\" , \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\" , \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\" , \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\" , \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\" , \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==2.0.0 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # Official Python Alpine Base image using Simple Tags # Image contains Python 3 and pip pre-installed, so no need to install them FROM python:3.9.5-alpine3.12 # Specify Working directory WORKDIR /usr/src/app # COPY requirements.txt /usr/src/app/ COPY requirements.txt ./ # Install Python Flask used by the Python app RUN pip install --no-cache-dir -r requirements.txt # Copy files required for the app to run COPY app.py ./ COPY templates/index.html ./templates/ # Make a record that the port number the container should be expose is: EXPOSE 5000 # run the application CMD [ \"python\" , \"./app.py\" ] 1.2 Build a Docker image \u00b6 Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step to your own Docker Hub. docker build -t <Docker-hub-user-name>/myfirstapp . Result Image has been buit Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 8080:5000 --name myfirstapp <Docker-hub-user-name>/myfirstapp Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Stop the container and remove it: docker rm -f myfirstapp 1.2.2 Publish Docker Image to Docker Hub \u00b6 One of the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <Docker-hub-user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry Note Image Tag of the created myfirstapp : docker images Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag <Docker-hub-user-name>/myfirstapp:v1 docker push <Docker-hub-user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <Docker-hub-user-name>/myfirstapp:latest docker pull <Docker-hub-user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <Docker-hub-user-name>/myfirstapp:v1 1.2.3 Pushing images to gcr.io \u00b6 In a similar manner we need to tag the image to prepare it to be pushed to gcr.io. We just need to change the registry, which is for gcr.io formatted as gcr.io/PROJECT_ID. Step 1 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 2 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 3 Tag the image: Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag gcr.io/${PROJECT_ID}/myfirstapp:v1 $docker_image_tag Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/myfirstapp:v1 Step 4 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 1.2.3 Pushing images to Local Repository \u00b6 First, we need to spin up a local docker registry. This could be a use case if you want to deploy basic registry On-Prem. This registry will luck security features such as Authentication, SSL, scanning. If you interested to use Enterprise ready solution On-Prem consider: Jfrog Artifactory, RedHa's Clair, Docker Enterprise or open source CNCF project Harbor. Step 1 Deploy local registry docker run -d -p 5000:5000 --name registry registry:2.7.1 Step 2 In order to upload an image to a registry, we need to tag it properly Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag localhost:5000/myfirstapp:v1 Step 3 Now that we have an image tagged correctly, we can push it to our local registry docker push localhost:5000/myfirstapp:v1 Step 4 Let\u2019s now delete the local image, and pull it again from the local registry To delete the image, we need to first remove the container that depends on that image. Run docker ps and get the Container_ID for the container that uses myfirstapp:v1 Kill and delete that container by running the following command, but make sure to replace CONTAINER_ID, with the actual ID. docker rm CONTAINER_ID Result: The command will print back the container ID, which is an indication it was successful. Step 5 Run docker images to validate docker images Step 6 Now we can delete the docker image docker rmi localhost:5000/myfirstapp:v1 Step 7 Although the image is deleted locally, it is still in the registry and we can pull it back, or use it to deploy containers. docker run -dp 8080:5000 --name myfirstapp localhost:5000/myfirstapp:v1 Run docker images again to check how the image is available locally again. docker images Step 8 Cleanup: docker rm -f myfirstapp 2 Follow Docker Best Practices \u00b6 2.1 Inspecting Dockerfiles with dockle \u00b6 Dockle - Container Image Linter for Security, Helping build the Best-Practice Docker Image, Easy to start Dockle helps you: Build Best Practice Docker images Build secure Docker images Checkpoints includes CIS Benchmarks Step 1 Install Dockle $ VERSION=$( curl --silent \"https://api.github.com/repos/goodwithtech/dockle/releases/latest\" | \\ grep '\"tag_name\":' | \\ sed -E 's/.*\"v([^\"]+)\".*/\\1/' \\ ) && curl -L -o dockle.deb https://github.com/goodwithtech/dockle/releases/download/v${VERSION}/dockle_${VERSION}_Linux-64bit.deb $ sudo dpkg -i dockle.deb && rm dockle.deb Step 2 Experiment with existing applications we've created in the class: $ dockle [YOUR_IMAGE_NAME] e.g. dockle archy/myfirstapp output: WARN - CIS-DI-0001: Create a user for the container * Last user should not be root WARN - DKL-DI-0006: Avoid latest tag * Avoid 'latest' tag INFO - CIS-DI-0005: Enable Content trust for Docker * export DOCKER_CONTENT_TRUST=1 before docker pull/build INFO - CIS-DI-0006: Add HEALTHCHECK instruction to the container image * not found HEALTHCHECK statement INFO - DKL-LI-0003: Only put necessary files * Suspicious directory : tmp 2.2 Automated Builds with Google Cloud Build \u00b6 Live Demo: GCR Image scanning Setting Up Docker Image Auto-Build with Google Cloud Build based on Push to Branch Auto Deployment of Image to Cloud Run","title":"Lab 4 Managing Docker Images"},{"location":"ycit019_Lab_4_Docker_Images/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ycit019_Lab_4_Docker_Images/#1-distributing-docker-images-with-container-registry","text":"In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: https://github.com/Cloud-Architects-Program/ycit019/tree/main/Module4/flask-app","title":"1 Distributing Docker images with Container Registry"},{"location":"ycit019_Lab_4_Docker_Images/#11-create-dockerfile","text":"Step 1 Clone git repo on you laptop: git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\" , \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\" , \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\" , \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\" , \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\" , \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\" , \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\" , \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\" , \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\" , \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==2.0.0 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # Official Python Alpine Base image using Simple Tags # Image contains Python 3 and pip pre-installed, so no need to install them FROM python:3.9.5-alpine3.12 # Specify Working directory WORKDIR /usr/src/app # COPY requirements.txt /usr/src/app/ COPY requirements.txt ./ # Install Python Flask used by the Python app RUN pip install --no-cache-dir -r requirements.txt # Copy files required for the app to run COPY app.py ./ COPY templates/index.html ./templates/ # Make a record that the port number the container should be expose is: EXPOSE 5000 # run the application CMD [ \"python\" , \"./app.py\" ]","title":"1.1 Create DOCKERFILE"},{"location":"ycit019_Lab_4_Docker_Images/#12-build-a-docker-image","text":"Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step to your own Docker Hub. docker build -t <Docker-hub-user-name>/myfirstapp . Result Image has been buit Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 8080:5000 --name myfirstapp <Docker-hub-user-name>/myfirstapp Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Stop the container and remove it: docker rm -f myfirstapp","title":"1.2 Build a Docker image"},{"location":"ycit019_Lab_4_Docker_Images/#122-publish-docker-image-to-docker-hub","text":"One of the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <Docker-hub-user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry Note Image Tag of the created myfirstapp : docker images Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag <Docker-hub-user-name>/myfirstapp:v1 docker push <Docker-hub-user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <Docker-hub-user-name>/myfirstapp:latest docker pull <Docker-hub-user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <Docker-hub-user-name>/myfirstapp:v1","title":"1.2.2 Publish Docker Image to Docker Hub"},{"location":"ycit019_Lab_4_Docker_Images/#123-pushing-images-to-gcrio","text":"In a similar manner we need to tag the image to prepare it to be pushed to gcr.io. We just need to change the registry, which is for gcr.io formatted as gcr.io/PROJECT_ID. Step 1 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 2 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 3 Tag the image: Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag gcr.io/${PROJECT_ID}/myfirstapp:v1 $docker_image_tag Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/myfirstapp:v1 Step 4 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"1.2.3 Pushing images to gcr.io"},{"location":"ycit019_Lab_4_Docker_Images/#123-pushing-images-to-local-repository","text":"First, we need to spin up a local docker registry. This could be a use case if you want to deploy basic registry On-Prem. This registry will luck security features such as Authentication, SSL, scanning. If you interested to use Enterprise ready solution On-Prem consider: Jfrog Artifactory, RedHa's Clair, Docker Enterprise or open source CNCF project Harbor. Step 1 Deploy local registry docker run -d -p 5000:5000 --name registry registry:2.7.1 Step 2 In order to upload an image to a registry, we need to tag it properly Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag localhost:5000/myfirstapp:v1 Step 3 Now that we have an image tagged correctly, we can push it to our local registry docker push localhost:5000/myfirstapp:v1 Step 4 Let\u2019s now delete the local image, and pull it again from the local registry To delete the image, we need to first remove the container that depends on that image. Run docker ps and get the Container_ID for the container that uses myfirstapp:v1 Kill and delete that container by running the following command, but make sure to replace CONTAINER_ID, with the actual ID. docker rm CONTAINER_ID Result: The command will print back the container ID, which is an indication it was successful. Step 5 Run docker images to validate docker images Step 6 Now we can delete the docker image docker rmi localhost:5000/myfirstapp:v1 Step 7 Although the image is deleted locally, it is still in the registry and we can pull it back, or use it to deploy containers. docker run -dp 8080:5000 --name myfirstapp localhost:5000/myfirstapp:v1 Run docker images again to check how the image is available locally again. docker images Step 8 Cleanup: docker rm -f myfirstapp","title":"1.2.3 Pushing images to Local Repository"},{"location":"ycit019_Lab_4_Docker_Images/#2-follow-docker-best-practices","text":"","title":"2 Follow Docker Best Practices"},{"location":"ycit019_Lab_4_Docker_Images/#21-inspecting-dockerfiles-with-dockle","text":"Dockle - Container Image Linter for Security, Helping build the Best-Practice Docker Image, Easy to start Dockle helps you: Build Best Practice Docker images Build secure Docker images Checkpoints includes CIS Benchmarks Step 1 Install Dockle $ VERSION=$( curl --silent \"https://api.github.com/repos/goodwithtech/dockle/releases/latest\" | \\ grep '\"tag_name\":' | \\ sed -E 's/.*\"v([^\"]+)\".*/\\1/' \\ ) && curl -L -o dockle.deb https://github.com/goodwithtech/dockle/releases/download/v${VERSION}/dockle_${VERSION}_Linux-64bit.deb $ sudo dpkg -i dockle.deb && rm dockle.deb Step 2 Experiment with existing applications we've created in the class: $ dockle [YOUR_IMAGE_NAME] e.g. dockle archy/myfirstapp output: WARN - CIS-DI-0001: Create a user for the container * Last user should not be root WARN - DKL-DI-0006: Avoid latest tag * Avoid 'latest' tag INFO - CIS-DI-0005: Enable Content trust for Docker * export DOCKER_CONTENT_TRUST=1 before docker pull/build INFO - CIS-DI-0006: Add HEALTHCHECK instruction to the container image * not found HEALTHCHECK statement INFO - DKL-LI-0003: Only put necessary files * Suspicious directory : tmp","title":"2.1 Inspecting Dockerfiles with dockle"},{"location":"ycit019_Lab_4_Docker_Images/#22-automated-builds-with-google-cloud-build","text":"Live Demo: GCR Image scanning Setting Up Docker Image Auto-Build with Google Cloud Build based on Push to Branch Auto Deployment of Image to Cloud Run","title":"2.2 Automated Builds with Google Cloud Build"},{"location":"ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/","text":"Lab 4 Managing Docker Images Objective: Learn to build docker images using Dockerfiles. Store images in Docker Hub Learn alternative registry solutions (Quya.io) Automate image build process with Docker Cloud 1 Building Docker Images \u00b6 In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a registry of Docker images. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: git clone https://github.com/archyufa/k8scanada 1.1 Create DOCKERFILE \u00b6 Step 1 Clone git repo on you laptop: git clone https://github.com/archyufa/k8scanada cd k8scanada/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26388-1381844103-11.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr01/15/9/anigif_enhanced-buzz-31540-1381844535-8.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26390-1381844163-18.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-1376-1381846217-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3391-1381844336-26.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-29111-1381845968-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3409-1381844582-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr02/15/9/anigif_enhanced-buzz-19667-1381844937-10.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26358-1381845043-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-18774-1381844645-6.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-25158-1381844793-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/10/anigif_enhanced-buzz-11980-1381846269-1.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==0.10.1 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # our base image FROM alpine:3.5 # Install python and pip RUN apk add --update py2-pip # upgrade pip RUN pip install --upgrade pip # install Python modules needed by the Python app COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt # copy files required for the app to run COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ # tell the port number the container should expose EXPOSE 5000 # run the application CMD [ \"python\" , \"/usr/src/app/app.py\" ] 1.2 Build a Docker image \u00b6 Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step. docker build -t <user-name>/myfirstapp . Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 80:5000 --name myfirstapp <user-name>/myfirstapp Step 4 Use your browser to open the address http:// and check that the application works. Step 5 Stop the container and remove it: docker stop myfirstapp docker rm myfirstapp myfirstapp 1.2.1 Share docker images with tar files \u00b6 Now ideally you want to share you freshly build docker image with someone or run it in different environment which for some reason don\u2019t have internet access so images can not be pulled from Online Docker registries. In that case Docker images can be shared as we share traditionally regular files by creating tarballs using docker save command as following Step 1 Create a tar file using docker save command: docker save <user-name>/myfirstapp > myfirstapp.tar Or docker save --output myfirstapp1.tar archyufa/myfirstapp ls -trh | grep tar Step 2 Transfer images to another environment using scp command. Hint You can also store this images in Object storages, e.g. Swift or Amazon S3 using version control. Step 3 Now you can restore this images using docker load, that will load a tarred repository from a file or the standard input stream. It restores both images and tags. docker load < myfirstapp.tar 23b9c7b43573: Loading layer [==================================================>] 4.23MB/4.23MB b3b5c1214f71: Loading layer [==================================================>] 52.87MB/52.87MB f877d8dd64d3: Loading layer [==================================================>] 8.636MB/8.636MB bba871f91589: Loading layer [==================================================>] 3.584kB/3.584kB 1c131e92eb5f: Loading layer [==================================================>] 5.053MB/5.053MB 3f6463bcb64c: Loading layer [==================================================>] 5.12kB/5.12kB 47c61110467a: Loading layer [==================================================>] 4.096kB/4.096kB Loaded image: archyufa/myfirstapp:latest docker images 1.2.2 Publish Docker Image to Docker Hub \u00b6 However the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry docker tag 5b45ce063cea <user-name>/myfirstapp:v1 docker push <user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <user-name>/myfirstapp:latest docker pull <user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <user-name>/myfirstapp:v1 1.2.3 Automated Builds with Docker Cloud \u00b6 Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud and notifications to slac Automated Tests with PR 1.2.4 Push Docker Images to quay.io \u00b6 Prerequisite: Register to quay.io with your Github user Step 1 Login to quay.io from CLI docker login quay.io Step 2 Build image with quay prefix docker build -t quay.io/archyufa/myfirstapp . Step 3 Push image to quay registry docker push quay.io/archyufa/myfirstapp 1.2.5 Demo quay.io \u00b6 Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud / Quai.io","title":"ycit019 Lab 4 Docker Images Docker Hub Quya io"},{"location":"ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#1-building-docker-images","text":"In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a registry of Docker images. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: git clone https://github.com/archyufa/k8scanada","title":"1 Building Docker Images"},{"location":"ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#11-create-dockerfile","text":"Step 1 Clone git repo on you laptop: git clone https://github.com/archyufa/k8scanada cd k8scanada/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26388-1381844103-11.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr01/15/9/anigif_enhanced-buzz-31540-1381844535-8.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26390-1381844163-18.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-1376-1381846217-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3391-1381844336-26.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-29111-1381845968-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3409-1381844582-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr02/15/9/anigif_enhanced-buzz-19667-1381844937-10.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26358-1381845043-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-18774-1381844645-6.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-25158-1381844793-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/10/anigif_enhanced-buzz-11980-1381846269-1.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==0.10.1 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # our base image FROM alpine:3.5 # Install python and pip RUN apk add --update py2-pip # upgrade pip RUN pip install --upgrade pip # install Python modules needed by the Python app COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt # copy files required for the app to run COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ # tell the port number the container should expose EXPOSE 5000 # run the application CMD [ \"python\" , \"/usr/src/app/app.py\" ]","title":"1.1 Create DOCKERFILE"},{"location":"ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#12-build-a-docker-image","text":"Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step. docker build -t <user-name>/myfirstapp . Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 80:5000 --name myfirstapp <user-name>/myfirstapp Step 4 Use your browser to open the address http:// and check that the application works. Step 5 Stop the container and remove it: docker stop myfirstapp docker rm myfirstapp myfirstapp","title":"1.2 Build a Docker image"},{"location":"ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#121-share-docker-images-with-tar-files","text":"Now ideally you want to share you freshly build docker image with someone or run it in different environment which for some reason don\u2019t have internet access so images can not be pulled from Online Docker registries. In that case Docker images can be shared as we share traditionally regular files by creating tarballs using docker save command as following Step 1 Create a tar file using docker save command: docker save <user-name>/myfirstapp > myfirstapp.tar Or docker save --output myfirstapp1.tar archyufa/myfirstapp ls -trh | grep tar Step 2 Transfer images to another environment using scp command. Hint You can also store this images in Object storages, e.g. Swift or Amazon S3 using version control. Step 3 Now you can restore this images using docker load, that will load a tarred repository from a file or the standard input stream. It restores both images and tags. docker load < myfirstapp.tar 23b9c7b43573: Loading layer [==================================================>] 4.23MB/4.23MB b3b5c1214f71: Loading layer [==================================================>] 52.87MB/52.87MB f877d8dd64d3: Loading layer [==================================================>] 8.636MB/8.636MB bba871f91589: Loading layer [==================================================>] 3.584kB/3.584kB 1c131e92eb5f: Loading layer [==================================================>] 5.053MB/5.053MB 3f6463bcb64c: Loading layer [==================================================>] 5.12kB/5.12kB 47c61110467a: Loading layer [==================================================>] 4.096kB/4.096kB Loaded image: archyufa/myfirstapp:latest docker images","title":"1.2.1 Share docker images with tar files"},{"location":"ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#122-publish-docker-image-to-docker-hub","text":"However the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry docker tag 5b45ce063cea <user-name>/myfirstapp:v1 docker push <user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <user-name>/myfirstapp:latest docker pull <user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <user-name>/myfirstapp:v1","title":"1.2.2 Publish Docker Image to Docker Hub"},{"location":"ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#123-automated-builds-with-docker-cloud","text":"Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud and notifications to slac Automated Tests with PR","title":"1.2.3 Automated Builds with Docker Cloud"},{"location":"ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#124-push-docker-images-to-quayio","text":"Prerequisite: Register to quay.io with your Github user Step 1 Login to quay.io from CLI docker login quay.io Step 2 Build image with quay prefix docker build -t quay.io/archyufa/myfirstapp . Step 3 Push image to quay registry docker push quay.io/archyufa/myfirstapp","title":"1.2.4 Push Docker Images to quay.io"},{"location":"ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#125-demo-quayio","text":"Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud / Quai.io","title":"1.2.5 Demo quay.io"},{"location":"ycit019_Lab_5_Docker_Compose/","text":"Lab 5 Docker Compose and Docker Security Objective: Practice to use Docker Compose, 1 Docker Security \u00b6 1.1 Scan images with Trivy \u00b6 Trivy (tri pronounced like trigger, vy pronounced like envy) is a simple and comprehensive vulnerability scanner for containers and other artifacts. A software vulnerability is a glitch, flaw, or weakness present in the software or in an Operating System. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and application dependencies (Bundler, Composer, npm, yarn, etc.). Trivy is easy to use. Just install the binary and you're ready to scan. All you need to do for scanning is to specify a target such as an image name of the container. Step 1 Install Trivy sudo apt-get install wget apt-transport-https gnupg lsb-release wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add - echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list sudo apt-get update sudo apt-get install trivy Step 2 Specify an image name (and a tag). $ trivy image [YOUR_IMAGE_NAME] For example: $ trivy image python:3.4-alpine 2019-05-16T01:20:43.180+0900 INFO Updating vulnerability database... 2019-05-16T01:20:53.029+0900 INFO Detecting Alpine vulnerabilities... python:3.4-alpine3.9 (alpine 3.9.2) =================================== Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0) +---------+------------------+----------+-------------------+---------------+--------------------------------+ | LIBRARY | VULNERABILITY ID | SEVERITY | INSTALLED VERSION | FIXED VERSION | TITLE | +---------+------------------+----------+-------------------+---------------+--------------------------------+ | openssl | CVE-2019-1543 | MEDIUM | 1.1.1a-r1 | 1.1.1b-r1 | openssl: ChaCha20-Poly1305 | | | | | | | with long nonces | +---------+------------------+----------+-------------------+---------------+--------------------------------+ Step 3 Explore local images in your environment. 2 Docker Compose \u00b6 In this module, will guide you through the process of building a multi-container application using docker compose. The application code is available at GitHub: https://github.com/Cloud-Architects-Program/ycit019 2.1 Deploy Guestbook app with Compose \u00b6 Let\u2019s build another application. This time we going to create famous Guestbook application. Guestbook consists of three services. A redis-master node, a set of redis-slave that can be scaled and find the redis-master via its DNS name. And a PHP frontend that exposes itself on port 80. The resulting application allows you to leave short messages which are stored in the redis cluster. Step 1 Change directory to the guestbook cd ~/ycit019/Module5/guestbook/ ls Step 2 Let\u2019s review the docker-guestbook.yml file version: \"2\" services: redis-master: image: gcr.io/google_containers/redis:e2e ports: - \"6379\" redis-slave: image: gcr.io/google_samples/gb-redisslave:v1 ports: - \"6379\" environment: - GET_HOSTS_FROM=dns frontend: image: gcr.io/google-samples/gb-frontend:v4 ports: - \"80:80\" environment: - GET_HOSTS_FROM=dns Step 3 Let\u2019s run docker-guestbook.yml with compose export LD_LIBRARY_PATH=/usr/local/lib docker-compose -f docker-guestbook.yml up -d Creating network \"examples_default\" with the default driver Creating examples_redis-slave_1 Creating examples_frontend_1 Creating examples_redis-master_1 Note -d - Detached mode: Run containers in the background, print new container names. -f - Specify an alternate compose file (default: docker-compose.yml) Step 4 Check that all containers are running: docker ps CONTAINER ID IMAGE COMMAND d1006d1beee5 gcr.io/google-samples/gb-frontend:v4 \"apache2-foreground\" fb3a15fde23f gcr.io/google_containers/redis:e2e \"redis-server /etc...\" 326b94d4cdd7 gcr.io/google_samples/gb-redisslave:v1 \"/entrypoint.sh /b...\" Step 5 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Success Nice you now have compose stuck up and running! Step 6 Cleanup environment: docker-compose -f docker-guestbook.yml down Stopping guestbook_frontend_1 ... done Stopping guestbook_redis-master_1 ... done Stopping guestbook_redis-slave_1 ... done Removing guestbook_frontend_1 ... done Removing guestbook_redis-master_1 ... done Removing guestbook_redis-slave_1 ... done Removing network guestbook_default 2.2 Deploy Voting App using Compose \u00b6 Step 1 Switch to Module5/example-voting-app folder : cd ~/ycit019/Module5/example-voting-app/ Step 2 The existing file docker-compose.yml defines several images: A voting-app container based on a Python image A result-app container based on a Node.js image A Redis container based on a redis image, to temporarily store the data. A worker app based on a dotnet image A Postgres container based on a postgres image App Architecture: Note that three of the containers are built from Dockerfiles, while the other two are images on Docker Hub. Let's review them closely: Step 3 Review files that going to be deployed with tree command. Alternatively view the files in gitrepo page here sudo apt install tree tree Step 5 Let\u2019s change the default port to expose. Edit the docker-compose.yml file and find the following lines: ports: - \"5000:80\" Change 5000 to 8080: ports: - \"8080:80\" Step 4 Verify Docker Compose version: docker-compose version Step 5 Use the docker-compose tool to launch your application: docker-compose up -d Step 6 Check that all containers are running, volumes created. Check compose state and logs : #Docker state docker ps docker volumes #Docker compose state docker-compose ps docker-compose logs Step 7 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 8 Cleanup up. docker-compose down Stopping examplevotingapp_worker_1 ... done Stopping examplevotingapp_redis_1 ... done Stopping examplevotingapp_result_1 ... done Stopping examplevotingapp_db_1 ... done Stopping examplevotingapp_vote_1 ... done Removing examplevotingapp_worker_1 ... done Removing examplevotingapp_redis_1 ... done Removing examplevotingapp_result_1 ... done Removing examplevotingapp_db_1 ... done Removing examplevotingapp_vote_1 ... done Removing network examplevotingapp_default Step 9 You Boss told you that the application has a bug. Update the the app by editing the vote/app.py file and change the following lines near the top of the file: vim vote/app.py Press 'i' option_a = os.getenv('OPTION_A', \"Cats\") option_b = os.getenv('OPTION_B', \"Dogs\") Step 10 Replace \u201cCats\u201d and \u201cDogs\u201d with two options of your choice. For example: option_a = os.getenv('OPTION_A', \"Java\") option_b = os.getenv('OPTION_B', \"Python\") Press 'wq!' Step 11 Use docker-compose tool to launch your Update application: docker-compose up -d Check the UI Bingo Let's see who wins the battle of Orchestrations! Step 8 Cleanup up docker-compose down Congratulations You are now docker expert! We were able to start 2 microservices application with docker compose. First microservice had 3 services. Second microservice had 5 servics written in 3 different languages and able to talk to each other. Summary So far we've learned docker-compose v2. docker-compose v3 is out of scope for this Lab. However you got the idea! Read the Docker-Compose documentation on new syntax. Also example of v3 version of voting-app is here for you reference.","title":"Lab 5 Docker Compose"},{"location":"ycit019_Lab_5_Docker_Compose/#1-docker-security","text":"","title":"1 Docker Security"},{"location":"ycit019_Lab_5_Docker_Compose/#11-scan-images-with-trivy","text":"Trivy (tri pronounced like trigger, vy pronounced like envy) is a simple and comprehensive vulnerability scanner for containers and other artifacts. A software vulnerability is a glitch, flaw, or weakness present in the software or in an Operating System. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and application dependencies (Bundler, Composer, npm, yarn, etc.). Trivy is easy to use. Just install the binary and you're ready to scan. All you need to do for scanning is to specify a target such as an image name of the container. Step 1 Install Trivy sudo apt-get install wget apt-transport-https gnupg lsb-release wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add - echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list sudo apt-get update sudo apt-get install trivy Step 2 Specify an image name (and a tag). $ trivy image [YOUR_IMAGE_NAME] For example: $ trivy image python:3.4-alpine 2019-05-16T01:20:43.180+0900 INFO Updating vulnerability database... 2019-05-16T01:20:53.029+0900 INFO Detecting Alpine vulnerabilities... python:3.4-alpine3.9 (alpine 3.9.2) =================================== Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0) +---------+------------------+----------+-------------------+---------------+--------------------------------+ | LIBRARY | VULNERABILITY ID | SEVERITY | INSTALLED VERSION | FIXED VERSION | TITLE | +---------+------------------+----------+-------------------+---------------+--------------------------------+ | openssl | CVE-2019-1543 | MEDIUM | 1.1.1a-r1 | 1.1.1b-r1 | openssl: ChaCha20-Poly1305 | | | | | | | with long nonces | +---------+------------------+----------+-------------------+---------------+--------------------------------+ Step 3 Explore local images in your environment.","title":"1.1 Scan images with Trivy"},{"location":"ycit019_Lab_5_Docker_Compose/#2-docker-compose","text":"In this module, will guide you through the process of building a multi-container application using docker compose. The application code is available at GitHub: https://github.com/Cloud-Architects-Program/ycit019","title":"2 Docker Compose"},{"location":"ycit019_Lab_5_Docker_Compose/#21-deploy-guestbook-app-with-compose","text":"Let\u2019s build another application. This time we going to create famous Guestbook application. Guestbook consists of three services. A redis-master node, a set of redis-slave that can be scaled and find the redis-master via its DNS name. And a PHP frontend that exposes itself on port 80. The resulting application allows you to leave short messages which are stored in the redis cluster. Step 1 Change directory to the guestbook cd ~/ycit019/Module5/guestbook/ ls Step 2 Let\u2019s review the docker-guestbook.yml file version: \"2\" services: redis-master: image: gcr.io/google_containers/redis:e2e ports: - \"6379\" redis-slave: image: gcr.io/google_samples/gb-redisslave:v1 ports: - \"6379\" environment: - GET_HOSTS_FROM=dns frontend: image: gcr.io/google-samples/gb-frontend:v4 ports: - \"80:80\" environment: - GET_HOSTS_FROM=dns Step 3 Let\u2019s run docker-guestbook.yml with compose export LD_LIBRARY_PATH=/usr/local/lib docker-compose -f docker-guestbook.yml up -d Creating network \"examples_default\" with the default driver Creating examples_redis-slave_1 Creating examples_frontend_1 Creating examples_redis-master_1 Note -d - Detached mode: Run containers in the background, print new container names. -f - Specify an alternate compose file (default: docker-compose.yml) Step 4 Check that all containers are running: docker ps CONTAINER ID IMAGE COMMAND d1006d1beee5 gcr.io/google-samples/gb-frontend:v4 \"apache2-foreground\" fb3a15fde23f gcr.io/google_containers/redis:e2e \"redis-server /etc...\" 326b94d4cdd7 gcr.io/google_samples/gb-redisslave:v1 \"/entrypoint.sh /b...\" Step 5 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Success Nice you now have compose stuck up and running! Step 6 Cleanup environment: docker-compose -f docker-guestbook.yml down Stopping guestbook_frontend_1 ... done Stopping guestbook_redis-master_1 ... done Stopping guestbook_redis-slave_1 ... done Removing guestbook_frontend_1 ... done Removing guestbook_redis-master_1 ... done Removing guestbook_redis-slave_1 ... done Removing network guestbook_default","title":"2.1 Deploy Guestbook app with Compose"},{"location":"ycit019_Lab_5_Docker_Compose/#22-deploy-voting-app-using-compose","text":"Step 1 Switch to Module5/example-voting-app folder : cd ~/ycit019/Module5/example-voting-app/ Step 2 The existing file docker-compose.yml defines several images: A voting-app container based on a Python image A result-app container based on a Node.js image A Redis container based on a redis image, to temporarily store the data. A worker app based on a dotnet image A Postgres container based on a postgres image App Architecture: Note that three of the containers are built from Dockerfiles, while the other two are images on Docker Hub. Let's review them closely: Step 3 Review files that going to be deployed with tree command. Alternatively view the files in gitrepo page here sudo apt install tree tree Step 5 Let\u2019s change the default port to expose. Edit the docker-compose.yml file and find the following lines: ports: - \"5000:80\" Change 5000 to 8080: ports: - \"8080:80\" Step 4 Verify Docker Compose version: docker-compose version Step 5 Use the docker-compose tool to launch your application: docker-compose up -d Step 6 Check that all containers are running, volumes created. Check compose state and logs : #Docker state docker ps docker volumes #Docker compose state docker-compose ps docker-compose logs Step 7 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 8 Cleanup up. docker-compose down Stopping examplevotingapp_worker_1 ... done Stopping examplevotingapp_redis_1 ... done Stopping examplevotingapp_result_1 ... done Stopping examplevotingapp_db_1 ... done Stopping examplevotingapp_vote_1 ... done Removing examplevotingapp_worker_1 ... done Removing examplevotingapp_redis_1 ... done Removing examplevotingapp_result_1 ... done Removing examplevotingapp_db_1 ... done Removing examplevotingapp_vote_1 ... done Removing network examplevotingapp_default Step 9 You Boss told you that the application has a bug. Update the the app by editing the vote/app.py file and change the following lines near the top of the file: vim vote/app.py Press 'i' option_a = os.getenv('OPTION_A', \"Cats\") option_b = os.getenv('OPTION_B', \"Dogs\") Step 10 Replace \u201cCats\u201d and \u201cDogs\u201d with two options of your choice. For example: option_a = os.getenv('OPTION_A', \"Java\") option_b = os.getenv('OPTION_B', \"Python\") Press 'wq!' Step 11 Use docker-compose tool to launch your Update application: docker-compose up -d Check the UI Bingo Let's see who wins the battle of Orchestrations! Step 8 Cleanup up docker-compose down Congratulations You are now docker expert! We were able to start 2 microservices application with docker compose. First microservice had 3 services. Second microservice had 5 servics written in 3 different languages and able to talk to each other. Summary So far we've learned docker-compose v2. docker-compose v3 is out of scope for this Lab. However you got the idea! Read the Docker-Compose documentation on new syntax. Also example of v3 version of voting-app is here for you reference.","title":"2.2 Deploy Voting App using Compose"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/","text":"Deploy Kubernetes \u00b6 In this Lab, we are going to: Deploy Single Node Kubernetes cluster using kubeadm on a Google Compute Engine node Deploy an application to Kubernetes 1. Deploy Kubernetes and Calico with Kubeadm \u00b6 In general to deploy Kubernetes with kubeadm it is required to use following official Kubernetes documentation . 1.1 Create a VM, and ssh into it \u00b6 Step 1 Create the instance gcloud compute instances create k8s-cluster \\ --zone us-central1-c \\ --machine-type=e2-standard-4 \\ --image=ubuntu-1804-bionic-v20210514 \\ --image-project=ubuntu-os-cloud Step 2 Capture the private IP address of the VM Record the IP address of the node, as we will need later. Step 3 Once the vm is created, you can ssh into it gcloud compute ssh k8s-cluster --zone us-central1-c You can also ssh into the node through the console if you prefer that. Step 4 Define the NODE_IP as an environment variable. export NODE_IP=<REPLACE_WITH_NODE_PRIVATE_IP> 1.2 Let iptables see bridged traffic \u00b6 cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 1.3 Install Docker \u00b6 Step 1 Update the apt package index and install packages: sudo su apt-get update apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Step 2 Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Step 3 Setup the stable repository echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Step 4 Install Docker Engine and containerd apt-get update apt-get install docker-ce docker-ce-cli containerd.io Step 5 Configure the Docker daemon to use systemd for the management of the container's cgroups. mkdir /etc/docker cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF Step 6 Restart Docker and enable on boot: systemctl enable docker systemctl daemon-reload systemctl restart docker 1.4 Install kubeadm and prerequisite packages on each node \u00b6 The next step is to install kubeadm and prerequisite packages as showed here. Step 1 Deploy kubeadm and prerequisite packages apt-get update && apt-get install -y apt-transport-https ca-certificates curl curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list apt-get update && apt-get install -y kubelet=1.20.6-00 kubeadm=1.20.6-00 kubectl=1.20.6-00 apt-mark hold kubelet kubeadm kubectl Step 2 Verify kubeadm version kubeadm version Output: kubeadm version: &version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:26:21Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"} Result Latest available version of Kubernetes/kubeadm has been installed from (GitHub Kubernetes repo release page.)[https://github.com/kubernetes/kubernetes/releases] 1.5 'Kubeadm init' the Master \u00b6 Run On the Master node only: Step 1: Build kubeadm Custom Config cat <<EOF > kubeadm.conf kind: ClusterConfiguration apiVersion: kubeadm.k8s.io/v1beta2 apiServer: extraArgs: advertise-address: $NODE_IP kubernetesVersion: v1.20.6 --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd EOF Make sure the IP address was updated: cat kubeadm.conf Note To expose custom Config you can create a kubeadm.conf and specify during kubecadm init execution. For instance: * ControllerManager configs * Custom Subnet * Custom version * Apiserver configs such as authentication, authorization and etc. Step 2: Create a cluster kubeadm init --config=kubeadm.conf Result Once the command completes, configure the KUBECONFIG env variable with the path to admin.conf (recommend adding it to your .bashrc): mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config export KUBECONFIG=$HOME/.kube/config Let's validate that the installation was successful. You should now be able to run kubectl commands and see that all cluster Pods are running (except DNS one): watch kubectl get pods --all-namespaces -o wide To exit back to the terminal, press ctrl+c 1.6 Deploy Calico Networking (via self-hosted, as a daemon set) \u00b6 Step 1 Lets download the Calico manifest: curl https://docs.projectcalico.org/manifests/calico.yaml -O Note You can customize Calico deployments based on you needs. For instance by changing: (Optional) In the manifest, change CALICO_IPV4POOL_IPIP from \"always\" to \"cross-subnet\". (Optional) In the manifest, change FELIX_IPINIPMTU to match your ethernet interface mtu (Optional) If you want a different address range for containers, change CALICO_IPV4POOL_CIDR to the same cidr range used in kubeadm init, for e.g., \"10.6.0.0/16\" This time we are not going to modify defaults Calico values. Step 2 Now lets deploy Calico as a daemon set. kubectl apply -f calico.yaml Watch the Calico/node pod for the master get created (hopefully successfully) watch kubectl get pods --all-namespaces -o wide 1.7 Join worker node \u00b6 If you have other nodes around you can run the 'kubeadm join ...' command from the output of kubeadm init on each worker node (incl token). Watch the calico/node pods get created for each worker node automatically. e.g. kubeadm join --token **** For this lab, we are creating a one node kubernetes clusters, so in order to be able to deploy applications on the same node as the control plane, we need to remove the taint that prevent such deployment. kubectl taint nodes --all node-role.kubernetes.io/master- 1.8 Now lets create a test deployment with 2 replicas \u00b6 kubectl create deployment nginx --replicas=2 --image=nginx --port=8080 Lets get some more detail about the deployment: kubectl describe deployment nginx kubectl get deployment nginx And pods that has been created by nginx deployment: kubectl get pods Congrats. Now you have a working Kubernetes+Calico cluster. 2.1 Verify Kubernetes components deployed by kubeadm \u00b6 2.1.1 Check Kubernetes version \u00b6 Step 1 Verify that Kubernetes is deployed and working. kubectl get nodes Result Kubernetes has single node for workload scheduling. Kubernetes running version 1.20.6 Note At Kubernetes community, we define 3 types of Kubernetes releases: Major (x.0.0) Minor (x.x.0) Patch (x.x.x) Note At a single point of time, we develop the new \"Major\"/\"Minor\" version of Kubernetes (today - Kubernetes 1.21), and we support three existing releases as the \"Patch\" releases (today - 1.19.x, 1.20.x and 1.21.x). 2.1.2 Verify Cluster default namespaces. \u00b6 Step 1 Verify namespaces created in K8s systems $ kubectl get ns NAME STATUS AGE default Active 5h50m kube-node-lease Active 5h50m kube-public Active 5h50m kube-system Active 5h50m Info Namespaces are intendent to isolate groups/teams and give them access to a set of resources. They avoid name collisions between resources. Namespaces provides with a soft Multitenancy, meaning they not provide full isolation. Result By default Kubernetes deployed by kubeadm starts with 4 namespaces: default The default namespace for objects with no other namespace. When listing resources with the kubectl get command, we\u2019ve never specified the namespace explicitly, so kubectl always defaulted to the default namespace, showing us just the objects inside that namespace. kube-system The namespace for objects created by the Kubernetes system kube-public Readable by all users, and mostly reserved for cluster usage. kube-node-lease This namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales. 2.1.3 Verify kubelet \u00b6 Step 1 Verify that kubelet installed in K8s Cluster: systemctl -l | grep kubelet systemctl status kubelet Note Service and its config file can be found in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Step 2 Find manifests file for other master Node components: Once kubelet is deployed, all the rest master node components are deployed as a static pods on Kubernetes Master node. Setting --pod-manifest-path= specifies from where to read Static Pod manifests used for spinning up the control plane. Step 3 List K8s components manifest files that is going to be used for cluster deployment and run as Static Pods by kubelet : sudo ls /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml Result We see etcd, api-server, controller-manager and scheduler that has been used to deploy on this cluster and managed by kubelet . Step 4 Verify K8s Components deployed as containers on K8s: kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-6d7b4db76c-h242g 1/1 Running 0 27m calico-node-gwnng 1/1 Running 0 27m coredns-74ff55c5b-5s7rp 1/1 Running 0 5h59m coredns-74ff55c5b-l6hd4 1/1 Running 0 5h59m etcd-k8s-cluster 1/1 Running 0 5h59m kube-apiserver-k8s-cluster 1/1 Running 0 5h59m kube-controller-manager-k8s-cluster 1/1 Running 0 5h59m kube-proxy-f8647 1/1 Running 0 5h59m kube-scheduler-k8s-cluster 1/1 Running 0 5h59m Result We can see that Kubernetes components: etcd, api-server, controller-manager and scheduler deployed on K8s cluster via kubelet. Calico Networking including calico-etcd, calico-node, calico-policy-controller has been deployed as a last step of kubeadm installation 2.1.4 Verify etcd database deployment. \u00b6 Step 1 Verify etcd config file sudo cat /etc/kubernetes/manifests/etcd.yaml Step 2 Overview etcd pod deployed on K8s cluster: kubectl get pods -n kube-system | grep etcd kubectl describe pods/etcd-k8s-cluste -n kube-system Result etcd has been deployed as a static pod. Annotation Priority Class Name: system-node-critical tells to K8s that this Pod is critical and will have highest QOS . Step 3 Check the location of etcd db and snapshot dumps. sudo ls /var/lib/etcd/member Result The data directory has two sub-directories in it: wal: write ahead log files are stored here. snap: log snapshots are stored here. When first started, etcd stores its configuration into a data directory specified by the data-dir configuration parameter. Configuration is stored in the write ahead log and includes: the local member ID, cluster ID, and initial cluster configuration. The write ahead log and snapshot files are used during member operation and to recover after a restart. 2.1.5 Verify api-server deployment on the K8s cluster. \u00b6 Step 1 Review configuration file: sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml Step 2 Overview api-server pod and its parameters. kubectl describe pods/kube-apiserver-k8s-cluster -n kube-system 2.1.6 Verify Controller-manager and scheduler deployment. \u00b6 Step 1 Controller-manager and scheduler deployed on K8s cluster via kubelet the same way api-server . Verify both configuration files and pods running on K8s Cluster. Summary K8s is an orchestration system for containers. Since most of the k8s components are the go binaries that can be containerized, K8s has been designed to run itself. This makes system itself HA, easily deployable, scaleable and upgradable.","title":"Lab 6 Deploy Kubernetes Cluster with Kubeadm"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#deploy-kubernetes","text":"In this Lab, we are going to: Deploy Single Node Kubernetes cluster using kubeadm on a Google Compute Engine node Deploy an application to Kubernetes","title":"Deploy Kubernetes"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#1-deploy-kubernetes-and-calico-with-kubeadm","text":"In general to deploy Kubernetes with kubeadm it is required to use following official Kubernetes documentation .","title":"1. Deploy Kubernetes and Calico with Kubeadm"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#11-create-a-vm-and-ssh-into-it","text":"Step 1 Create the instance gcloud compute instances create k8s-cluster \\ --zone us-central1-c \\ --machine-type=e2-standard-4 \\ --image=ubuntu-1804-bionic-v20210514 \\ --image-project=ubuntu-os-cloud Step 2 Capture the private IP address of the VM Record the IP address of the node, as we will need later. Step 3 Once the vm is created, you can ssh into it gcloud compute ssh k8s-cluster --zone us-central1-c You can also ssh into the node through the console if you prefer that. Step 4 Define the NODE_IP as an environment variable. export NODE_IP=<REPLACE_WITH_NODE_PRIVATE_IP>","title":"1.1 Create a VM, and ssh into it"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#12-let-iptables-see-bridged-traffic","text":"cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system","title":"1.2 Let iptables see bridged traffic"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#13-install-docker","text":"Step 1 Update the apt package index and install packages: sudo su apt-get update apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Step 2 Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Step 3 Setup the stable repository echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Step 4 Install Docker Engine and containerd apt-get update apt-get install docker-ce docker-ce-cli containerd.io Step 5 Configure the Docker daemon to use systemd for the management of the container's cgroups. mkdir /etc/docker cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF Step 6 Restart Docker and enable on boot: systemctl enable docker systemctl daemon-reload systemctl restart docker","title":"1.3 Install Docker"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#14-install-kubeadm-and-prerequisite-packages-on-each-node","text":"The next step is to install kubeadm and prerequisite packages as showed here. Step 1 Deploy kubeadm and prerequisite packages apt-get update && apt-get install -y apt-transport-https ca-certificates curl curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list apt-get update && apt-get install -y kubelet=1.20.6-00 kubeadm=1.20.6-00 kubectl=1.20.6-00 apt-mark hold kubelet kubeadm kubectl Step 2 Verify kubeadm version kubeadm version Output: kubeadm version: &version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:26:21Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"} Result Latest available version of Kubernetes/kubeadm has been installed from (GitHub Kubernetes repo release page.)[https://github.com/kubernetes/kubernetes/releases]","title":"1.4 Install kubeadm and prerequisite packages on each node"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#15-kubeadm-init-the-master","text":"Run On the Master node only: Step 1: Build kubeadm Custom Config cat <<EOF > kubeadm.conf kind: ClusterConfiguration apiVersion: kubeadm.k8s.io/v1beta2 apiServer: extraArgs: advertise-address: $NODE_IP kubernetesVersion: v1.20.6 --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd EOF Make sure the IP address was updated: cat kubeadm.conf Note To expose custom Config you can create a kubeadm.conf and specify during kubecadm init execution. For instance: * ControllerManager configs * Custom Subnet * Custom version * Apiserver configs such as authentication, authorization and etc. Step 2: Create a cluster kubeadm init --config=kubeadm.conf Result Once the command completes, configure the KUBECONFIG env variable with the path to admin.conf (recommend adding it to your .bashrc): mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config export KUBECONFIG=$HOME/.kube/config Let's validate that the installation was successful. You should now be able to run kubectl commands and see that all cluster Pods are running (except DNS one): watch kubectl get pods --all-namespaces -o wide To exit back to the terminal, press ctrl+c","title":"1.5 'Kubeadm init' the Master"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#16-deploy-calico-networking-via-self-hosted-as-a-daemon-set","text":"Step 1 Lets download the Calico manifest: curl https://docs.projectcalico.org/manifests/calico.yaml -O Note You can customize Calico deployments based on you needs. For instance by changing: (Optional) In the manifest, change CALICO_IPV4POOL_IPIP from \"always\" to \"cross-subnet\". (Optional) In the manifest, change FELIX_IPINIPMTU to match your ethernet interface mtu (Optional) If you want a different address range for containers, change CALICO_IPV4POOL_CIDR to the same cidr range used in kubeadm init, for e.g., \"10.6.0.0/16\" This time we are not going to modify defaults Calico values. Step 2 Now lets deploy Calico as a daemon set. kubectl apply -f calico.yaml Watch the Calico/node pod for the master get created (hopefully successfully) watch kubectl get pods --all-namespaces -o wide","title":"1.6 Deploy Calico Networking (via self-hosted, as a daemon set)"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#17-join-worker-node","text":"If you have other nodes around you can run the 'kubeadm join ...' command from the output of kubeadm init on each worker node (incl token). Watch the calico/node pods get created for each worker node automatically. e.g. kubeadm join --token **** For this lab, we are creating a one node kubernetes clusters, so in order to be able to deploy applications on the same node as the control plane, we need to remove the taint that prevent such deployment. kubectl taint nodes --all node-role.kubernetes.io/master-","title":"1.7 Join worker node"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#18-now-lets-create-a-test-deployment-with-2-replicas","text":"kubectl create deployment nginx --replicas=2 --image=nginx --port=8080 Lets get some more detail about the deployment: kubectl describe deployment nginx kubectl get deployment nginx And pods that has been created by nginx deployment: kubectl get pods Congrats. Now you have a working Kubernetes+Calico cluster.","title":"1.8 Now lets create a test deployment with 2 replicas"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#21-verify-kubernetes-components-deployed-by-kubeadm","text":"","title":"2.1 Verify Kubernetes components deployed by kubeadm"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#211-check-kubernetes-version","text":"Step 1 Verify that Kubernetes is deployed and working. kubectl get nodes Result Kubernetes has single node for workload scheduling. Kubernetes running version 1.20.6 Note At Kubernetes community, we define 3 types of Kubernetes releases: Major (x.0.0) Minor (x.x.0) Patch (x.x.x) Note At a single point of time, we develop the new \"Major\"/\"Minor\" version of Kubernetes (today - Kubernetes 1.21), and we support three existing releases as the \"Patch\" releases (today - 1.19.x, 1.20.x and 1.21.x).","title":"2.1.1 Check Kubernetes version"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#212-verify-cluster-default-namespaces","text":"Step 1 Verify namespaces created in K8s systems $ kubectl get ns NAME STATUS AGE default Active 5h50m kube-node-lease Active 5h50m kube-public Active 5h50m kube-system Active 5h50m Info Namespaces are intendent to isolate groups/teams and give them access to a set of resources. They avoid name collisions between resources. Namespaces provides with a soft Multitenancy, meaning they not provide full isolation. Result By default Kubernetes deployed by kubeadm starts with 4 namespaces: default The default namespace for objects with no other namespace. When listing resources with the kubectl get command, we\u2019ve never specified the namespace explicitly, so kubectl always defaulted to the default namespace, showing us just the objects inside that namespace. kube-system The namespace for objects created by the Kubernetes system kube-public Readable by all users, and mostly reserved for cluster usage. kube-node-lease This namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales.","title":"2.1.2 Verify Cluster default namespaces."},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#213-verify-kubelet","text":"Step 1 Verify that kubelet installed in K8s Cluster: systemctl -l | grep kubelet systemctl status kubelet Note Service and its config file can be found in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Step 2 Find manifests file for other master Node components: Once kubelet is deployed, all the rest master node components are deployed as a static pods on Kubernetes Master node. Setting --pod-manifest-path= specifies from where to read Static Pod manifests used for spinning up the control plane. Step 3 List K8s components manifest files that is going to be used for cluster deployment and run as Static Pods by kubelet : sudo ls /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml Result We see etcd, api-server, controller-manager and scheduler that has been used to deploy on this cluster and managed by kubelet . Step 4 Verify K8s Components deployed as containers on K8s: kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-6d7b4db76c-h242g 1/1 Running 0 27m calico-node-gwnng 1/1 Running 0 27m coredns-74ff55c5b-5s7rp 1/1 Running 0 5h59m coredns-74ff55c5b-l6hd4 1/1 Running 0 5h59m etcd-k8s-cluster 1/1 Running 0 5h59m kube-apiserver-k8s-cluster 1/1 Running 0 5h59m kube-controller-manager-k8s-cluster 1/1 Running 0 5h59m kube-proxy-f8647 1/1 Running 0 5h59m kube-scheduler-k8s-cluster 1/1 Running 0 5h59m Result We can see that Kubernetes components: etcd, api-server, controller-manager and scheduler deployed on K8s cluster via kubelet. Calico Networking including calico-etcd, calico-node, calico-policy-controller has been deployed as a last step of kubeadm installation","title":"2.1.3 Verify kubelet"},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#214-verify-etcd-database-deployment","text":"Step 1 Verify etcd config file sudo cat /etc/kubernetes/manifests/etcd.yaml Step 2 Overview etcd pod deployed on K8s cluster: kubectl get pods -n kube-system | grep etcd kubectl describe pods/etcd-k8s-cluste -n kube-system Result etcd has been deployed as a static pod. Annotation Priority Class Name: system-node-critical tells to K8s that this Pod is critical and will have highest QOS . Step 3 Check the location of etcd db and snapshot dumps. sudo ls /var/lib/etcd/member Result The data directory has two sub-directories in it: wal: write ahead log files are stored here. snap: log snapshots are stored here. When first started, etcd stores its configuration into a data directory specified by the data-dir configuration parameter. Configuration is stored in the write ahead log and includes: the local member ID, cluster ID, and initial cluster configuration. The write ahead log and snapshot files are used during member operation and to recover after a restart.","title":"2.1.4 Verify etcd database deployment."},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#215-verify-api-server-deployment-on-the-k8s-cluster","text":"Step 1 Review configuration file: sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml Step 2 Overview api-server pod and its parameters. kubectl describe pods/kube-apiserver-k8s-cluster -n kube-system","title":"2.1.5 Verify api-server deployment on the K8s cluster."},{"location":"ycit019_Lab_6_Deploy_Kubernetes_kubeadm/#216-verify-controller-manager-and-scheduler-deployment","text":"Step 1 Controller-manager and scheduler deployed on K8s cluster via kubelet the same way api-server . Verify both configuration files and pods running on K8s Cluster. Summary K8s is an orchestration system for containers. Since most of the k8s components are the go binaries that can be containerized, K8s has been designed to run itself. This makes system itself HA, easily deployable, scaleable and upgradable.","title":"2.1.6 Verify Controller-manager and scheduler deployment."},{"location":"ycit019_Lab_7_Kubernetes_Concepts/","text":"Kubernetes Concepts \u00b6 Objective: Learn basic Kubernetes concepts: Create a GKE Cluster Pods Labels, Selectors and Annotations Create Deployments Create Services namespaces 0 Create GKE Cluster \u00b6 Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c 1 Pods \u00b6 1.1 Create a Pod with manifest \u00b6 Reference: Pod Overview Step 1 Printout explanation of the object and lists of attributes: kubectl explain pods See all possible fields available for the pods: kubectl explain pods.spec --recursive Note It's not require to provide all possible fields for the Pods or any other resources. Most of the fields will be added by default if not specified. For the Pods at minimum it is required to specify image , name , ports inside of spec.containers. Step 2 Define a new pod in the file echoserver-pod.yaml : cat <<EOF > echoserver-pod.yaml apiVersion: v1 kind: Pod metadata: name: echoserver labels: app: echoserver spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.10 ports: - containerPort: 8080 EOF Here, we use the existing image echoserver . This is a simple server that responds with the http headers it received. It runs on nginx server and implemented using lua in the nginx configuration: https://github.com/kubernetes/contrib/tree/master/ingress/echoheaders Step 3 Create the echoserver pod: kubectl apply -f echoserver-pod.yaml Step 4 Use kubectl get pods to watch the pod get created: kubectl get pods Result: NAME READY STATUS RESTARTS AGE echoserver 1/1 Running 0 5s Step 5 Use kubectl describe pods/podname to watch the details about scheduled pod: kubectl describe pods/echoserver Note Review and discuss the following fields: Namespace Status Containers QoS Class Events Step 6 Now let\u2019s get the pod definition back from Kubernetes: kubectl get pods echoserver -o yaml > echoserver-pod-created.yaml cat echoserver-pod-created.yaml Compare echoserver-pod.yaml and echoserver-pod-created.yaml to see additional properties that have been added to the original pod definition. 2 Labels & Selectors \u00b6 Organizing pods and other resources with labels. 2.1 Label and Select Pods \u00b6 Reference: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ Step 1 Label Pod hello-world with label app=hello and env=test kubectl label pods echoserver dep=sales kubectl label pods echoserver env=test Step 2 See all Pods and all their Labels. kubectl get pods --show-labels Step 3 Select all Pods with labels env=test kubectl get pods -l env=test 2.2 Label Nodes \u00b6 Reference: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ Step 1 List available nodes kubectl get nodes Step 2 List a detailed view of nodes kubectl get nodes -o wide Step 3 List Nodes and their labels kubectl get nodes --show-labels Step 4 Label the node as size: small . Make sure to replace YOUR_NODE_NAME with one of the nodes you have. kubectl label node YOUR_NODE_NAME size=small Step 5 Check the labels for this node kubectl get node YOUR_NODE_NAME --show-labels | grep size Note In the upcoming classes we will use node labels to make sure our applications run on eligible nodes only. 3 Services \u00b6 3.1 Create a Service \u00b6 We have three running echoserver pods, but we cannot access them yet, because the container ports are not accessible. Let\u2019s define a new service that will expose echoserver ports and make them accessible. Step 1 Create a new file echoserver-service.yaml with the following content: cat <<EOF > echoserver-service.yaml apiVersion: v1 kind: Service metadata: name: echoserver spec: selector: app: echoserver type: \"NodePort\" ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: echoserver EOF Step 2 Create a new service: kubectl create -f echoserver-service.yaml Step 3 Check the service details: kubectl describe services/echoserver Output: Name: echoserver Namespace: default Labels: <none> Selector: app=echoserver Type: NodePort IP: ... Port: <unset> 8080/TCP NodePort: <unset> 30366/TCP Endpoints: ...:8080,...:8080,..:8080 Session Affinity: None No events. Note The above output contains one endpoint and a node port, 30366, but it can be different in your case. Remember this port to use it in the next step. Step 4 We need to open the node port on one of the cluster nodes to be able to access the service externally. Let's first find the exteran IP address of one of the nodes. kubectl get nodes -o wide Output: NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gke-k8s-concepts-default-pool-ad96fd50-1rf1 Ready <none> 20m v1.19.9-gke.1400 10.128.0.32 34.136.1.22 gke-k8s-concepts-default-pool-ad96fd50-jpd2 Ready <none> 20m v1.19.9-gke.1400 10.128.0.31 34.69.114.67 Step 5 Create a firewall rule to allow TCP traffic on your node port. Make sure to replace YOUR_NODE_PORT. gcloud compute firewall-rules create echoserver-node-port --allow tcp:YOUR_NODE_PORT Step 6 To access a service exposed via a node port, specify the node port from the previous step and use one of the IP addresses of the cluster nodes. Make sure to replace both NODE_IP and YOUR_NODE_PORT curl http://NODE_IP:YOUR_NODE_PORT 3.2 Cleanup Services and Pods \u00b6 Step 1 Before diving into Kubernetes deployment, let\u2019s delete our service and pods. To delete the service execute the following command: kubectl delete service echoserver Step 2 delete the pod kubectl delete pod echoserver Step 3 Check that there are no running pods: kubectl get pods 4 Deployments \u00b6 4.1 Deploy hello-app on Kubernetes using Deployments \u00b6 4.1.1 Create a Deployment \u00b6 Step 1 The simplest way to create a new deployment for a single-container pod is to use kubectl run : kubectl create deployment hello-app \\ --image=gcr.io/google-samples/hello-app:1.0 \\ --port=8080 \\ --replicas=2 Note --port Deployment opens port 8080 for use by the Pods. --replicas number of replicas. Step 2 Check pods: kubectl get pods Step 3 To access the hello-app deployment, create a new service of type LoadBalancer this time using kubectl expose deployment : kubectl expose deployment hello-app --type=LoadBalancer To get the external IP for the loadbalancer that got created: kubectl get services/hello-app The Loadbalancer might take few minutes to get created, and it'll show pending status. Step 4 Check that the hello-app is accessible: Make sure to replace the LB_IP. curl http://LB_IP:8080 Output: Hello, world! Version: 1.0.0 Hostname: hello-app-76f778987d-rdhr7 Step 5 You can open the app in the browser by navigating to LB_IP:8080 Summary We learned how to create a deployment and expose our container. 4.1.2 Scale a Deployment \u00b6 Now, let's scale our application as our website get popular. Step 1 Deployments using replica set (RS) to scale the containers. Let's check how replica set (RS) looks like: kubectl get rs,deploy NAME DESIRED CURRENT READY AGE replicaset.apps/hello-app-76f778987d 2 2 2 5m12s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hello-app 2/2 2 2 5m12s Step 2 Let\u2019s scale number of pods in replica for the deployment. Use going to use kubectl scale to change the number of replicas to 5: kubectl scale deployment hello-app --replicas=5 Step 3 View the deployment details: kubectl describe deployment hello-app Step 4 Check that there are 5 running pods: kubectl get pods 4.1.3 Rolling Update of Containers \u00b6 To perform rolling upgrade we need a new version of our application and then perform Rolling Upgrade using deployments Step 4 Use kubectl rollout history deployment to see revisions of the deployment: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> Result Since we've just deployed there is only 1 revision that currenly running. Step 5 Now we want to replace our hello-app with a new implementation. We want to use a new version of hello-app image. We are going to use kubectl set command this time around. Hint kubectl set used only to change image name/version. You can use this for command for CI/CD pipeline. Suppose that we want to update the webk8sbirthday Pods to use the hello-app:2.0 image instead of the hello-app:1.0 image. kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record kubectl get pods Note It is a good practice to paste --record at the end of the rolling upgrade command as it will record the action in the rollout history Result: We can see that the Rolling Upgraded was recorded: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> 2 kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record=true Step 6 Refresh browser and see new version of app deployed http://LB_IP:8080 Step 7 Let's assume there was something wrong with this new version and we need to rollback with kubectl rollout undo our deployment: kubectl rollout undo deployment/hello-app Refresh the browser again to see how we rolledback to version 1.0.0 We have successfully rolled back the deployment and our pods are based on the hello-app:1.0.0 image again. Step 8 Let's delete the deployment and the service: kubectl delete deployment hello-app kubectl delete services/hello-app Success You are now up to speed with Kubernetes Concepts such as Pods, Services and Deployments. Let's move on to Kubernetes Features to learn what else Kubernetes is capable of! 5 NameSpace \u00b6 Namespace can be used for: Splitting complex systems with several components into smaller groups Separating resources in a multi-tenant env: production, development and QA environments Separating resources per production Separating per-user or per-department or any other logical group Some other rules and regulations: Resource names only need to be unique within a namespace. Two different namespaces can contain resources of the same name. Most of the Kubernetes resources (e.g. pods, svc, rcs, and others) are namespaced. However, some resource can be cluster-wide e.g nodes, persistentVolumes and PodSecurityPolicy. 5.1 Viewing namespaces \u00b6 Step 1 List the current namespaces in a cluster using: kubectl get ns Output: NAME STATUS AGE default Active 71m kube-node-lease Active 71m kube-public Active 71m kube-system Active 71m Step 2 You can also get the summary of a specific namespace using: kubectl get namespaces <name> Or you can get detailed information with: kubectl describe namespaces <name> A namespace can be in one of two phases: Active the namespace is in use Terminating the namespace is being deleted, and can not be used for new objects Note These details show both resource quota (if present) as well as resource and limit ranges. Resource quota tracks aggregate usage of resources in the Namespace and allows cluster operators to define Hard resource usage limits that a Namespace may consume. A limit range defines min/max constraints on the amount of resources a single entity can consume in a Namespace . Step 3 Let\u2019s have a look at the pods that belong to the kube-system namespace, by telling kubectl to list pods in that namespace: kubectl get po --namespace kube-system Output: NAME READY STATUS RESTARTS AGE event-exporter-gke-67986489c8-5fsdv 2/2 Running 0 71m fluentbit-gke-fqcsx 2/2 Running 0 71m fluentbit-gke-ppb9j 2/2 Running 0 71m gke-metrics-agent-5vl7t 1/1 Running 0 71m gke-metrics-agent-bxt2r 1/1 Running 0 71m kube-dns-5d54b45645-9srx6 4/4 Running 0 71m kube-dns-5d54b45645-b7njm 4/4 Running 0 71m kube-dns-autoscaler-58cbd4f75c-2scrv 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-1rf1 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-jpd2 1/1 Running 0 71m l7-default-backend-66579f5d7-dsbdt 1/1 Running 0 71m metrics-server-v0.3.6-6c47ffd7d7-mtls4 2/2 Running 0 71m pdcsi-node-knlqp 2/2 Running 0 71m pdcsi-node-vh4tx 2/2 Running 0 71m stackdriver-metadata-agent-cluster-level-6f7d66dc98-zcd25 2/2 Running 0 71m Tip You can also use -n instead of --namespace Yot may already know some of the pods, the rest we will cover later. It\u2019s clear from the name of the namespace, that resources inside kube-system related to the Kubernetes system itself. By having them in this separate namespace, it keeps everything nicely organized. If they were all in the default namespace, mixed in with the resources we create ourselves, we\u2019d have a hard time seeing what belongs where and we might inadvertently delete some system resources. Step 4 Now you know how to view resources in specific namespaces. Additionally, it is also possible to view list all resources in all namespaces. For example below is example to list all pods in all namespaces: kubectl get pods --all-namespaces 5.2 Creating Namespaces \u00b6 A namespace is a Kubernetes resource, therefore it is possible to create it by posting a YAML file to the Kubernetes API server or using kubectl create ns . Step 1 First, create a custom-namespace.yaml file with the following content: cat <<EOF > custom-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: custom-namespace EOF Step 2 Than, use kubectl to post the file to the Kubernetes API server: kubectl create -f custom-namespace.yaml Step 3 A much easier and faster way to create a namespaces using kubectl create ns command, as shown below: kubectl create namespace custom-namespace2 5.3 Setting the namespace preference. \u00b6 By default, a Kubernetes cluster will instantiate a default namespace when provisioning the cluster to hold the default set of Pods, Services, and Deployments used by the cluster. So by default all kubectl calls such as list or create resources will end up in default namespace . However sometimes you want to list or create resources in other namespaces than default namespace . As we discussed in previous exercise this can be done by specifying -n or --namespace to point in which namespaces action has to be done. However it is not convenient to do this action every time. Below example will show how to create 2 namespaces dev and prod and switch between each other. Step 1 kubectl API uses so called kubeconfig context where you can controls which namespace, user or cluster needs to be accessed. In order to display which context is currently in use run: KUBECONFIG=~/.kube/config kubectl config current-context Result We running in kubernetes-admin@kubernetes context, which is default for lab environment. Step 3 To see full view of the kubeconfig context run: kubectl config view Result Our context named as kubernetes-admin@kubernetes uses cluster cluster , and user kubernetes-admin Step 4 The result of the above command comes from kubeconfig file, in our case we defined it under ~/.kube/config . echo $KUBECONFIG Result KUBECONFIG is configured to use following ~/.kube/config file cat ~/.kube/config Note The KUBECONFIG environment variable is a list of paths to configuration files. The list is colon-delimited for Linux and Mac, and semicolon-delimited for Windows. We already set KUBECONFIG environment variable in a first step of exercise to be ~/.kube/config Tip You can use use multiple kubeconfig files at the same time and view merged config: $ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view Step 5 Create two new namespaces dev : kubectl create namespace dev And prod namespace: kubectl create namespace prod Step 7 Let\u2019s switch to operate in the development namespace: kubectl config set-context --current --namespace=dev We can see now that our current context is switched to dev: kubectl config view | grep namespace Output: namespace: dev Result At this point, all requests we make to the Kubernetes cluster from the command line are scoped to the development namespace. Step 8 Let's test that all resources going to be created in dev namespace. kubectl run devsnowflake --image=nginx Step 9 Verify result of creation: kubectl get pods Success Developers are able to do what they want, and they do not have to worry about affecting content in the production namespace. Step 10 Now switch to the production namespace and show how resources in one namespace are hidden from the other. kubectl config set-context --current --namespace=prod The production namespace should be empty, and the following commands should return nothing. kubectl get pods Step 11 Let's create some production workloads: kubectl run prodapp --image=nginx kubectl get pods -n prod Summary At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace. 6.4 Deleting Namespaces \u00b6 Step 1 Delete a namespace with kubectl delete namespaces custom-namespace kubectl delete namespaces dev kubectl delete namespaces prod Warning Unlike with OpenStack where when you delete a project/tenant, underlining resources will still exist as zombies and not deleted. In Kubernetes when you delete namespace it deletes everything under it (pods, svc, rc, and etc.)! This is called resource garbage collection in Kubernetes. Delete process is asynchronous, so you may see Terminating state for some time. 6.5 Create a pod in a different namespace \u00b6 Create test namespace: kubectl create ns test Create a pod in this namespaces: cat <<EOF > echoserver-pod_ns.yaml apiVersion: v1 kind: Pod metadata: name: echoserverns spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.4 ports: - containerPort: 8080 EOF Create Pod in namespaces: kubectl create -f echoserver-pod_ns.yaml -n test Verify Pods created in specified namespaces: kubectl get pods -n test 6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts Step 2 Delete the firewall rule gcloud compute firewall-rules delete echoserver-node-port","title":"Lab 7 Kubernetes Concepts"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#kubernetes-concepts","text":"Objective: Learn basic Kubernetes concepts: Create a GKE Cluster Pods Labels, Selectors and Annotations Create Deployments Create Services namespaces","title":"Kubernetes Concepts"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#0-create-gke-cluster","text":"Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c","title":"0 Create GKE Cluster"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#1-pods","text":"","title":"1 Pods"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#11-create-a-pod-with-manifest","text":"Reference: Pod Overview Step 1 Printout explanation of the object and lists of attributes: kubectl explain pods See all possible fields available for the pods: kubectl explain pods.spec --recursive Note It's not require to provide all possible fields for the Pods or any other resources. Most of the fields will be added by default if not specified. For the Pods at minimum it is required to specify image , name , ports inside of spec.containers. Step 2 Define a new pod in the file echoserver-pod.yaml : cat <<EOF > echoserver-pod.yaml apiVersion: v1 kind: Pod metadata: name: echoserver labels: app: echoserver spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.10 ports: - containerPort: 8080 EOF Here, we use the existing image echoserver . This is a simple server that responds with the http headers it received. It runs on nginx server and implemented using lua in the nginx configuration: https://github.com/kubernetes/contrib/tree/master/ingress/echoheaders Step 3 Create the echoserver pod: kubectl apply -f echoserver-pod.yaml Step 4 Use kubectl get pods to watch the pod get created: kubectl get pods Result: NAME READY STATUS RESTARTS AGE echoserver 1/1 Running 0 5s Step 5 Use kubectl describe pods/podname to watch the details about scheduled pod: kubectl describe pods/echoserver Note Review and discuss the following fields: Namespace Status Containers QoS Class Events Step 6 Now let\u2019s get the pod definition back from Kubernetes: kubectl get pods echoserver -o yaml > echoserver-pod-created.yaml cat echoserver-pod-created.yaml Compare echoserver-pod.yaml and echoserver-pod-created.yaml to see additional properties that have been added to the original pod definition.","title":"1.1 Create a Pod with manifest"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#2-labels-selectors","text":"Organizing pods and other resources with labels.","title":"2 Labels &amp; Selectors"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#21-label-and-select-pods","text":"Reference: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ Step 1 Label Pod hello-world with label app=hello and env=test kubectl label pods echoserver dep=sales kubectl label pods echoserver env=test Step 2 See all Pods and all their Labels. kubectl get pods --show-labels Step 3 Select all Pods with labels env=test kubectl get pods -l env=test","title":"2.1 Label and Select Pods"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#22-label-nodes","text":"Reference: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ Step 1 List available nodes kubectl get nodes Step 2 List a detailed view of nodes kubectl get nodes -o wide Step 3 List Nodes and their labels kubectl get nodes --show-labels Step 4 Label the node as size: small . Make sure to replace YOUR_NODE_NAME with one of the nodes you have. kubectl label node YOUR_NODE_NAME size=small Step 5 Check the labels for this node kubectl get node YOUR_NODE_NAME --show-labels | grep size Note In the upcoming classes we will use node labels to make sure our applications run on eligible nodes only.","title":"2.2 Label Nodes"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#3-services","text":"","title":"3 Services"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#31-create-a-service","text":"We have three running echoserver pods, but we cannot access them yet, because the container ports are not accessible. Let\u2019s define a new service that will expose echoserver ports and make them accessible. Step 1 Create a new file echoserver-service.yaml with the following content: cat <<EOF > echoserver-service.yaml apiVersion: v1 kind: Service metadata: name: echoserver spec: selector: app: echoserver type: \"NodePort\" ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: echoserver EOF Step 2 Create a new service: kubectl create -f echoserver-service.yaml Step 3 Check the service details: kubectl describe services/echoserver Output: Name: echoserver Namespace: default Labels: <none> Selector: app=echoserver Type: NodePort IP: ... Port: <unset> 8080/TCP NodePort: <unset> 30366/TCP Endpoints: ...:8080,...:8080,..:8080 Session Affinity: None No events. Note The above output contains one endpoint and a node port, 30366, but it can be different in your case. Remember this port to use it in the next step. Step 4 We need to open the node port on one of the cluster nodes to be able to access the service externally. Let's first find the exteran IP address of one of the nodes. kubectl get nodes -o wide Output: NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gke-k8s-concepts-default-pool-ad96fd50-1rf1 Ready <none> 20m v1.19.9-gke.1400 10.128.0.32 34.136.1.22 gke-k8s-concepts-default-pool-ad96fd50-jpd2 Ready <none> 20m v1.19.9-gke.1400 10.128.0.31 34.69.114.67 Step 5 Create a firewall rule to allow TCP traffic on your node port. Make sure to replace YOUR_NODE_PORT. gcloud compute firewall-rules create echoserver-node-port --allow tcp:YOUR_NODE_PORT Step 6 To access a service exposed via a node port, specify the node port from the previous step and use one of the IP addresses of the cluster nodes. Make sure to replace both NODE_IP and YOUR_NODE_PORT curl http://NODE_IP:YOUR_NODE_PORT","title":"3.1 Create a Service"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#32-cleanup-services-and-pods","text":"Step 1 Before diving into Kubernetes deployment, let\u2019s delete our service and pods. To delete the service execute the following command: kubectl delete service echoserver Step 2 delete the pod kubectl delete pod echoserver Step 3 Check that there are no running pods: kubectl get pods","title":"3.2 Cleanup Services and Pods"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#4-deployments","text":"","title":"4 Deployments"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#41-deploy-hello-app-on-kubernetes-using-deployments","text":"","title":"4.1 Deploy hello-app on Kubernetes using Deployments"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#411-create-a-deployment","text":"Step 1 The simplest way to create a new deployment for a single-container pod is to use kubectl run : kubectl create deployment hello-app \\ --image=gcr.io/google-samples/hello-app:1.0 \\ --port=8080 \\ --replicas=2 Note --port Deployment opens port 8080 for use by the Pods. --replicas number of replicas. Step 2 Check pods: kubectl get pods Step 3 To access the hello-app deployment, create a new service of type LoadBalancer this time using kubectl expose deployment : kubectl expose deployment hello-app --type=LoadBalancer To get the external IP for the loadbalancer that got created: kubectl get services/hello-app The Loadbalancer might take few minutes to get created, and it'll show pending status. Step 4 Check that the hello-app is accessible: Make sure to replace the LB_IP. curl http://LB_IP:8080 Output: Hello, world! Version: 1.0.0 Hostname: hello-app-76f778987d-rdhr7 Step 5 You can open the app in the browser by navigating to LB_IP:8080 Summary We learned how to create a deployment and expose our container.","title":"4.1.1 Create a Deployment"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#412-scale-a-deployment","text":"Now, let's scale our application as our website get popular. Step 1 Deployments using replica set (RS) to scale the containers. Let's check how replica set (RS) looks like: kubectl get rs,deploy NAME DESIRED CURRENT READY AGE replicaset.apps/hello-app-76f778987d 2 2 2 5m12s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hello-app 2/2 2 2 5m12s Step 2 Let\u2019s scale number of pods in replica for the deployment. Use going to use kubectl scale to change the number of replicas to 5: kubectl scale deployment hello-app --replicas=5 Step 3 View the deployment details: kubectl describe deployment hello-app Step 4 Check that there are 5 running pods: kubectl get pods","title":"4.1.2 Scale a Deployment"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#413-rolling-update-of-containers","text":"To perform rolling upgrade we need a new version of our application and then perform Rolling Upgrade using deployments Step 4 Use kubectl rollout history deployment to see revisions of the deployment: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> Result Since we've just deployed there is only 1 revision that currenly running. Step 5 Now we want to replace our hello-app with a new implementation. We want to use a new version of hello-app image. We are going to use kubectl set command this time around. Hint kubectl set used only to change image name/version. You can use this for command for CI/CD pipeline. Suppose that we want to update the webk8sbirthday Pods to use the hello-app:2.0 image instead of the hello-app:1.0 image. kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record kubectl get pods Note It is a good practice to paste --record at the end of the rolling upgrade command as it will record the action in the rollout history Result: We can see that the Rolling Upgraded was recorded: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> 2 kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record=true Step 6 Refresh browser and see new version of app deployed http://LB_IP:8080 Step 7 Let's assume there was something wrong with this new version and we need to rollback with kubectl rollout undo our deployment: kubectl rollout undo deployment/hello-app Refresh the browser again to see how we rolledback to version 1.0.0 We have successfully rolled back the deployment and our pods are based on the hello-app:1.0.0 image again. Step 8 Let's delete the deployment and the service: kubectl delete deployment hello-app kubectl delete services/hello-app Success You are now up to speed with Kubernetes Concepts such as Pods, Services and Deployments. Let's move on to Kubernetes Features to learn what else Kubernetes is capable of!","title":"4.1.3 Rolling Update of Containers"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#5-namespace","text":"Namespace can be used for: Splitting complex systems with several components into smaller groups Separating resources in a multi-tenant env: production, development and QA environments Separating resources per production Separating per-user or per-department or any other logical group Some other rules and regulations: Resource names only need to be unique within a namespace. Two different namespaces can contain resources of the same name. Most of the Kubernetes resources (e.g. pods, svc, rcs, and others) are namespaced. However, some resource can be cluster-wide e.g nodes, persistentVolumes and PodSecurityPolicy.","title":"5 NameSpace"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#51-viewing-namespaces","text":"Step 1 List the current namespaces in a cluster using: kubectl get ns Output: NAME STATUS AGE default Active 71m kube-node-lease Active 71m kube-public Active 71m kube-system Active 71m Step 2 You can also get the summary of a specific namespace using: kubectl get namespaces <name> Or you can get detailed information with: kubectl describe namespaces <name> A namespace can be in one of two phases: Active the namespace is in use Terminating the namespace is being deleted, and can not be used for new objects Note These details show both resource quota (if present) as well as resource and limit ranges. Resource quota tracks aggregate usage of resources in the Namespace and allows cluster operators to define Hard resource usage limits that a Namespace may consume. A limit range defines min/max constraints on the amount of resources a single entity can consume in a Namespace . Step 3 Let\u2019s have a look at the pods that belong to the kube-system namespace, by telling kubectl to list pods in that namespace: kubectl get po --namespace kube-system Output: NAME READY STATUS RESTARTS AGE event-exporter-gke-67986489c8-5fsdv 2/2 Running 0 71m fluentbit-gke-fqcsx 2/2 Running 0 71m fluentbit-gke-ppb9j 2/2 Running 0 71m gke-metrics-agent-5vl7t 1/1 Running 0 71m gke-metrics-agent-bxt2r 1/1 Running 0 71m kube-dns-5d54b45645-9srx6 4/4 Running 0 71m kube-dns-5d54b45645-b7njm 4/4 Running 0 71m kube-dns-autoscaler-58cbd4f75c-2scrv 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-1rf1 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-jpd2 1/1 Running 0 71m l7-default-backend-66579f5d7-dsbdt 1/1 Running 0 71m metrics-server-v0.3.6-6c47ffd7d7-mtls4 2/2 Running 0 71m pdcsi-node-knlqp 2/2 Running 0 71m pdcsi-node-vh4tx 2/2 Running 0 71m stackdriver-metadata-agent-cluster-level-6f7d66dc98-zcd25 2/2 Running 0 71m Tip You can also use -n instead of --namespace Yot may already know some of the pods, the rest we will cover later. It\u2019s clear from the name of the namespace, that resources inside kube-system related to the Kubernetes system itself. By having them in this separate namespace, it keeps everything nicely organized. If they were all in the default namespace, mixed in with the resources we create ourselves, we\u2019d have a hard time seeing what belongs where and we might inadvertently delete some system resources. Step 4 Now you know how to view resources in specific namespaces. Additionally, it is also possible to view list all resources in all namespaces. For example below is example to list all pods in all namespaces: kubectl get pods --all-namespaces","title":"5.1 Viewing namespaces"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#52-creating-namespaces","text":"A namespace is a Kubernetes resource, therefore it is possible to create it by posting a YAML file to the Kubernetes API server or using kubectl create ns . Step 1 First, create a custom-namespace.yaml file with the following content: cat <<EOF > custom-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: custom-namespace EOF Step 2 Than, use kubectl to post the file to the Kubernetes API server: kubectl create -f custom-namespace.yaml Step 3 A much easier and faster way to create a namespaces using kubectl create ns command, as shown below: kubectl create namespace custom-namespace2","title":"5.2 Creating Namespaces"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#53-setting-the-namespace-preference","text":"By default, a Kubernetes cluster will instantiate a default namespace when provisioning the cluster to hold the default set of Pods, Services, and Deployments used by the cluster. So by default all kubectl calls such as list or create resources will end up in default namespace . However sometimes you want to list or create resources in other namespaces than default namespace . As we discussed in previous exercise this can be done by specifying -n or --namespace to point in which namespaces action has to be done. However it is not convenient to do this action every time. Below example will show how to create 2 namespaces dev and prod and switch between each other. Step 1 kubectl API uses so called kubeconfig context where you can controls which namespace, user or cluster needs to be accessed. In order to display which context is currently in use run: KUBECONFIG=~/.kube/config kubectl config current-context Result We running in kubernetes-admin@kubernetes context, which is default for lab environment. Step 3 To see full view of the kubeconfig context run: kubectl config view Result Our context named as kubernetes-admin@kubernetes uses cluster cluster , and user kubernetes-admin Step 4 The result of the above command comes from kubeconfig file, in our case we defined it under ~/.kube/config . echo $KUBECONFIG Result KUBECONFIG is configured to use following ~/.kube/config file cat ~/.kube/config Note The KUBECONFIG environment variable is a list of paths to configuration files. The list is colon-delimited for Linux and Mac, and semicolon-delimited for Windows. We already set KUBECONFIG environment variable in a first step of exercise to be ~/.kube/config Tip You can use use multiple kubeconfig files at the same time and view merged config: $ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view Step 5 Create two new namespaces dev : kubectl create namespace dev And prod namespace: kubectl create namespace prod Step 7 Let\u2019s switch to operate in the development namespace: kubectl config set-context --current --namespace=dev We can see now that our current context is switched to dev: kubectl config view | grep namespace Output: namespace: dev Result At this point, all requests we make to the Kubernetes cluster from the command line are scoped to the development namespace. Step 8 Let's test that all resources going to be created in dev namespace. kubectl run devsnowflake --image=nginx Step 9 Verify result of creation: kubectl get pods Success Developers are able to do what they want, and they do not have to worry about affecting content in the production namespace. Step 10 Now switch to the production namespace and show how resources in one namespace are hidden from the other. kubectl config set-context --current --namespace=prod The production namespace should be empty, and the following commands should return nothing. kubectl get pods Step 11 Let's create some production workloads: kubectl run prodapp --image=nginx kubectl get pods -n prod Summary At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace.","title":"5.3 Setting the namespace preference."},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#64-deleting-namespaces","text":"Step 1 Delete a namespace with kubectl delete namespaces custom-namespace kubectl delete namespaces dev kubectl delete namespaces prod Warning Unlike with OpenStack where when you delete a project/tenant, underlining resources will still exist as zombies and not deleted. In Kubernetes when you delete namespace it deletes everything under it (pods, svc, rc, and etc.)! This is called resource garbage collection in Kubernetes. Delete process is asynchronous, so you may see Terminating state for some time.","title":"6.4 Deleting Namespaces"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#65-create-a-pod-in-a-different-namespace","text":"Create test namespace: kubectl create ns test Create a pod in this namespaces: cat <<EOF > echoserver-pod_ns.yaml apiVersion: v1 kind: Pod metadata: name: echoserverns spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.4 ports: - containerPort: 8080 EOF Create Pod in namespaces: kubectl create -f echoserver-pod_ns.yaml -n test Verify Pods created in specified namespaces: kubectl get pods -n test","title":"6.5 Create a pod in a different namespace"},{"location":"ycit019_Lab_7_Kubernetes_Concepts/#6-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts Step 2 Delete the firewall rule gcloud compute firewall-rules delete echoserver-node-port","title":"6 Cleaning Up"},{"location":"ycit019_Lab_8_Kubernetes_Features/","text":"Lab 8 Kubernetes Features Objective Use Liveness Probes to healthcheck you application while it is running Learn about secrets and configmaps Deploy a Daemonset and jobs 0 Create GKE Cluster \u00b6 Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-features \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-features us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-features --zone us-central1-c 1 Kubernetes Features \u00b6 1.1 Using Liveness Probes \u00b6 Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes p rovides liveness probes to detect and remedy such situations. As we already discussed Kubernetes provides 3 types of Probes to perform Liveness checks: HTTP GET EXEC tcpSocket In below example we are going to use HTTP GET probe for a Pod that runs a container based on the gcr.io/google_containers/liveness image. Step 1 Create http-liveness.yaml manifest with below content: cat <<EOF > http-liveness.yaml apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: gcr.io/google_containers/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 EOF Based on the manifest, we can see that the Pod has a single Container. The periodSeconds field specifies that the kubelet should perform a liveness probe every 3 seconds. The initialDelaySeconds field tells the kubelet that it should wait 3 seconds before performing the first probe. To perform a probe, the kubelet sends an HTTP GET request to the server that is running in the Container and listening on port 8080. If the handler for the server's /healthz path returns a success code, the kubelet considers the Container to be alive and healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it. Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure. Full code for for a reference in server.go . For the first 10 seconds that the Container is alive, the /healthz handler returns a status of 200. After that, the handler returns a status of 500. http . HandleFunc ( \"/healthz\" , func ( w http . ResponseWriter , r * http . Request ) { duration := time . Now (). Sub ( started ) if duration . Seconds () > 10 { w . WriteHeader ( 500 ) w . Write ([] byte ( fmt . Sprintf ( \"error: %v\" , duration . Seconds ()))) } else { w . WriteHeader ( 200 ) w . Write ([] byte ( \"ok\" )) } }) The kubelet starts performing health checks 3 seconds after the Container starts. So the first couple of health checks will succeed. But after 10 seconds, the health checks will fail, and the kubelet will kill and restart the Container. Step 2 Let's create a Pod and see how HTTP liveness check works: kubectl create -f http-liveness.yaml Step 3 Monitor the Pod watch kubectl get pod Result After 10 seconds Pods has beed restarted. Exit the shell session by using ctrl+c Step 4 Verify status of the Pod and review the Events happened after restart kubectl describe pod liveness-http Result Pod events shows that liveness probes have failed and the Container has been restarted. Step 5 Clean up kubectl delete -f http-liveness.yaml 1.2 Using ConfigMaps \u00b6 In Kubernetes ConfigMaps could be use in several cases: Storing configuration values as key-values in ConfigMap and referencing them in a Pod as environment variables Storing configurations as a file inside of ConfigMap and referencing it in a Pod as a Volume Let's try second option and deploy nginx pod while storing its config in a ConfigMap. Step 1 Create nginx my-nginx-config.conf config file as below: cat <<EOF > my-nginx-config.conf server { listen 80; server_name www.cloudnative.tech; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } EOF Step 2 Create ConfigMap from this file kubectl create configmap nginxconfig --from-file=my-nginx-config.conf Step 3 Review the ConfigMap kubectl describe cm nginxconfig Result: ``` Name: nginxconfig Namespace: default Labels: <none> Annotations: <none> Data ==== my-nginx-config.conf: ---- server { listen 80; server_name _; gzip off; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } Events: <none> ``` Step 4 Create Nginx Pod website.yaml file, where ConfigMap referenced as a Volume cat <<EOF > website.yaml apiVersion: v1 kind: Pod metadata: name: website spec: containers: - image: nginx:alpine name: website volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: false - name: config mountPath: /etc/nginx/conf.d readOnly: true volumes: - name: html emptyDir: {} - name: config configMap: name: nginxconfig EOF Step 5 Deploy Nginx Pod website.yaml kubectl apply -f website.yaml You can expose a service as we usually do with NodePort or LoadBalancer or by using the port-forward technicque in the next step Step 6 Open second SSH terminal by pressing \"+\" icon in the cloud shell and run following command kubectl port-forward website 8080:80 Result This opens a tunnel and expose our application on port '80' to localhost:8080. Step 7 Test that website is running Navigate back to the first terminal tab and run the following curl localhost:8080 Result: ``` <html> <head><title>403 Forbidden</title></head> <body> <center><h1>403 Forbidden</h1></center> <hr><center>nginx/1.21.0</center> </body> </html> ``` Success You've just learned how to use ConfigMaps. You can also use the preview feature with the console Step 8 start a shell in the pod by using exec kubectl exec website -it -- sh Your terminal now should have /# Step 9 view the content of the folder where we added the volume ls /etc/nginx/conf.d Step 10 check the content of the file in that folder and notice that it's the same as the config file for nginx we mounted. cat /etc/nginx/conf.d/my-nginx-config.conf Step 11 Exit the shell exit Make sure you're back to the cloud shell terminal. 1.3 Using Secrets \u00b6 1.3.1 Kubernetes Secrets \u00b6 Kubernetes secrets allow users to define sensitive information outside of containers and expose that information to containers through environment variables as well as files within Pods. In this section we will declare and create secrets to hold our database connection information that will be used by Wordpress to connect to its backend database. Step 1 Open up two terminal windows. We will use one window to generate encoded strings that will contain our sensitive data. The other window will be used to create the secrets YAML declaration. Step 2 In the first terminal window, execute the following commands to encode our strings: echo -n \"admin\" | base64 echo -n \"t0p-Secret\" | base64 Step 3 create the secret. For this lab, we added the encoded values for you. Feel free to change the username and password, and replace the values in the file below: cat <<EOF > app-secrets.yaml apiVersion: v1 kind: Secret metadata: name: app-secret type: Opaque data: username: YWRtaW4= password: dDBwLVNlY3JldA== EOF Step 4 Create the secret: kubectl create -f app-secrets.yaml Step 5 Verify secret creation and get details: kubectl get secrets Step 6 Get details of this secret: kubectl describe secrets/app-secrets Result: Name: app-secret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== password: 10 bytes username: 5 bytes Step 7 Create a pod that will reference the secret as a volume. Use either nano or vim to create secret-pod.yaml, and copy the following to it apiVersion: v1 kind: Pod metadata: name: secret-test-pod spec: containers: - name: test-container image: alpine command: [\"/bin/sh\", \"-ec\", \"export LOGIN=$(cat /etc/secret-volume/username);export PWD=$(cat /etc/secret-volume/password);while true; do echo hello $LOGIN your password is $PWD; sleep 10; done\"] volumeMounts: - name: secret-volume mountPath: /etc/secret-volume volumes: - name: secret-volume secret: secretName: app-secret restartPolicy: Never Step 8 View the logs of the pod. kubectl logs secret-test-pod Notice how we were able to pull the content of the secret using a command. Not very secure, right? Step 9 Delete both the pod and the secret kubectl delete pod secret-test-pod kubectl delete secret app-secret Summary Secrets has been created and they not visiable when you view them via kubectl describe This does not make Kubernetes secrets secure as we have experienced. Additonal measures need to be in place to protect secrets. 1.4 Jobs \u00b6 A Job creates one or more pods and ensures that a specified number of them successfully complete. A job keeps track of successful completion of a pod. When the specified number of pods have successfully completed, the job itself is complete. The job will start a new pod if the pod fails or is deleted due to hardware failure. A successful completion of the specified number of pods means the job is complete. This is different from a replica set or a deployment which ensures that a certain number of pods are always running. So if a pod in a replica set or deployment terminates, then it is restarted again. This makes replica set or deployment as long-running processes. This is well suited for a web server, such as NGINX. But a job is completed if the specified number of pods successfully completes. This is well suited for tasks that need to run only once. For example, a job may convert an image format from one to another. Restarting this pod in replication controller would not only cause redundant work but may be harmful in certain cases. Jobs are complementary to Replica Set. A Replica Set manages pods which are not expected to terminate (e.g. web servers), and a Job manages pods that are expected to terminate (e.g. batch jobs). 1.4.1 Non-parallel Job \u00b6 Only one pod per job is started, unless the pod fails. Job is complete as soon as the pod terminates successfully. Use the image \"busybox\" and have it sleep for 10 seconds and then complete. Run your job to be sure it works. Step 1 Create a Job Manifest cat <<EOF > busybox.yaml apiVersion: batch/v1 kind: Job metadata: name: busybox spec: template: spec: containers: - name: busybox image: busybox command: [\"sleep\", \"20\"] restartPolicy: Never EOF Step 2 Run a Job kubectl create -f busybox.yaml Step 3 Look at the job: kubectl get jobs Output: NAME DESIRED SUCCESSFUL AGE busybox 1 0 0s Result The output shows that the job is not successful yet. Step 4 Watch the pod status kubectl get -w pods Output: NAME READY STATUS RESTARTS AGE busybox-lk49x 1/1 Running 0 7s busybox-lk49x 0/1 Completed 0 24s Result It starts with pod for the job is Running . Then pod successfully exits after a few seconds and shows the Completed status. Step 5 Watch the job status again: kubectl get jobs Output: NAME COMPLETIONS DURATION AGE busybox 1/1 21s 1m Step 6 Delete a Job kubectl delete -f busybox.yaml 1.4.2 Parallel Job \u00b6 Non-parallel jobs run only one pod per job. This API is used to run multiple pods in parallel for the job. The number of pods to complete is defined by .spec.completions attribute in the configuration file. The number of pods to run in parallel is defined by .spec.parallelism attribute in the configuration file. The default value for both of these attributes is 1. The job is complete when there is one successful pod for each value in the range in 1 to .spec.completions . For that reason, it is also called as fixed completion count job. Step 1 Create a Job Manifest cat <<EOF > job-parallel.yaml apiVersion: batch/v1 kind: Job metadata: name: wait spec: completions: 6 parallelism: 2 template: metadata: name: wait spec: containers: - name: wait image: ubuntu command: [\"sleep\", \"10\"] restartPolicy: Never EOF Note This job specification is similar to the non-parallel job specification above. However it has two new attributes added: .spec.completions and .spec.parallelism . This means the job will be complete when six pods have successfully completed. A maximum of two pods will run in parallel at a given time. Step 2 Create a parallel job using the command: kubectl apply -f job-parallel.yaml Step 3 Watch the status of the job as shown: kubectl get -w jobs Output: NAME COMPLETIONS DURATION AGE wait 0/6 6s 6s wait 1/6 12s 12s wait 2/6 12s 12s wait 3/6 24s 24s wait 4/6 24s 24s wait 5/6 36s 36s wait 6/6 36s 36s Results The output shows that 2 pods are created about every 12 seconds. Step 4 In another terminal window, watch the status of pods created: kubectl get -w pods -l job-name=wait Output: NAME READY STATUS RESTARTS AGE wait-5blwm 0/1 Completed 0 17s wait-stmk4 0/1 Completed 0 17s wait-ts6xt 1/1 Running 0 5s wait-xlhl6 1/1 Running 0 5s wait-xlhl6 0/1 Completed 0 12s wait-rq6z5 0/1 Pending 0 0s wait-ts6xt 0/1 Completed 0 12s wait-rq6z5 0/1 Pending 0 0s wait-rq6z5 0/1 ContainerCreating 0 0s wait-f85bj 0/1 Pending 0 0s wait-f85bj 0/1 Pending 0 0s wait-f85bj 0/1 ContainerCreating 0 0s wait-rq6z5 1/1 Running 0 2s wait-f85bj 1/1 Running 0 2s wait-f85bj 0/1 Completed 0 12s wait-rq6z5 0/1 Completed 0 12s Step 6 Once the job is completed, you can get the list of completed pods kubectl get pods -l job-name=wait Result: NAME READY STATUS RESTARTS AGE wait-5blwm 0/1 Completed 0 2m55s wait-f85bj 0/1 Completed 0 2m31s wait-rq6z5 0/1 Completed 0 2m31s wait-stmk4 0/1 Completed 0 2m55s wait-ts6xt 0/1 Completed 0 2m43s wait-xlhl6 0/1 Completed 0 2m43s Step 5 Similarly, kubectl get jobs shows the status of the job after it has completed: kubectl get jobs Result: NAME COMPLETIONS DURATION AGE wait 6/6 36s 3m54s Step 6 Deleting a job deletes all the pods as well. Delete the job as: kubectl delete -f job-parallel.yaml 1.5 Cron Jobs \u00b6 A Cron Job is a job that runs on a given schedule, written in Cron format. There are two primary use cases: Run jobs once at a specified point in time Repeatedly at a specified point in time 1.5.1 Create Cron Job \u00b6 Step 1 Create CronJob manifest that prints the current timestamp and the message \" Hello World \" every minute. cat <<EOF > cronjob.yaml apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: metadata: labels: app: hello-cronpod spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello World! restartPolicy: OnFailure EOF Step 2 Create the Cron Job as shown in the command: kubectl create -f cronjob.yaml Step 3 Watch the status of the job as shown: kubectl get -w cronjobs Output : NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hello */1 * * * * False 0 <none> 6s hello */1 * * * * False 1 5s 39s hello */1 * * * * False 0 15s 49s hello */1 * * * * False 1 5s 99s hello */1 * * * * False 0 15s 109s hello */1 * * * * False 1 6s 2m40s hello */1 * * * * False 0 16s 2m50s Step 4 In another terminal window, watch the status of pods created: kubectl get -w pods -l app=hello-cronpod Output : NAME READY STATUS RESTARTS AGE hello-1622584020-kc46c 0/1 Completed 0 118s hello-1622584080-c2pcq 0/1 Completed 0 58s hello-1622584140-hxnv2 0/1 Pending 0 0s hello-1622584140-hxnv2 0/1 Pending 0 0s hello-1622584140-hxnv2 0/1 ContainerCreating 0 0s hello-1622584140-hxnv2 1/1 Running 0 1s hello-1622584140-hxnv2 0/1 Completed 0 2s Step 5 Get logs from one of the pods: kubectl logs hello-1622584140-hxnv2 Output : Tue Jun 1 21:49:07 UTC 2021 Hello World! Step 6 Delete Cron Job kubectl delete -f cronjob.yaml 1.6 Daemon Set \u00b6 Daemon Set ensures that a copy of the pod runs on a selected set of nodes. By default, all nodes in the cluster are selected. A selection critieria may be specified to select a limited number of nodes. As new nodes are added to the cluster, pods are started on them. As nodes are removed, pods are removed through garbage collection. The following is an example DaemonSet that runs a Prometheus exporter container that used for collecting machine metrics from each node. Step 1 Create a DaemonSet manifest cat <<EOF > daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: prometheus-daemonset spec: selector: matchLabels: tier: monitoring name: prometheus-exporter template: metadata: labels: tier: monitoring name: prometheus-exporter spec: containers: - name: prometheus image: prom/node-exporter ports: - containerPort: 80 EOF Step 2 Run the following command to create the ReplicaSet and pods: kubectl create -f daemonset.yaml --record Note The --record flag will track changes made through each revision. Step 3 Get basic details about the DaemonSet: kubectl get daemonsets/prometheus-daemonset Output: NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE-SELECTOR AGE prometheus-daemonset 1 1 1 1 1 <none> 5s Step 4 Get more details about the DaemonSet: kubectl describe daemonset/prometheus-daemonset Output: Name: prometheus-daemonset Selector: name=prometheus-exporter,tier=monitoring Node-Selector: <none> Labels: <none> Annotations: deprecated.daemonset.template.generation: 1 kubernetes.io/change-cause: kubectl create --filename=daemonset.yaml --record=true Desired Number of Nodes Scheduled: 2 Current Number of Nodes Scheduled: 2 Number of Nodes Scheduled with Up-to-date Pods: 2 Number of Nodes Scheduled with Available Pods: 2 Number of Nodes Misscheduled: 0 Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: name=prometheus-exporter tier=monitoring Containers: prometheus: Image: prom/node-exporter Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 9m daemonset-controller Created pod: prometheus-daemonset-f4xl2 Normal SuccessfulCreate 2m18s daemonset-controller Created pod: prometheus-daemonset-w596z Step 5 Get pods in the DaemonSet: kubectl get pods -l name=prometheus-exporter Output: NAME READY STATUS RESTARTS AGE prometheus-daemonset-f4xl2 1/1 Running 0 8m27s prometheus-daemonset-w596z 1/1 Running 0 105s Step 6 Verify that the Prometheus pod was successfully deployed to the cluster nodes: kubectl get pods -o wide Output: NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES prometheus-daemonset-f4xl2 1/1 Running 0 6m51s 10.0.0.19 gke-k8s-features-default-pool-73e09df7-m2lf <none> <none> prometheus-daemonset-w596z 1/1 Running 0 9s 10.0.1.2 gke-k8s-features-default-pool-73e09df7-k7hj <none> <none> Notes It is possible to Limit DaemonSets to specific nodes by changing the spec.template.spec to include a nodeSelector to matche node label. Step 7 Delete a DaemonSet kubectl delete -f daemonset.yaml 1.7 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"Lab 8 Kubernetes Features"},{"location":"ycit019_Lab_8_Kubernetes_Features/#0-create-gke-cluster","text":"Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-features \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-features us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-features --zone us-central1-c","title":"0 Create GKE Cluster"},{"location":"ycit019_Lab_8_Kubernetes_Features/#1-kubernetes-features","text":"","title":"1 Kubernetes Features"},{"location":"ycit019_Lab_8_Kubernetes_Features/#11-using-liveness-probes","text":"Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes p rovides liveness probes to detect and remedy such situations. As we already discussed Kubernetes provides 3 types of Probes to perform Liveness checks: HTTP GET EXEC tcpSocket In below example we are going to use HTTP GET probe for a Pod that runs a container based on the gcr.io/google_containers/liveness image. Step 1 Create http-liveness.yaml manifest with below content: cat <<EOF > http-liveness.yaml apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: gcr.io/google_containers/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 EOF Based on the manifest, we can see that the Pod has a single Container. The periodSeconds field specifies that the kubelet should perform a liveness probe every 3 seconds. The initialDelaySeconds field tells the kubelet that it should wait 3 seconds before performing the first probe. To perform a probe, the kubelet sends an HTTP GET request to the server that is running in the Container and listening on port 8080. If the handler for the server's /healthz path returns a success code, the kubelet considers the Container to be alive and healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it. Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure. Full code for for a reference in server.go . For the first 10 seconds that the Container is alive, the /healthz handler returns a status of 200. After that, the handler returns a status of 500. http . HandleFunc ( \"/healthz\" , func ( w http . ResponseWriter , r * http . Request ) { duration := time . Now (). Sub ( started ) if duration . Seconds () > 10 { w . WriteHeader ( 500 ) w . Write ([] byte ( fmt . Sprintf ( \"error: %v\" , duration . Seconds ()))) } else { w . WriteHeader ( 200 ) w . Write ([] byte ( \"ok\" )) } }) The kubelet starts performing health checks 3 seconds after the Container starts. So the first couple of health checks will succeed. But after 10 seconds, the health checks will fail, and the kubelet will kill and restart the Container. Step 2 Let's create a Pod and see how HTTP liveness check works: kubectl create -f http-liveness.yaml Step 3 Monitor the Pod watch kubectl get pod Result After 10 seconds Pods has beed restarted. Exit the shell session by using ctrl+c Step 4 Verify status of the Pod and review the Events happened after restart kubectl describe pod liveness-http Result Pod events shows that liveness probes have failed and the Container has been restarted. Step 5 Clean up kubectl delete -f http-liveness.yaml","title":"1.1 Using Liveness Probes"},{"location":"ycit019_Lab_8_Kubernetes_Features/#12-using-configmaps","text":"In Kubernetes ConfigMaps could be use in several cases: Storing configuration values as key-values in ConfigMap and referencing them in a Pod as environment variables Storing configurations as a file inside of ConfigMap and referencing it in a Pod as a Volume Let's try second option and deploy nginx pod while storing its config in a ConfigMap. Step 1 Create nginx my-nginx-config.conf config file as below: cat <<EOF > my-nginx-config.conf server { listen 80; server_name www.cloudnative.tech; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } EOF Step 2 Create ConfigMap from this file kubectl create configmap nginxconfig --from-file=my-nginx-config.conf Step 3 Review the ConfigMap kubectl describe cm nginxconfig Result: ``` Name: nginxconfig Namespace: default Labels: <none> Annotations: <none> Data ==== my-nginx-config.conf: ---- server { listen 80; server_name _; gzip off; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } Events: <none> ``` Step 4 Create Nginx Pod website.yaml file, where ConfigMap referenced as a Volume cat <<EOF > website.yaml apiVersion: v1 kind: Pod metadata: name: website spec: containers: - image: nginx:alpine name: website volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: false - name: config mountPath: /etc/nginx/conf.d readOnly: true volumes: - name: html emptyDir: {} - name: config configMap: name: nginxconfig EOF Step 5 Deploy Nginx Pod website.yaml kubectl apply -f website.yaml You can expose a service as we usually do with NodePort or LoadBalancer or by using the port-forward technicque in the next step Step 6 Open second SSH terminal by pressing \"+\" icon in the cloud shell and run following command kubectl port-forward website 8080:80 Result This opens a tunnel and expose our application on port '80' to localhost:8080. Step 7 Test that website is running Navigate back to the first terminal tab and run the following curl localhost:8080 Result: ``` <html> <head><title>403 Forbidden</title></head> <body> <center><h1>403 Forbidden</h1></center> <hr><center>nginx/1.21.0</center> </body> </html> ``` Success You've just learned how to use ConfigMaps. You can also use the preview feature with the console Step 8 start a shell in the pod by using exec kubectl exec website -it -- sh Your terminal now should have /# Step 9 view the content of the folder where we added the volume ls /etc/nginx/conf.d Step 10 check the content of the file in that folder and notice that it's the same as the config file for nginx we mounted. cat /etc/nginx/conf.d/my-nginx-config.conf Step 11 Exit the shell exit Make sure you're back to the cloud shell terminal.","title":"1.2 Using ConfigMaps"},{"location":"ycit019_Lab_8_Kubernetes_Features/#13-using-secrets","text":"","title":"1.3 Using Secrets"},{"location":"ycit019_Lab_8_Kubernetes_Features/#131-kubernetes-secrets","text":"Kubernetes secrets allow users to define sensitive information outside of containers and expose that information to containers through environment variables as well as files within Pods. In this section we will declare and create secrets to hold our database connection information that will be used by Wordpress to connect to its backend database. Step 1 Open up two terminal windows. We will use one window to generate encoded strings that will contain our sensitive data. The other window will be used to create the secrets YAML declaration. Step 2 In the first terminal window, execute the following commands to encode our strings: echo -n \"admin\" | base64 echo -n \"t0p-Secret\" | base64 Step 3 create the secret. For this lab, we added the encoded values for you. Feel free to change the username and password, and replace the values in the file below: cat <<EOF > app-secrets.yaml apiVersion: v1 kind: Secret metadata: name: app-secret type: Opaque data: username: YWRtaW4= password: dDBwLVNlY3JldA== EOF Step 4 Create the secret: kubectl create -f app-secrets.yaml Step 5 Verify secret creation and get details: kubectl get secrets Step 6 Get details of this secret: kubectl describe secrets/app-secrets Result: Name: app-secret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== password: 10 bytes username: 5 bytes Step 7 Create a pod that will reference the secret as a volume. Use either nano or vim to create secret-pod.yaml, and copy the following to it apiVersion: v1 kind: Pod metadata: name: secret-test-pod spec: containers: - name: test-container image: alpine command: [\"/bin/sh\", \"-ec\", \"export LOGIN=$(cat /etc/secret-volume/username);export PWD=$(cat /etc/secret-volume/password);while true; do echo hello $LOGIN your password is $PWD; sleep 10; done\"] volumeMounts: - name: secret-volume mountPath: /etc/secret-volume volumes: - name: secret-volume secret: secretName: app-secret restartPolicy: Never Step 8 View the logs of the pod. kubectl logs secret-test-pod Notice how we were able to pull the content of the secret using a command. Not very secure, right? Step 9 Delete both the pod and the secret kubectl delete pod secret-test-pod kubectl delete secret app-secret Summary Secrets has been created and they not visiable when you view them via kubectl describe This does not make Kubernetes secrets secure as we have experienced. Additonal measures need to be in place to protect secrets.","title":"1.3.1 Kubernetes Secrets"},{"location":"ycit019_Lab_8_Kubernetes_Features/#14-jobs","text":"A Job creates one or more pods and ensures that a specified number of them successfully complete. A job keeps track of successful completion of a pod. When the specified number of pods have successfully completed, the job itself is complete. The job will start a new pod if the pod fails or is deleted due to hardware failure. A successful completion of the specified number of pods means the job is complete. This is different from a replica set or a deployment which ensures that a certain number of pods are always running. So if a pod in a replica set or deployment terminates, then it is restarted again. This makes replica set or deployment as long-running processes. This is well suited for a web server, such as NGINX. But a job is completed if the specified number of pods successfully completes. This is well suited for tasks that need to run only once. For example, a job may convert an image format from one to another. Restarting this pod in replication controller would not only cause redundant work but may be harmful in certain cases. Jobs are complementary to Replica Set. A Replica Set manages pods which are not expected to terminate (e.g. web servers), and a Job manages pods that are expected to terminate (e.g. batch jobs).","title":"1.4 Jobs"},{"location":"ycit019_Lab_8_Kubernetes_Features/#141-non-parallel-job","text":"Only one pod per job is started, unless the pod fails. Job is complete as soon as the pod terminates successfully. Use the image \"busybox\" and have it sleep for 10 seconds and then complete. Run your job to be sure it works. Step 1 Create a Job Manifest cat <<EOF > busybox.yaml apiVersion: batch/v1 kind: Job metadata: name: busybox spec: template: spec: containers: - name: busybox image: busybox command: [\"sleep\", \"20\"] restartPolicy: Never EOF Step 2 Run a Job kubectl create -f busybox.yaml Step 3 Look at the job: kubectl get jobs Output: NAME DESIRED SUCCESSFUL AGE busybox 1 0 0s Result The output shows that the job is not successful yet. Step 4 Watch the pod status kubectl get -w pods Output: NAME READY STATUS RESTARTS AGE busybox-lk49x 1/1 Running 0 7s busybox-lk49x 0/1 Completed 0 24s Result It starts with pod for the job is Running . Then pod successfully exits after a few seconds and shows the Completed status. Step 5 Watch the job status again: kubectl get jobs Output: NAME COMPLETIONS DURATION AGE busybox 1/1 21s 1m Step 6 Delete a Job kubectl delete -f busybox.yaml","title":"1.4.1 Non-parallel Job"},{"location":"ycit019_Lab_8_Kubernetes_Features/#142-parallel-job","text":"Non-parallel jobs run only one pod per job. This API is used to run multiple pods in parallel for the job. The number of pods to complete is defined by .spec.completions attribute in the configuration file. The number of pods to run in parallel is defined by .spec.parallelism attribute in the configuration file. The default value for both of these attributes is 1. The job is complete when there is one successful pod for each value in the range in 1 to .spec.completions . For that reason, it is also called as fixed completion count job. Step 1 Create a Job Manifest cat <<EOF > job-parallel.yaml apiVersion: batch/v1 kind: Job metadata: name: wait spec: completions: 6 parallelism: 2 template: metadata: name: wait spec: containers: - name: wait image: ubuntu command: [\"sleep\", \"10\"] restartPolicy: Never EOF Note This job specification is similar to the non-parallel job specification above. However it has two new attributes added: .spec.completions and .spec.parallelism . This means the job will be complete when six pods have successfully completed. A maximum of two pods will run in parallel at a given time. Step 2 Create a parallel job using the command: kubectl apply -f job-parallel.yaml Step 3 Watch the status of the job as shown: kubectl get -w jobs Output: NAME COMPLETIONS DURATION AGE wait 0/6 6s 6s wait 1/6 12s 12s wait 2/6 12s 12s wait 3/6 24s 24s wait 4/6 24s 24s wait 5/6 36s 36s wait 6/6 36s 36s Results The output shows that 2 pods are created about every 12 seconds. Step 4 In another terminal window, watch the status of pods created: kubectl get -w pods -l job-name=wait Output: NAME READY STATUS RESTARTS AGE wait-5blwm 0/1 Completed 0 17s wait-stmk4 0/1 Completed 0 17s wait-ts6xt 1/1 Running 0 5s wait-xlhl6 1/1 Running 0 5s wait-xlhl6 0/1 Completed 0 12s wait-rq6z5 0/1 Pending 0 0s wait-ts6xt 0/1 Completed 0 12s wait-rq6z5 0/1 Pending 0 0s wait-rq6z5 0/1 ContainerCreating 0 0s wait-f85bj 0/1 Pending 0 0s wait-f85bj 0/1 Pending 0 0s wait-f85bj 0/1 ContainerCreating 0 0s wait-rq6z5 1/1 Running 0 2s wait-f85bj 1/1 Running 0 2s wait-f85bj 0/1 Completed 0 12s wait-rq6z5 0/1 Completed 0 12s Step 6 Once the job is completed, you can get the list of completed pods kubectl get pods -l job-name=wait Result: NAME READY STATUS RESTARTS AGE wait-5blwm 0/1 Completed 0 2m55s wait-f85bj 0/1 Completed 0 2m31s wait-rq6z5 0/1 Completed 0 2m31s wait-stmk4 0/1 Completed 0 2m55s wait-ts6xt 0/1 Completed 0 2m43s wait-xlhl6 0/1 Completed 0 2m43s Step 5 Similarly, kubectl get jobs shows the status of the job after it has completed: kubectl get jobs Result: NAME COMPLETIONS DURATION AGE wait 6/6 36s 3m54s Step 6 Deleting a job deletes all the pods as well. Delete the job as: kubectl delete -f job-parallel.yaml","title":"1.4.2 Parallel Job"},{"location":"ycit019_Lab_8_Kubernetes_Features/#15-cron-jobs","text":"A Cron Job is a job that runs on a given schedule, written in Cron format. There are two primary use cases: Run jobs once at a specified point in time Repeatedly at a specified point in time","title":"1.5 Cron Jobs"},{"location":"ycit019_Lab_8_Kubernetes_Features/#151-create-cron-job","text":"Step 1 Create CronJob manifest that prints the current timestamp and the message \" Hello World \" every minute. cat <<EOF > cronjob.yaml apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: metadata: labels: app: hello-cronpod spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello World! restartPolicy: OnFailure EOF Step 2 Create the Cron Job as shown in the command: kubectl create -f cronjob.yaml Step 3 Watch the status of the job as shown: kubectl get -w cronjobs Output : NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hello */1 * * * * False 0 <none> 6s hello */1 * * * * False 1 5s 39s hello */1 * * * * False 0 15s 49s hello */1 * * * * False 1 5s 99s hello */1 * * * * False 0 15s 109s hello */1 * * * * False 1 6s 2m40s hello */1 * * * * False 0 16s 2m50s Step 4 In another terminal window, watch the status of pods created: kubectl get -w pods -l app=hello-cronpod Output : NAME READY STATUS RESTARTS AGE hello-1622584020-kc46c 0/1 Completed 0 118s hello-1622584080-c2pcq 0/1 Completed 0 58s hello-1622584140-hxnv2 0/1 Pending 0 0s hello-1622584140-hxnv2 0/1 Pending 0 0s hello-1622584140-hxnv2 0/1 ContainerCreating 0 0s hello-1622584140-hxnv2 1/1 Running 0 1s hello-1622584140-hxnv2 0/1 Completed 0 2s Step 5 Get logs from one of the pods: kubectl logs hello-1622584140-hxnv2 Output : Tue Jun 1 21:49:07 UTC 2021 Hello World! Step 6 Delete Cron Job kubectl delete -f cronjob.yaml","title":"1.5.1 Create Cron Job"},{"location":"ycit019_Lab_8_Kubernetes_Features/#16-daemon-set","text":"Daemon Set ensures that a copy of the pod runs on a selected set of nodes. By default, all nodes in the cluster are selected. A selection critieria may be specified to select a limited number of nodes. As new nodes are added to the cluster, pods are started on them. As nodes are removed, pods are removed through garbage collection. The following is an example DaemonSet that runs a Prometheus exporter container that used for collecting machine metrics from each node. Step 1 Create a DaemonSet manifest cat <<EOF > daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: prometheus-daemonset spec: selector: matchLabels: tier: monitoring name: prometheus-exporter template: metadata: labels: tier: monitoring name: prometheus-exporter spec: containers: - name: prometheus image: prom/node-exporter ports: - containerPort: 80 EOF Step 2 Run the following command to create the ReplicaSet and pods: kubectl create -f daemonset.yaml --record Note The --record flag will track changes made through each revision. Step 3 Get basic details about the DaemonSet: kubectl get daemonsets/prometheus-daemonset Output: NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE-SELECTOR AGE prometheus-daemonset 1 1 1 1 1 <none> 5s Step 4 Get more details about the DaemonSet: kubectl describe daemonset/prometheus-daemonset Output: Name: prometheus-daemonset Selector: name=prometheus-exporter,tier=monitoring Node-Selector: <none> Labels: <none> Annotations: deprecated.daemonset.template.generation: 1 kubernetes.io/change-cause: kubectl create --filename=daemonset.yaml --record=true Desired Number of Nodes Scheduled: 2 Current Number of Nodes Scheduled: 2 Number of Nodes Scheduled with Up-to-date Pods: 2 Number of Nodes Scheduled with Available Pods: 2 Number of Nodes Misscheduled: 0 Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: name=prometheus-exporter tier=monitoring Containers: prometheus: Image: prom/node-exporter Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 9m daemonset-controller Created pod: prometheus-daemonset-f4xl2 Normal SuccessfulCreate 2m18s daemonset-controller Created pod: prometheus-daemonset-w596z Step 5 Get pods in the DaemonSet: kubectl get pods -l name=prometheus-exporter Output: NAME READY STATUS RESTARTS AGE prometheus-daemonset-f4xl2 1/1 Running 0 8m27s prometheus-daemonset-w596z 1/1 Running 0 105s Step 6 Verify that the Prometheus pod was successfully deployed to the cluster nodes: kubectl get pods -o wide Output: NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES prometheus-daemonset-f4xl2 1/1 Running 0 6m51s 10.0.0.19 gke-k8s-features-default-pool-73e09df7-m2lf <none> <none> prometheus-daemonset-w596z 1/1 Running 0 9s 10.0.1.2 gke-k8s-features-default-pool-73e09df7-k7hj <none> <none> Notes It is possible to Limit DaemonSets to specific nodes by changing the spec.template.spec to include a nodeSelector to matche node label. Step 7 Delete a DaemonSet kubectl delete -f daemonset.yaml","title":"1.6 Daemon Set"},{"location":"ycit019_Lab_8_Kubernetes_Features/#17-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"1.7 Cleaning Up"},{"location":"ycit019_Lab_9_Kubernetes_Scaling/","text":"1 Kubernetes Autoscaling \u00b6 Objective Resource & Limits Scheduling HPA VPA Cluster Autoscaling Node Auto provisioning (NAP) 0 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-scaling \\ --zone us-central1-c \\ --enable-vertical-pod-autoscaling \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-scaling --zone us-central1-c 1.1 Resource and Limits \u00b6 Step 1: Inspecting a node\u2019s capacity kubectl describe nodes | grep -A15 Capacity: The output shows two sets of amounts related to the available resources on the node: the node\u2019s capacity and allocatable resources. The capacity represents the total resources of a node, which may not all be available to pods. Certain resources may be reserved for Kubernetes and/or system components. The Scheduler bases its decisions only on the allocatable resource amounts. Step 2: Show metrics for a given node kubectl top nodes kubectl top pods -n kube-system Result CPU and Memory information is available for pods and node through the metrics API. Step 3 Create a deployment best_effort.yaml as showed below. This is regular deployment with resources configured cat <<EOF > best_effort.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs EOF Step 4 Deploy application kubectl create -f best_effort.yaml Step 5 Verify what is the QOS for this pod: kubectl describe pods | grep QoS Result If you don't specify request/limits K8s provides Best Effort QOS Step 6 Cleanup kubectl delete -f best_effort.yaml Step 7 Create a deployment guaranteed.yaml as showed below. This is regular deployment with resources configured cat <<EOF > guaranteed.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs resources: requests: cpu: 100m memory: 200Mi limits: cpu: 100m memory: 200Mi EOF Step 8 Deploy application kubectl create -f guaranteed.yaml Step 9 Verify what is the QOS for this pod: kubectl describe pods | grep QoS Result If you request = limits K8s provides guaranteed QOS Step 10 Cleanup kubectl delete -f guaranteed.yaml Step 11 Create a deployment burstable.yaml as showed below. This is regular deployment with resources configured cat <<EOF > burstable.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs resources: requests: cpu: 3000 EOF Step 12 Deploy application kubectl create -f burstable.yaml Step 13 Verify what is the QOS for this pod: kubectl describe pods | grep QoS Result If you specify request > or < limits K8s provides Burstable QOS Step 14 Check status of the Pods kubectl get pods Pending Why the deployment failed ??? Step 15 Cleanup kubectl delete -f burstable.yaml 1.2 Creating a Horizontal Pod Autoscaler based on CPU usage \u00b6 Prerequisites: Ensure metrics api is running in your cluster. kubectl get pod -n kube-system Check the status of metrics-server-***** pod status. It should be Running kubectl top nodes kubectl top pods -n kube-system Result CPU and Memory information is available for pods and node through the metrics API. Let\u2019s create a horizontal pod autoscaler now and configure it to scale pods based on their CPU utilization. Step 1 Create a deployment.yaml as showed below. This is regular deployment with resources configured cat <<EOF > deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs resources: requests: cpu: 100m EOF Step 2 Deploy application kubectl create -f deployment.yaml Step 3 After creating the deployment, to enable horizontal autoscaling of its pods, you need to create a HorizontalPodAutoscaler (HPA) object and point it to the deployment. kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5 Note This creates the HPA object for us and sets the deployment called kubia as the scaling target. We\u2019re setting the target CPU utilization of the pods to 30% and specifying the minimum and maximum number of replicas. The autoscaler will thus constantly keep adjusting the number of replicas to keep their CPU utilization around 30%, but it will never scale down to less than 1 or scale up to more than 5 replicas. Step 4 Verify definition of the Horizontal Pod Autoscaler resource to gain a better understanding of it: kubectl get hpa kubia -o yaml Result: apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler ... spec: maxReplicas: 5 minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: kubia targetCPUUtilizationPercentage: 30 status: currentReplicas: 0 desiredReplicas: 0 Step 5 Take a closer look at the HPA and notice that it is still not ready to do the autoscaling. kubectl describe hpa kubia Results Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedGetResourceMetric 2m29s horizontal-pod-autoscaler unable to get metrics for resource cpu: no metrics returned from resource metrics API Warning FailedComputeMetricsReplicas 2m29s horizontal-pod-autoscaler failed to compute desired number of replicas based on listed metrics for Deployment/default/kubia: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API Warning FailedGetResourceMetric 118s (x3 over 2m14s) horizontal-pod-autoscaler did not receive metrics for any ready pods Warning FailedComputeMetricsReplicas 118s (x3 over 2m14s) horizontal-pod-autoscaler failed to compute desired number of replicas based on listed metrics for Deployment/default/kubia: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: did not receive metrics for any ready pods Given that historical data is not available yet, you will see the above in the events section. Give it a minute or so and try again. Eventually, you will see the following in the Events section. Normal SuccessfulRescale 41s horizontal-pod-autoscaler New size: 1; reason: All metrics below target If you take a look at the kubia deployment, you will see it was scaled down from 3 pods to 1 pod. Step 6 Create a service kubectl expose deployment kubia --port=80 --target-port=8080 Step 7 Start another terminal session and run: watch -n 1 kubectl get hpa,deployment Step 8 Generate load to the Application kubectl run -it --rm --restart=Never loadgenerator --image=busybox \\ -- sh -c \"while true; do wget -O - -q http://kubia.default; done\" Step 9 Observe autoscaling In the other terminal you will start noticing that the deployment is being scaled up. Step 10 Terminate both sessions by pressing Ctrl+c 1.3 Scale size of pods with Vertical Pod Autoscaling \u00b6 Step 1 Verify that Vertical Pod Autoscaling has already been enabled on the cluster. We enabled VPA when we created the cluster, by using --enable-vertical-pod-autoscaling . This command can be handy if you want to check VPA on an existing cluster. gcloud container clusters describe k8s-scaling --zone us-central1-c | grep ^verticalPodAutoscaling -A 1 Step 2 Apply the hello-server deployment to your cluster kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:2.0 Step 3 Ensure the deployment was successfully created kubectl get deployment hello-server Step 4 Assign a CPU resource request of 100m to the deployment kubectl set resources deployment hello-server --requests=cpu=100m Step 5 Inspect the container specifics of the hello-server pods, find Requests section, and notice that this pod is currently requesting the 450m CPU we assigned. kubectl describe pod hello-server | sed -n \"/Containers:$/,/Conditions:/p\" Output Containers: hello-app: Image: gcr.io/google-samples/hello-app:2.0 Port: <none> Host Port: <none> Requests: cpu: 100m Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-rw2gr (ro) Conditions: Containers: hello-app: Container ID: containerd://e9bb428186f5d6a6572e81a5c0a9c37118fd2855f22173aa791d8429f35169a6 Image: gcr.io/google-samples/hello-app:2.0 Image ID: gcr.io/google-samples/hello-app@sha256:37e5287945774f27b418ce567cd77f4bbc9ef44a1bcd1a2312369f31f9cce567 Port: <none> Host Port: <none> State: Running Started: Wed, 09 Jun 2021 11:34:15 +0000 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-rw2gr (ro) Conditions: Step 6 Create a manifest for you Vertical Pod Autoscale cat << EOF > hello-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: hello-server-vpa spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: hello-server updatePolicy: updateMode: \"Off\" EOF Step 7 Apply the manifest for hello-vpa kubectl apply -f hello-vpa.yaml Step 8 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa hello-server-vpa Step 9 Locate the \"Container Recommendations\" at the end of the output from the describe command. If you don't see it, wait a little longer and try the previous command again. When it appears, you'll see several different recommendation types, each with values for CPU and memory: Lower Bound: this is the lower bound number VPA looks at for triggering a resize. If your pod utilization goes below this, VPA will delete the pod and scale it down. Target: this is the value VPA will use when resizing the pod. Uncapped Target: if no minimum or maximum capacity is assigned to the VPA, this will be the target utilization for VPA. Upper Bound: this is the upper bound number VPA looks at for triggering a resize. If your pod utilization goes above this, VPA will delete the pod and scale it up. Notice that the VPA is recommending new values for CPU instead of what we set, and also giving you a suggested number for how much memory should be requested. We can at this point manually apply these suggestions, or allow VPA to apply them. Step 10 Update the manifest to set the policy to Auto and apply the configuration sed -i 's/Off/Auto/g' hello-vpa.yaml kubectl apply -f hello-vpa.yaml In order to resize a pod, Vertical Pod Autoscaler will need to delete that pod and recreate it with the new size. By default, to avoid downtime, VPA will not delete and resize the last active pod. Because of this, you will need at least 2 replicas to see VPA make any changes. Step 11 Scale hello-server deployment to 2 replicas: kubectl scale deployment hello-server --replicas=2 Step 12 Watch your pods kubectl get pods -w Step 13 The VPA should have resized your pods in the hello-server deployment. Inspect your pods: kubectl describe pod hello-server | sed -n \"/Containers:$/,/Conditions:/p\" 1.7 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"Lab 9 Kubernetes Autoscaling"},{"location":"ycit019_Lab_9_Kubernetes_Scaling/#1-kubernetes-autoscaling","text":"Objective Resource & Limits Scheduling HPA VPA Cluster Autoscaling Node Auto provisioning (NAP)","title":"1 Kubernetes Autoscaling"},{"location":"ycit019_Lab_9_Kubernetes_Scaling/#0-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-scaling \\ --zone us-central1-c \\ --enable-vertical-pod-autoscaling \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-scaling --zone us-central1-c","title":"0 Create GKE Cluster"},{"location":"ycit019_Lab_9_Kubernetes_Scaling/#11-resource-and-limits","text":"Step 1: Inspecting a node\u2019s capacity kubectl describe nodes | grep -A15 Capacity: The output shows two sets of amounts related to the available resources on the node: the node\u2019s capacity and allocatable resources. The capacity represents the total resources of a node, which may not all be available to pods. Certain resources may be reserved for Kubernetes and/or system components. The Scheduler bases its decisions only on the allocatable resource amounts. Step 2: Show metrics for a given node kubectl top nodes kubectl top pods -n kube-system Result CPU and Memory information is available for pods and node through the metrics API. Step 3 Create a deployment best_effort.yaml as showed below. This is regular deployment with resources configured cat <<EOF > best_effort.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs EOF Step 4 Deploy application kubectl create -f best_effort.yaml Step 5 Verify what is the QOS for this pod: kubectl describe pods | grep QoS Result If you don't specify request/limits K8s provides Best Effort QOS Step 6 Cleanup kubectl delete -f best_effort.yaml Step 7 Create a deployment guaranteed.yaml as showed below. This is regular deployment with resources configured cat <<EOF > guaranteed.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs resources: requests: cpu: 100m memory: 200Mi limits: cpu: 100m memory: 200Mi EOF Step 8 Deploy application kubectl create -f guaranteed.yaml Step 9 Verify what is the QOS for this pod: kubectl describe pods | grep QoS Result If you request = limits K8s provides guaranteed QOS Step 10 Cleanup kubectl delete -f guaranteed.yaml Step 11 Create a deployment burstable.yaml as showed below. This is regular deployment with resources configured cat <<EOF > burstable.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs resources: requests: cpu: 3000 EOF Step 12 Deploy application kubectl create -f burstable.yaml Step 13 Verify what is the QOS for this pod: kubectl describe pods | grep QoS Result If you specify request > or < limits K8s provides Burstable QOS Step 14 Check status of the Pods kubectl get pods Pending Why the deployment failed ??? Step 15 Cleanup kubectl delete -f burstable.yaml","title":"1.1 Resource and Limits"},{"location":"ycit019_Lab_9_Kubernetes_Scaling/#12-creating-a-horizontal-pod-autoscaler-based-on-cpu-usage","text":"Prerequisites: Ensure metrics api is running in your cluster. kubectl get pod -n kube-system Check the status of metrics-server-***** pod status. It should be Running kubectl top nodes kubectl top pods -n kube-system Result CPU and Memory information is available for pods and node through the metrics API. Let\u2019s create a horizontal pod autoscaler now and configure it to scale pods based on their CPU utilization. Step 1 Create a deployment.yaml as showed below. This is regular deployment with resources configured cat <<EOF > deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs resources: requests: cpu: 100m EOF Step 2 Deploy application kubectl create -f deployment.yaml Step 3 After creating the deployment, to enable horizontal autoscaling of its pods, you need to create a HorizontalPodAutoscaler (HPA) object and point it to the deployment. kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5 Note This creates the HPA object for us and sets the deployment called kubia as the scaling target. We\u2019re setting the target CPU utilization of the pods to 30% and specifying the minimum and maximum number of replicas. The autoscaler will thus constantly keep adjusting the number of replicas to keep their CPU utilization around 30%, but it will never scale down to less than 1 or scale up to more than 5 replicas. Step 4 Verify definition of the Horizontal Pod Autoscaler resource to gain a better understanding of it: kubectl get hpa kubia -o yaml Result: apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler ... spec: maxReplicas: 5 minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: kubia targetCPUUtilizationPercentage: 30 status: currentReplicas: 0 desiredReplicas: 0 Step 5 Take a closer look at the HPA and notice that it is still not ready to do the autoscaling. kubectl describe hpa kubia Results Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedGetResourceMetric 2m29s horizontal-pod-autoscaler unable to get metrics for resource cpu: no metrics returned from resource metrics API Warning FailedComputeMetricsReplicas 2m29s horizontal-pod-autoscaler failed to compute desired number of replicas based on listed metrics for Deployment/default/kubia: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API Warning FailedGetResourceMetric 118s (x3 over 2m14s) horizontal-pod-autoscaler did not receive metrics for any ready pods Warning FailedComputeMetricsReplicas 118s (x3 over 2m14s) horizontal-pod-autoscaler failed to compute desired number of replicas based on listed metrics for Deployment/default/kubia: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: did not receive metrics for any ready pods Given that historical data is not available yet, you will see the above in the events section. Give it a minute or so and try again. Eventually, you will see the following in the Events section. Normal SuccessfulRescale 41s horizontal-pod-autoscaler New size: 1; reason: All metrics below target If you take a look at the kubia deployment, you will see it was scaled down from 3 pods to 1 pod. Step 6 Create a service kubectl expose deployment kubia --port=80 --target-port=8080 Step 7 Start another terminal session and run: watch -n 1 kubectl get hpa,deployment Step 8 Generate load to the Application kubectl run -it --rm --restart=Never loadgenerator --image=busybox \\ -- sh -c \"while true; do wget -O - -q http://kubia.default; done\" Step 9 Observe autoscaling In the other terminal you will start noticing that the deployment is being scaled up. Step 10 Terminate both sessions by pressing Ctrl+c","title":"1.2 Creating a Horizontal Pod Autoscaler based on CPU usage"},{"location":"ycit019_Lab_9_Kubernetes_Scaling/#13-scale-size-of-pods-with-vertical-pod-autoscaling","text":"Step 1 Verify that Vertical Pod Autoscaling has already been enabled on the cluster. We enabled VPA when we created the cluster, by using --enable-vertical-pod-autoscaling . This command can be handy if you want to check VPA on an existing cluster. gcloud container clusters describe k8s-scaling --zone us-central1-c | grep ^verticalPodAutoscaling -A 1 Step 2 Apply the hello-server deployment to your cluster kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:2.0 Step 3 Ensure the deployment was successfully created kubectl get deployment hello-server Step 4 Assign a CPU resource request of 100m to the deployment kubectl set resources deployment hello-server --requests=cpu=100m Step 5 Inspect the container specifics of the hello-server pods, find Requests section, and notice that this pod is currently requesting the 450m CPU we assigned. kubectl describe pod hello-server | sed -n \"/Containers:$/,/Conditions:/p\" Output Containers: hello-app: Image: gcr.io/google-samples/hello-app:2.0 Port: <none> Host Port: <none> Requests: cpu: 100m Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-rw2gr (ro) Conditions: Containers: hello-app: Container ID: containerd://e9bb428186f5d6a6572e81a5c0a9c37118fd2855f22173aa791d8429f35169a6 Image: gcr.io/google-samples/hello-app:2.0 Image ID: gcr.io/google-samples/hello-app@sha256:37e5287945774f27b418ce567cd77f4bbc9ef44a1bcd1a2312369f31f9cce567 Port: <none> Host Port: <none> State: Running Started: Wed, 09 Jun 2021 11:34:15 +0000 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-rw2gr (ro) Conditions: Step 6 Create a manifest for you Vertical Pod Autoscale cat << EOF > hello-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: hello-server-vpa spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: hello-server updatePolicy: updateMode: \"Off\" EOF Step 7 Apply the manifest for hello-vpa kubectl apply -f hello-vpa.yaml Step 8 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa hello-server-vpa Step 9 Locate the \"Container Recommendations\" at the end of the output from the describe command. If you don't see it, wait a little longer and try the previous command again. When it appears, you'll see several different recommendation types, each with values for CPU and memory: Lower Bound: this is the lower bound number VPA looks at for triggering a resize. If your pod utilization goes below this, VPA will delete the pod and scale it down. Target: this is the value VPA will use when resizing the pod. Uncapped Target: if no minimum or maximum capacity is assigned to the VPA, this will be the target utilization for VPA. Upper Bound: this is the upper bound number VPA looks at for triggering a resize. If your pod utilization goes above this, VPA will delete the pod and scale it up. Notice that the VPA is recommending new values for CPU instead of what we set, and also giving you a suggested number for how much memory should be requested. We can at this point manually apply these suggestions, or allow VPA to apply them. Step 10 Update the manifest to set the policy to Auto and apply the configuration sed -i 's/Off/Auto/g' hello-vpa.yaml kubectl apply -f hello-vpa.yaml In order to resize a pod, Vertical Pod Autoscaler will need to delete that pod and recreate it with the new size. By default, to avoid downtime, VPA will not delete and resize the last active pod. Because of this, you will need at least 2 replicas to see VPA make any changes. Step 11 Scale hello-server deployment to 2 replicas: kubectl scale deployment hello-server --replicas=2 Step 12 Watch your pods kubectl get pods -w Step 13 The VPA should have resized your pods in the hello-server deployment. Inspect your pods: kubectl describe pod hello-server | sed -n \"/Containers:$/,/Conditions:/p\"","title":"1.3 Scale size of pods with Vertical Pod Autoscaling"},{"location":"ycit019_Lab_9_Kubernetes_Scaling/#17-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"1.7 Cleaning Up"},{"location":"ycit019_ass1/","text":"1 Containerize Applications \u00b6 Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1.1 Overview of the Sample Application \u00b6 This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application. 1.1 Build Dockers image for frontend application \u00b6 Step 1 Locate and review the go source code: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Setup vim editor with appropriate color schema Note Skip this step if you use another editor. echo \"colo elflord\" > ~/.vimrc Step 3 Write Dockerfile for your frontend application cd ~/ycit019/Assignment1/gowebapp Create a file named Dockerfile in this directory for the frontend Go app. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1 1.2 Build Docker image for backend application \u00b6 Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally 1.3 Test application by running with Docker Engine. \u00b6 Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network named gowebapp with subnet range 172.19.0.0/16 in which to run the frontend and backend containers: #TODO docker xxx Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Container needs to run on network: `gowebapp` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine #TODO Container needs to run on network: `gowebapp` Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit; 1.5 Cleanup running applications and unused networks \u00b6 ### TODO docker xxx","title":"Assignment1"},{"location":"ycit019_ass1/#1-containerize-applications","text":"Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process","title":"1 Containerize Applications"},{"location":"ycit019_ass1/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ycit019_ass1/#11-overview-of-the-sample-application","text":"This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application.","title":"1.1 Overview of the Sample Application"},{"location":"ycit019_ass1/#11-build-dockers-image-for-frontend-application","text":"Step 1 Locate and review the go source code: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Setup vim editor with appropriate color schema Note Skip this step if you use another editor. echo \"colo elflord\" > ~/.vimrc Step 3 Write Dockerfile for your frontend application cd ~/ycit019/Assignment1/gowebapp Create a file named Dockerfile in this directory for the frontend Go app. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1","title":"1.1 Build Dockers image for frontend application"},{"location":"ycit019_ass1/#12-build-docker-image-for-backend-application","text":"Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally","title":"1.2 Build Docker image for backend application"},{"location":"ycit019_ass1/#13-test-application-by-running-with-docker-engine","text":"Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network named gowebapp with subnet range 172.19.0.0/16 in which to run the frontend and backend containers: #TODO docker xxx Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Container needs to run on network: `gowebapp` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine #TODO Container needs to run on network: `gowebapp` Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit;","title":"1.3 Test application by running with Docker Engine."},{"location":"ycit019_ass1/#15-cleanup-running-applications-and-unused-networks","text":"### TODO docker xxx","title":"1.5 Cleanup running applications and unused networks"},{"location":"ycit019_ass1_sol/","text":"1 Containerize Applications \u00b6 Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1.1 Overview of the Sample Application \u00b6 This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application. 1.1 Build Dockers image for frontend application \u00b6 Step 1 Locate and review the go source code folders: https://github.com/Cloud-Architects-Program/k8s cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Write Dockerfile for your frontend application Create a file named Dockerfile in this directory for the frontend Go application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: cd ~/ycit019/Assignment1/gowebapp vim Dockerfile FROM golang:1.15.11 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 3 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. docker build -t <user-name>/gowebapp:v1 . 1.2 Build Docker image for backend application \u00b6 Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1//gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Build gowebapp-mysql Docker image locally docker build -t <user-name>/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally 1.3 Store images in the Dockerhub \u00b6 docker login docker push <user-name>/gowebapp-mysql docker push <user-name>/gowebapp 1.4 Test application by running with Docker Engine. \u00b6 Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd <user-name>/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp <user-name>/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: docker exec -it gowebapp-mysql bash Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit; 1.5 Cleanup running applications and unused networks \u00b6 docker rm -f $(docker ps -q)","title":"Assignment1 - Solution"},{"location":"ycit019_ass1_sol/#1-containerize-applications","text":"Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process","title":"1 Containerize Applications"},{"location":"ycit019_ass1_sol/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ycit019_ass1_sol/#11-overview-of-the-sample-application","text":"This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application.","title":"1.1 Overview of the Sample Application"},{"location":"ycit019_ass1_sol/#11-build-dockers-image-for-frontend-application","text":"Step 1 Locate and review the go source code folders: https://github.com/Cloud-Architects-Program/k8s cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Write Dockerfile for your frontend application Create a file named Dockerfile in this directory for the frontend Go application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: cd ~/ycit019/Assignment1/gowebapp vim Dockerfile FROM golang:1.15.11 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 3 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. docker build -t <user-name>/gowebapp:v1 .","title":"1.1 Build Dockers image for frontend application"},{"location":"ycit019_ass1_sol/#12-build-docker-image-for-backend-application","text":"Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1//gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Build gowebapp-mysql Docker image locally docker build -t <user-name>/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally","title":"1.2 Build Docker image for backend application"},{"location":"ycit019_ass1_sol/#13-store-images-in-the-dockerhub","text":"docker login docker push <user-name>/gowebapp-mysql docker push <user-name>/gowebapp","title":"1.3 Store images in the Dockerhub"},{"location":"ycit019_ass1_sol/#14-test-application-by-running-with-docker-engine","text":"Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd <user-name>/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp <user-name>/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: docker exec -it gowebapp-mysql bash Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit;","title":"1.4 Test application by running with Docker Engine."},{"location":"ycit019_ass1_sol/#15-cleanup-running-applications-and-unused-networks","text":"docker rm -f $(docker ps -q)","title":"1.5 Cleanup running applications and unused networks"},{"location":"ycit019_ass2/","text":"1 Containerize Applications \u00b6 Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1 Configure Cloud Source Repository \u00b6 Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console. 1.1 Create a repository with Cloud Source Repository \u00b6 Step 1 Start a new session in Cloud Shell and clone repository with Assignment2: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 Result Two folders with go app and mysql config has been reviewed. Step 2 Run the following command to create a new Cloud Source Repository named $student_id-notepad, where $student_id - is you mcgill student-ID: gcloud source repos create $student_id-notepad You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: MY_REPO=$student_id-notepad gcloud source repos clone $MY_REPO The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: MY_REPO=$student_id-notepad cd $MY_REPO Step 4 Copy Assignment 2 code to you repo: cp -r ~/ycit019/Assignment2/* . Step 5 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" git add . git commit -m \"Repo Structure\" Output: [master (root-commit) 48c4f03] Repo Structure 82 files changed, 3686 insertions(+) ... Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Git pushes the sample application files from the master branch to the origin remote: Output: Enumerating objects: 119, done. Counting objects: 100% (119/119), done. Delta compression using up to 4 threads Compressing objects: 100% (104/104), done. Writing objects: 100% (119/119), 224.31 KiB | 5.47 MiB/s, done. Total 119 (delta 22), reused 0 (delta 0) remote: Resolving deltas: 100% (22/22) To https://source.developers.google.com/p/mcgil-stundent/r/notepad-gowebapp * [new branch] master -> master Browse files in the Google Cloud Source repository Step 7 Browse files in the Google Cloud Source repository Use the Google Cloud Source Repositories source code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Assignment 2 files you pushed to the repository by opening the Navigation menu and selecting Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. Step 8 View a file in the Google Cloud repository Click $MY_REPO > gowebapp to view the file's contents in the source code browser Click $MY_REPO > gowebapp-mysql to view the file's contents in the source code browser 2 Build and push Docker images to Google Container Registry (GCR) \u00b6 2.1 Build and push gowebapp-mysql Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 2.2 Build and push gowebapp Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 2.3 Test application by running with Docker Engine. \u00b6 Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. 2.4 Cleanup running applications and unused networks \u00b6 ### TODO docker xxx 3 Docker Compose \u00b6 3.1 Test application locally with Docker Compose \u00b6 Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v2 documentations Note Starting from Docker Compose 1.27.0+, v2 and v3 format has been merged, there for if you using latest version above 1.27.0+ you do NOT need to specify version of docker-compose, otherwise you need to specify version 2 or 3. Where version 3 mainly used for docker swarm deployment. Implementation Ensure that mysql start first and then webapp services Ensure that mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Use version 2 or 3 of compose Step 1 Create compose file cd ~/$MY_REPO/ vim docker-compose.yaml Create structure as following, replace #TODO with correct values. version: '2.4' services: gowebapp-mysql: build: #TODO ... gowebapp: build: #TODO ... networks: #TODO docker-compose up -d Step 3 Test application Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!! 3.2 Commit docker-compose file to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.3 Grant viewing permissions for a repository to Instructors/Teachers \u00b6 Reference document gcloud projects add-iam-policy-binding mcgil-stundent --member='user:ayrat.khayretdinov@gmail.com' --role=roles/viewer gcloud projects add-iam-policy-binding mcgil-stundent --member='user:dima.kassab@gmail.com' --role=roles/viewer Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"Assignment2"},{"location":"ycit019_ass2/#1-containerize-applications","text":"Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose","title":"1 Containerize Applications"},{"location":"ycit019_ass2/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ycit019_ass2/#1-configure-cloud-source-repository","text":"Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console.","title":"1 Configure Cloud Source Repository"},{"location":"ycit019_ass2/#11-create-a-repository-with-cloud-source-repository","text":"Step 1 Start a new session in Cloud Shell and clone repository with Assignment2: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 Result Two folders with go app and mysql config has been reviewed. Step 2 Run the following command to create a new Cloud Source Repository named $student_id-notepad, where $student_id - is you mcgill student-ID: gcloud source repos create $student_id-notepad You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: MY_REPO=$student_id-notepad gcloud source repos clone $MY_REPO The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: MY_REPO=$student_id-notepad cd $MY_REPO Step 4 Copy Assignment 2 code to you repo: cp -r ~/ycit019/Assignment2/* . Step 5 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" git add . git commit -m \"Repo Structure\" Output: [master (root-commit) 48c4f03] Repo Structure 82 files changed, 3686 insertions(+) ... Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Git pushes the sample application files from the master branch to the origin remote: Output: Enumerating objects: 119, done. Counting objects: 100% (119/119), done. Delta compression using up to 4 threads Compressing objects: 100% (104/104), done. Writing objects: 100% (119/119), 224.31 KiB | 5.47 MiB/s, done. Total 119 (delta 22), reused 0 (delta 0) remote: Resolving deltas: 100% (22/22) To https://source.developers.google.com/p/mcgil-stundent/r/notepad-gowebapp * [new branch] master -> master Browse files in the Google Cloud Source repository Step 7 Browse files in the Google Cloud Source repository Use the Google Cloud Source Repositories source code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Assignment 2 files you pushed to the repository by opening the Navigation menu and selecting Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. Step 8 View a file in the Google Cloud repository Click $MY_REPO > gowebapp to view the file's contents in the source code browser Click $MY_REPO > gowebapp-mysql to view the file's contents in the source code browser","title":"1.1 Create a repository with Cloud Source Repository"},{"location":"ycit019_ass2/#2-build-and-push-docker-images-to-google-container-registry-gcr","text":"","title":"2 Build and push Docker images to Google Container Registry (GCR)"},{"location":"ycit019_ass2/#21-build-and-push-gowebapp-mysql-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"2.1 Build and push gowebapp-mysql Image to GCR"},{"location":"ycit019_ass2/#22-build-and-push-gowebapp-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"2.2 Build and push gowebapp Image to GCR"},{"location":"ycit019_ass2/#23-test-application-by-running-with-docker-engine","text":"Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed.","title":"2.3 Test application by running with Docker Engine."},{"location":"ycit019_ass2/#24-cleanup-running-applications-and-unused-networks","text":"### TODO docker xxx","title":"2.4 Cleanup running applications and unused networks"},{"location":"ycit019_ass2/#3-docker-compose","text":"","title":"3 Docker Compose"},{"location":"ycit019_ass2/#31-test-application-locally-with-docker-compose","text":"Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v2 documentations Note Starting from Docker Compose 1.27.0+, v2 and v3 format has been merged, there for if you using latest version above 1.27.0+ you do NOT need to specify version of docker-compose, otherwise you need to specify version 2 or 3. Where version 3 mainly used for docker swarm deployment. Implementation Ensure that mysql start first and then webapp services Ensure that mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Use version 2 or 3 of compose Step 1 Create compose file cd ~/$MY_REPO/ vim docker-compose.yaml Create structure as following, replace #TODO with correct values. version: '2.4' services: gowebapp-mysql: build: #TODO ... gowebapp: build: #TODO ... networks: #TODO docker-compose up -d Step 3 Test application Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!!","title":"3.1 Test application locally with Docker Compose"},{"location":"ycit019_ass2/#32-commit-docker-compose-file-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"3.2 Commit docker-compose file to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass2/#33-grant-viewing-permissions-for-a-repository-to-instructorsteachers","text":"Reference document gcloud projects add-iam-policy-binding mcgil-stundent --member='user:ayrat.khayretdinov@gmail.com' --role=roles/viewer gcloud projects add-iam-policy-binding mcgil-stundent --member='user:dima.kassab@gmail.com' --role=roles/viewer Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"3.3 Grant viewing permissions for a repository to Instructors/Teachers"},{"location":"ycit019_ass2_solution/","text":"1 Containerize Applications \u00b6 Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. Cloud Source Repositories: Qwik Start 1 Configure Cloud Source Repository \u00b6 Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console. 1.1 Create a repository with Cloud Source Repository \u00b6 Step 1 Start a new session in Cloud Shell and clone repository with Assignment2: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 Result Two folders with go app and mysql config has been reviewed. Step 2 Run the following command to create a new Cloud Source Repository named $student_id-notepad, where $student_id - is you mcgill student-ID: gcloud source repos create $student_id-notepad You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: MY_REPO=$student_id-notepad gcloud source repos clone $MY_REPO The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: cd $MY_REPO Step 4 Copy Assignment 2 code to you repo: cp -r ~/ycit019/Assignment2/* . Step 5 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" git add . git commit -m \"Repo Structure\" Output: [master (root-commit) 48c4f03] Repo Structure 82 files changed, 3686 insertions(+) ... Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Git pushes the sample application files from the master branch to the origin remote: Output: Enumerating objects: 119, done. Counting objects: 100% (119/119), done. Delta compression using up to 4 threads Compressing objects: 100% (104/104), done. Writing objects: 100% (119/119), 224.31 KiB | 5.47 MiB/s, done. Total 119 (delta 22), reused 0 (delta 0) remote: Resolving deltas: 100% (22/22) To https://source.developers.google.com/p/mcgil-stundent/r/notepad-gowebapp * [new branch] master -> master Browse files in the Google Cloud Source repository Step 7 Browse files in the Google Cloud Source repository Use the Google Cloud Source Repositories source code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Assignment 2 files you pushed to the repository by opening the Navigation menu and selecting Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. Step 8 View a file in the Google Cloud repository Click $MY_REPO > gowebapp to view the file's contents in the source code browser Click $MY_REPO > gowebapp-mysql to view the file's contents in the source code browser 2 Build and push Docker images to Google Container Registry (GCR) \u00b6 2.1 Build and push gowebapp-mysql Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 2.1 Build and push gowebapp Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 2.3 Test application by running with Docker Engine. \u00b6 Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. 2.4 Cleanup running applications and unused networks \u00b6 ### TODO docker xxx 3 Docker Compose \u00b6 3.1 Test application locally with Docker Compose \u00b6 Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v2 documentations Note Starting from Docker Compose 1.27.0+, v2 and v3 format has been merged, there for if you using latest version above 1.27.0+ you do need to specify version of docker-compose. Implementation Ensure that Mysql start first and then webapp services Ensure that Mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Step 1 Create compose file cd ~/$MY_REPO/ vim docker-compose.yaml Create structure as following: version: '2.4' services: gowebapp-mysql: build: ./gowebapp-mysql environment: MYSQL_ROOT_PASSWORD: rootpasswd container_name: gowebapp-mysql healthcheck: test: \"/usr/bin/mysql --user=root --password=rootpasswd --execute \\\"SHOW DATABASES;\\\"\" interval: 2s timeout: 5s retries: 30 networks: - gowebapp1 gowebapp: build: ./gowebapp container_name: gowebapp ports: - \"8080:80\" depends_on: gowebapp-mysql: condition: service_healthy networks: - gowebapp1 networks: gowebapp1: driver: bridge Note In this case we using Compose v2.4 version due to support of healthcheck feature. Step 2 Run compose file export CLOUDSDK_PYTHON=/usr/bin/python # https://github.com/google-github-actions/setup-gcloud/issues/128 docker-compose up -d Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!! 3.2 Commit docker-compose file to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.3 Grant viewing permissions for a repository to Instructors/Teachers \u00b6 Reference document gcloud projects add-iam-policy-binding mcgil-stundent --member='user:ayrat.khayretdinov@gmail.com' --role=roles/viewer gcloud projects add-iam-policy-binding mcgil-stundent --member='user:dima.kassab@gmail.com' --role=roles/viewer Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"Assignment2 - Solution"},{"location":"ycit019_ass2_solution/#1-containerize-applications","text":"Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose","title":"1 Containerize Applications"},{"location":"ycit019_ass2_solution/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. Cloud Source Repositories: Qwik Start","title":"Prepare Lab Environment"},{"location":"ycit019_ass2_solution/#1-configure-cloud-source-repository","text":"Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console.","title":"1 Configure Cloud Source Repository"},{"location":"ycit019_ass2_solution/#11-create-a-repository-with-cloud-source-repository","text":"Step 1 Start a new session in Cloud Shell and clone repository with Assignment2: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 Result Two folders with go app and mysql config has been reviewed. Step 2 Run the following command to create a new Cloud Source Repository named $student_id-notepad, where $student_id - is you mcgill student-ID: gcloud source repos create $student_id-notepad You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: MY_REPO=$student_id-notepad gcloud source repos clone $MY_REPO The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: cd $MY_REPO Step 4 Copy Assignment 2 code to you repo: cp -r ~/ycit019/Assignment2/* . Step 5 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" git add . git commit -m \"Repo Structure\" Output: [master (root-commit) 48c4f03] Repo Structure 82 files changed, 3686 insertions(+) ... Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Git pushes the sample application files from the master branch to the origin remote: Output: Enumerating objects: 119, done. Counting objects: 100% (119/119), done. Delta compression using up to 4 threads Compressing objects: 100% (104/104), done. Writing objects: 100% (119/119), 224.31 KiB | 5.47 MiB/s, done. Total 119 (delta 22), reused 0 (delta 0) remote: Resolving deltas: 100% (22/22) To https://source.developers.google.com/p/mcgil-stundent/r/notepad-gowebapp * [new branch] master -> master Browse files in the Google Cloud Source repository Step 7 Browse files in the Google Cloud Source repository Use the Google Cloud Source Repositories source code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Assignment 2 files you pushed to the repository by opening the Navigation menu and selecting Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. Step 8 View a file in the Google Cloud repository Click $MY_REPO > gowebapp to view the file's contents in the source code browser Click $MY_REPO > gowebapp-mysql to view the file's contents in the source code browser","title":"1.1 Create a repository with Cloud Source Repository"},{"location":"ycit019_ass2_solution/#2-build-and-push-docker-images-to-google-container-registry-gcr","text":"","title":"2 Build and push Docker images to Google Container Registry (GCR)"},{"location":"ycit019_ass2_solution/#21-build-and-push-gowebapp-mysql-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"2.1 Build and push gowebapp-mysql Image to GCR"},{"location":"ycit019_ass2_solution/#21-build-and-push-gowebapp-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"2.1 Build and push gowebapp Image to GCR"},{"location":"ycit019_ass2_solution/#23-test-application-by-running-with-docker-engine","text":"Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed.","title":"2.3 Test application by running with Docker Engine."},{"location":"ycit019_ass2_solution/#24-cleanup-running-applications-and-unused-networks","text":"### TODO docker xxx","title":"2.4 Cleanup running applications and unused networks"},{"location":"ycit019_ass2_solution/#3-docker-compose","text":"","title":"3 Docker Compose"},{"location":"ycit019_ass2_solution/#31-test-application-locally-with-docker-compose","text":"Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v2 documentations Note Starting from Docker Compose 1.27.0+, v2 and v3 format has been merged, there for if you using latest version above 1.27.0+ you do need to specify version of docker-compose. Implementation Ensure that Mysql start first and then webapp services Ensure that Mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Step 1 Create compose file cd ~/$MY_REPO/ vim docker-compose.yaml Create structure as following: version: '2.4' services: gowebapp-mysql: build: ./gowebapp-mysql environment: MYSQL_ROOT_PASSWORD: rootpasswd container_name: gowebapp-mysql healthcheck: test: \"/usr/bin/mysql --user=root --password=rootpasswd --execute \\\"SHOW DATABASES;\\\"\" interval: 2s timeout: 5s retries: 30 networks: - gowebapp1 gowebapp: build: ./gowebapp container_name: gowebapp ports: - \"8080:80\" depends_on: gowebapp-mysql: condition: service_healthy networks: - gowebapp1 networks: gowebapp1: driver: bridge Note In this case we using Compose v2.4 version due to support of healthcheck feature. Step 2 Run compose file export CLOUDSDK_PYTHON=/usr/bin/python # https://github.com/google-github-actions/setup-gcloud/issues/128 docker-compose up -d Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!!","title":"3.1 Test application locally with Docker Compose"},{"location":"ycit019_ass2_solution/#32-commit-docker-compose-file-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"3.2 Commit docker-compose file to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass2_solution/#33-grant-viewing-permissions-for-a-repository-to-instructorsteachers","text":"Reference document gcloud projects add-iam-policy-binding mcgil-stundent --member='user:ayrat.khayretdinov@gmail.com' --role=roles/viewer gcloud projects add-iam-policy-binding mcgil-stundent --member='user:dima.kassab@gmail.com' --role=roles/viewer Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"3.3 Grant viewing permissions for a repository to Instructors/Teachers"},{"location":"ycit019_ass3/","text":"1 Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating NameSpaces Review process of changing Context Review process of creating K8s: Services Labels, Selectors Deployments Rolling Updates 1.1 Development Tools \u00b6 Note This steps is optional Step 1 Choose one of the following option to develop the YAML manifests: Option 1: You can develop in Google Cloud Shell Editor Option 2: You can also develop locally on your laptop using VCScode . We recommend to use it in conjunction with VSC YAML extension from Redhat Option 3: Use your preferred text editor on Linux VM (vim, nano). 2.1 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c 2.2 Setup KUBECTL AUTOCOMPLETE \u00b6 Since we going to use a lot of kubectl cli let's setup autocomplete. source <(kubectl completion bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 2.3 Create 'dev' namespace and make it default. \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl #TO DO(create namespace `dev`) Step 2 Use dev context to create K8s resources inside this namespace. kubectl #TO DO (make `dev` context default) Step 3 Verify current context: kubectl config current-context Result dev 2.4 Create Service Object for MySQL \u00b6 Step 1 Locate directory where kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment3/ ls Step 2 Go into the local repository you've created: cd ~/$MY_REPO Step 3 Copy Assignment 3 deploy folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019/Assignment3/deploy . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests in deploy folder\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Define a Kubernetes Service object for the backend MySQL database. cd ~/$MY_REPO/deploy Follow instructions below to populate gowebapp-mysql-service.yaml For reference, please see Service docs : https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service Additionally, you can use kubectl built-in docs for any type of resources: kubectl explain service vim gowebapp-mysql-service.yaml Note You can also use VCS or Cloud Code to work with yaml manifest. #TODO: Specify Kubernetes API apiVersion #TODO: Identify the kind of Object metadata: #TODO: Give the service a name: \"gowebapp-mysql\" labels: #TODO: Add a label KV \"run: gowebapp-mysql\" spec: #TODO: leave the clusterIP to None. We allow k8s to assign clusterIP. ports: #TODO: Define a \"port\" as 3306 #TODO: Define a \"targetPort\" as 3306 #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp-mysql\" Step 3 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 4 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\" 2.5 Create Deployment object for the backend MySQL database \u00b6 Step 1 Follow instructions below to populate gowebapp-mysql-deployment.yaml For reference, please see Deployment doc: https://kubernetes.io/docs/concepts/workloads/controllers/deployment vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 #TODO: Identify the type of Object metadata: #TODO: Give the Deployment a name \"gowebapp-mysql\" labels: #TODO: Add a label KV \"run: gowebapp-mysql\" #TODO: give the Deployment a label: tier: backend spec: #TODO: Define number of replicas, set it to 1 #TODO: Starting from Deplloyment v1 selectors are mandatory #add selector KV \"run: gowebapp-mysql\" strategy: type: # Set strategy type as `Recreate` template: metadata: #TODO: Add a label called \"run\" with the name of the service: \"gowebapp-mysql\" spec: containers: - env: - name: # TODO add MYSQL_ROOT_PASSWORD env value image: #TODO define mysql image created in previous assignment, located in gcr registry name: gowebapp-mysql ports: #TODO: define containerPort: 3306 Step 2 Create a Deployment object for MySQL kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp-mysql\" Step 3 Check mysql pod logs: List mysql Pods and note the name the pod : kubectl get pods -l \"run=gowebapp-mysql\" Ensure Mysql is up by looking at pod logs: kubectl logs <Pod_name> Result We have created Service and Deployment for backend application. 2.5 Create a K8s Service for the frontend gowebapp. \u00b6 Step 1 Follow instructions below to populate gowebapp-service.yaml vim gowebapp-service.yaml apiVersion: v1 kind: Service metadata: name: gowebapp labels: #TODO: give the Service a label: run: gowebapp #TODO: give the Service a label: tier: frontend spec: #TODO: Define a \"ports\" array with the \"port\" attribute: 9000 and \"targetPort\" attributes: 80 #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp\" #TODO: Add a Service Type of LoadBalancer #If you need help, see reference: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types Step 2 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 3 Check to make sure it worked kubectl get service -l \"run=gowebapp\" 2.6 Create a K8s Deployment object for the frontend gowebapp \u00b6 Step 1 Follow instructions below to populate gowebapp-deployment.yaml vim gowebapp-deployment.yaml apiVersion: apps/v1 #TODO: define the kind of object as Deployment metadata: #TODO: Add a name attribute for the service as \"gowebapp\" labels: #TODO: give the Deployment a label: run: gowebapp #TODO: give the Deployment a label: tier: frontend spec: #TODO: Define number of replicas, set it to 2 #TODO: add selector KV \"run: gowebapp\" template: metadata: labels: run: gowebapp tier: frontend spec: containers: - env: - #TODO: define name as MYSQL_ROOT_PASSWORD #TODO: define value as cloudops #TODO: Replace <user-name> with value you Docker-Hub ID. image: #TODO define gowebapp image created in previous assignment, located in gcr registry name: gowebapp ports: - #TODO: define the container port as 80 Step 2 Create a Deployment object for gowebapp kubectl apply -f gowebapp-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp\" Step 4 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 5 Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes 2.7 Fix gowebapp code bugs and build a new image. \u00b6 Task: As you've noticed gowebapp frontend app has YCIT019 logo in it. Since you may want to use application for you personal needs, let's change YCIT019 logo to you Name . Step 1 Modify gowebapp frontend so that it has name of you company and link to company web page e.g. vim ~/$MY_REPO/gowebapp/code/template/partial/footer.tmpl Step 2 Build a new version of Image cd ~/$MY_REPO/gowebapp docker build -t gcr.io/${PROJECT_ID}/gowebapp:v2 . docker push gcr.io/${PROJECT_ID}/gowebapp:v2 . 2.7 Rolling Upgrade \u00b6 For gowebapp frontend deployment manifest we've not specified any upgrade strategy type. It means application will use default Upgrade strategy called RollingUpdate . RollingUpdate strategy - updates Pods in a rolling update fashion. maxUnavailable - is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. By default, it ensures that at least 25% less than the desired number of Pods are up (25% max unavailable). Max Surge - is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. By default, it ensures that at most 25% more than the desired number of Pods are up (25% max surge). Step 1 Locate directory with manifest cd ~/$MY_REPO/deploy Step 2 Trigger rolling upgrade using kubectl set command #TO DO Step 3 Verify rollout history #TO DO Step 4 Perform Rollback to v1 #TO DO 2.8 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 2.9 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"Assignment3"},{"location":"ycit019_ass3/#1-deploy-applications-on-kubernetes","text":"Objective: Review process of creating NameSpaces Review process of changing Context Review process of creating K8s: Services Labels, Selectors Deployments Rolling Updates","title":"1 Deploy Applications on Kubernetes"},{"location":"ycit019_ass3/#11-development-tools","text":"Note This steps is optional Step 1 Choose one of the following option to develop the YAML manifests: Option 1: You can develop in Google Cloud Shell Editor Option 2: You can also develop locally on your laptop using VCScode . We recommend to use it in conjunction with VSC YAML extension from Redhat Option 3: Use your preferred text editor on Linux VM (vim, nano).","title":"1.1 Development Tools"},{"location":"ycit019_ass3/#21-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c","title":"2.1 Create GKE Cluster"},{"location":"ycit019_ass3/#22-setup-kubectl-autocomplete","text":"Since we going to use a lot of kubectl cli let's setup autocomplete. source <(kubectl completion bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"2.2 Setup KUBECTL AUTOCOMPLETE"},{"location":"ycit019_ass3/#23-create-dev-namespace-and-make-it-default","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl #TO DO(create namespace `dev`) Step 2 Use dev context to create K8s resources inside this namespace. kubectl #TO DO (make `dev` context default) Step 3 Verify current context: kubectl config current-context Result dev","title":"2.3 Create 'dev' namespace and make it default."},{"location":"ycit019_ass3/#24-create-service-object-for-mysql","text":"Step 1 Locate directory where kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment3/ ls Step 2 Go into the local repository you've created: cd ~/$MY_REPO Step 3 Copy Assignment 3 deploy folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019/Assignment3/deploy . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests in deploy folder\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Define a Kubernetes Service object for the backend MySQL database. cd ~/$MY_REPO/deploy Follow instructions below to populate gowebapp-mysql-service.yaml For reference, please see Service docs : https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service Additionally, you can use kubectl built-in docs for any type of resources: kubectl explain service vim gowebapp-mysql-service.yaml Note You can also use VCS or Cloud Code to work with yaml manifest. #TODO: Specify Kubernetes API apiVersion #TODO: Identify the kind of Object metadata: #TODO: Give the service a name: \"gowebapp-mysql\" labels: #TODO: Add a label KV \"run: gowebapp-mysql\" spec: #TODO: leave the clusterIP to None. We allow k8s to assign clusterIP. ports: #TODO: Define a \"port\" as 3306 #TODO: Define a \"targetPort\" as 3306 #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp-mysql\" Step 3 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 4 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\"","title":"2.4 Create Service Object for MySQL"},{"location":"ycit019_ass3/#25-create-deployment-object-for-the-backend-mysql-database","text":"Step 1 Follow instructions below to populate gowebapp-mysql-deployment.yaml For reference, please see Deployment doc: https://kubernetes.io/docs/concepts/workloads/controllers/deployment vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 #TODO: Identify the type of Object metadata: #TODO: Give the Deployment a name \"gowebapp-mysql\" labels: #TODO: Add a label KV \"run: gowebapp-mysql\" #TODO: give the Deployment a label: tier: backend spec: #TODO: Define number of replicas, set it to 1 #TODO: Starting from Deplloyment v1 selectors are mandatory #add selector KV \"run: gowebapp-mysql\" strategy: type: # Set strategy type as `Recreate` template: metadata: #TODO: Add a label called \"run\" with the name of the service: \"gowebapp-mysql\" spec: containers: - env: - name: # TODO add MYSQL_ROOT_PASSWORD env value image: #TODO define mysql image created in previous assignment, located in gcr registry name: gowebapp-mysql ports: #TODO: define containerPort: 3306 Step 2 Create a Deployment object for MySQL kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp-mysql\" Step 3 Check mysql pod logs: List mysql Pods and note the name the pod : kubectl get pods -l \"run=gowebapp-mysql\" Ensure Mysql is up by looking at pod logs: kubectl logs <Pod_name> Result We have created Service and Deployment for backend application.","title":"2.5 Create Deployment object for the backend MySQL database"},{"location":"ycit019_ass3/#25-create-a-k8s-service-for-the-frontend-gowebapp","text":"Step 1 Follow instructions below to populate gowebapp-service.yaml vim gowebapp-service.yaml apiVersion: v1 kind: Service metadata: name: gowebapp labels: #TODO: give the Service a label: run: gowebapp #TODO: give the Service a label: tier: frontend spec: #TODO: Define a \"ports\" array with the \"port\" attribute: 9000 and \"targetPort\" attributes: 80 #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp\" #TODO: Add a Service Type of LoadBalancer #If you need help, see reference: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types Step 2 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 3 Check to make sure it worked kubectl get service -l \"run=gowebapp\"","title":"2.5 Create a K8s Service for the frontend gowebapp."},{"location":"ycit019_ass3/#26-create-a-k8s-deployment-object-for-the-frontend-gowebapp","text":"Step 1 Follow instructions below to populate gowebapp-deployment.yaml vim gowebapp-deployment.yaml apiVersion: apps/v1 #TODO: define the kind of object as Deployment metadata: #TODO: Add a name attribute for the service as \"gowebapp\" labels: #TODO: give the Deployment a label: run: gowebapp #TODO: give the Deployment a label: tier: frontend spec: #TODO: Define number of replicas, set it to 2 #TODO: add selector KV \"run: gowebapp\" template: metadata: labels: run: gowebapp tier: frontend spec: containers: - env: - #TODO: define name as MYSQL_ROOT_PASSWORD #TODO: define value as cloudops #TODO: Replace <user-name> with value you Docker-Hub ID. image: #TODO define gowebapp image created in previous assignment, located in gcr registry name: gowebapp ports: - #TODO: define the container port as 80 Step 2 Create a Deployment object for gowebapp kubectl apply -f gowebapp-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp\" Step 4 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 5 Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes","title":"2.6 Create a K8s Deployment object for the frontend gowebapp"},{"location":"ycit019_ass3/#27-fix-gowebapp-code-bugs-and-build-a-new-image","text":"Task: As you've noticed gowebapp frontend app has YCIT019 logo in it. Since you may want to use application for you personal needs, let's change YCIT019 logo to you Name . Step 1 Modify gowebapp frontend so that it has name of you company and link to company web page e.g. vim ~/$MY_REPO/gowebapp/code/template/partial/footer.tmpl Step 2 Build a new version of Image cd ~/$MY_REPO/gowebapp docker build -t gcr.io/${PROJECT_ID}/gowebapp:v2 . docker push gcr.io/${PROJECT_ID}/gowebapp:v2 .","title":"2.7 Fix gowebapp code bugs and build a new image."},{"location":"ycit019_ass3/#27-rolling-upgrade","text":"For gowebapp frontend deployment manifest we've not specified any upgrade strategy type. It means application will use default Upgrade strategy called RollingUpdate . RollingUpdate strategy - updates Pods in a rolling update fashion. maxUnavailable - is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. By default, it ensures that at least 25% less than the desired number of Pods are up (25% max unavailable). Max Surge - is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. By default, it ensures that at most 25% more than the desired number of Pods are up (25% max surge). Step 1 Locate directory with manifest cd ~/$MY_REPO/deploy Step 2 Trigger rolling upgrade using kubectl set command #TO DO Step 3 Verify rollout history #TO DO Step 4 Perform Rollback to v1 #TO DO","title":"2.7 Rolling Upgrade"},{"location":"ycit019_ass3/#28-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"2.8 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass3/#29-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"2.9 Cleaning Up"},{"location":"ycit019_ass3_solution/","text":"1 Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating NameSpaces Review process of changing Context Review process of creating K8s: Services Labels, Selectors Deployments Rolling Updates 1.1 Development Tools \u00b6 Note This steps is optional Step 1 Choose one of the following option to develop the YAML manifests: Option 1: You can develop in Google Cloud Shell Editor Option 2: You can also develop locally on your laptop using VCScode . We recommend to use it in conjunction with VSC YAML extension from Redhat Option 3: Use your preferred text editor on Linux VM (vim, nano). 2.1 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c 2.2 Setup KUBECTL AUTOCOMPLETE \u00b6 Since we going to use a lot of kubectl cli let's setup autocomplete. source <(kubectl completion bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc 2.3 Create 'dev' namespace and make it default. \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 2 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 3 Verify current context: kubectl config view | grep namespace Result dev 2.4 Create Service Object for MySQL \u00b6 Step 1 Locate directory where kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment3/ ls Step 2 Go into the local repository you've created: cd ~/$MY_REPO Step 3 Copy Assignment 3 deploy folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019/Assignment3/deploy . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests in deploy folder\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Define a Kubernetes Service object for the backend MySQL database. cd ~/$MY_REPO/deploy Follow instructions below to populate gowebapp-mysql-service.yaml For reference, please see Service docs : https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service Additionally, you can use kubectl built-in docs for any type of resources: kubectl explain service vim gowebapp-mysql-service.yaml Note You can also use VCS or Cloud Code to work with yaml manifest. apiVersion: v1 kind: Service metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: clusterIP: None ports: - port: 3306 targetPort: 3306 selector: run: gowebapp-mysql Step 3 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 4 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\" 2.5 Create Deployment object for the backend MySQL database \u00b6 Step 1 Follow instructions below to populate gowebapp-mysql-deployment.yaml For reference, please see Deployment doc: https://kubernetes.io/docs/concepts/workloads/controllers/deployment vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: rootpasswd image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 Step 2 Create a Deployment object for MySQL kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp-mysql\" Step 3 Check mysql pod logs: List mysql Pods and note the name the pod : kubectl get pods -l \"run=gowebapp-mysql\" Ensure Mysql is up by looking at pod logs: kubectl logs <Pod_name> Result We have created Service and Deployment for backend application. 2.5 Create a K8s Service for the frontend gowebapp. \u00b6 Step 1 Follow instructions below to populate gowebapp-service.yaml vim gowebapp-service.yaml apiVersion: v1 kind: Service metadata: name: gowebapp labels: run: gowebapp spec: ports: - port: 9000 targetPort: 80 selector: run: gowebapp type: LoadBalancer Step 2 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 3 Check to make sure it worked kubectl get service -l \"run=gowebapp\" 2.6 Create a K8s Deployment object for the frontend gowebapp \u00b6 Step 1 Follow instructions below to populate gowebapp-deployment.yaml vim gowebapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: rootpasswd image: gcr.io/${PROJECT_ID}/gowebapp:v1 name: gowebapp ports: - containerPort: 80 Step 2 Create a Deployment object for gowebapp kubectl apply -f gowebapp-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp\" Step 4 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 5 Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes 2.7 Fix gowebapp code bugs and build a new image. \u00b6 Task: As you've noticed gowebapp frontend app has YCIT019 logo in it. Since you may want to use application for you personal needs, let's change YCIT019 logo to you Name . Step 1 Modify gowebapp frontend so that it has name of you company and link to company web page e.g. vim ~/$MY_REPO/gowebapp/code/template/partial/footer.tmpl Step 2 Build a new version of Image cd ~/$MY_REPO/gowebapp docker build -t gcr.io/${PROJECT_ID}/gowebapp:v2 . docker push gcr.io/${PROJECT_ID}/gowebapp:v2 . 2.7 Rolling Upgrade \u00b6 For gowebapp frontend deployment manifest we've not specified any upgrade strategy type. It means application will use default Upgrade strategy called RollingUpdate . RollingUpdate strategy - updates Pods in a rolling update fashion. maxUnavailable - is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. By default, it ensures that at least 25% less than the desired number of Pods are up (25% max unavailable). Max Surge - is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. By default, it ensures that at most 25% more than the desired number of Pods are up (25% max surge). Step 1 Locate directory with manifest cd ~/$MY_REPO/deploy Step 2 Trigger rolling upgrade using kubectl set command kubectl set image deployments/gowebapp gowebapp=gcr.io/${PROJECT_ID}/gowebapp:v2 Step 3 Verify rollout history kubectl rollout history deployment/gowebapp Step 4 Perform Rollback to v1 kubectl rollout undo deployment/gowebapp --to-revision=1 2.8 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 2.9 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"Assignment3 - Solution"},{"location":"ycit019_ass3_solution/#1-deploy-applications-on-kubernetes","text":"Objective: Review process of creating NameSpaces Review process of changing Context Review process of creating K8s: Services Labels, Selectors Deployments Rolling Updates","title":"1 Deploy Applications on Kubernetes"},{"location":"ycit019_ass3_solution/#11-development-tools","text":"Note This steps is optional Step 1 Choose one of the following option to develop the YAML manifests: Option 1: You can develop in Google Cloud Shell Editor Option 2: You can also develop locally on your laptop using VCScode . We recommend to use it in conjunction with VSC YAML extension from Redhat Option 3: Use your preferred text editor on Linux VM (vim, nano).","title":"1.1 Development Tools"},{"location":"ycit019_ass3_solution/#21-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c","title":"2.1 Create GKE Cluster"},{"location":"ycit019_ass3_solution/#22-setup-kubectl-autocomplete","text":"Since we going to use a lot of kubectl cli let's setup autocomplete. source <(kubectl completion bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc","title":"2.2 Setup KUBECTL AUTOCOMPLETE"},{"location":"ycit019_ass3_solution/#23-create-dev-namespace-and-make-it-default","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 2 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 3 Verify current context: kubectl config view | grep namespace Result dev","title":"2.3 Create 'dev' namespace and make it default."},{"location":"ycit019_ass3_solution/#24-create-service-object-for-mysql","text":"Step 1 Locate directory where kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment3/ ls Step 2 Go into the local repository you've created: cd ~/$MY_REPO Step 3 Copy Assignment 3 deploy folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019/Assignment3/deploy . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests in deploy folder\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Define a Kubernetes Service object for the backend MySQL database. cd ~/$MY_REPO/deploy Follow instructions below to populate gowebapp-mysql-service.yaml For reference, please see Service docs : https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service Additionally, you can use kubectl built-in docs for any type of resources: kubectl explain service vim gowebapp-mysql-service.yaml Note You can also use VCS or Cloud Code to work with yaml manifest. apiVersion: v1 kind: Service metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: clusterIP: None ports: - port: 3306 targetPort: 3306 selector: run: gowebapp-mysql Step 3 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 4 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\"","title":"2.4 Create Service Object for MySQL"},{"location":"ycit019_ass3_solution/#25-create-deployment-object-for-the-backend-mysql-database","text":"Step 1 Follow instructions below to populate gowebapp-mysql-deployment.yaml For reference, please see Deployment doc: https://kubernetes.io/docs/concepts/workloads/controllers/deployment vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: rootpasswd image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 Step 2 Create a Deployment object for MySQL kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp-mysql\" Step 3 Check mysql pod logs: List mysql Pods and note the name the pod : kubectl get pods -l \"run=gowebapp-mysql\" Ensure Mysql is up by looking at pod logs: kubectl logs <Pod_name> Result We have created Service and Deployment for backend application.","title":"2.5 Create Deployment object for the backend MySQL database"},{"location":"ycit019_ass3_solution/#25-create-a-k8s-service-for-the-frontend-gowebapp","text":"Step 1 Follow instructions below to populate gowebapp-service.yaml vim gowebapp-service.yaml apiVersion: v1 kind: Service metadata: name: gowebapp labels: run: gowebapp spec: ports: - port: 9000 targetPort: 80 selector: run: gowebapp type: LoadBalancer Step 2 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 3 Check to make sure it worked kubectl get service -l \"run=gowebapp\"","title":"2.5 Create a K8s Service for the frontend gowebapp."},{"location":"ycit019_ass3_solution/#26-create-a-k8s-deployment-object-for-the-frontend-gowebapp","text":"Step 1 Follow instructions below to populate gowebapp-deployment.yaml vim gowebapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: rootpasswd image: gcr.io/${PROJECT_ID}/gowebapp:v1 name: gowebapp ports: - containerPort: 80 Step 2 Create a Deployment object for gowebapp kubectl apply -f gowebapp-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp\" Step 4 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 5 Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes","title":"2.6 Create a K8s Deployment object for the frontend gowebapp"},{"location":"ycit019_ass3_solution/#27-fix-gowebapp-code-bugs-and-build-a-new-image","text":"Task: As you've noticed gowebapp frontend app has YCIT019 logo in it. Since you may want to use application for you personal needs, let's change YCIT019 logo to you Name . Step 1 Modify gowebapp frontend so that it has name of you company and link to company web page e.g. vim ~/$MY_REPO/gowebapp/code/template/partial/footer.tmpl Step 2 Build a new version of Image cd ~/$MY_REPO/gowebapp docker build -t gcr.io/${PROJECT_ID}/gowebapp:v2 . docker push gcr.io/${PROJECT_ID}/gowebapp:v2 .","title":"2.7 Fix gowebapp code bugs and build a new image."},{"location":"ycit019_ass3_solution/#27-rolling-upgrade","text":"For gowebapp frontend deployment manifest we've not specified any upgrade strategy type. It means application will use default Upgrade strategy called RollingUpdate . RollingUpdate strategy - updates Pods in a rolling update fashion. maxUnavailable - is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. By default, it ensures that at least 25% less than the desired number of Pods are up (25% max unavailable). Max Surge - is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. By default, it ensures that at most 25% more than the desired number of Pods are up (25% max surge). Step 1 Locate directory with manifest cd ~/$MY_REPO/deploy Step 2 Trigger rolling upgrade using kubectl set command kubectl set image deployments/gowebapp gowebapp=gcr.io/${PROJECT_ID}/gowebapp:v2 Step 3 Verify rollout history kubectl rollout history deployment/gowebapp Step 4 Perform Rollback to v1 kubectl rollout undo deployment/gowebapp --to-revision=1","title":"2.7 Rolling Upgrade"},{"location":"ycit019_ass3_solution/#28-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"2.8 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass3_solution/#29-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"2.9 Cleaning Up"},{"location":"ycit019_ass4/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Startup Probes Liveness Probes Readiness Probes Secrets ConfigMaps Externalize Web Application Configuration 1.1 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c 1.2 Locate Assignment 4 \u00b6 Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment4/ ls Result You can see 4 Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 4 deploy_a4 folder to your repo: cp -r ~/ycit019/Assignment4/deploy_a4 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 4\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 2.1 Externalize Web Application Configuration \u00b6 Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~ mkdir $MY_REPO/gowebapp/config cp $MY_REPO/gowebapp/code/config/config.json \\ $MY_REPO/gowebapp/config Remove config folder that is located in code directory rm -rf $MY_REPO/gowebapp/code/config Result Your gowebapp folder should look like following: $ ls gowebapp code config Dockerfile Step 2: Modify app to support setting DB password through environment variable Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor of your choice (vim, VS code) to modify: vim ~/$MY_REPO/gowebapp/code/vendor/app/shared/database/database.go Note To see the line number in vim you can enable them by running :set number Add an import for the \"os\" package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") } 2.2 Build new Docker image for your frontend application \u00b6 Step 1: Update Dockerfile for your gowebapp frontend application: cd ~/$MY_REPO/gowebapp FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go #TODO --- add an environment variable declaration for a default DB_PASSWORD of \"rootpasswd\" #https://docs.docker.com/engine/reference/builder/#env COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- add a volume declaration for the container configuration path we # want to mount at runtime from the host file system: # $GOPATH/src/gowebapp/config #https://docs.docker.com/engine/reference/builder/#volume ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd ~/$MY_REPO/gowebapp Build and push the gowebapp image to GCR. Make sure to include \u201c.\u201c at the end of build command. docker build -t gcr.io/${PROJECT_ID}/gowebapp:v3 . docker push gcr.io/${PROJECT_ID}/gowebapp:v3 2.3 Run and test new Docker image locally \u00b6 Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker network create gowebapp -d bridge docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 8080:80 \\ -v ~/$MY_REPO/gowebapp/config:/go/src/gowebapp/config \\ --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v3 Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes Step 5 Cleanup environment docker rm -f $(docker ps -q) docker network rm gowebapp 2.4 Update DockerCompose file to support changes \u00b6 Step 1 Edit docker-compose file cd ~/$MY_REPO/ vim docker-compose.yml #TODO Define gowebapp configuration as volume #Ref: https://docs.docker.com/compose/compose-file/compose-file-v2/#volumes #Path Hint: ./gowebapp/config:/go/src/gowebapp/config Step 3 Test application export CLOUDSDK_PYTHON=/usr/bin/python docker-compose up -d Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Tear down environment docker-compose down 3.1 Create a Namespace dev \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 3.2 Create a Secret \u00b6 Step 1 Base64 Encode MySQL password rootpasswd . See Lab 8 for more details. Step 2 Edit a secret for the MySQL password in dev namespaces. cd ~/$MY_REPO/deploy_a4 vim secret-mysql.yaml kind: Secret apiVersion: v1 # TODO1 Create secret name: mysql # TODO2 Secret should be using arbitrary user defined type stringData: https://kubernetes.io/docs/concepts/configuration/secret/#secret-types # TODO3 Define Mysql Password in base64 encoded format kubectl apply -f secret-mysql.yaml kubectl describe secret mysql Step 2 Update gowebapp-mysql-deployment.yaml under ~/$MY_REPO/deploy_a4 vim ~/$MY_REPO/deploy_a4/gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: #TODO: replace value: rootpasswd with secretKeyRef #TODO: name is mysql #TODO: key is password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 #TODO add a livenessProbe which performs tcpSocket probe #aginst port 3306 with an initial # deay of 30 seconds, and a timeout of 2 seconds #TODO add a readinessProbe for tcpSocket port 3306 with a 25 second #initial delay, and a timeout of 2 seconds Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Check if pods are running kubectl get pods Step 5 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 6 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\" 3.3 Create ConfigMap and Probes for gowebapp \u00b6 Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json 3.4 Deploy webapp by Referencing Secret, ConfigMap and define Probes \u00b6 Step 1: Update gowebapp-deployment.yaml under ~/$MY_REPO/deploy_a4/ cd ~/$MY_REPO/deploy_a4/ vim gowebapp-deployment.yaml In this exercise, we will add liveness/readiness probes to our deployments. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: name: DB_PASSWORD #TODO: replace value: `rootpasswd` with valueFrom: secretKeyRef: #TODO: name is mysql #TODO: key is password image: gcr.io/${PROJECT_ID}/gowebapp:v3 name: gowebapp ports: - containerPort: 80 livenessProbe: #TODO add a livenessProbe which performs httpGet #aginst the /register endpoint on port 80 with an initial # deay of 15 seconds, and a timeout of 5 seconds #TODO add a livenessProbe which performs httpGet # aginst the /register endpoint on port 80 with an initial # deay of 25 seconds, and a timeout of 5 seconds volumeMounts: - #TODO: give the volume a name:config-volume #TODO: specify the mountPath: /go/src/gowebapp/config volumes: - #TODO: define volume name: config-volume configMap: #TODO: identify your ConfigMap name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6 Check if pods are running kubectl get pods Step 7 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 8 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes 3.5 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 4\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"Assignment4"},{"location":"ycit019_ass4/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Startup Probes Liveness Probes Readiness Probes Secrets ConfigMaps Externalize Web Application Configuration","title":"Deploy Applications on Kubernetes"},{"location":"ycit019_ass4/#11-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c","title":"1.1 Create GKE Cluster"},{"location":"ycit019_ass4/#12-locate-assignment-4","text":"Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment4/ ls Result You can see 4 Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 4 deploy_a4 folder to your repo: cp -r ~/ycit019/Assignment4/deploy_a4 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 4\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"1.2 Locate Assignment 4"},{"location":"ycit019_ass4/#21-externalize-web-application-configuration","text":"Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~ mkdir $MY_REPO/gowebapp/config cp $MY_REPO/gowebapp/code/config/config.json \\ $MY_REPO/gowebapp/config Remove config folder that is located in code directory rm -rf $MY_REPO/gowebapp/code/config Result Your gowebapp folder should look like following: $ ls gowebapp code config Dockerfile Step 2: Modify app to support setting DB password through environment variable Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor of your choice (vim, VS code) to modify: vim ~/$MY_REPO/gowebapp/code/vendor/app/shared/database/database.go Note To see the line number in vim you can enable them by running :set number Add an import for the \"os\" package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") }","title":"2.1 Externalize Web Application Configuration"},{"location":"ycit019_ass4/#22-build-new-docker-image-for-your-frontend-application","text":"Step 1: Update Dockerfile for your gowebapp frontend application: cd ~/$MY_REPO/gowebapp FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go #TODO --- add an environment variable declaration for a default DB_PASSWORD of \"rootpasswd\" #https://docs.docker.com/engine/reference/builder/#env COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- add a volume declaration for the container configuration path we # want to mount at runtime from the host file system: # $GOPATH/src/gowebapp/config #https://docs.docker.com/engine/reference/builder/#volume ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd ~/$MY_REPO/gowebapp Build and push the gowebapp image to GCR. Make sure to include \u201c.\u201c at the end of build command. docker build -t gcr.io/${PROJECT_ID}/gowebapp:v3 . docker push gcr.io/${PROJECT_ID}/gowebapp:v3","title":"2.2 Build new Docker image for your frontend application"},{"location":"ycit019_ass4/#23-run-and-test-new-docker-image-locally","text":"Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker network create gowebapp -d bridge docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 8080:80 \\ -v ~/$MY_REPO/gowebapp/config:/go/src/gowebapp/config \\ --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v3 Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes Step 5 Cleanup environment docker rm -f $(docker ps -q) docker network rm gowebapp","title":"2.3 Run and test new Docker image locally"},{"location":"ycit019_ass4/#24-update-dockercompose-file-to-support-changes","text":"Step 1 Edit docker-compose file cd ~/$MY_REPO/ vim docker-compose.yml #TODO Define gowebapp configuration as volume #Ref: https://docs.docker.com/compose/compose-file/compose-file-v2/#volumes #Path Hint: ./gowebapp/config:/go/src/gowebapp/config Step 3 Test application export CLOUDSDK_PYTHON=/usr/bin/python docker-compose up -d Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Tear down environment docker-compose down","title":"2.4 Update DockerCompose file to support changes"},{"location":"ycit019_ass4/#31-create-a-namespace-dev","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"3.1 Create a Namespace dev"},{"location":"ycit019_ass4/#32-create-a-secret","text":"Step 1 Base64 Encode MySQL password rootpasswd . See Lab 8 for more details. Step 2 Edit a secret for the MySQL password in dev namespaces. cd ~/$MY_REPO/deploy_a4 vim secret-mysql.yaml kind: Secret apiVersion: v1 # TODO1 Create secret name: mysql # TODO2 Secret should be using arbitrary user defined type stringData: https://kubernetes.io/docs/concepts/configuration/secret/#secret-types # TODO3 Define Mysql Password in base64 encoded format kubectl apply -f secret-mysql.yaml kubectl describe secret mysql Step 2 Update gowebapp-mysql-deployment.yaml under ~/$MY_REPO/deploy_a4 vim ~/$MY_REPO/deploy_a4/gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: #TODO: replace value: rootpasswd with secretKeyRef #TODO: name is mysql #TODO: key is password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 #TODO add a livenessProbe which performs tcpSocket probe #aginst port 3306 with an initial # deay of 30 seconds, and a timeout of 2 seconds #TODO add a readinessProbe for tcpSocket port 3306 with a 25 second #initial delay, and a timeout of 2 seconds Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Check if pods are running kubectl get pods Step 5 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 6 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\"","title":"3.2 Create a Secret"},{"location":"ycit019_ass4/#33-create-configmap-and-probes-for-gowebapp","text":"Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json","title":"3.3 Create ConfigMap and Probes for gowebapp"},{"location":"ycit019_ass4/#34-deploy-webapp-by-referencing-secret-configmap-and-define-probes","text":"Step 1: Update gowebapp-deployment.yaml under ~/$MY_REPO/deploy_a4/ cd ~/$MY_REPO/deploy_a4/ vim gowebapp-deployment.yaml In this exercise, we will add liveness/readiness probes to our deployments. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: name: DB_PASSWORD #TODO: replace value: `rootpasswd` with valueFrom: secretKeyRef: #TODO: name is mysql #TODO: key is password image: gcr.io/${PROJECT_ID}/gowebapp:v3 name: gowebapp ports: - containerPort: 80 livenessProbe: #TODO add a livenessProbe which performs httpGet #aginst the /register endpoint on port 80 with an initial # deay of 15 seconds, and a timeout of 5 seconds #TODO add a livenessProbe which performs httpGet # aginst the /register endpoint on port 80 with an initial # deay of 25 seconds, and a timeout of 5 seconds volumeMounts: - #TODO: give the volume a name:config-volume #TODO: specify the mountPath: /go/src/gowebapp/config volumes: - #TODO: define volume name: config-volume configMap: #TODO: identify your ConfigMap name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6 Check if pods are running kubectl get pods Step 7 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 8 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes","title":"3.4 Deploy webapp by Referencing Secret, ConfigMap and define Probes"},{"location":"ycit019_ass4/#35-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 4\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"3.5 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass4/#36-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"3.6 Cleaning Up"},{"location":"ycit019_ass4_sol/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Startup Probes Liveness Probes Readiness Probes Secrets ConfigMaps Externalize Web Application Configuration 1.1 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c 1.2 Locate Assignment 4 \u00b6 Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment4/ ls Result You can see 4 Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 4 deploy_a4 folder to your repo: cp -r ~/ycit019/Assignment4/deploy_a4 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 4\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 2.1 Externalize Web Application Configuration \u00b6 Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~ mkdir $MY_REPO/gowebapp/config cp $MY_REPO/gowebapp/code/config/config.json \\ $MY_REPO/gowebapp/config Remove config folder that is located in code directory rm -rf $MY_REPO/gowebapp/code/config Result Your gowebapp folder should look like following: $ ls gowebapp code config Dockerfile Step 2: Modify app to support setting DB password through environment variable Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor of your choice (vim, VS code) to modify: vim ~/$MY_REPO/gowebapp/code/vendor/app/shared/database/database.go Note To see the line number in vim you can enable them by running :set number Add an import for the \"os\" package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") } 2.2 Build new Docker image for your frontend application \u00b6 Step 1: Update Dockerfile for your gowebapp frontend application: cd ~/$MY_REPO/gowebapp FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go ENV PASSWORD=rootpasswd COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install VOLUME $GOPATH/src/gowebapp/config ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd ~/$MY_REPO/gowebapp Build and push the gowebapp image to GCR. Make sure to include \u201c.\u201c at the end of build command. docker build -t gcr.io/${PROJECT_ID}/gowebapp:v3 . docker push gcr.io/${PROJECT_ID}/gowebapp:v3 2.3 Run and test new Docker image locally \u00b6 Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker network create gowebapp -d bridge docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 8080:80 \\ -v ~/$MY_REPO/gowebapp/config:/go/src/gowebapp/config \\ --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v3 Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes Step 5 Cleanup environment docker rm -f $(docker ps -q) docker network rm gowebapp 2.4 Update DockerCompose file to support changes \u00b6 Step 1 Edit docker-compose file cd ~/$MY_REPO/ vim docker-compose.yml version: '2.4' services: gowebapp-mysql: container_name: gowebapp-mysql build: ./gowebapp-mysql environment: - MYSQL_ROOT_PASSWORD=rootpasswd healthcheck: test: [ \"CMD\", \"mysqladmin\" ,\"ping\", \"-h\", \"localhost\" ] interval: 30s timeout: 5s retries: 3 networks: - gowebapp gowebapp: container_name: gowebapp build: ./gowebapp ports: - 8080:80 depends_on: gowebapp-mysql: condition: service_healthy volumes: - ./gowebapp/config:/go/src/gowebapp/config networks: - gowebapp networks: gowebapp1: driver: bridge Step 3 Test application export CLOUDSDK_PYTHON=/usr/bin/python docker-compose up -d Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Tear down environment docker-compose down 3.1 Create a Namespace dev \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 3.2 Create a Secret \u00b6 Step 1 Base64 Encode MySQL password rootpasswd . See Lab 8 for more details. Step 2 Edit a secret for the MySQL password in dev namespaces. cd ~/$MY_REPO/deploy_a4 vim secret-mysql.yaml kind: Secret apiVersion: v1 metadata: name: mysql type: Opaque data: password: cm9vdHBhc3N3ZA== kubectl apply -f secret-mysql.yaml kubectl describe secret mysql Step 2 Update gowebapp-mysql-deployment.yaml under ~/$MY_REPO/deploy_a4 vim ~/$MY_REPO/deploy_a4/gowebapp-mysql-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: replicas: 1 strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Check if pods are running kubectl get pods Step 5 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 6 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\" 3.3 Create ConfigMap and Probes for gowebapp \u00b6 Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json 3.4 Deploy webapp by Referencing Secret, ConfigMap and define Probes \u00b6 Step 1: Update gowebapp-deployment.yaml under ~/$MY_REPO/deploy_a4/ cd ~/$MY_REPO/deploy_a4/ vim gowebapp-deployment.yaml In this exercise, we will add liveness/readiness probes to our deployments. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6 Check if pods are running kubectl get pods Step 7 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 8 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes 3.5 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 4\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"Assignment4 - Solution"},{"location":"ycit019_ass4_sol/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Startup Probes Liveness Probes Readiness Probes Secrets ConfigMaps Externalize Web Application Configuration","title":"Deploy Applications on Kubernetes"},{"location":"ycit019_ass4_sol/#11-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c","title":"1.1 Create GKE Cluster"},{"location":"ycit019_ass4_sol/#12-locate-assignment-4","text":"Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment4/ ls Result You can see 4 Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 4 deploy_a4 folder to your repo: cp -r ~/ycit019/Assignment4/deploy_a4 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 4\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"1.2 Locate Assignment 4"},{"location":"ycit019_ass4_sol/#21-externalize-web-application-configuration","text":"Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~ mkdir $MY_REPO/gowebapp/config cp $MY_REPO/gowebapp/code/config/config.json \\ $MY_REPO/gowebapp/config Remove config folder that is located in code directory rm -rf $MY_REPO/gowebapp/code/config Result Your gowebapp folder should look like following: $ ls gowebapp code config Dockerfile Step 2: Modify app to support setting DB password through environment variable Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor of your choice (vim, VS code) to modify: vim ~/$MY_REPO/gowebapp/code/vendor/app/shared/database/database.go Note To see the line number in vim you can enable them by running :set number Add an import for the \"os\" package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") }","title":"2.1 Externalize Web Application Configuration"},{"location":"ycit019_ass4_sol/#22-build-new-docker-image-for-your-frontend-application","text":"Step 1: Update Dockerfile for your gowebapp frontend application: cd ~/$MY_REPO/gowebapp FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go ENV PASSWORD=rootpasswd COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install VOLUME $GOPATH/src/gowebapp/config ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd ~/$MY_REPO/gowebapp Build and push the gowebapp image to GCR. Make sure to include \u201c.\u201c at the end of build command. docker build -t gcr.io/${PROJECT_ID}/gowebapp:v3 . docker push gcr.io/${PROJECT_ID}/gowebapp:v3","title":"2.2 Build new Docker image for your frontend application"},{"location":"ycit019_ass4_sol/#23-run-and-test-new-docker-image-locally","text":"Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker network create gowebapp -d bridge docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 8080:80 \\ -v ~/$MY_REPO/gowebapp/config:/go/src/gowebapp/config \\ --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v3 Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes Step 5 Cleanup environment docker rm -f $(docker ps -q) docker network rm gowebapp","title":"2.3 Run and test new Docker image locally"},{"location":"ycit019_ass4_sol/#24-update-dockercompose-file-to-support-changes","text":"Step 1 Edit docker-compose file cd ~/$MY_REPO/ vim docker-compose.yml version: '2.4' services: gowebapp-mysql: container_name: gowebapp-mysql build: ./gowebapp-mysql environment: - MYSQL_ROOT_PASSWORD=rootpasswd healthcheck: test: [ \"CMD\", \"mysqladmin\" ,\"ping\", \"-h\", \"localhost\" ] interval: 30s timeout: 5s retries: 3 networks: - gowebapp gowebapp: container_name: gowebapp build: ./gowebapp ports: - 8080:80 depends_on: gowebapp-mysql: condition: service_healthy volumes: - ./gowebapp/config:/go/src/gowebapp/config networks: - gowebapp networks: gowebapp1: driver: bridge Step 3 Test application export CLOUDSDK_PYTHON=/usr/bin/python docker-compose up -d Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Tear down environment docker-compose down","title":"2.4 Update DockerCompose file to support changes"},{"location":"ycit019_ass4_sol/#31-create-a-namespace-dev","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"3.1 Create a Namespace dev"},{"location":"ycit019_ass4_sol/#32-create-a-secret","text":"Step 1 Base64 Encode MySQL password rootpasswd . See Lab 8 for more details. Step 2 Edit a secret for the MySQL password in dev namespaces. cd ~/$MY_REPO/deploy_a4 vim secret-mysql.yaml kind: Secret apiVersion: v1 metadata: name: mysql type: Opaque data: password: cm9vdHBhc3N3ZA== kubectl apply -f secret-mysql.yaml kubectl describe secret mysql Step 2 Update gowebapp-mysql-deployment.yaml under ~/$MY_REPO/deploy_a4 vim ~/$MY_REPO/deploy_a4/gowebapp-mysql-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: replicas: 1 strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Check if pods are running kubectl get pods Step 5 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 6 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\"","title":"3.2 Create a Secret"},{"location":"ycit019_ass4_sol/#33-create-configmap-and-probes-for-gowebapp","text":"Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json","title":"3.3 Create ConfigMap and Probes for gowebapp"},{"location":"ycit019_ass4_sol/#34-deploy-webapp-by-referencing-secret-configmap-and-define-probes","text":"Step 1: Update gowebapp-deployment.yaml under ~/$MY_REPO/deploy_a4/ cd ~/$MY_REPO/deploy_a4/ vim gowebapp-deployment.yaml In this exercise, we will add liveness/readiness probes to our deployments. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6 Check if pods are running kubectl get pods Step 7 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 8 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes","title":"3.4 Deploy webapp by Referencing Secret, ConfigMap and define Probes"},{"location":"ycit019_ass4_sol/#35-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 4\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"3.5 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass4_sol/#36-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"3.6 Cleaning Up"},{"location":"ycit019_ass4_solution/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Secrets ConfigMaps Persistent Volumes (PV) Persisten Volume Claims (PVC) Jobs CronJobs HPA Externalize Web Application Configuration 3.1 Externalize Web Application Configuration \u00b6 Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~/k8scanada/ git pull mkdir $HOME/k8scanada/Assignment3/gowebapp/config cp $HOME/k8scanada/Assignment3/gowebapp/code/config/config.json \\ $HOME/k8scanada/Assignment3/gowebapp/config rm -rf $HOME/k8scanada/Assignment3/gowebapp/code/config/ Step 2: Modify app to support setting DB password through environment variable. Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor to modify: vim $HOME/k8scanada/Assignment3/gowebapp/code/vendor/app/shared/database/database.go Add an import for the os package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") } 3.2 Build new Docker image for your frontend application \u00b6 Step 1: Update Dockerfile for your frontend application FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go ENV PASSWORD=rootpasswd COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install VOLUME $GOPATH/src/gowebapp/config ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd $HOME/k8scanada/Assignment3/gowebapp/ Build the gowebapp image locally. Make sure to include \u201c.\u201c at the end. Notice the new version label. docker build -t gowebapp:v2 . 3.3 Run and test new Docker image locally \u00b6 Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=cloudops user-name/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 30005:80 \\ -v $HOME/k8scanada/Assignment3/gowebapp/config:/go/src/gowebapp/config \\ -e DB_PASSWORD=cloudops --net gowebapp -d --name gowebapp \\ --hostname gowebapp user-name/gowebapp:v2 Now that we\u2019ve launched the application containers, let\u2019s try to test the web application locally. You should be able to access the application at http://Public_IP:30005. Step 3: Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes 3.4 Publish New Image \u00b6 Step 1 We built the second version of gowebapp from the last exercise and tested it locally. Now we can tag and push gowebapp:v2 to private repository. docker tag gowebapp:v2 <user-name>/gowebapp:v2 docker push <user-name>/gowebapp:v2 3.5 Create a Secret \u00b6 Step 1 Create a secret for the MySQL password kind: Secret apiVersion: v1 metadata: name: mysql type: Opaque data: password: cm9vdHBhc3N3ZA== 3.6 Create a Volume for Mysql \u00b6 Step 1: Dynamically provisioning Persistent Volume Define a Persistent Volume Claim object to use with MySQL in a file named pvc.yaml under $HOME/k8scanada/Assignment3/deploy . In this case, we are not explicitly defining a Persistent Volume (pv) object for this PVC to use. This approach is more portable than explicitly defining and hard-wiring volume types. Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims cd $HOME/k8scanada/Assignment3/deploy vim pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysqlpvc labels: run: gowebapp-mysql spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi Step 2: Create PVC object kubectl apply -f pvc.yaml --record Step 3: View your PVC metadata kubectl get pvc mysqlpvc kubectl describe pv 3.6 Create Mysql deployment \u00b6 Create Mysql deployment using Secrets, liveness, readiness, resources, limits and Persistent Volume. Step 1 Update gowebapp-mysql-deployment.yaml under $HOME/k8scanada/Assignment3/deploy vim $HOME/k8scanada/Assignment3/deploy/gowebapp-mysql-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: replicas: 1 strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: archyufa/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 256m memory: 256Mi limits: cpu: 512m memory: 512Mi volumeMounts: - mountPath: /var/lib/mysql name: mysql volumes: - name: mysql persistentVolumeClaim: claimName: mysqlpvc Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 5 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 6 Get rollout history details for specific revision (use number show in output to previous command) Notice the value for MYSQL_ROOT_PASSWORD kubectl rollout history deploy gowebapp-mysql \\ --revision=<latest_version_number> 3.5 Create ConfigMap and Resources and Probes to MySQL \u00b6 Step 1: create ConfigMap for gowebapp's config.json file kubectl create configmap gowebapp --from-file=webapp-config-json=/home/cca-user/k8scanada/Assignment3/gowebapp/config/config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json Step 2: Update gowebapp-deployment.yaml under /home/cca-user/k8scanada/Assignment3/deploy In this exercise, we will add liveness/readiness probes to our deployments, as well as resource limits. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: <#TODO_user-name>/gowebapp:v1 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: request: cpu: 200m memory: 128Mi limits: cpu: 250m memory: 256Mi volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6: Access your application: <http://VM_Public_IP:NodePort. Register for an account, login and use the Notepad. If you need to lookup the VM_Public_IP for your environment, use the following command: kubectl get svc gowebapp -o wide ## 3.6 Define a Job to purge data from the web application database Create a Job object definition in a file called reset-db.yaml which connects to the MySQL database and deletes all the data from the note and user tables. Replace the TODOs below with the missing elements of the definition. If you need more help, refer to: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion Step 1: Create job reset-db.yaml under /home/cca-user/k8scanada/Assignment3/deploy vim reset-db.yaml apVersion: batch/v1 kind: Job metadata: name: reset-db labels: run: reset-db spec: activeDeadlineSeconds: 10 template: metadata: name: reset-db spec: restartPolicy: OnFailure containers: - name: database-cleaner image: mysql:5.6 env: - name: DB_USER value: root - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password - name: DB_HOST value: gowebapp-mysql - name: DB value: gowebapp command: - /bin/sh args: - -c - mysql -h $(DB_HOST) -u $(DB_USER) -p$(DB_PASSWORD) $(DB) -e 'delete from note; delete from user;' Step 2: Create the Job in Kubernetes kubectl apply -f reset-db.yaml --record Step 3: View Job metadata and status The job should run and complete immediately. kubectl get job reset-db kubectl describe job reset-db Step 4: Verify that data has been deleted Open the web application at http://Public_IP:NodePort and verify that the accounts and notes you created above no longer exist.","title":"Deploy Applications on Kubernetes"},{"location":"ycit019_ass4_solution/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Secrets ConfigMaps Persistent Volumes (PV) Persisten Volume Claims (PVC) Jobs CronJobs HPA Externalize Web Application Configuration","title":"Deploy Applications on Kubernetes"},{"location":"ycit019_ass4_solution/#31-externalize-web-application-configuration","text":"Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~/k8scanada/ git pull mkdir $HOME/k8scanada/Assignment3/gowebapp/config cp $HOME/k8scanada/Assignment3/gowebapp/code/config/config.json \\ $HOME/k8scanada/Assignment3/gowebapp/config rm -rf $HOME/k8scanada/Assignment3/gowebapp/code/config/ Step 2: Modify app to support setting DB password through environment variable. Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor to modify: vim $HOME/k8scanada/Assignment3/gowebapp/code/vendor/app/shared/database/database.go Add an import for the os package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") }","title":"3.1 Externalize Web Application Configuration"},{"location":"ycit019_ass4_solution/#32-build-new-docker-image-for-your-frontend-application","text":"Step 1: Update Dockerfile for your frontend application FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go ENV PASSWORD=rootpasswd COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install VOLUME $GOPATH/src/gowebapp/config ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd $HOME/k8scanada/Assignment3/gowebapp/ Build the gowebapp image locally. Make sure to include \u201c.\u201c at the end. Notice the new version label. docker build -t gowebapp:v2 .","title":"3.2 Build new Docker image for your frontend application"},{"location":"ycit019_ass4_solution/#33-run-and-test-new-docker-image-locally","text":"Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=cloudops user-name/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 30005:80 \\ -v $HOME/k8scanada/Assignment3/gowebapp/config:/go/src/gowebapp/config \\ -e DB_PASSWORD=cloudops --net gowebapp -d --name gowebapp \\ --hostname gowebapp user-name/gowebapp:v2 Now that we\u2019ve launched the application containers, let\u2019s try to test the web application locally. You should be able to access the application at http://Public_IP:30005. Step 3: Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes","title":"3.3 Run and test new Docker image locally"},{"location":"ycit019_ass4_solution/#34-publish-new-image","text":"Step 1 We built the second version of gowebapp from the last exercise and tested it locally. Now we can tag and push gowebapp:v2 to private repository. docker tag gowebapp:v2 <user-name>/gowebapp:v2 docker push <user-name>/gowebapp:v2","title":"3.4 Publish New Image"},{"location":"ycit019_ass4_solution/#35-create-a-secret","text":"Step 1 Create a secret for the MySQL password kind: Secret apiVersion: v1 metadata: name: mysql type: Opaque data: password: cm9vdHBhc3N3ZA==","title":"3.5 Create a Secret"},{"location":"ycit019_ass4_solution/#36-create-a-volume-for-mysql","text":"Step 1: Dynamically provisioning Persistent Volume Define a Persistent Volume Claim object to use with MySQL in a file named pvc.yaml under $HOME/k8scanada/Assignment3/deploy . In this case, we are not explicitly defining a Persistent Volume (pv) object for this PVC to use. This approach is more portable than explicitly defining and hard-wiring volume types. Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims cd $HOME/k8scanada/Assignment3/deploy vim pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysqlpvc labels: run: gowebapp-mysql spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi Step 2: Create PVC object kubectl apply -f pvc.yaml --record Step 3: View your PVC metadata kubectl get pvc mysqlpvc kubectl describe pv","title":"3.6 Create a Volume for Mysql"},{"location":"ycit019_ass4_solution/#36-create-mysql-deployment","text":"Create Mysql deployment using Secrets, liveness, readiness, resources, limits and Persistent Volume. Step 1 Update gowebapp-mysql-deployment.yaml under $HOME/k8scanada/Assignment3/deploy vim $HOME/k8scanada/Assignment3/deploy/gowebapp-mysql-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: replicas: 1 strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: archyufa/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 256m memory: 256Mi limits: cpu: 512m memory: 512Mi volumeMounts: - mountPath: /var/lib/mysql name: mysql volumes: - name: mysql persistentVolumeClaim: claimName: mysqlpvc Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 5 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 6 Get rollout history details for specific revision (use number show in output to previous command) Notice the value for MYSQL_ROOT_PASSWORD kubectl rollout history deploy gowebapp-mysql \\ --revision=<latest_version_number>","title":"3.6 Create Mysql deployment"},{"location":"ycit019_ass4_solution/#35-create-configmap-and-resources-and-probes-to-mysql","text":"Step 1: create ConfigMap for gowebapp's config.json file kubectl create configmap gowebapp --from-file=webapp-config-json=/home/cca-user/k8scanada/Assignment3/gowebapp/config/config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json Step 2: Update gowebapp-deployment.yaml under /home/cca-user/k8scanada/Assignment3/deploy In this exercise, we will add liveness/readiness probes to our deployments, as well as resource limits. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: <#TODO_user-name>/gowebapp:v1 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: request: cpu: 200m memory: 128Mi limits: cpu: 250m memory: 256Mi volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6: Access your application: <http://VM_Public_IP:NodePort. Register for an account, login and use the Notepad. If you need to lookup the VM_Public_IP for your environment, use the following command: kubectl get svc gowebapp -o wide ## 3.6 Define a Job to purge data from the web application database Create a Job object definition in a file called reset-db.yaml which connects to the MySQL database and deletes all the data from the note and user tables. Replace the TODOs below with the missing elements of the definition. If you need more help, refer to: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion Step 1: Create job reset-db.yaml under /home/cca-user/k8scanada/Assignment3/deploy vim reset-db.yaml apVersion: batch/v1 kind: Job metadata: name: reset-db labels: run: reset-db spec: activeDeadlineSeconds: 10 template: metadata: name: reset-db spec: restartPolicy: OnFailure containers: - name: database-cleaner image: mysql:5.6 env: - name: DB_USER value: root - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password - name: DB_HOST value: gowebapp-mysql - name: DB value: gowebapp command: - /bin/sh args: - -c - mysql -h $(DB_HOST) -u $(DB_USER) -p$(DB_PASSWORD) $(DB) -e 'delete from note; delete from user;' Step 2: Create the Job in Kubernetes kubectl apply -f reset-db.yaml --record Step 3: View Job metadata and status The job should run and complete immediately. kubectl get job reset-db kubectl describe job reset-db Step 4: Verify that data has been deleted Open the web application at http://Public_IP:NodePort and verify that the accounts and notes you created above no longer exist.","title":"3.5 Create ConfigMap and Resources and Probes to MySQL"},{"location":"ycit019_ass5/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Resource Limits HPA VPA Limit Range 0 Create GKE Cluster with Cluster and Vertical Autoscaling Support \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-scaling \\ --zone us-central1-c \\ --enable-vertical-pod-autoscaling \\ --num-nodes 2 \\ --enable-autoscaling --min-nodes 1 --max-nodes 3 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-scaling --zone us-central1-c 1.2 Locate Assignment 5 \u00b6 Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement5 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment5/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 5 deploy_a5 folder to your repo: cp -r ~/ycit019/Assignment5/deploy_a5 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 5\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 1.3 Create a Namespace dev \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 2.1 Configure VPA for gowebapp-mysql to find optimal Resource Request and Limits values \u00b6 requests and limits is the way Kubernetes set's QoS for Pods, as well as enable's features like HPA, CA, Resource Quota's and more. However setting best values for resource requests and limits is hard, VPA is here to help. Set VPA for gowebapp and observe usage recommendation for requests and limits Step 1 Deploy gowebapp-mysql app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 2 Edit a manifest for gowebapp-mysql Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp-mysql #TODO: Configure VPA with updateMode:OFF Step 3 Apply the manifest for gowebapp-mysql-vpa kubectl apply -f gowebapp-mysql-vpa.yaml Step 4 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp-mysql Note If you don't see it, wait a little longer and try the previous command again. Step 5 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value. 2.2 Set Recommended Request and Limits values to gowebapp-mysql \u00b6 Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 #TODO define a resource request and limits based on VPA Recommender Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-mysql-deployment.yaml kubectl apply -f gowebapp-mysql-deployment.yaml 2.3 Configure VPA for gowebapp to find optimal Resource Request and Limits values \u00b6 Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 3 Edit a manifest for gowebapp Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp #TODO: Ref: https://cloud.google.com/kubernetes-engine/docs/how-to/vertical-pod-autoscaling #TODO: Configure VPA with updateMode:OFF Step 4 Apply the manifest for gowebapp-vpa kubectl apply -f gowebapp-vpa.yaml Step 5 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp Note If you don't see it, wait a little longer and try the previous command again. Step 6 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value. 2.4 Set Recommended Request and Limits values to gowebapp \u00b6 Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp:v3 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: #TODO define a resource request and limits based on VPA Recommender volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-deployment.yaml kubectl apply -f gowebapp-deployment.yaml 2.5 Configure HPA for gowebapp \u00b6 Our NotePad Application is going to Production soon. To make sure our application can scale based on requests we will set HPA for our deployment resource using Horizontal Pod Autoscaler. Step 1 Create HPA for gowebapp based on CPU with minReplicas 1 and maxReplicas 5 with target 50. cd ~/$MY_REPO/deploy_a5 vim gowebapp-hpa.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: gowebapp-hpa spec: scaleTargetRef: #TODO: Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/ #TODO Create HPA based on CPU with minReplicas `1` and maxReplicas `5` with target 50. Step 2 Apply the manifest for gowebapp-hpa kubectl apply -f gowebapp-hpa.yaml Step 3 Take a closer look at the HPA and observe autoscaling or downscaling if any. kubectl describe hpa kubia Note It will take some time to collect metrics information about current cpu usage and since our does't have real load it might not trigger any scaling 2.6 Configure LimitRanges for Namespace dev \u00b6 In order to prevent developers accidentally forget to set values for request and limits . Ops team decide to create a Configuration LimitRange , that will enforce some default values for request and limits if they have not been set, as well as Minimum and maximum requests/limits a container can have to prevent resources abuse. Step 1 Edit a manifest for limit-range that can be used for dev namespace with following requirements: cd ~/$MY_REPO/deploy_a5/ vim limit-range.yaml apiVersion: v1 kind: LimitRange metadata: name: gowebapp-system #TODO Specify Namespace spec: #TODO Ref: https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/ #TODO If not specified the Container's `memory` limit is set to 256Mi, which is the default memory limit for the namespace. #TODO If not specified default limit `cpu` is 250m per container #TODO If not specified set the Container's Default memory requests to 256Mi #TODO If not specified set the Container's Default cpu requests to 250m #TODO Configure Minimum `20m` and Maximum `800m` CPU Constraints for a Namespace 2.7 Viewing cluster autoscaler events \u00b6 Our cluster was configured to use Cluster Autoscaler, verify if during you assignment, cluster did went through autoscaling process? To view the logs, perform the following: Step 1: In the Cloud Console, go to the Logs Viewer page. Step 2: Search for the logs using the basic or advanced query interface. To search for logs using the basic query interface, perform the following: a. From the resources drop-down list, select Kubernetes Cluster, then select the location of your cluster, and the name of your cluster. b. From the logs type drop-down list, select container.googleapis.com/cluster-autoscaler-visibility. c. From the time-range drop-down list, select the desired time range. OR search for logs using the advanced query interface, apply the following advanced filter: resource.type=\"k8s_cluster\" resource.labels.location=\"cluster-location\" resource.labels.cluster_name=\"cluster-name\" logName=\"projects/project-id/logs/container.googleapis.com%2Fcluster-autoscaler-visibility\" Reference link 2.7 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 5\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"Assignment5"},{"location":"ycit019_ass5/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Resource Limits HPA VPA Limit Range","title":"Deploy Applications on Kubernetes"},{"location":"ycit019_ass5/#0-create-gke-cluster-with-cluster-and-vertical-autoscaling-support","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-scaling \\ --zone us-central1-c \\ --enable-vertical-pod-autoscaling \\ --num-nodes 2 \\ --enable-autoscaling --min-nodes 1 --max-nodes 3 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-scaling --zone us-central1-c","title":"0 Create GKE Cluster with Cluster and Vertical Autoscaling Support"},{"location":"ycit019_ass5/#12-locate-assignment-5","text":"Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement5 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment5/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 5 deploy_a5 folder to your repo: cp -r ~/ycit019/Assignment5/deploy_a5 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 5\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"1.2 Locate Assignment 5"},{"location":"ycit019_ass5/#13-create-a-namespace-dev","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"1.3 Create a Namespace dev"},{"location":"ycit019_ass5/#21-configure-vpa-for-gowebapp-mysql-to-find-optimal-resource-request-and-limits-values","text":"requests and limits is the way Kubernetes set's QoS for Pods, as well as enable's features like HPA, CA, Resource Quota's and more. However setting best values for resource requests and limits is hard, VPA is here to help. Set VPA for gowebapp and observe usage recommendation for requests and limits Step 1 Deploy gowebapp-mysql app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 2 Edit a manifest for gowebapp-mysql Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp-mysql #TODO: Configure VPA with updateMode:OFF Step 3 Apply the manifest for gowebapp-mysql-vpa kubectl apply -f gowebapp-mysql-vpa.yaml Step 4 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp-mysql Note If you don't see it, wait a little longer and try the previous command again. Step 5 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value.","title":"2.1 Configure VPA for gowebapp-mysql to find optimal Resource Request and Limits values"},{"location":"ycit019_ass5/#22-set-recommended-request-and-limits-values-to-gowebapp-mysql","text":"Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 #TODO define a resource request and limits based on VPA Recommender Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-mysql-deployment.yaml kubectl apply -f gowebapp-mysql-deployment.yaml","title":"2.2 Set Recommended Request and Limits values to gowebapp-mysql"},{"location":"ycit019_ass5/#23-configure-vpa-for-gowebapp-to-find-optimal-resource-request-and-limits-values","text":"Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 3 Edit a manifest for gowebapp Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp #TODO: Ref: https://cloud.google.com/kubernetes-engine/docs/how-to/vertical-pod-autoscaling #TODO: Configure VPA with updateMode:OFF Step 4 Apply the manifest for gowebapp-vpa kubectl apply -f gowebapp-vpa.yaml Step 5 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp Note If you don't see it, wait a little longer and try the previous command again. Step 6 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value.","title":"2.3 Configure VPA for gowebapp to find optimal Resource Request and Limits values"},{"location":"ycit019_ass5/#24-set-recommended-request-and-limits-values-to-gowebapp","text":"Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp:v3 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: #TODO define a resource request and limits based on VPA Recommender volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-deployment.yaml kubectl apply -f gowebapp-deployment.yaml","title":"2.4 Set Recommended Request and Limits values to gowebapp"},{"location":"ycit019_ass5/#25-configure-hpa-for-gowebapp","text":"Our NotePad Application is going to Production soon. To make sure our application can scale based on requests we will set HPA for our deployment resource using Horizontal Pod Autoscaler. Step 1 Create HPA for gowebapp based on CPU with minReplicas 1 and maxReplicas 5 with target 50. cd ~/$MY_REPO/deploy_a5 vim gowebapp-hpa.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: gowebapp-hpa spec: scaleTargetRef: #TODO: Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/ #TODO Create HPA based on CPU with minReplicas `1` and maxReplicas `5` with target 50. Step 2 Apply the manifest for gowebapp-hpa kubectl apply -f gowebapp-hpa.yaml Step 3 Take a closer look at the HPA and observe autoscaling or downscaling if any. kubectl describe hpa kubia Note It will take some time to collect metrics information about current cpu usage and since our does't have real load it might not trigger any scaling","title":"2.5 Configure HPA for gowebapp"},{"location":"ycit019_ass5/#26-configure-limitranges-for-namespace-dev","text":"In order to prevent developers accidentally forget to set values for request and limits . Ops team decide to create a Configuration LimitRange , that will enforce some default values for request and limits if they have not been set, as well as Minimum and maximum requests/limits a container can have to prevent resources abuse. Step 1 Edit a manifest for limit-range that can be used for dev namespace with following requirements: cd ~/$MY_REPO/deploy_a5/ vim limit-range.yaml apiVersion: v1 kind: LimitRange metadata: name: gowebapp-system #TODO Specify Namespace spec: #TODO Ref: https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/ #TODO If not specified the Container's `memory` limit is set to 256Mi, which is the default memory limit for the namespace. #TODO If not specified default limit `cpu` is 250m per container #TODO If not specified set the Container's Default memory requests to 256Mi #TODO If not specified set the Container's Default cpu requests to 250m #TODO Configure Minimum `20m` and Maximum `800m` CPU Constraints for a Namespace","title":"2.6 Configure LimitRanges for Namespace dev"},{"location":"ycit019_ass5/#27-viewing-cluster-autoscaler-events","text":"Our cluster was configured to use Cluster Autoscaler, verify if during you assignment, cluster did went through autoscaling process? To view the logs, perform the following: Step 1: In the Cloud Console, go to the Logs Viewer page. Step 2: Search for the logs using the basic or advanced query interface. To search for logs using the basic query interface, perform the following: a. From the resources drop-down list, select Kubernetes Cluster, then select the location of your cluster, and the name of your cluster. b. From the logs type drop-down list, select container.googleapis.com/cluster-autoscaler-visibility. c. From the time-range drop-down list, select the desired time range. OR search for logs using the advanced query interface, apply the following advanced filter: resource.type=\"k8s_cluster\" resource.labels.location=\"cluster-location\" resource.labels.cluster_name=\"cluster-name\" logName=\"projects/project-id/logs/container.googleapis.com%2Fcluster-autoscaler-visibility\" Reference link","title":"2.7 Viewing cluster autoscaler events"},{"location":"ycit019_ass5/#27-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 5\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"2.7 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass5/#36-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"3.6 Cleaning Up"},{"location":"ycit019_ass5_solution/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Resource Limits HPA VPA Limit Range 0 Create GKE Cluster with Cluster and Vertical Autoscaling Support \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-scaling \\ --zone us-central1-c \\ --enable-vertical-pod-autoscaling \\ --num-nodes 2 \\ --enable-autoscaling --min-nodes 1 --max-nodes 3 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-scaling --zone us-central1-c 1.2 Locate Assignment 5 \u00b6 Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement5 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment5/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 5 deploy_a5 folder to your repo: cp -r ~/ycit019/Assignment5/deploy_a5 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 5\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 1.3 Create a Namespace dev \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 2.1 Configure VPA for gowebapp-mysql to find optimal Resource Request and Limits values \u00b6 requests and limits is the way Kubernetes set's QoS for Pods, as well as enable's features like HPA, CA, Resource Quota's and more. However setting best values for resource requests and limits is hard, VPA is here to help. Set VPA for gowebapp and observe usage recommendation for requests and limits Step 1 Deploy gowebapp-mysql app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 2 Edit a manifest for gowebapp-mysql Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp-mysql spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: gowebapp-mysql updatePolicy: updateMode: \"Off\" Step 3 Apply the manifest for gowebapp-mysql-vpa kubectl apply -f gowebapp-mysql-vpa.yaml Step 4 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp-mysql Note If you don't see it, wait a little longer and try the previous command again. Step 5 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value. 2.2 Set Recommended Request and Limits values to gowebapp-mysql \u00b6 Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 20m memory: 252M limits: cpu: 1000m memory: 2G Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-mysql-deployment.yaml kubectl apply -f gowebapp-mysql-deployment.yaml 2.3 Configure VPA for gowebapp to find optimal Resource Request and Limits values \u00b6 Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 3 Edit a manifest for gowebapp Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp spec: targetRef: apiVersion: apps/v1 kind: Deployment name: gowebapp updatePolicy: updateMode: \"Off\" Step 4 Apply the manifest for gowebapp-vpa kubectl apply -f gowebapp-vpa.yaml Step 5 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp Note If you don't see it, wait a little longer and try the previous command again. Step 6 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value. 2.4 Set Recommended Request and Limits values to gowebapp \u00b6 Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp:v3 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: requests: cpu: \"20m\" memory: \"10M\" limits: cpu: \"500m\" memory: \"2G\" volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-deployment.yaml kubectl apply -f gowebapp-deployment.yaml 2.5 Configure HPA for gowebapp \u00b6 Our NotePad Application is going to Production soon. To make sure our application can scale based on requests we will set HPA for our deployment resource using Horizontal Pod Autoscaler. Step 1 Create HPA for gowebapp based on CPU with minReplicas 1 and maxReplicas 5 with target 50. cd ~/$MY_REPO/deploy_a5 vim gowebapp-hpa.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: gowebapp-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: gowebapp minReplicas: 1 maxReplicas: 5 targetCPUUtilizationPercentage: 50 Step 2 Apply the manifest for gowebapp-hpa kubectl apply -f gowebapp-hpa.yaml Step 3 Take a closer look at the HPA and observe autoscaling or downscaling if any. kubectl describe hpa kubia Note It will take some time to collect metrics information about current cpu usage and since our does't have real load it might not trigger any scaling 2.6 Configure LimitRanges for Namespace dev \u00b6 In order to prevent developers accidentally forget to set values for request and limits . Ops team decide to create a Configuration LimitRange , that will enforce some default values for request and limits if they have not been set, as well as Minimum and maximum requests/limits a container can have to prevent resources abuse. Step 1 Edit a manifest for limit-range that can be used for dev namespace with following requirements: cd ~/$MY_REPO/deploy_a5/ vim limit-range.yaml apiVersion: v1 kind: LimitRange metadata: name: gowebapp-system namespace: dev spec: limits: - default: cpu: \"250m\" # If not specified default limit is 500m per container memory: \"256Mi\" # If not specified the Container's memory limit is set to 512Mi defaultRequest: cpu: \"250m\" # If not specified default it will take from whatever specified in limits.default.cpu memory: \"256Mi\" #If not specified default it will take from whatever specified in limits.default.memory max: cpu: 800m min: cpu: 200m type: Container 2.7 Viewing cluster autoscaler events \u00b6 Our cluster was configured to use Cluster Autoscaler, verify if during you assignment, cluster did went through autoscaling process? To view the logs, perform the following: Step 1: In the Cloud Console, go to the Logs Viewer page. Step 2: Search for the logs using the basic or advanced query interface. To search for logs using the basic query interface, perform the following: a. From the resources drop-down list, select Kubernetes Cluster, then select the location of your cluster, and the name of your cluster. b. From the logs type drop-down list, select container.googleapis.com/cluster-autoscaler-visibility. c. From the time-range drop-down list, select the desired time range. OR search for logs using the advanced query interface, apply the following advanced filter: resource.type=\"k8s_cluster\" resource.labels.location=\"cluster-location\" resource.labels.cluster_name=\"cluster-name\" logName=\"projects/project-id/logs/container.googleapis.com%2Fcluster-autoscaler-visibility\" Reference link 2.7 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 5\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"Assignment5 - Solution"},{"location":"ycit019_ass5_solution/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Resource Limits HPA VPA Limit Range","title":"Deploy Applications on Kubernetes"},{"location":"ycit019_ass5_solution/#0-create-gke-cluster-with-cluster-and-vertical-autoscaling-support","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-scaling \\ --zone us-central1-c \\ --enable-vertical-pod-autoscaling \\ --num-nodes 2 \\ --enable-autoscaling --min-nodes 1 --max-nodes 3 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-scaling --zone us-central1-c","title":"0 Create GKE Cluster with Cluster and Vertical Autoscaling Support"},{"location":"ycit019_ass5_solution/#12-locate-assignment-5","text":"Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement5 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment5/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 5 deploy_a5 folder to your repo: cp -r ~/ycit019/Assignment5/deploy_a5 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 5\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"1.2 Locate Assignment 5"},{"location":"ycit019_ass5_solution/#13-create-a-namespace-dev","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"1.3 Create a Namespace dev"},{"location":"ycit019_ass5_solution/#21-configure-vpa-for-gowebapp-mysql-to-find-optimal-resource-request-and-limits-values","text":"requests and limits is the way Kubernetes set's QoS for Pods, as well as enable's features like HPA, CA, Resource Quota's and more. However setting best values for resource requests and limits is hard, VPA is here to help. Set VPA for gowebapp and observe usage recommendation for requests and limits Step 1 Deploy gowebapp-mysql app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 2 Edit a manifest for gowebapp-mysql Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp-mysql spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: gowebapp-mysql updatePolicy: updateMode: \"Off\" Step 3 Apply the manifest for gowebapp-mysql-vpa kubectl apply -f gowebapp-mysql-vpa.yaml Step 4 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp-mysql Note If you don't see it, wait a little longer and try the previous command again. Step 5 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value.","title":"2.1 Configure VPA for gowebapp-mysql to find optimal Resource Request and Limits values"},{"location":"ycit019_ass5_solution/#22-set-recommended-request-and-limits-values-to-gowebapp-mysql","text":"Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 20m memory: 252M limits: cpu: 1000m memory: 2G Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-mysql-deployment.yaml kubectl apply -f gowebapp-mysql-deployment.yaml","title":"2.2 Set Recommended Request and Limits values to gowebapp-mysql"},{"location":"ycit019_ass5_solution/#23-configure-vpa-for-gowebapp-to-find-optimal-resource-request-and-limits-values","text":"Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 3 Edit a manifest for gowebapp Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp spec: targetRef: apiVersion: apps/v1 kind: Deployment name: gowebapp updatePolicy: updateMode: \"Off\" Step 4 Apply the manifest for gowebapp-vpa kubectl apply -f gowebapp-vpa.yaml Step 5 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp Note If you don't see it, wait a little longer and try the previous command again. Step 6 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value.","title":"2.3 Configure VPA for gowebapp to find optimal Resource Request and Limits values"},{"location":"ycit019_ass5_solution/#24-set-recommended-request-and-limits-values-to-gowebapp","text":"Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp:v3 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: requests: cpu: \"20m\" memory: \"10M\" limits: cpu: \"500m\" memory: \"2G\" volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-deployment.yaml kubectl apply -f gowebapp-deployment.yaml","title":"2.4 Set Recommended Request and Limits values to gowebapp"},{"location":"ycit019_ass5_solution/#25-configure-hpa-for-gowebapp","text":"Our NotePad Application is going to Production soon. To make sure our application can scale based on requests we will set HPA for our deployment resource using Horizontal Pod Autoscaler. Step 1 Create HPA for gowebapp based on CPU with minReplicas 1 and maxReplicas 5 with target 50. cd ~/$MY_REPO/deploy_a5 vim gowebapp-hpa.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: gowebapp-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: gowebapp minReplicas: 1 maxReplicas: 5 targetCPUUtilizationPercentage: 50 Step 2 Apply the manifest for gowebapp-hpa kubectl apply -f gowebapp-hpa.yaml Step 3 Take a closer look at the HPA and observe autoscaling or downscaling if any. kubectl describe hpa kubia Note It will take some time to collect metrics information about current cpu usage and since our does't have real load it might not trigger any scaling","title":"2.5 Configure HPA for gowebapp"},{"location":"ycit019_ass5_solution/#26-configure-limitranges-for-namespace-dev","text":"In order to prevent developers accidentally forget to set values for request and limits . Ops team decide to create a Configuration LimitRange , that will enforce some default values for request and limits if they have not been set, as well as Minimum and maximum requests/limits a container can have to prevent resources abuse. Step 1 Edit a manifest for limit-range that can be used for dev namespace with following requirements: cd ~/$MY_REPO/deploy_a5/ vim limit-range.yaml apiVersion: v1 kind: LimitRange metadata: name: gowebapp-system namespace: dev spec: limits: - default: cpu: \"250m\" # If not specified default limit is 500m per container memory: \"256Mi\" # If not specified the Container's memory limit is set to 512Mi defaultRequest: cpu: \"250m\" # If not specified default it will take from whatever specified in limits.default.cpu memory: \"256Mi\" #If not specified default it will take from whatever specified in limits.default.memory max: cpu: 800m min: cpu: 200m type: Container","title":"2.6 Configure LimitRanges for Namespace dev"},{"location":"ycit019_ass5_solution/#27-viewing-cluster-autoscaler-events","text":"Our cluster was configured to use Cluster Autoscaler, verify if during you assignment, cluster did went through autoscaling process? To view the logs, perform the following: Step 1: In the Cloud Console, go to the Logs Viewer page. Step 2: Search for the logs using the basic or advanced query interface. To search for logs using the basic query interface, perform the following: a. From the resources drop-down list, select Kubernetes Cluster, then select the location of your cluster, and the name of your cluster. b. From the logs type drop-down list, select container.googleapis.com/cluster-autoscaler-visibility. c. From the time-range drop-down list, select the desired time range. OR search for logs using the advanced query interface, apply the following advanced filter: resource.type=\"k8s_cluster\" resource.labels.location=\"cluster-location\" resource.labels.cluster_name=\"cluster-name\" logName=\"projects/project-id/logs/container.googleapis.com%2Fcluster-autoscaler-visibility\" Reference link","title":"2.7 Viewing cluster autoscaler events"},{"location":"ycit019_ass5_solution/#27-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 5\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"2.7 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass5_solution/#36-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"3.6 Cleaning Up"},{"location":"ycit019_ass6/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Network Policy Ingress PVC, PV 1. Prepare Environment \u00b6 1.1 Create GKE Cluster with Cluster Network Policy Support \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-networking \\ --zone us-central1-c \\ --enable-ip-alias \\ --create-subnetwork=\"\" \\ --network=default \\ --enable-network-policy \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-networking --zone us-central1-c 1.2 Locate Assignment 6 \u00b6 Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement6 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment6/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 6 deploy_a6 folder to your repo: cp -r ~/ycit019/Assignment6/deploy_a6 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 6\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 1.3 Create a Namespace dev \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 2 Configure Volumes \u00b6 2.1 Create Persistent Volume for gowebapp-mysql \u00b6 Step 1 Edit a Persistent Volume Claim (PVC) gowebapp-mysql-pvc.yaml manifest so that it will Dynamically creates 5G GCP PD Persistent Volume (PV), using balanced persistent disk Provisioner with Access Mode ReadWriteOnce . cd ~/$MY_REPO/deploy_a6 edit gowebapp-mysql-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: gowebapp-mysql-pvc labels: run: gowebapp-mysql spec: #TODO Access Mode `ReadWriteOnce` #TODO Request 5G GCP PD Persistent Storage #TODO Balanced PD CSI Storage Class #TODO GKE CSI Reference: https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/gce-pd-csi-driver Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims Step 2 Apply the manifest for gowebapp-mysql-pvc to Dynamically provision Persistent Volume: kubectl apply -f gowebapp-mysql-pvc.yaml Step 3 Verify STATUS of PVC kubectl get pvc Output: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongodb-pvc Pending standard-rwo 5s List PVs: kubectl get pv List GCP Disks: gcloud compute disks list 2.2 Create Mysql deployment \u00b6 Create Mysql deployment using created Persistent Volume (PV) Step 1 Update gowebapp-mysql-deployment.yaml and add Volume cd ~/$MY_REPO/deploy_a6 edit gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 20m memory: 252M limits: cpu: 1000m memory: 2G #TODO add the definition for volumeMounts: - #TODO: add mountPath as /var/lib/mysql name: mysql #TODO Configure Pods access to storage by using the claim as a volume #TODO: define persistentVolumeClaim #TODO: claimName is the name defined in gowebapp-mysql-pvc.yaml #TODO Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes Step 2 Deploy gowebapp-mysql app under ~/$MY_REPO/deploy_a6/ cd ~/$MY_REPO/deploy_a6/ kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment Check Status of the mysql Pod: kubectl get pods Check Events from the mysql Pod: kubectl get describe $POD Note Notice the sequence of events: Scheduler selects node Volume mounted to Pod kubelet pulls image kubelet creates and starts container Result Our Mysql Deployment is up and running with Volume mounted 2.4 Create GoWebApp deployment \u00b6 Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_a6/ cd ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl get pods Step 3 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 4 Access Loadbalancer IP via browser: EXTERNAL-IP:9000 Result Congrats!!! You've deployed you code to Kubernetes 3.1 Ingress \u00b6 3.1 Expose gowebapp with Ingress resource \u00b6 Step 1 Delete the LoadBalancer gowebapp service. #TODO delete the gowebapp service Step 2 Modify gowebapp service manifest gowebapp-service.yaml to change the service to a NodePort service instead of LoadBalancer service. cd ~/$MY_REPO/deploy_a6/ edit gowebapp-service.yaml Result Cloud Shell Editor opens. Make sure to update the service and save it! apiVersion: v1 kind: Service metadata: name: gowebapp labels: run: gowebapp annotations: cloud.google.com/neg: '{\"ingress\": true}' spec: ports: - port: 9000 targetPort: 80 selector: run: gowebapp #TODO add the appropriate type Go back to Cloud Shell Terminal. kubectl apply -f gowebapp-service.yaml #Re-Create the service Step 3 Create an Ingress resource for the gowebapp service to expose it externally at the path /* cat > gowebapp-ingress.yaml << EOF apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: gowebapp-ingress spec: rules: - http: paths: - path: #TODO add the correct path pathType: #TODO add the pathType appropriate for a GKE Ingress backend: service: name: #TODO add the appropriate service name port: number: #TODO add the appropriate service port EOF Step 4 Deploy gowebapp-ingress app under ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-ingress.yaml #Create Ingress Step 5 Verify the Ingress After you have deployed a workload, grouped its Pods into a Service, and created an Ingress for the Service, you should verify that the Ingress has provisioned the container-native load balancer successfully. kubectl describe ingress gowebapp-ingress Step 6 Test that application is reachable via Ingress Note Wait several minutes for the HTTP(S) load balancer to be configured. Get the Ingress IP address, run the following command: kubectl get ing gowebapp-ingress In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser $ADDRESS/* 4 Kubernetes Network Policy \u00b6 Let's secure our 2 tier application using Kubernetes Network Policy! Task: We need to ensure the following is configured dev namespace deny by default all Ingress traffic including from outside the cluster (Internet), With-in the namespace, and from inside the Cluster mysql pod can be accessed from the gowebapp pod gowebapp pod can be accessed from the Internet, in our case GKE Ingress (HTTPs Loadbalancer) and it must be configure to allow the appropriate HTTP(S) load balancer health check IP ranges FROM CIDR: 35.191.0.0/16 and 130.211.0.0/22 or to make it less secure From any Endpoint CIDR: 0.0.0.0/0 Prerequisite \u00b6 In order to ensure that Kubernetes can configure Network Policy we need to make sure our CNI supports this feature. As we've learned from class Calico, WeaveNet and Cilium are CNIs that support network Policy. Our GKE cluster is already deployed using Calico CNI. To check that calico is installed in your cluster, run: kubectl get pods -n kube-system | grep calico Output calico-node-2n65c 1/1 Running 0 20h calico-node-prhgk 1/1 Running 0 20h calico-node-vertical-autoscaler-57bfddcfd8-85ltc 1/1 Running 0 20h calico-typha-67c885c7b7-7vf6d 1/1 Running 0 20h calico-typha-67c885c7b7-zwkll 1/1 Running 0 20h calico-typha-horizontal-autoscaler-58b8486cd4-pnt7c 1/1 Running 0 20h calico-typha-vertical-autoscaler-7595df8859-zxzmp 1/1 Running 0 20h 4.1 Configure deny-all Network Policy for dev Namespace \u00b6 cd ~/$MY_REPO/deploy_a6/ Step 1 Create a policy so that dev namespace deny by default all Ingress traffic including from the Internet, with-in the namespace, and from inside the Cluster cat > default-deny-dev.yaml << EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny spec: podSelector: matchLabels: #TODO add the appropriate match labels to block all traffic policyTypes: #TODO Default deny all ingress traffic https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-traffic EOF Edit the required fields and save: edit default-deny-dev.yaml Step 2 Deploy default-deny-dev Policy app under ~/$MY_REPO/deploy_a6/ kubectl apply -f default-deny-dev.yaml # deny-all Ingress Traffic to dev Namespace Verify Policy: kubectl describe netpol default-deny Result - blocking the specific traffic to all pods in this namespace Step 3 Verify that you can't access the application through Ingress anymore: Visit the IP address in a web browser $Ingress IP Result can't access the app through ingress loadbalancer anymore. 4.2 Configure Network Policy for gowebapp-mysql \u00b6 Step 1 Create a backend-policy network policy to allow access to gowebapp-mysql pod from the gowebapp pods cd ~/$MY_REPO/deploy_a6/ cat > gowebapp-mysql-netpol.yaml << EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: backend-policy spec: podSelector: matchLabels: #TODO add the appropriate match labels ingress: - from: - podSelector: matchLabels: #TODO add the appropriate match labels EOF Edit the required fields and save: edit gowebapp-mysql-netpol.yaml Step 2 Using Cilium Editor test you Policy for Ingress traffic only Open Browser, Upload created Policy YAML. Result mysql pod can only communicate with gowebapp pod Step 3 Deploy gowebapp-mysql-netpol Policy app under ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-mysql-netpol.yaml # `mysql` pod can be accessed from the `gowebapp` pod Verify Policy: kubectl describe netpol backend-policy Result run=gowebapp-mysql Allowing ingress traffic only From PodSelector: run=gowebapp pod 4.3 Configure Network Policy for gowebapp \u00b6 Step 1 Configure a Network Policy to allow access for the Healthcheck IP ranges needed for the Ingress Loadbalancer, and hence allow access through the Ingress Loadbalancer. The IP rangers you need to enable access from are 35.191.0.0/16 and 130.211.0.0/22 cd ~/$MY_REPO/deploy_a6/ cat > gowebapp-netpol.yaml << EOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: frontend-policy spec: podSelector: matchLabels: run: gowebapp ingress: #TODO add the appropriate rules to enable access from IP ranges policyTypes: - Ingress EOF Edit the required fields and save: edit gowebapp-netpol.yaml Step 2 Using Cilium Editor test you Policy for Ingress traffic only Open Browser, Upload created Policy YAML. Result gowebapp pod can only get ingress traffic from CIDRs: 35.191.0.0/16 and 130.211.0.0/22 Step 3 Deploy gowebapp-netpol Policy app under ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-netpol.yaml # `gowebapp` pod from Internet on CIDR: `35.191.0.0/16` and `130.211.0.0/22` Verify Policy: kubectl describe netpol frontend-policy Result run=gowebapp Allowing ingress traffic from CIDR: 35.191.0.0/16 and 130.211.0.0/22 Step 4 Test that application is reachable via Ingress after all Network Policy has been applied. Get the Ingress IP address, run the following command: kubectl get ing gowebapp-ingress In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser $ADDRESS/* Step 5 Verify that NotePad application is functional (e.g can login and create new entries) 5 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 6\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"Assignment6"},{"location":"ycit019_ass6/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Network Policy Ingress PVC, PV","title":"Deploy Applications on Kubernetes"},{"location":"ycit019_ass6/#1-prepare-environment","text":"","title":"1. Prepare Environment"},{"location":"ycit019_ass6/#11-create-gke-cluster-with-cluster-network-policy-support","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-networking \\ --zone us-central1-c \\ --enable-ip-alias \\ --create-subnetwork=\"\" \\ --network=default \\ --enable-network-policy \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-networking --zone us-central1-c","title":"1.1 Create GKE Cluster with Cluster Network Policy Support"},{"location":"ycit019_ass6/#12-locate-assignment-6","text":"Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement6 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment6/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 6 deploy_a6 folder to your repo: cp -r ~/ycit019/Assignment6/deploy_a6 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 6\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"1.2 Locate Assignment 6"},{"location":"ycit019_ass6/#13-create-a-namespace-dev","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"1.3 Create a Namespace dev"},{"location":"ycit019_ass6/#2-configure-volumes","text":"","title":"2 Configure Volumes"},{"location":"ycit019_ass6/#21-create-persistent-volume-for-gowebapp-mysql","text":"Step 1 Edit a Persistent Volume Claim (PVC) gowebapp-mysql-pvc.yaml manifest so that it will Dynamically creates 5G GCP PD Persistent Volume (PV), using balanced persistent disk Provisioner with Access Mode ReadWriteOnce . cd ~/$MY_REPO/deploy_a6 edit gowebapp-mysql-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: gowebapp-mysql-pvc labels: run: gowebapp-mysql spec: #TODO Access Mode `ReadWriteOnce` #TODO Request 5G GCP PD Persistent Storage #TODO Balanced PD CSI Storage Class #TODO GKE CSI Reference: https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/gce-pd-csi-driver Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims Step 2 Apply the manifest for gowebapp-mysql-pvc to Dynamically provision Persistent Volume: kubectl apply -f gowebapp-mysql-pvc.yaml Step 3 Verify STATUS of PVC kubectl get pvc Output: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongodb-pvc Pending standard-rwo 5s List PVs: kubectl get pv List GCP Disks: gcloud compute disks list","title":"2.1 Create Persistent Volume for gowebapp-mysql"},{"location":"ycit019_ass6/#22-create-mysql-deployment","text":"Create Mysql deployment using created Persistent Volume (PV) Step 1 Update gowebapp-mysql-deployment.yaml and add Volume cd ~/$MY_REPO/deploy_a6 edit gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 20m memory: 252M limits: cpu: 1000m memory: 2G #TODO add the definition for volumeMounts: - #TODO: add mountPath as /var/lib/mysql name: mysql #TODO Configure Pods access to storage by using the claim as a volume #TODO: define persistentVolumeClaim #TODO: claimName is the name defined in gowebapp-mysql-pvc.yaml #TODO Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes Step 2 Deploy gowebapp-mysql app under ~/$MY_REPO/deploy_a6/ cd ~/$MY_REPO/deploy_a6/ kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment Check Status of the mysql Pod: kubectl get pods Check Events from the mysql Pod: kubectl get describe $POD Note Notice the sequence of events: Scheduler selects node Volume mounted to Pod kubelet pulls image kubelet creates and starts container Result Our Mysql Deployment is up and running with Volume mounted","title":"2.2 Create Mysql deployment"},{"location":"ycit019_ass6/#24-create-gowebapp-deployment","text":"Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_a6/ cd ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl get pods Step 3 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 4 Access Loadbalancer IP via browser: EXTERNAL-IP:9000 Result Congrats!!! You've deployed you code to Kubernetes","title":"2.4 Create GoWebApp deployment"},{"location":"ycit019_ass6/#31-ingress","text":"","title":"3.1 Ingress"},{"location":"ycit019_ass6/#31-expose-gowebapp-with-ingress-resource","text":"Step 1 Delete the LoadBalancer gowebapp service. #TODO delete the gowebapp service Step 2 Modify gowebapp service manifest gowebapp-service.yaml to change the service to a NodePort service instead of LoadBalancer service. cd ~/$MY_REPO/deploy_a6/ edit gowebapp-service.yaml Result Cloud Shell Editor opens. Make sure to update the service and save it! apiVersion: v1 kind: Service metadata: name: gowebapp labels: run: gowebapp annotations: cloud.google.com/neg: '{\"ingress\": true}' spec: ports: - port: 9000 targetPort: 80 selector: run: gowebapp #TODO add the appropriate type Go back to Cloud Shell Terminal. kubectl apply -f gowebapp-service.yaml #Re-Create the service Step 3 Create an Ingress resource for the gowebapp service to expose it externally at the path /* cat > gowebapp-ingress.yaml << EOF apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: gowebapp-ingress spec: rules: - http: paths: - path: #TODO add the correct path pathType: #TODO add the pathType appropriate for a GKE Ingress backend: service: name: #TODO add the appropriate service name port: number: #TODO add the appropriate service port EOF Step 4 Deploy gowebapp-ingress app under ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-ingress.yaml #Create Ingress Step 5 Verify the Ingress After you have deployed a workload, grouped its Pods into a Service, and created an Ingress for the Service, you should verify that the Ingress has provisioned the container-native load balancer successfully. kubectl describe ingress gowebapp-ingress Step 6 Test that application is reachable via Ingress Note Wait several minutes for the HTTP(S) load balancer to be configured. Get the Ingress IP address, run the following command: kubectl get ing gowebapp-ingress In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser $ADDRESS/*","title":"3.1 Expose gowebapp with Ingress resource"},{"location":"ycit019_ass6/#4-kubernetes-network-policy","text":"Let's secure our 2 tier application using Kubernetes Network Policy! Task: We need to ensure the following is configured dev namespace deny by default all Ingress traffic including from outside the cluster (Internet), With-in the namespace, and from inside the Cluster mysql pod can be accessed from the gowebapp pod gowebapp pod can be accessed from the Internet, in our case GKE Ingress (HTTPs Loadbalancer) and it must be configure to allow the appropriate HTTP(S) load balancer health check IP ranges FROM CIDR: 35.191.0.0/16 and 130.211.0.0/22 or to make it less secure From any Endpoint CIDR: 0.0.0.0/0","title":"4 Kubernetes Network Policy"},{"location":"ycit019_ass6/#prerequisite","text":"In order to ensure that Kubernetes can configure Network Policy we need to make sure our CNI supports this feature. As we've learned from class Calico, WeaveNet and Cilium are CNIs that support network Policy. Our GKE cluster is already deployed using Calico CNI. To check that calico is installed in your cluster, run: kubectl get pods -n kube-system | grep calico Output calico-node-2n65c 1/1 Running 0 20h calico-node-prhgk 1/1 Running 0 20h calico-node-vertical-autoscaler-57bfddcfd8-85ltc 1/1 Running 0 20h calico-typha-67c885c7b7-7vf6d 1/1 Running 0 20h calico-typha-67c885c7b7-zwkll 1/1 Running 0 20h calico-typha-horizontal-autoscaler-58b8486cd4-pnt7c 1/1 Running 0 20h calico-typha-vertical-autoscaler-7595df8859-zxzmp 1/1 Running 0 20h","title":"Prerequisite"},{"location":"ycit019_ass6/#41-configure-deny-all-network-policy-for-dev-namespace","text":"cd ~/$MY_REPO/deploy_a6/ Step 1 Create a policy so that dev namespace deny by default all Ingress traffic including from the Internet, with-in the namespace, and from inside the Cluster cat > default-deny-dev.yaml << EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny spec: podSelector: matchLabels: #TODO add the appropriate match labels to block all traffic policyTypes: #TODO Default deny all ingress traffic https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-traffic EOF Edit the required fields and save: edit default-deny-dev.yaml Step 2 Deploy default-deny-dev Policy app under ~/$MY_REPO/deploy_a6/ kubectl apply -f default-deny-dev.yaml # deny-all Ingress Traffic to dev Namespace Verify Policy: kubectl describe netpol default-deny Result - blocking the specific traffic to all pods in this namespace Step 3 Verify that you can't access the application through Ingress anymore: Visit the IP address in a web browser $Ingress IP Result can't access the app through ingress loadbalancer anymore.","title":"4.1 Configure deny-all Network Policy for dev Namespace"},{"location":"ycit019_ass6/#42-configure-network-policy-for-gowebapp-mysql","text":"Step 1 Create a backend-policy network policy to allow access to gowebapp-mysql pod from the gowebapp pods cd ~/$MY_REPO/deploy_a6/ cat > gowebapp-mysql-netpol.yaml << EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: backend-policy spec: podSelector: matchLabels: #TODO add the appropriate match labels ingress: - from: - podSelector: matchLabels: #TODO add the appropriate match labels EOF Edit the required fields and save: edit gowebapp-mysql-netpol.yaml Step 2 Using Cilium Editor test you Policy for Ingress traffic only Open Browser, Upload created Policy YAML. Result mysql pod can only communicate with gowebapp pod Step 3 Deploy gowebapp-mysql-netpol Policy app under ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-mysql-netpol.yaml # `mysql` pod can be accessed from the `gowebapp` pod Verify Policy: kubectl describe netpol backend-policy Result run=gowebapp-mysql Allowing ingress traffic only From PodSelector: run=gowebapp pod","title":"4.2 Configure Network Policy for gowebapp-mysql"},{"location":"ycit019_ass6/#43-configure-network-policy-for-gowebapp","text":"Step 1 Configure a Network Policy to allow access for the Healthcheck IP ranges needed for the Ingress Loadbalancer, and hence allow access through the Ingress Loadbalancer. The IP rangers you need to enable access from are 35.191.0.0/16 and 130.211.0.0/22 cd ~/$MY_REPO/deploy_a6/ cat > gowebapp-netpol.yaml << EOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: frontend-policy spec: podSelector: matchLabels: run: gowebapp ingress: #TODO add the appropriate rules to enable access from IP ranges policyTypes: - Ingress EOF Edit the required fields and save: edit gowebapp-netpol.yaml Step 2 Using Cilium Editor test you Policy for Ingress traffic only Open Browser, Upload created Policy YAML. Result gowebapp pod can only get ingress traffic from CIDRs: 35.191.0.0/16 and 130.211.0.0/22 Step 3 Deploy gowebapp-netpol Policy app under ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-netpol.yaml # `gowebapp` pod from Internet on CIDR: `35.191.0.0/16` and `130.211.0.0/22` Verify Policy: kubectl describe netpol frontend-policy Result run=gowebapp Allowing ingress traffic from CIDR: 35.191.0.0/16 and 130.211.0.0/22 Step 4 Test that application is reachable via Ingress after all Network Policy has been applied. Get the Ingress IP address, run the following command: kubectl get ing gowebapp-ingress In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser $ADDRESS/* Step 5 Verify that NotePad application is functional (e.g can login and create new entries)","title":"4.3 Configure Network Policy for gowebapp"},{"location":"ycit019_ass6/#5-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 6\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"5 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass6/#6-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"6 Cleaning Up"},{"location":"ycit019_ass6_solution/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Network Policy Ingress PVC, PV 1. Prepare Environment \u00b6 1.1 Create GKE Cluster with Cluster Network Policy Support \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-networking \\ --zone us-central1-c \\ --enable-ip-alias \\ --create-subnetwork=\"\" \\ --network=default \\ --enable-network-policy \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-networking --zone us-central1-c 1.2 Locate Assignment 6 \u00b6 Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement6 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment6/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 6 deploy_a6 folder to your repo: cp -r ~/ycit019/Assignment6/deploy_a6 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 6\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 1.3 Create a Namespace dev \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 2 Configure Volumes \u00b6 2.1 Create Persistent Volume for gowebapp-mysql \u00b6 Step 1 Edit a Persistent Volume Claim (PVC) gowebapp-mysql-pvc.yaml manifest so that it will Dynamically creates 5G GCP PD Persistent Volume (PV), using balanced persistent disk Provisioner with Access Mode ReadWriteOnce . cd ~/$MY_REPO/deploy_a6 edit gowebapp-mysql-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: gowebapp-mysql-pvc labels: run: gowebapp-mysql spec: #TODO Access Mode `ReadWriteOnce` #TODO Request 5G GCP PD Persistent Storage #TODO Balanced PD CSI Storage Class #TODO GKE CSI Reference: https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/gce-pd-csi-driver storageClassName: standard-rwo accessModes: - ReadWriteOnce resources: requests: storage: 5Gi Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims Save the file! Step 2 Apply the manifest for gowebapp-mysql-pvc to Dynamically provision Persistent Volume: kubectl apply -f gowebapp-mysql-pvc.yaml Step 3 Verify STATUS of PVC kubectl get pvc Output: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongodb-pvc Pending standard-rwo 5s List PVs: kubectl get pv List GCP Disks: gcloud compute disks list 2.2 Create Mysql deployment \u00b6 Create Mysql deployment using created Persistent Volume (PV) Step 1 Update gowebapp-mysql-deployment.yaml and add Volume cd ~/$MY_REPO/deploy_a6 edit gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 20m memory: 252M limits: cpu: 1000m memory: 2G #TODO add the definition for volumeMounts: volumeMounts: - #TODO: add mountPath as /var/lib/mysql - mountPath: /var/lib/mysql name: mysql #TODO Configure Pods access to storage by using the claim as a volume #TODO: define persistentVolumeClaim #TODO: claimName is the name defined in gowebapp-mysql-pvc.yaml #TODO Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes volumes: - name: mysql persistentVolumeClaim: claimName: gowebapp-mysql-pvc Save the file! Step 2 Deploy gowebapp-mysql app under ~/$MY_REPO/deploy_a6/ cd ~/$MY_REPO/deploy_a6/ kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment Check Status of the mysql Pod: kubectl get pods Check Events from the mysql Pod: kubectl get describe $POD Note Notice the sequence of events: Scheduler selects node Volume mounted to Pod kubelet pulls image kubelet creates and starts container Result Our Mysql Deployment is up and running with Volume mounted 2.4 Create GoWebApp deployment \u00b6 Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_a6/ cd ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl get pods Step 3 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 5 Access Loadbalancer IP via browser: EXTERNAL-IP:9000 Result Congrats!!! You've deployed you code to Kubernetes 3 Ingress \u00b6 3.1 Expose gowebapp with Ingress resource \u00b6 Step 1 Delete the LoadBalancer gowebapp service. kubectl delete svc gowebapp Step 2 Modify gowebapp service manifest gowebapp-service.yaml to change the service to a ClusterIP service instead of LoadBalancer service. cd ~/$MY_REPO/deploy_a6/ edit gowebapp-service.yaml Result Cloud Shell Editor opens. Make sure update service and save it! apiVersion: v1 kind: Service metadata: name: gowebapp labels: run: gowebapp spec: ports: - port: 9000 targetPort: 80 selector: run: gowebapp type: NodePort Go back to Cloud Shell Terminal. kubectl apply -f gowebapp-service.yaml #Re-Create Service with ClusterIP Step 3 Create an Ingress resource for the gowebapp service to expose it externally at the path /* cat > gowebapp-ingress.yaml << EOF apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: gowebapp-ingress spec: rules: - http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: gowebapp port: number: 9000 EOF Step 4 Deploy gowebapp-ingress app under ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-ingress.yaml #Create Ingress Note When you create Ingress first time it takes several minutes, as Ingress Resource will create backing Loadbalancer for it. Step 5 Verify the Ingress After you have deployed a workload, grouped its Pods into a Service, and created an Ingress for the Service, you should verify that the Ingress has provisioned the container-native load balancer successfully. kubectl describe ingress gowebapp-ingress Step 6 Test load balancer functionality Note Wait several minutes for the HTTP(S) load balancer to be configured. Get the Ingress IP address, run the following command: kubectl get ing gowebapp-ingress In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser $ADDRESS/gowebapp 4 Kubernetes Network Policy \u00b6 Let's secure our 2 tier application using Kubernetes Network Policy! Task: We need to ensure following is configured dev namespace Denys by default All Ingress traffic including from Outside, With In Namespace, and in Cluster mysql pod can be accessed from the gowebapp pod gowebapp pod can be accessed from the Internet, in our case GKE Ingress (HTTPs Loadbalancer) and it must be configure to allow the appropriate HTTP(S) load balancer health check IP ranges FROM CIDR: 35.191.0.0/16 and 130.211.0.0/22 or to make it less secure From any Endpoint CIDR: 0.0.0.0/0 Prerequisite \u00b6 In order to ensure that Kubernetes can configure Network Policy we need to make sure our CNI supports this feature. As we've learned from class Calico, WeaveNet and Cilium are CNIs that support network Policy. Our GKE cluster already deployed using Calico CNI. To check that you calico installed you cluster run: kubectl get pods -n kube-system | grep calico Output calico-node-2n65c 1/1 Running 0 20h calico-node-prhgk 1/1 Running 0 20h calico-node-vertical-autoscaler-57bfddcfd8-85ltc 1/1 Running 0 20h calico-typha-67c885c7b7-7vf6d 1/1 Running 0 20h calico-typha-67c885c7b7-zwkll 1/1 Running 0 20h calico-typha-horizontal-autoscaler-58b8486cd4-pnt7c 1/1 Running 0 20h calico-typha-vertical-autoscaler-7595df8859-zxzmp 1/1 Running 0 20h 4.1 Configure deny-all Network Policy for dev Namespace \u00b6 Step 1 Locate Directory cd ~/$MY_REPO/deploy_a6/ Step 1 Create Policy so that dev namespace deny by default all Ingress traffic including from Outside Internet, With In Namespace, and inside of Cluster cat > default-deny-dev.yaml << EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny spec: podSelector: matchLabels: {} policyTypes: - Ingress ingress: [] EOF Edit the required fields and save: edit default-deny-dev.yaml Step 2 Deploy default-deny-dev Policy app under ~/$MY_REPO/deploy_a6/ kubectl apply -f default-deny-dev.yaml # deny-all Ingress Traffic to dev Namespace Verify Policy: kubectl describe netpol default-deny Step 3 Verify that you can't access application from Ingress anymore: Visit the IP address in a web browser $Ingress IP 4.2 Configure Network Policy for gowebapp-mysql \u00b6 Step 1 Create a backend-policy network policy to allow access to gowebapp-mysql pod from the gowebapp pod cd ~/$MY_REPO/deploy_a6/ cat > gowebapp-mysql-netpol.yaml << EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: backend-policy spec: podSelector: matchLabels: run: gowebapp-mysql ingress: - from: - podSelector: matchLabels: run: gowebapp EOF Edit the required fields and save: edit gowebapp-mysql-netpol.yaml Step 2 Using Cilium Editor test you Policy for Ingress traffic only Open Browser, Upload created Policy YAML. Result mysql pod can only communicate with gowebapp pod Step 3 Deploy gowebapp-mysql-netpol Policy app under ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-mysql-netpol.yaml # `mysql` pod can be accessed from the `gowebapp` pod Verify Policy: kubectl describe netpol backend-policy Result run=gowebapp-mysql Allowing ingress traffic only From PodSelector: run=gowebapp pod 4.3 Configure Network Policy for gowebapp \u00b6 Step 1 Configure a frontend-policy Network Policy to allow access for the Healthcheck IP ranges needed for the Ingress Loadbalancer, and hence allow access through the Ingress Loadbalancer. The IP rangers you need to enable access from are 35.191.0.0/16 and 130.211.0.0/22 cd ~/$MY_REPO/deploy_a6/ cat > gowebapp-netpol.yaml << EOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: frontend-policy spec: podSelector: matchLabels: run: gowebapp ingress: policyTypes: - Ingress ingress: - from: - ipBlock: cidr: 35.191.0.0/16 - ipBlock: cidr: 130.211.0.0/22 policyTypes: - Ingress EOF Edit the required fields and save: edit gowebapp-netpol.yaml Step 2 Using Cilium Editor test you Policy for Ingress traffic only Open Browser, Upload created Policy YAML. Result gowebapp pod can only get ingress traffic from CIDRs: 35.191.0.0/16 and 130.211.0.0/22 Step 3 Deploy gowebapp-netpol Policy app under ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-netpol.yaml # `gowebapp` pod from Internet on CIDR: `35.191.0.0/16` and `130.211.0.0/22` Verify Policy: kubectl describe netpol frontend-policy Result run=gowebapp Allowing ingress traffic from CIDR: 35.191.0.0/16 and 130.211.0.0/22 Step 4 Test that application is reachable via Ingress after all Network Policy has been applied. Get the Ingress IP address, run the following command: kubectl get ing gowebapp-ingress In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser $ADDRESS/* Step 5 Verify that NotePad application is functional (e.g can login and create new entries) 5 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 6\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"Assignment6 - Solution"},{"location":"ycit019_ass6_solution/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Network Policy Ingress PVC, PV","title":"Deploy Applications on Kubernetes"},{"location":"ycit019_ass6_solution/#1-prepare-environment","text":"","title":"1. Prepare Environment"},{"location":"ycit019_ass6_solution/#11-create-gke-cluster-with-cluster-network-policy-support","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-networking \\ --zone us-central1-c \\ --enable-ip-alias \\ --create-subnetwork=\"\" \\ --network=default \\ --enable-network-policy \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-networking --zone us-central1-c","title":"1.1 Create GKE Cluster with Cluster Network Policy Support"},{"location":"ycit019_ass6_solution/#12-locate-assignment-6","text":"Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement6 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment6/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 6 deploy_a6 folder to your repo: cp -r ~/ycit019/Assignment6/deploy_a6 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 6\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"1.2 Locate Assignment 6"},{"location":"ycit019_ass6_solution/#13-create-a-namespace-dev","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"1.3 Create a Namespace dev"},{"location":"ycit019_ass6_solution/#2-configure-volumes","text":"","title":"2 Configure Volumes"},{"location":"ycit019_ass6_solution/#21-create-persistent-volume-for-gowebapp-mysql","text":"Step 1 Edit a Persistent Volume Claim (PVC) gowebapp-mysql-pvc.yaml manifest so that it will Dynamically creates 5G GCP PD Persistent Volume (PV), using balanced persistent disk Provisioner with Access Mode ReadWriteOnce . cd ~/$MY_REPO/deploy_a6 edit gowebapp-mysql-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: gowebapp-mysql-pvc labels: run: gowebapp-mysql spec: #TODO Access Mode `ReadWriteOnce` #TODO Request 5G GCP PD Persistent Storage #TODO Balanced PD CSI Storage Class #TODO GKE CSI Reference: https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/gce-pd-csi-driver storageClassName: standard-rwo accessModes: - ReadWriteOnce resources: requests: storage: 5Gi Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims Save the file! Step 2 Apply the manifest for gowebapp-mysql-pvc to Dynamically provision Persistent Volume: kubectl apply -f gowebapp-mysql-pvc.yaml Step 3 Verify STATUS of PVC kubectl get pvc Output: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongodb-pvc Pending standard-rwo 5s List PVs: kubectl get pv List GCP Disks: gcloud compute disks list","title":"2.1 Create Persistent Volume for gowebapp-mysql"},{"location":"ycit019_ass6_solution/#22-create-mysql-deployment","text":"Create Mysql deployment using created Persistent Volume (PV) Step 1 Update gowebapp-mysql-deployment.yaml and add Volume cd ~/$MY_REPO/deploy_a6 edit gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 20m memory: 252M limits: cpu: 1000m memory: 2G #TODO add the definition for volumeMounts: volumeMounts: - #TODO: add mountPath as /var/lib/mysql - mountPath: /var/lib/mysql name: mysql #TODO Configure Pods access to storage by using the claim as a volume #TODO: define persistentVolumeClaim #TODO: claimName is the name defined in gowebapp-mysql-pvc.yaml #TODO Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes volumes: - name: mysql persistentVolumeClaim: claimName: gowebapp-mysql-pvc Save the file! Step 2 Deploy gowebapp-mysql app under ~/$MY_REPO/deploy_a6/ cd ~/$MY_REPO/deploy_a6/ kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment Check Status of the mysql Pod: kubectl get pods Check Events from the mysql Pod: kubectl get describe $POD Note Notice the sequence of events: Scheduler selects node Volume mounted to Pod kubelet pulls image kubelet creates and starts container Result Our Mysql Deployment is up and running with Volume mounted","title":"2.2 Create Mysql deployment"},{"location":"ycit019_ass6_solution/#24-create-gowebapp-deployment","text":"Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_a6/ cd ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl get pods Step 3 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 5 Access Loadbalancer IP via browser: EXTERNAL-IP:9000 Result Congrats!!! You've deployed you code to Kubernetes","title":"2.4 Create GoWebApp deployment"},{"location":"ycit019_ass6_solution/#3-ingress","text":"","title":"3 Ingress"},{"location":"ycit019_ass6_solution/#31-expose-gowebapp-with-ingress-resource","text":"Step 1 Delete the LoadBalancer gowebapp service. kubectl delete svc gowebapp Step 2 Modify gowebapp service manifest gowebapp-service.yaml to change the service to a ClusterIP service instead of LoadBalancer service. cd ~/$MY_REPO/deploy_a6/ edit gowebapp-service.yaml Result Cloud Shell Editor opens. Make sure update service and save it! apiVersion: v1 kind: Service metadata: name: gowebapp labels: run: gowebapp spec: ports: - port: 9000 targetPort: 80 selector: run: gowebapp type: NodePort Go back to Cloud Shell Terminal. kubectl apply -f gowebapp-service.yaml #Re-Create Service with ClusterIP Step 3 Create an Ingress resource for the gowebapp service to expose it externally at the path /* cat > gowebapp-ingress.yaml << EOF apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: gowebapp-ingress spec: rules: - http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: gowebapp port: number: 9000 EOF Step 4 Deploy gowebapp-ingress app under ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-ingress.yaml #Create Ingress Note When you create Ingress first time it takes several minutes, as Ingress Resource will create backing Loadbalancer for it. Step 5 Verify the Ingress After you have deployed a workload, grouped its Pods into a Service, and created an Ingress for the Service, you should verify that the Ingress has provisioned the container-native load balancer successfully. kubectl describe ingress gowebapp-ingress Step 6 Test load balancer functionality Note Wait several minutes for the HTTP(S) load balancer to be configured. Get the Ingress IP address, run the following command: kubectl get ing gowebapp-ingress In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser $ADDRESS/gowebapp","title":"3.1 Expose gowebapp with Ingress resource"},{"location":"ycit019_ass6_solution/#4-kubernetes-network-policy","text":"Let's secure our 2 tier application using Kubernetes Network Policy! Task: We need to ensure following is configured dev namespace Denys by default All Ingress traffic including from Outside, With In Namespace, and in Cluster mysql pod can be accessed from the gowebapp pod gowebapp pod can be accessed from the Internet, in our case GKE Ingress (HTTPs Loadbalancer) and it must be configure to allow the appropriate HTTP(S) load balancer health check IP ranges FROM CIDR: 35.191.0.0/16 and 130.211.0.0/22 or to make it less secure From any Endpoint CIDR: 0.0.0.0/0","title":"4 Kubernetes Network Policy"},{"location":"ycit019_ass6_solution/#prerequisite","text":"In order to ensure that Kubernetes can configure Network Policy we need to make sure our CNI supports this feature. As we've learned from class Calico, WeaveNet and Cilium are CNIs that support network Policy. Our GKE cluster already deployed using Calico CNI. To check that you calico installed you cluster run: kubectl get pods -n kube-system | grep calico Output calico-node-2n65c 1/1 Running 0 20h calico-node-prhgk 1/1 Running 0 20h calico-node-vertical-autoscaler-57bfddcfd8-85ltc 1/1 Running 0 20h calico-typha-67c885c7b7-7vf6d 1/1 Running 0 20h calico-typha-67c885c7b7-zwkll 1/1 Running 0 20h calico-typha-horizontal-autoscaler-58b8486cd4-pnt7c 1/1 Running 0 20h calico-typha-vertical-autoscaler-7595df8859-zxzmp 1/1 Running 0 20h","title":"Prerequisite"},{"location":"ycit019_ass6_solution/#41-configure-deny-all-network-policy-for-dev-namespace","text":"Step 1 Locate Directory cd ~/$MY_REPO/deploy_a6/ Step 1 Create Policy so that dev namespace deny by default all Ingress traffic including from Outside Internet, With In Namespace, and inside of Cluster cat > default-deny-dev.yaml << EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny spec: podSelector: matchLabels: {} policyTypes: - Ingress ingress: [] EOF Edit the required fields and save: edit default-deny-dev.yaml Step 2 Deploy default-deny-dev Policy app under ~/$MY_REPO/deploy_a6/ kubectl apply -f default-deny-dev.yaml # deny-all Ingress Traffic to dev Namespace Verify Policy: kubectl describe netpol default-deny Step 3 Verify that you can't access application from Ingress anymore: Visit the IP address in a web browser $Ingress IP","title":"4.1 Configure deny-all Network Policy for dev Namespace"},{"location":"ycit019_ass6_solution/#42-configure-network-policy-for-gowebapp-mysql","text":"Step 1 Create a backend-policy network policy to allow access to gowebapp-mysql pod from the gowebapp pod cd ~/$MY_REPO/deploy_a6/ cat > gowebapp-mysql-netpol.yaml << EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: backend-policy spec: podSelector: matchLabels: run: gowebapp-mysql ingress: - from: - podSelector: matchLabels: run: gowebapp EOF Edit the required fields and save: edit gowebapp-mysql-netpol.yaml Step 2 Using Cilium Editor test you Policy for Ingress traffic only Open Browser, Upload created Policy YAML. Result mysql pod can only communicate with gowebapp pod Step 3 Deploy gowebapp-mysql-netpol Policy app under ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-mysql-netpol.yaml # `mysql` pod can be accessed from the `gowebapp` pod Verify Policy: kubectl describe netpol backend-policy Result run=gowebapp-mysql Allowing ingress traffic only From PodSelector: run=gowebapp pod","title":"4.2 Configure Network Policy for gowebapp-mysql"},{"location":"ycit019_ass6_solution/#43-configure-network-policy-for-gowebapp","text":"Step 1 Configure a frontend-policy Network Policy to allow access for the Healthcheck IP ranges needed for the Ingress Loadbalancer, and hence allow access through the Ingress Loadbalancer. The IP rangers you need to enable access from are 35.191.0.0/16 and 130.211.0.0/22 cd ~/$MY_REPO/deploy_a6/ cat > gowebapp-netpol.yaml << EOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: frontend-policy spec: podSelector: matchLabels: run: gowebapp ingress: policyTypes: - Ingress ingress: - from: - ipBlock: cidr: 35.191.0.0/16 - ipBlock: cidr: 130.211.0.0/22 policyTypes: - Ingress EOF Edit the required fields and save: edit gowebapp-netpol.yaml Step 2 Using Cilium Editor test you Policy for Ingress traffic only Open Browser, Upload created Policy YAML. Result gowebapp pod can only get ingress traffic from CIDRs: 35.191.0.0/16 and 130.211.0.0/22 Step 3 Deploy gowebapp-netpol Policy app under ~/$MY_REPO/deploy_a6/ kubectl apply -f gowebapp-netpol.yaml # `gowebapp` pod from Internet on CIDR: `35.191.0.0/16` and `130.211.0.0/22` Verify Policy: kubectl describe netpol frontend-policy Result run=gowebapp Allowing ingress traffic from CIDR: 35.191.0.0/16 and 130.211.0.0/22 Step 4 Test that application is reachable via Ingress after all Network Policy has been applied. Get the Ingress IP address, run the following command: kubectl get ing gowebapp-ingress In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser $ADDRESS/* Step 5 Verify that NotePad application is functional (e.g can login and create new entries)","title":"4.3 Configure Network Policy for gowebapp"},{"location":"ycit019_ass6_solution/#5-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 6\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"5 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass6_solution/#6-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"6 Cleaning Up"},{"location":"ycit019_assignment1/","text":"1 Containerize Applications \u00b6 Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1.1 Overview of the Sample Application \u00b6 This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application. 1.1 Build Dockers image for frontend application \u00b6 Step 1 Locate and review the go source code: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Setup vim editor with appropriate color schema Note Skip this step if you use another editor. echo \"colo elflord\" > ~/.vimrc Step 3 Write Dockerfile for your frontend application cd ~/ycit019/Assignment1/gowebapp Create a file named Dockerfile in this directory for the frontend Go app. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1 1.2 Build Docker image for backend application \u00b6 Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally 1.3 Test application by running with Docker Engine. \u00b6 Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network named gowebapp with subnet range 172.19.0.0/16 in which to run the frontend and backend containers: #TODO docker xxx Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Container needs to run on network: `gowebapp` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine #TODO Container needs to run on network: `gowebapp` Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit; 1.5 Cleanup running applications and unused networks \u00b6 ### TODO docker xxx","title":"1 Containerize Applications"},{"location":"ycit019_assignment1/#1-containerize-applications","text":"Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process","title":"1 Containerize Applications"},{"location":"ycit019_assignment1/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ycit019_assignment1/#11-overview-of-the-sample-application","text":"This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application.","title":"1.1 Overview of the Sample Application"},{"location":"ycit019_assignment1/#11-build-dockers-image-for-frontend-application","text":"Step 1 Locate and review the go source code: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Setup vim editor with appropriate color schema Note Skip this step if you use another editor. echo \"colo elflord\" > ~/.vimrc Step 3 Write Dockerfile for your frontend application cd ~/ycit019/Assignment1/gowebapp Create a file named Dockerfile in this directory for the frontend Go app. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1","title":"1.1 Build Dockers image for frontend application"},{"location":"ycit019_assignment1/#12-build-docker-image-for-backend-application","text":"Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally","title":"1.2 Build Docker image for backend application"},{"location":"ycit019_assignment1/#13-test-application-by-running-with-docker-engine","text":"Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network named gowebapp with subnet range 172.19.0.0/16 in which to run the frontend and backend containers: #TODO docker xxx Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Container needs to run on network: `gowebapp` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine #TODO Container needs to run on network: `gowebapp` Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit;","title":"1.3 Test application by running with Docker Engine."},{"location":"ycit019_assignment1/#15-cleanup-running-applications-and-unused-networks","text":"### TODO docker xxx","title":"1.5 Cleanup running applications and unused networks"},{"location":"ycit019_assignment3_sol/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Secrets ConfigMaps Persistent Volumes (PV) Persisten Volume Claims (PVC) Jobs CronJobs HPA Externalize Web Application Configuration 3.1 Externalize Web Application Configuration \u00b6 Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~/k8scanada/ git pull mkdir $HOME/k8scanada/Assignment3/gowebapp/config cp $HOME/k8scanada/Assignment3/gowebapp/code/config/config.json \\ $HOME/k8scanada/Assignment3/gowebapp/config rm -rf $HOME/k8scanada/Assignment3/gowebapp/code/config/ Step 2: Modify app to support setting DB password through environment variable. Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor to modify: vim $HOME/k8scanada/Assignment3/gowebapp/code/vendor/app/shared/database/database.go Add an import for the os package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") } 3.2 Build new Docker image for your frontend application \u00b6 Step 1: Update Dockerfile for your frontend application FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go ENV PASSWORD=rootpasswd COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install VOLUME $GOPATH/src/gowebapp/config ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd $HOME/k8scanada/Assignment3/gowebapp/ Build the gowebapp image locally. Make sure to include \u201c.\u201c at the end. Notice the new version label. docker build -t gowebapp:v2 . 3.3 Run and test new Docker image locally \u00b6 Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=cloudops user-name/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 30005:80 \\ -v $HOME/k8scanada/Assignment3/gowebapp/config:/go/src/gowebapp/config \\ -e DB_PASSWORD=cloudops --net gowebapp -d --name gowebapp \\ --hostname gowebapp user-name/gowebapp:v2 Now that we\u2019ve launched the application containers, let\u2019s try to test the web application locally. You should be able to access the application at http://Public_IP:30005. Step 3: Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes 3.4 Publish New Image \u00b6 Step 1 We built the second version of gowebapp from the last exercise and tested it locally. Now we can tag and push gowebapp:v2 to private repository. docker tag gowebapp:v2 <user-name>/gowebapp:v2 docker push <user-name>/gowebapp:v2 3.5 Create a Secret \u00b6 Step 1 Create a secret for the MySQL password kind: Secret apiVersion: v1 metadata: name: mysql type: Opaque data: password: cm9vdHBhc3N3ZA== 3.6 Create a Volume for Mysql \u00b6 Step 1: Dynamically provisioning Persistent Volume Define a Persistent Volume Claim object to use with MySQL in a file named pvc.yaml under $HOME/k8scanada/Assignment3/deploy . In this case, we are not explicitly defining a Persistent Volume (pv) object for this PVC to use. This approach is more portable than explicitly defining and hard-wiring volume types. Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims cd $HOME/k8scanada/Assignment3/deploy vim pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysqlpvc labels: run: gowebapp-mysql spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi Step 2: Create PVC object kubectl apply -f pvc.yaml --record Step 3: View your PVC metadata kubectl get pvc mysqlpvc kubectl describe pv 3.6 Create Mysql deployment \u00b6 Create Mysql deployment using Secrets, liveness, readiness, resources, limits and Persistent Volume. Step 1 Update gowebapp-mysql-deployment.yaml under $HOME/k8scanada/Assignment3/deploy vim $HOME/k8scanada/Assignment3/deploy/gowebapp-mysql-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: replicas: 1 strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: archyufa/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 256m memory: 256Mi limits: cpu: 512m memory: 512Mi volumeMounts: - mountPath: /var/lib/mysql name: mysql volumes: - name: mysql persistentVolumeClaim: claimName: mysqlpvc Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 5 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 6 Get rollout history details for specific revision (use number show in output to previous command) Notice the value for MYSQL_ROOT_PASSWORD kubectl rollout history deploy gowebapp-mysql \\ --revision=<latest_version_number> 3.5 Create ConfigMap and Resources and Probes to MySQL \u00b6 Step 1: create ConfigMap for gowebapp's config.json file kubectl create configmap gowebapp --from-file=webapp-config-json=/home/cca-user/k8scanada/Assignment3/gowebapp/config/config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json Step 2: Update gowebapp-deployment.yaml under /home/cca-user/k8scanada/Assignment3/deploy In this exercise, we will add liveness/readiness probes to our deployments, as well as resource limits. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: <#TODO_user-name>/gowebapp:v1 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: request: cpu: 200m memory: 128Mi limits: cpu: 250m memory: 256Mi volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6: Access your application: <http://VM_Public_IP:NodePort. Register for an account, login and use the Notepad. If you need to lookup the VM_Public_IP for your environment, use the following command: kubectl get svc gowebapp -o wide ## 3.6 Define a Job to purge data from the web application database Create a Job object definition in a file called reset-db.yaml which connects to the MySQL database and deletes all the data from the note and user tables. Replace the TODOs below with the missing elements of the definition. If you need more help, refer to: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion Step 1: Create job reset-db.yaml under /home/cca-user/k8scanada/Assignment3/deploy vim reset-db.yaml apVersion: batch/v1 kind: Job metadata: name: reset-db labels: run: reset-db spec: activeDeadlineSeconds: 10 template: metadata: name: reset-db spec: restartPolicy: OnFailure containers: - name: database-cleaner image: mysql:5.6 env: - name: DB_USER value: root - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password - name: DB_HOST value: gowebapp-mysql - name: DB value: gowebapp command: - /bin/sh args: - -c - mysql -h $(DB_HOST) -u $(DB_USER) -p$(DB_PASSWORD) $(DB) -e 'delete from note; delete from user;' Step 2: Create the Job in Kubernetes kubectl apply -f reset-db.yaml --record Step 3: View Job metadata and status The job should run and complete immediately. kubectl get job reset-db kubectl describe job reset-db Step 4: Verify that data has been deleted Open the web application at http://Public_IP:NodePort and verify that the accounts and notes you created above no longer exist.","title":"Deploy Applications on Kubernetes"},{"location":"ycit019_assignment3_sol/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Secrets ConfigMaps Persistent Volumes (PV) Persisten Volume Claims (PVC) Jobs CronJobs HPA Externalize Web Application Configuration","title":"Deploy Applications on Kubernetes"},{"location":"ycit019_assignment3_sol/#31-externalize-web-application-configuration","text":"Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~/k8scanada/ git pull mkdir $HOME/k8scanada/Assignment3/gowebapp/config cp $HOME/k8scanada/Assignment3/gowebapp/code/config/config.json \\ $HOME/k8scanada/Assignment3/gowebapp/config rm -rf $HOME/k8scanada/Assignment3/gowebapp/code/config/ Step 2: Modify app to support setting DB password through environment variable. Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor to modify: vim $HOME/k8scanada/Assignment3/gowebapp/code/vendor/app/shared/database/database.go Add an import for the os package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") }","title":"3.1 Externalize Web Application Configuration"},{"location":"ycit019_assignment3_sol/#32-build-new-docker-image-for-your-frontend-application","text":"Step 1: Update Dockerfile for your frontend application FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go ENV PASSWORD=rootpasswd COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install VOLUME $GOPATH/src/gowebapp/config ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd $HOME/k8scanada/Assignment3/gowebapp/ Build the gowebapp image locally. Make sure to include \u201c.\u201c at the end. Notice the new version label. docker build -t gowebapp:v2 .","title":"3.2 Build new Docker image for your frontend application"},{"location":"ycit019_assignment3_sol/#33-run-and-test-new-docker-image-locally","text":"Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=cloudops user-name/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 30005:80 \\ -v $HOME/k8scanada/Assignment3/gowebapp/config:/go/src/gowebapp/config \\ -e DB_PASSWORD=cloudops --net gowebapp -d --name gowebapp \\ --hostname gowebapp user-name/gowebapp:v2 Now that we\u2019ve launched the application containers, let\u2019s try to test the web application locally. You should be able to access the application at http://Public_IP:30005. Step 3: Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes","title":"3.3 Run and test new Docker image locally"},{"location":"ycit019_assignment3_sol/#34-publish-new-image","text":"Step 1 We built the second version of gowebapp from the last exercise and tested it locally. Now we can tag and push gowebapp:v2 to private repository. docker tag gowebapp:v2 <user-name>/gowebapp:v2 docker push <user-name>/gowebapp:v2","title":"3.4 Publish New Image"},{"location":"ycit019_assignment3_sol/#35-create-a-secret","text":"Step 1 Create a secret for the MySQL password kind: Secret apiVersion: v1 metadata: name: mysql type: Opaque data: password: cm9vdHBhc3N3ZA==","title":"3.5 Create a Secret"},{"location":"ycit019_assignment3_sol/#36-create-a-volume-for-mysql","text":"Step 1: Dynamically provisioning Persistent Volume Define a Persistent Volume Claim object to use with MySQL in a file named pvc.yaml under $HOME/k8scanada/Assignment3/deploy . In this case, we are not explicitly defining a Persistent Volume (pv) object for this PVC to use. This approach is more portable than explicitly defining and hard-wiring volume types. Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims cd $HOME/k8scanada/Assignment3/deploy vim pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysqlpvc labels: run: gowebapp-mysql spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi Step 2: Create PVC object kubectl apply -f pvc.yaml --record Step 3: View your PVC metadata kubectl get pvc mysqlpvc kubectl describe pv","title":"3.6 Create a Volume for Mysql"},{"location":"ycit019_assignment3_sol/#36-create-mysql-deployment","text":"Create Mysql deployment using Secrets, liveness, readiness, resources, limits and Persistent Volume. Step 1 Update gowebapp-mysql-deployment.yaml under $HOME/k8scanada/Assignment3/deploy vim $HOME/k8scanada/Assignment3/deploy/gowebapp-mysql-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: replicas: 1 strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: archyufa/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 256m memory: 256Mi limits: cpu: 512m memory: 512Mi volumeMounts: - mountPath: /var/lib/mysql name: mysql volumes: - name: mysql persistentVolumeClaim: claimName: mysqlpvc Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 5 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 6 Get rollout history details for specific revision (use number show in output to previous command) Notice the value for MYSQL_ROOT_PASSWORD kubectl rollout history deploy gowebapp-mysql \\ --revision=<latest_version_number>","title":"3.6 Create Mysql deployment"},{"location":"ycit019_assignment3_sol/#35-create-configmap-and-resources-and-probes-to-mysql","text":"Step 1: create ConfigMap for gowebapp's config.json file kubectl create configmap gowebapp --from-file=webapp-config-json=/home/cca-user/k8scanada/Assignment3/gowebapp/config/config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json Step 2: Update gowebapp-deployment.yaml under /home/cca-user/k8scanada/Assignment3/deploy In this exercise, we will add liveness/readiness probes to our deployments, as well as resource limits. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: <#TODO_user-name>/gowebapp:v1 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: request: cpu: 200m memory: 128Mi limits: cpu: 250m memory: 256Mi volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6: Access your application: <http://VM_Public_IP:NodePort. Register for an account, login and use the Notepad. If you need to lookup the VM_Public_IP for your environment, use the following command: kubectl get svc gowebapp -o wide ## 3.6 Define a Job to purge data from the web application database Create a Job object definition in a file called reset-db.yaml which connects to the MySQL database and deletes all the data from the note and user tables. Replace the TODOs below with the missing elements of the definition. If you need more help, refer to: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion Step 1: Create job reset-db.yaml under /home/cca-user/k8scanada/Assignment3/deploy vim reset-db.yaml apVersion: batch/v1 kind: Job metadata: name: reset-db labels: run: reset-db spec: activeDeadlineSeconds: 10 template: metadata: name: reset-db spec: restartPolicy: OnFailure containers: - name: database-cleaner image: mysql:5.6 env: - name: DB_USER value: root - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password - name: DB_HOST value: gowebapp-mysql - name: DB value: gowebapp command: - /bin/sh args: - -c - mysql -h $(DB_HOST) -u $(DB_USER) -p$(DB_PASSWORD) $(DB) -e 'delete from note; delete from user;' Step 2: Create the Job in Kubernetes kubectl apply -f reset-db.yaml --record Step 3: View Job metadata and status The job should run and complete immediately. kubectl get job reset-db kubectl describe job reset-db Step 4: Verify that data has been deleted Open the web application at http://Public_IP:NodePort and verify that the accounts and notes you created above no longer exist.","title":"3.5 Create ConfigMap and Resources and Probes to MySQL"},{"location":"ycit020_Lab_1_istio_traffic_control/","text":"Istio Traffic Control - Canary Deployments \u00b6 Objective: Install Istio Deploy an application and enable automatic sidecar injection Deploy a canary service and use Istio to route some traffic to the new service 0 Create a Cluster and Deploy Istio \u00b6 Follow the steps in the assignment to create a GKE cluster and deploy Istio 1 Deploy an Application \u00b6 Step 1 Get the source code for a sample application. For this lab, we will use the Online Boutique sample app. cd ~ git clone https://github.com/GoogleCloudPlatform/microservices-demo.git Step 2 Create a namespace online-boutique for example kubectl create namespace online-boutique Step 3 Enable automatic sidecar injection for the online-boutique namespace kubectl label namespace online-boutique istio-injection=enabled --overwrite Step 4 Deploy the application by creating the kubernetes manifests kubectl apply -f ./release/kubernetes-manifests.yaml -n online-boutique Step 5 Watch the pods and verify that all pods were started successfully with the sidecar injected watch kubectl get pods -n online-boutique 1 Allow Traffic to Online Boutique Application \u00b6 In order for traffic to flow to the frontend of the application, we need to create an ingress gateway and a virtual service attached to it. In the istio-manifest directory, we already have the required resources to accomplish this: * frontend-gateway.yaml: configures the Ingressgateway and the virtualService to allow external traffic into the mesh and route it to the frontend serivce * frontend.yaml: defines another virtual service to be used by our load generator to make sure traffic directed to the frontend from within the mesh stays internal, and doesn't go through the gateway. * Whitelist-egress-googleapis.yaml: Used to whitelist various google APIs used by services within the mesh. Step 1 Apply the istio manifests kubectl apply -f ./istio-manifests -n online-boutique Step 2 Get the external IP address of the ingress gateway and check the application kubectl get -n istio-system service istio-ingressgateway 2 Canary Deployments with Istio \u00b6 Step 1 Patch the Recommendation service deployment to give it a production label kubectl patch deployment -n online-boutique recommendationservice --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/metadata/labels/version\", \"value\": \"production\"}]' Step 2 Check the labels on the deployment kubectl get pod -n online-boutique recommendationservice --show-labels Step 3 Create a DestinationRule that defines two subsets production and canary cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: recommendationservice namespace: online-boutique spec: host: recommendationservice subsets: - name: production labels: version: production - name: canary labels: version: canary EOF Step 4 Make a change to the Recommendation service cd ~/microservices-demo sed -i 's/max_responses = 5/max_responses = 2/g' \\ src/recommendationservice/recommendation_server.py Step 5 Build a new version of the image PROJECT_ID=$(gcloud config list --format \"value(core.project)\") docker build -t \\ gcr.io/${PROJECT_ID}/microservices_demo/recommendationservice:canary \\ src/recommendationservice Step 6 Push the image to GCR docker push gcr.io/${PROJECT_ID}/microservices_demo/recommendationservice:canary Step 7 Deploy the canary version of the Recommendation service cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: recommendationservice version: canary name: recommendationservice-canary namespace: online-boutique spec: replicas: 1 selector: matchLabels: app: recommendationservice version: canary template: metadata: labels: app: recommendationservice version: canary spec: containers: - env: - name: PORT value: \"8080\" - name: PRODUCT_CATALOG_SERVICE_ADDR value: productcatalogservice:3550 - name: ENABLE_PROFILER value: \"0\" image: gcr.io/${PROJECT_ID}/microservices_demo/recommendationservice:canary imagePullPolicy: Always livenessProbe: exec: command: - /bin/grpc_health_probe - -addr=:8080 failureThreshold: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 name: server ports: - containerPort: 8080 protocol: TCP readinessProbe: exec: command: - /bin/grpc_health_probe - -addr=:8080 failureThreshold: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 resources: limits: cpu: 200m memory: 450Mi requests: cpu: 100m memory: 220Mi EOF Step 8 Deploy a virtual service to direct 20% of the traffic to the canary version cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendationservice namespace: online-boutique spec: hosts: - recommendationservice http: - route: - destination: host: recommendationservice port: number: 8080 subset: production weight: 80 - destination: host: recommendationservice port: number: 8080 subset: canary weight: 20 EOF Step 9 Refresh the page for one of the products where recommendations are shown multiple times, and notice how most of the time you get 5 recommendations while if 20% of the time you get only 2 recommendations.","title":"Istio Traffic Control - Canary Deployments"},{"location":"ycit020_Lab_1_istio_traffic_control/#istio-traffic-control-canary-deployments","text":"Objective: Install Istio Deploy an application and enable automatic sidecar injection Deploy a canary service and use Istio to route some traffic to the new service","title":"Istio Traffic Control - Canary Deployments"},{"location":"ycit020_Lab_1_istio_traffic_control/#0-create-a-cluster-and-deploy-istio","text":"Follow the steps in the assignment to create a GKE cluster and deploy Istio","title":"0 Create a Cluster and Deploy Istio"},{"location":"ycit020_Lab_1_istio_traffic_control/#1-deploy-an-application","text":"Step 1 Get the source code for a sample application. For this lab, we will use the Online Boutique sample app. cd ~ git clone https://github.com/GoogleCloudPlatform/microservices-demo.git Step 2 Create a namespace online-boutique for example kubectl create namespace online-boutique Step 3 Enable automatic sidecar injection for the online-boutique namespace kubectl label namespace online-boutique istio-injection=enabled --overwrite Step 4 Deploy the application by creating the kubernetes manifests kubectl apply -f ./release/kubernetes-manifests.yaml -n online-boutique Step 5 Watch the pods and verify that all pods were started successfully with the sidecar injected watch kubectl get pods -n online-boutique","title":"1 Deploy an Application"},{"location":"ycit020_Lab_1_istio_traffic_control/#1-allow-traffic-to-online-boutique-application","text":"In order for traffic to flow to the frontend of the application, we need to create an ingress gateway and a virtual service attached to it. In the istio-manifest directory, we already have the required resources to accomplish this: * frontend-gateway.yaml: configures the Ingressgateway and the virtualService to allow external traffic into the mesh and route it to the frontend serivce * frontend.yaml: defines another virtual service to be used by our load generator to make sure traffic directed to the frontend from within the mesh stays internal, and doesn't go through the gateway. * Whitelist-egress-googleapis.yaml: Used to whitelist various google APIs used by services within the mesh. Step 1 Apply the istio manifests kubectl apply -f ./istio-manifests -n online-boutique Step 2 Get the external IP address of the ingress gateway and check the application kubectl get -n istio-system service istio-ingressgateway","title":"1 Allow Traffic to Online Boutique Application"},{"location":"ycit020_Lab_1_istio_traffic_control/#2-canary-deployments-with-istio","text":"Step 1 Patch the Recommendation service deployment to give it a production label kubectl patch deployment -n online-boutique recommendationservice --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/metadata/labels/version\", \"value\": \"production\"}]' Step 2 Check the labels on the deployment kubectl get pod -n online-boutique recommendationservice --show-labels Step 3 Create a DestinationRule that defines two subsets production and canary cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: recommendationservice namespace: online-boutique spec: host: recommendationservice subsets: - name: production labels: version: production - name: canary labels: version: canary EOF Step 4 Make a change to the Recommendation service cd ~/microservices-demo sed -i 's/max_responses = 5/max_responses = 2/g' \\ src/recommendationservice/recommendation_server.py Step 5 Build a new version of the image PROJECT_ID=$(gcloud config list --format \"value(core.project)\") docker build -t \\ gcr.io/${PROJECT_ID}/microservices_demo/recommendationservice:canary \\ src/recommendationservice Step 6 Push the image to GCR docker push gcr.io/${PROJECT_ID}/microservices_demo/recommendationservice:canary Step 7 Deploy the canary version of the Recommendation service cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: recommendationservice version: canary name: recommendationservice-canary namespace: online-boutique spec: replicas: 1 selector: matchLabels: app: recommendationservice version: canary template: metadata: labels: app: recommendationservice version: canary spec: containers: - env: - name: PORT value: \"8080\" - name: PRODUCT_CATALOG_SERVICE_ADDR value: productcatalogservice:3550 - name: ENABLE_PROFILER value: \"0\" image: gcr.io/${PROJECT_ID}/microservices_demo/recommendationservice:canary imagePullPolicy: Always livenessProbe: exec: command: - /bin/grpc_health_probe - -addr=:8080 failureThreshold: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 name: server ports: - containerPort: 8080 protocol: TCP readinessProbe: exec: command: - /bin/grpc_health_probe - -addr=:8080 failureThreshold: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 resources: limits: cpu: 200m memory: 450Mi requests: cpu: 100m memory: 220Mi EOF Step 8 Deploy a virtual service to direct 20% of the traffic to the canary version cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendationservice namespace: online-boutique spec: hosts: - recommendationservice http: - route: - destination: host: recommendationservice port: number: 8080 subset: production weight: 80 - destination: host: recommendationservice port: number: 8080 subset: canary weight: 20 EOF Step 9 Refresh the page for one of the products where recommendations are shown multiple times, and notice how most of the time you get 5 recommendations while if 20% of the time you get only 2 recommendations.","title":"2 Canary Deployments with Istio"}]}