{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"YCIT019 - Cloud Architecture \u00b6","title":"Home"},{"location":"#ycit019-cloud-architecture","text":"","title":"YCIT019 - Cloud Architecture"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/","text":"Lab 4 Managing Docker Images Objective: Learn to build docker images using Dockerfiles. Store images in Docker Hub Learn alternative registry solutions (Quya.io) Automate image build process with Docker Cloud 1 Building Docker Images \u00b6 In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a registry of Docker images. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: git clone https://github.com/archyufa/k8scanada 1.1 Create DOCKERFILE \u00b6 Step 1 Clone git repo on you laptop: git clone https://github.com/archyufa/k8scanada cd k8scanada/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26388-1381844103-11.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr01/15/9/anigif_enhanced-buzz-31540-1381844535-8.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26390-1381844163-18.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-1376-1381846217-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3391-1381844336-26.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-29111-1381845968-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3409-1381844582-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr02/15/9/anigif_enhanced-buzz-19667-1381844937-10.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26358-1381845043-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-18774-1381844645-6.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-25158-1381844793-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/10/anigif_enhanced-buzz-11980-1381846269-1.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==0.10.1 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # our base image FROM alpine:3.5 # Install python and pip RUN apk add --update py2-pip # upgrade pip RUN pip install --upgrade pip # install Python modules needed by the Python app COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt # copy files required for the app to run COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ # tell the port number the container should expose EXPOSE 5000 # run the application CMD [ \"python\" , \"/usr/src/app/app.py\" ] 1.2 Build a Docker image \u00b6 Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step. docker build -t <user-name>/myfirstapp . Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 80:5000 --name myfirstapp <user-name>/myfirstapp Step 4 Use your browser to open the address http:// and check that the application works. Step 5 Stop the container and remove it: docker stop myfirstapp docker rm myfirstapp myfirstapp 1.2.1 Share docker images with tar files \u00b6 Now ideally you want to share you freshly build docker image with someone or run it in different environment which for some reason don\u2019t have internet access so images can not be pulled from Online Docker registries. In that case Docker images can be shared as we share traditionally regular files by creating tarballs using docker save command as following Step 1 Create a tar file using docker save command: docker save <user-name>/myfirstapp > myfirstapp.tar Or docker save --output myfirstapp1.tar archyufa/myfirstapp ls -trh | grep tar Step 2 Transfer images to another environment using scp command. Hint You can also store this images in Object storages, e.g. Swift or Amazon S3 using version control. Step 3 Now you can restore this images using docker load, that will load a tarred repository from a file or the standard input stream. It restores both images and tags. docker load < myfirstapp.tar 23b9c7b43573: Loading layer [==================================================>] 4.23MB/4.23MB b3b5c1214f71: Loading layer [==================================================>] 52.87MB/52.87MB f877d8dd64d3: Loading layer [==================================================>] 8.636MB/8.636MB bba871f91589: Loading layer [==================================================>] 3.584kB/3.584kB 1c131e92eb5f: Loading layer [==================================================>] 5.053MB/5.053MB 3f6463bcb64c: Loading layer [==================================================>] 5.12kB/5.12kB 47c61110467a: Loading layer [==================================================>] 4.096kB/4.096kB Loaded image: archyufa/myfirstapp:latest docker images 1.2.2 Publish Docker Image to Docker Hub \u00b6 However the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry docker tag 5b45ce063cea <user-name>/myfirstapp:v1 docker push <user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <user-name>/myfirstapp:latest docker pull <user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <user-name>/myfirstapp:v1 1.2.3 Automated Builds with Docker Cloud \u00b6 Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud and notifications to slac Automated Tests with PR 1.2.4 Push Docker Images to quay.io \u00b6 Prerequisite: Register to quay.io with your Github user Step 1 Login to quay.io from CLI docker login quay.io Step 2 Build image with quay prefix docker build -t quay.io/archyufa/myfirstapp . Step 3 Push image to quay registry docker push quay.io/archyufa/myfirstapp 1.2.5 Demo quay.io \u00b6 Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud / Quai.io","title":"backup ycit019 Lab 4 Docker Images Docker Hub Quya io"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#1-building-docker-images","text":"In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a registry of Docker images. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: git clone https://github.com/archyufa/k8scanada","title":"1 Building Docker Images"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#11-create-dockerfile","text":"Step 1 Clone git repo on you laptop: git clone https://github.com/archyufa/k8scanada cd k8scanada/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26388-1381844103-11.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr01/15/9/anigif_enhanced-buzz-31540-1381844535-8.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26390-1381844163-18.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-1376-1381846217-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3391-1381844336-26.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-29111-1381845968-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3409-1381844582-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr02/15/9/anigif_enhanced-buzz-19667-1381844937-10.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26358-1381845043-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-18774-1381844645-6.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-25158-1381844793-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/10/anigif_enhanced-buzz-11980-1381846269-1.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==0.10.1 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # our base image FROM alpine:3.5 # Install python and pip RUN apk add --update py2-pip # upgrade pip RUN pip install --upgrade pip # install Python modules needed by the Python app COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt # copy files required for the app to run COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ # tell the port number the container should expose EXPOSE 5000 # run the application CMD [ \"python\" , \"/usr/src/app/app.py\" ]","title":"1.1 Create DOCKERFILE"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#12-build-a-docker-image","text":"Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step. docker build -t <user-name>/myfirstapp . Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 80:5000 --name myfirstapp <user-name>/myfirstapp Step 4 Use your browser to open the address http:// and check that the application works. Step 5 Stop the container and remove it: docker stop myfirstapp docker rm myfirstapp myfirstapp","title":"1.2 Build a Docker image"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#121-share-docker-images-with-tar-files","text":"Now ideally you want to share you freshly build docker image with someone or run it in different environment which for some reason don\u2019t have internet access so images can not be pulled from Online Docker registries. In that case Docker images can be shared as we share traditionally regular files by creating tarballs using docker save command as following Step 1 Create a tar file using docker save command: docker save <user-name>/myfirstapp > myfirstapp.tar Or docker save --output myfirstapp1.tar archyufa/myfirstapp ls -trh | grep tar Step 2 Transfer images to another environment using scp command. Hint You can also store this images in Object storages, e.g. Swift or Amazon S3 using version control. Step 3 Now you can restore this images using docker load, that will load a tarred repository from a file or the standard input stream. It restores both images and tags. docker load < myfirstapp.tar 23b9c7b43573: Loading layer [==================================================>] 4.23MB/4.23MB b3b5c1214f71: Loading layer [==================================================>] 52.87MB/52.87MB f877d8dd64d3: Loading layer [==================================================>] 8.636MB/8.636MB bba871f91589: Loading layer [==================================================>] 3.584kB/3.584kB 1c131e92eb5f: Loading layer [==================================================>] 5.053MB/5.053MB 3f6463bcb64c: Loading layer [==================================================>] 5.12kB/5.12kB 47c61110467a: Loading layer [==================================================>] 4.096kB/4.096kB Loaded image: archyufa/myfirstapp:latest docker images","title":"1.2.1 Share docker images with tar files"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#122-publish-docker-image-to-docker-hub","text":"However the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry docker tag 5b45ce063cea <user-name>/myfirstapp:v1 docker push <user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <user-name>/myfirstapp:latest docker pull <user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <user-name>/myfirstapp:v1","title":"1.2.2 Publish Docker Image to Docker Hub"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#123-automated-builds-with-docker-cloud","text":"Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud and notifications to slac Automated Tests with PR","title":"1.2.3 Automated Builds with Docker Cloud"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#124-push-docker-images-to-quayio","text":"Prerequisite: Register to quay.io with your Github user Step 1 Login to quay.io from CLI docker login quay.io Step 2 Build image with quay prefix docker build -t quay.io/archyufa/myfirstapp . Step 3 Push image to quay registry docker push quay.io/archyufa/myfirstapp","title":"1.2.4 Push Docker Images to quay.io"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#125-demo-quayio","text":"Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud / Quai.io","title":"1.2.5 Demo quay.io"},{"location":"ycit019_Lab_2_Docker_basics/","text":"Lab 2 Docker basics Objective: Practice to run Docker containers Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1 Docker basics \u00b6 1.1 Show running containers \u00b6 Step 1 Run docker ps to show running containers: docker ps Step 2 The output shows that there are no running containers at the moment. Use the command docker ps -a to list all containers including the ones has been stopped: docker ps -a Output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e6db2a24a8e hello-world \"/hello\" 15 minutes ago Exited (0) 15 min dreamy_nobel Review the collumns CONTAINER ID , STATUS , COMMAND , PORTS , NAMES . In the previous section we started one container and the command docker ps -a shows it as Exited . Note You can name your own containers with --name when you use docker run. If you do not provide a name, Docker will generate a random one like the one you have. Question Why Docker names are random? How docker containers named? Step 3 Let\u2019s run the command docker images to show all the images on your local system: docker images As you see, there is only one image that was downloaded from the Docker Hub. 1.2 Specify a container main process \u00b6 Step 1 Let\u2019s run our own \"hello world\" container. For that we will use the official Ubuntu image : docker run ubuntu /bin/echo 'Hello world' Output: Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:latest Hello world As you see, Docker downloaded the image ubuntu because it was not on the local machine. Step 2 Let\u2019s run the command docker images again: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Step 3 If you run the same \"hello world\" container again, Docker will use a local copy of the image: docker run ubuntu /bin/echo 'Hello world' Output: Hello world Question Compare Ubuntu Docker image with ISO image or with Cloud VM image. Why the size is so different ? Summary Pulling docker images from Docker Hub takes sometime. This time depends on: How large is the image? How fast is the network to Internet ? However, it is still much faster than booting traditional OS with Ubuntu on VM. If image already pulled on local host it takes fraction of a second to start a container. Running application in docker containers considered as a best practice for running CI/CD pipelines as it considerably faster than using VMs and reduce time for deploying a test environments. 1.3 Specify an image version \u00b6 Step 1 As you see, Docker has downloaded the ubuntu:latest image. You can see Ubuntu version by running the following command: docker run ubuntu /bin/cat /etc/issue.net Output: Ubuntu 16.04 LTS Let\u2019s say you need a previous Ubuntu LTS release. In this case, you can specify the version you need: docker run ubuntu:14.04 /bin/cat /etc/issue.net Output: Unable to find image 'ubuntu:14.04' locally 14.04: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:14.04 Ubuntu 14.04.4 LTS Step 2 The docker images command should show that we have 3 Ubuntu images downloaded locally: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB ubuntu 14.04 0ccb13bf1954 11 days ago 188 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Tip Running CI/CD pipeline with Docker using latest tag considered as a Bad Practice. Instead consider using: Versioning SHA tagging. 1.4 Run an interactive container \u00b6 Step 1 Let\u2019s use the ubuntu image to run an interactive bash session and inspect what is running inside our docker image. To achive that we going to use -i and -t flags. The -i is shorthand for --interactive , which instructs Docker to keep stdin open so that we can send commands to the sprocess. The -t flag is short for --tty and allocates a pseudo-TTY or terminal inside of the session. docker run -it ubuntu /bin/bash root@17d8bdeda98e:/# Result We get a bash shell prompt inside of the container. Note Bash prompt is not availabe for all docker images. Step 2 Let's print the system information of the latest Ubuntu image: root@17d8bdeda98e:/# uname -a Linux 17d8bdeda98e 3.19.0-31-generic ... Step 3 Let's verify what Ubuntu version is run by latest image of ubuntu: root@17d8bdeda98e:/# lsb_release -a bash: lsb_release: command not found Failure Why the standard Ubuntu command that checks version of OS is not working as expeced ? Step 4 Let's verify Ubuntu version using alternative way by checking /etc/lsb-release file. root@8cbcbd0fe8d2:/# cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.3 LTS\" Step 5 Let's compare the number of executable binaries availabe inside of the docker image versus Cloud VM that we running our class environment. First, run ls command on /bin and /usr/bin directories inside of the running ubuntu container as well as dpkg --list command that shows total number of installed packages: root@8cbcbd0fe8d2:/# ls /bin | wc -l 86 root@8cbcbd0fe8d2:/# ls /usr/bin | wc -l 233 root@eb11cd0b4106:/# dpkg --list | wc -l 101 Step 6 Use the exit command or press Ctrl-D to exit the interactive bash session back to Cloud VM. root@eb11cd0b4106:/# exit Step 7 Now run ls command on /bin and /usr/bin directories on Cloud VM that we using as our class environment: cca-user@userx-docker-vm:~$ ls /bin | wc -l 171 cca-user@userx-docker-vm:~$ ls /usr/bin | wc -l 660 cca-user@userx-docker-vm:~$ dpkg --list | wc -l 463 Result Official Docker container has much less binaries and packages installed vs Ubuntu Cloud Image. Summary Some of the use cases running docker containers in interactive mode are: Troubleshooting containerized applications Deploying and running containerized application on the existing production systems without affecting it. We've also learned that an official Docker \"minimal\" ubuntu image, does not include lsb_release command, as well as many other commands and packages that can be found in Official Ubuntu ISO image . The docker images are ment to contain only required core system commands and functions to make Images as light as possible. That say you can still install required packages using apt-get install , however this may increase size of docker image considerably. Hint While Docker Ubuntu image we used so far or Docker Centos image are very familiar to users and can be good starting point for learning docker containers. Using them in production or development considered as a Bad Practice. This is due those images still considered as heavy and potentially contain a lot more valnurabilities compare to specialized images. To reduce image pull time from docker hub and follow the best secuirity practices consider using specialized images that works well with you underlining code (Node image for NodeJS applications and etc.). Examples of specialized images are: Alpine Linux Node Atomic In fact, not so long ago all the official Docker Images in Docker-Hub has been moved to use Alpine Image . Step 8 Finally let\u2019s check that when the shell process has finished, the container stops: docker ps 1.5 Run a container in a background \u00b6 Now we know how to connect to running container and execute commands in it. However in most cases you just want run a container in a background so it can do a specific action. Step 1 Run a container in a background using the -d command line argument: docker run -d ubuntu /bin/sh -c \"while true; do date; echo hello world; sleep 1; done\" Result Command should return the container ID. Step 2 Let\u2019s use the docker ps command to see running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac231579e57f ubuntu \"/bin/sh -c 'while tr\" 1 minute ago Up 11 minute evil_golick Note Container id is going to be different in your case Hint Instead of using full container-id when building commands, it is possible simply type first few characters of container-id, to make things nice and easy. Step 3 Let\u2019s use container-id to show the container standard output: docker logs <container-id> Thu Jan 26 00:23:45 UTC 2017 hello world Thu Jan 26 00:23:46 UTC 2017 hello world Thu Jan 26 00:23:47 UTC 2017 hello world ... As you can see, in the docker ps command output, the auto generated container name is evil_golick (your container can have a different name). Step 4 Now, instead of using docker contaier-id use container name to show the container standard output: docker logs <name> Thu Jan 26 00:23:51 UTC 2017 hello world Thu Jan 26 00:23:52 UTC 2017 hello world Thu Jan 26 00:23:53 UTC 2017 hello world ... Step 5 Finally, let\u2019s stop our container: docker stop <name> Step 6 Check, that there are no running containers: docker ps Summary docker logs is a very usefull command to troubleshoot containers, and going to be used very often both for Docker and Kubernertes troubleshooting. 1.6 Accessing Containers from the Internet \u00b6 Step 1 Let\u2019s run a simple web application. We will use the existing image training/webapp, which contains a Python Flask application: docker run -d -P training/webapp python app.py ... Status: Downloaded newer image for training/webapp:latest 6e88f42d3d853762edcbfe1fe73fdc5c48865275bc6df759b83b0939d5bd2456 In the command above we specified the main process (python app.py), the -d command line argument, which tells Docker to run the container in the background. The -P command line argument tells Docker to map any required network ports inside our container to our host. This allows us to access the web application in the container. Step 2 Use the docker ps command to list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e88f42d3d85 training/webapp \"python app.py\" 3 minutes ago Up 3 minutes 0.0.0.0:32768->5000/tcp determined_torvalds The PORTS column contains the mapped ports. In our case, Docker has exposed port 5000 (the default Python Flask port) on port 32768 (can be different in your case). Step 3 The docker port command shows the exposed port. We will use the container name (determined_torvalds in the example above, it can be different in your case): docker port <name> 5000 0.0.0.0:32768 Step 4 Let\u2019s check that we can access the web application exposed port: curl http://localhost:<port>/ Result Hello world! Step 5 Let\u2019s stop our web application for now: docker stop <name> Step 6 We want to manually specify the local port to expose (-p argument). Let\u2019s use the standard HTTP port 80. We also want to specify the container name (--name argument): docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 7 Let\u2019s check that the port 80 is exposed: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp curl http://localhost/ Result `Hello world!`` Step 8 You can also observe Hello world! webapp from you laptop, for that you need to use you public VM IP that can be gather from VMs list: Your_VM_Public_IP Than paste VM Public IP address in you browser. Result Our web-app can be accessed from Internet! 1.7 Restart a container \u00b6 Step 1 Let\u2019s stop the container with web application: docker stop webapp The main process inside of the container will receive SIGTERM, and after a grace period, SIGKILL. Step 2 You can start the container later using the docker start command: docker start webapp Step 3 Check that the web application works: curl http://localhost/ Hello world! Step 4 You also can restart the running container using the docker restart command. docker restart webapp Step 4 Run docker ps command and check STATUS field: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS 6e400179070f training/webapp \"python app.py\" 25 minutes ago Up 3 seconds 1.8 Ensuring Container Uptime \u00b6 Docker considers any containers to exit with a non-zero exit code to have crashed. By default a crashed container will remain stopped. Step 1 Start the container that outputs a message and then exits with code 1 to simulate a crash. docker run -d --name restart-default scrapbook/docker-restart-example docker ps -a | grep restart-default CONTAINER ID IMAGE CREATED STATUS NAMES c854289d2f39 scrapbook/docker-restart-example 5 seconds ago Exited 3 sec ago restart-default $ docker logs restart-default Sun Sep 17 20:34:55 UTC 2017 Booting up... Result Container crushed and exited. However, there are several ways to ensure that you container up and running even if it\u2019s restarts. Step 2 The option --restart=on-failure : allows you to say how many times Docker should try again: docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example docker logs restart-3 Thu Apr 20 14:01:27 UTC 2017 Booting up... Thu Apr 20 14:01:28 UTC 2017 Booting up... Thu Apr 20 14:01:29 UTC 2017 Booting up... Thu Apr 20 14:01:31 UTC 2017 Booting up... Step 3 Finally, Docker can always restart a failed container. In this case, Docker will keep trying until the container is explicitly told to stop. docker run -d --name restart-always --restart=always scrapbook/docker-restart-example docker logs restart-always Step 4 After sometime stop running docker container, as it will be keep failing and starting again: docker stop restart-always 1.9 Inspect a container \u00b6 Step 1 You can use the docker inspect command to see the configuration and status information for the specified container: docker inspect webapp [ { \"Id\": \"249476631f7d...\", \"Created\": \"2016-08-02T23:42:56.932135327Z\", \"Path\": \"python\", \"Args\": [ \"app.py\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 16055, \"ExitCode\": 0, \"Error\": \"\", ... Step 2 You can specify a filter (-f command line argument) to show only specific elements. For example: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp 172.17.0.2 The command returns the IP address of the container. 1.10 Interacting with containers \u00b6 In some cases using docker log is not enough to undertand issues and you want to login inside of running VM. Also sometimes you package you applicaiton and in order to run it you need to login inside of container and execute and leave it running in background. Below provded few ways to interacting with containers that can help to achive descrined use cases. 1.10.1 Detach from Interactive container \u00b6 In Module, 1.4 Run an interactive container we run an Ubuntu container with -it flag and able directly login inside of the container to interact with it, however after we exited contianer using Ctrl-D or exit command container stopped. However you can exit from Interactive mode without stoping a container. Let's demonstrate how this works: Step 1 Start Ubunu container in interactive mode: docker run -it ubuntu /bin/bash Step 2 Run watch date command inside running container in order to exit date command every 2 seconds. root@1d688a9f4ed4:/# watch date Step 3 Detach from a container and leave it running using the CTRL-p CTRL-q key sequence. Step 4 Verify that Ubuntu container is still running: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS NAMES 1d688a9f4ed4 ubuntu \"/bin/bash\" 1 minutes ago Up 1 minutes admiring_lovelace Result Great you were able to detach from Docker container without stopping it, while it is executing a process in it. What about attaching back to container ? Important CTRL-p CTRL-q sequence key only works if docker contaienr started with -it command! 1.11.2 Attach to a container \u00b6 Now let's get back and attach to our running Ubuntu image. For that docker provides docker attach command. docker attach <container name> Every 2.0s: date Mon Sep 18 00:08:57 2017 Summary docker attach attaches your contairs terminal\u2019s standard input, output, and error (or any combination of the 3) to a running container. This allows you to view its ongoing output or to control it interactively, as though the commands were running directly in your terminal. 1.11.3 Execute a process in a container \u00b6 Step 1 Let verify if webapp container is still running docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp If not running start it with following command: $ docker run -d -p 80:5000 --name webapp training/webapp python app.py other wise skip to next step . Step 2 Use the docker exec command to execute a command in the running container. For example: docker exec webapp ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 26 0.0 0.0 15572 2104 ? Rs 00:12 0:00 ps aux The same command with the -it command line argument can be used to run an interactive session in the container: docker exec -it webapp bash root@249476631f7d:/opt/webapp# ps auxw ps auxw USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 32 0.0 0.0 18144 3064 ? Ss 00:14 0:00 bash root 47 0.0 0.0 15572 2076 ? R+ 00:16 0:00 ps auxw Step 2 Use the exit command or press Ctrl-D to exit the interactive bash session: root@249476631f7d:/opt/webapp# exit Summary docker exec is one of the most usefull docker commands used for troubleshooting containers. 1.12 Copy files to/from container \u00b6 The docker cp command allows you to copy files from the container to the local machine or from the local file system to the container. This command works for a running or stopped container. Step 1 Let\u2019s copy the container\u2019s app.py file to the local machine: docker cp webapp:/opt/webapp/app.py . Step 2 Edit the local app.py file. For example, change the line return 'Hello '+provider+'!' to return 'Hello '+provider+'!!!'. Copy the modified file back and restart the container: docker cp app.py webapp:/opt/webapp/ docker restart webapp Step 3 Check that the modified web application works:: curl http://localhost/ Result `Hello world!!!`` 1.12 Remove containers \u00b6 Now let's clean up the environment and at the same time learn how delete containers. Step 1 First list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 81c4c66baaf9 training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp Step 2 Than try to delete running container using docker rm <container_id> docker rm $container_id Error response from daemon: You cannot remove a running container 81c4c66baaf9. Stop the container before attempting removal or force remove. Failure Docker containers needs to be first stopped or deleted using --force flag. docker rm $container_id -f Alternatively, you can run stop and rm in sequence: docker stop 81c4c66baaf9 docker rm 81c4c66baaf9 Summary We've learned a lot of docker commands which are very handy to know both when using Docker and Kubernetes. We've also learned how to create Docker Images from DOCKERFILE.","title":"ycit019 Lab 2 Docker basics"},{"location":"ycit019_Lab_2_Docker_basics/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ycit019_Lab_2_Docker_basics/#1-docker-basics","text":"","title":"1 Docker basics"},{"location":"ycit019_Lab_2_Docker_basics/#11-show-running-containers","text":"Step 1 Run docker ps to show running containers: docker ps Step 2 The output shows that there are no running containers at the moment. Use the command docker ps -a to list all containers including the ones has been stopped: docker ps -a Output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e6db2a24a8e hello-world \"/hello\" 15 minutes ago Exited (0) 15 min dreamy_nobel Review the collumns CONTAINER ID , STATUS , COMMAND , PORTS , NAMES . In the previous section we started one container and the command docker ps -a shows it as Exited . Note You can name your own containers with --name when you use docker run. If you do not provide a name, Docker will generate a random one like the one you have. Question Why Docker names are random? How docker containers named? Step 3 Let\u2019s run the command docker images to show all the images on your local system: docker images As you see, there is only one image that was downloaded from the Docker Hub.","title":"1.1 Show running containers"},{"location":"ycit019_Lab_2_Docker_basics/#12-specify-a-container-main-process","text":"Step 1 Let\u2019s run our own \"hello world\" container. For that we will use the official Ubuntu image : docker run ubuntu /bin/echo 'Hello world' Output: Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:latest Hello world As you see, Docker downloaded the image ubuntu because it was not on the local machine. Step 2 Let\u2019s run the command docker images again: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Step 3 If you run the same \"hello world\" container again, Docker will use a local copy of the image: docker run ubuntu /bin/echo 'Hello world' Output: Hello world Question Compare Ubuntu Docker image with ISO image or with Cloud VM image. Why the size is so different ? Summary Pulling docker images from Docker Hub takes sometime. This time depends on: How large is the image? How fast is the network to Internet ? However, it is still much faster than booting traditional OS with Ubuntu on VM. If image already pulled on local host it takes fraction of a second to start a container. Running application in docker containers considered as a best practice for running CI/CD pipelines as it considerably faster than using VMs and reduce time for deploying a test environments.","title":"1.2 Specify a container main process"},{"location":"ycit019_Lab_2_Docker_basics/#13-specify-an-image-version","text":"Step 1 As you see, Docker has downloaded the ubuntu:latest image. You can see Ubuntu version by running the following command: docker run ubuntu /bin/cat /etc/issue.net Output: Ubuntu 16.04 LTS Let\u2019s say you need a previous Ubuntu LTS release. In this case, you can specify the version you need: docker run ubuntu:14.04 /bin/cat /etc/issue.net Output: Unable to find image 'ubuntu:14.04' locally 14.04: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:14.04 Ubuntu 14.04.4 LTS Step 2 The docker images command should show that we have 3 Ubuntu images downloaded locally: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB ubuntu 14.04 0ccb13bf1954 11 days ago 188 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Tip Running CI/CD pipeline with Docker using latest tag considered as a Bad Practice. Instead consider using: Versioning SHA tagging.","title":"1.3 Specify an image version"},{"location":"ycit019_Lab_2_Docker_basics/#14-run-an-interactive-container","text":"Step 1 Let\u2019s use the ubuntu image to run an interactive bash session and inspect what is running inside our docker image. To achive that we going to use -i and -t flags. The -i is shorthand for --interactive , which instructs Docker to keep stdin open so that we can send commands to the sprocess. The -t flag is short for --tty and allocates a pseudo-TTY or terminal inside of the session. docker run -it ubuntu /bin/bash root@17d8bdeda98e:/# Result We get a bash shell prompt inside of the container. Note Bash prompt is not availabe for all docker images. Step 2 Let's print the system information of the latest Ubuntu image: root@17d8bdeda98e:/# uname -a Linux 17d8bdeda98e 3.19.0-31-generic ... Step 3 Let's verify what Ubuntu version is run by latest image of ubuntu: root@17d8bdeda98e:/# lsb_release -a bash: lsb_release: command not found Failure Why the standard Ubuntu command that checks version of OS is not working as expeced ? Step 4 Let's verify Ubuntu version using alternative way by checking /etc/lsb-release file. root@8cbcbd0fe8d2:/# cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.3 LTS\" Step 5 Let's compare the number of executable binaries availabe inside of the docker image versus Cloud VM that we running our class environment. First, run ls command on /bin and /usr/bin directories inside of the running ubuntu container as well as dpkg --list command that shows total number of installed packages: root@8cbcbd0fe8d2:/# ls /bin | wc -l 86 root@8cbcbd0fe8d2:/# ls /usr/bin | wc -l 233 root@eb11cd0b4106:/# dpkg --list | wc -l 101 Step 6 Use the exit command or press Ctrl-D to exit the interactive bash session back to Cloud VM. root@eb11cd0b4106:/# exit Step 7 Now run ls command on /bin and /usr/bin directories on Cloud VM that we using as our class environment: cca-user@userx-docker-vm:~$ ls /bin | wc -l 171 cca-user@userx-docker-vm:~$ ls /usr/bin | wc -l 660 cca-user@userx-docker-vm:~$ dpkg --list | wc -l 463 Result Official Docker container has much less binaries and packages installed vs Ubuntu Cloud Image. Summary Some of the use cases running docker containers in interactive mode are: Troubleshooting containerized applications Deploying and running containerized application on the existing production systems without affecting it. We've also learned that an official Docker \"minimal\" ubuntu image, does not include lsb_release command, as well as many other commands and packages that can be found in Official Ubuntu ISO image . The docker images are ment to contain only required core system commands and functions to make Images as light as possible. That say you can still install required packages using apt-get install , however this may increase size of docker image considerably. Hint While Docker Ubuntu image we used so far or Docker Centos image are very familiar to users and can be good starting point for learning docker containers. Using them in production or development considered as a Bad Practice. This is due those images still considered as heavy and potentially contain a lot more valnurabilities compare to specialized images. To reduce image pull time from docker hub and follow the best secuirity practices consider using specialized images that works well with you underlining code (Node image for NodeJS applications and etc.). Examples of specialized images are: Alpine Linux Node Atomic In fact, not so long ago all the official Docker Images in Docker-Hub has been moved to use Alpine Image . Step 8 Finally let\u2019s check that when the shell process has finished, the container stops: docker ps","title":"1.4 Run an interactive container"},{"location":"ycit019_Lab_2_Docker_basics/#15-run-a-container-in-a-background","text":"Now we know how to connect to running container and execute commands in it. However in most cases you just want run a container in a background so it can do a specific action. Step 1 Run a container in a background using the -d command line argument: docker run -d ubuntu /bin/sh -c \"while true; do date; echo hello world; sleep 1; done\" Result Command should return the container ID. Step 2 Let\u2019s use the docker ps command to see running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac231579e57f ubuntu \"/bin/sh -c 'while tr\" 1 minute ago Up 11 minute evil_golick Note Container id is going to be different in your case Hint Instead of using full container-id when building commands, it is possible simply type first few characters of container-id, to make things nice and easy. Step 3 Let\u2019s use container-id to show the container standard output: docker logs <container-id> Thu Jan 26 00:23:45 UTC 2017 hello world Thu Jan 26 00:23:46 UTC 2017 hello world Thu Jan 26 00:23:47 UTC 2017 hello world ... As you can see, in the docker ps command output, the auto generated container name is evil_golick (your container can have a different name). Step 4 Now, instead of using docker contaier-id use container name to show the container standard output: docker logs <name> Thu Jan 26 00:23:51 UTC 2017 hello world Thu Jan 26 00:23:52 UTC 2017 hello world Thu Jan 26 00:23:53 UTC 2017 hello world ... Step 5 Finally, let\u2019s stop our container: docker stop <name> Step 6 Check, that there are no running containers: docker ps Summary docker logs is a very usefull command to troubleshoot containers, and going to be used very often both for Docker and Kubernertes troubleshooting.","title":"1.5 Run a container in a background"},{"location":"ycit019_Lab_2_Docker_basics/#16-accessing-containers-from-the-internet","text":"Step 1 Let\u2019s run a simple web application. We will use the existing image training/webapp, which contains a Python Flask application: docker run -d -P training/webapp python app.py ... Status: Downloaded newer image for training/webapp:latest 6e88f42d3d853762edcbfe1fe73fdc5c48865275bc6df759b83b0939d5bd2456 In the command above we specified the main process (python app.py), the -d command line argument, which tells Docker to run the container in the background. The -P command line argument tells Docker to map any required network ports inside our container to our host. This allows us to access the web application in the container. Step 2 Use the docker ps command to list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e88f42d3d85 training/webapp \"python app.py\" 3 minutes ago Up 3 minutes 0.0.0.0:32768->5000/tcp determined_torvalds The PORTS column contains the mapped ports. In our case, Docker has exposed port 5000 (the default Python Flask port) on port 32768 (can be different in your case). Step 3 The docker port command shows the exposed port. We will use the container name (determined_torvalds in the example above, it can be different in your case): docker port <name> 5000 0.0.0.0:32768 Step 4 Let\u2019s check that we can access the web application exposed port: curl http://localhost:<port>/ Result Hello world! Step 5 Let\u2019s stop our web application for now: docker stop <name> Step 6 We want to manually specify the local port to expose (-p argument). Let\u2019s use the standard HTTP port 80. We also want to specify the container name (--name argument): docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 7 Let\u2019s check that the port 80 is exposed: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp curl http://localhost/ Result `Hello world!`` Step 8 You can also observe Hello world! webapp from you laptop, for that you need to use you public VM IP that can be gather from VMs list: Your_VM_Public_IP Than paste VM Public IP address in you browser. Result Our web-app can be accessed from Internet!","title":"1.6 Accessing Containers from the Internet"},{"location":"ycit019_Lab_2_Docker_basics/#17-restart-a-container","text":"Step 1 Let\u2019s stop the container with web application: docker stop webapp The main process inside of the container will receive SIGTERM, and after a grace period, SIGKILL. Step 2 You can start the container later using the docker start command: docker start webapp Step 3 Check that the web application works: curl http://localhost/ Hello world! Step 4 You also can restart the running container using the docker restart command. docker restart webapp Step 4 Run docker ps command and check STATUS field: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS 6e400179070f training/webapp \"python app.py\" 25 minutes ago Up 3 seconds","title":"1.7 Restart a container"},{"location":"ycit019_Lab_2_Docker_basics/#18-ensuring-container-uptime","text":"Docker considers any containers to exit with a non-zero exit code to have crashed. By default a crashed container will remain stopped. Step 1 Start the container that outputs a message and then exits with code 1 to simulate a crash. docker run -d --name restart-default scrapbook/docker-restart-example docker ps -a | grep restart-default CONTAINER ID IMAGE CREATED STATUS NAMES c854289d2f39 scrapbook/docker-restart-example 5 seconds ago Exited 3 sec ago restart-default $ docker logs restart-default Sun Sep 17 20:34:55 UTC 2017 Booting up... Result Container crushed and exited. However, there are several ways to ensure that you container up and running even if it\u2019s restarts. Step 2 The option --restart=on-failure : allows you to say how many times Docker should try again: docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example docker logs restart-3 Thu Apr 20 14:01:27 UTC 2017 Booting up... Thu Apr 20 14:01:28 UTC 2017 Booting up... Thu Apr 20 14:01:29 UTC 2017 Booting up... Thu Apr 20 14:01:31 UTC 2017 Booting up... Step 3 Finally, Docker can always restart a failed container. In this case, Docker will keep trying until the container is explicitly told to stop. docker run -d --name restart-always --restart=always scrapbook/docker-restart-example docker logs restart-always Step 4 After sometime stop running docker container, as it will be keep failing and starting again: docker stop restart-always","title":"1.8 Ensuring Container Uptime"},{"location":"ycit019_Lab_2_Docker_basics/#19-inspect-a-container","text":"Step 1 You can use the docker inspect command to see the configuration and status information for the specified container: docker inspect webapp [ { \"Id\": \"249476631f7d...\", \"Created\": \"2016-08-02T23:42:56.932135327Z\", \"Path\": \"python\", \"Args\": [ \"app.py\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 16055, \"ExitCode\": 0, \"Error\": \"\", ... Step 2 You can specify a filter (-f command line argument) to show only specific elements. For example: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp 172.17.0.2 The command returns the IP address of the container.","title":"1.9 Inspect a container"},{"location":"ycit019_Lab_2_Docker_basics/#110-interacting-with-containers","text":"In some cases using docker log is not enough to undertand issues and you want to login inside of running VM. Also sometimes you package you applicaiton and in order to run it you need to login inside of container and execute and leave it running in background. Below provded few ways to interacting with containers that can help to achive descrined use cases.","title":"1.10 Interacting with containers"},{"location":"ycit019_Lab_2_Docker_basics/#1101-detach-from-interactive-container","text":"In Module, 1.4 Run an interactive container we run an Ubuntu container with -it flag and able directly login inside of the container to interact with it, however after we exited contianer using Ctrl-D or exit command container stopped. However you can exit from Interactive mode without stoping a container. Let's demonstrate how this works: Step 1 Start Ubunu container in interactive mode: docker run -it ubuntu /bin/bash Step 2 Run watch date command inside running container in order to exit date command every 2 seconds. root@1d688a9f4ed4:/# watch date Step 3 Detach from a container and leave it running using the CTRL-p CTRL-q key sequence. Step 4 Verify that Ubuntu container is still running: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS NAMES 1d688a9f4ed4 ubuntu \"/bin/bash\" 1 minutes ago Up 1 minutes admiring_lovelace Result Great you were able to detach from Docker container without stopping it, while it is executing a process in it. What about attaching back to container ? Important CTRL-p CTRL-q sequence key only works if docker contaienr started with -it command!","title":"1.10.1 Detach from Interactive container"},{"location":"ycit019_Lab_2_Docker_basics/#1112-attach-to-a-container","text":"Now let's get back and attach to our running Ubuntu image. For that docker provides docker attach command. docker attach <container name> Every 2.0s: date Mon Sep 18 00:08:57 2017 Summary docker attach attaches your contairs terminal\u2019s standard input, output, and error (or any combination of the 3) to a running container. This allows you to view its ongoing output or to control it interactively, as though the commands were running directly in your terminal.","title":"1.11.2 Attach to a container"},{"location":"ycit019_Lab_2_Docker_basics/#1113-execute-a-process-in-a-container","text":"Step 1 Let verify if webapp container is still running docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp If not running start it with following command: $ docker run -d -p 80:5000 --name webapp training/webapp python app.py other wise skip to next step . Step 2 Use the docker exec command to execute a command in the running container. For example: docker exec webapp ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 26 0.0 0.0 15572 2104 ? Rs 00:12 0:00 ps aux The same command with the -it command line argument can be used to run an interactive session in the container: docker exec -it webapp bash root@249476631f7d:/opt/webapp# ps auxw ps auxw USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 32 0.0 0.0 18144 3064 ? Ss 00:14 0:00 bash root 47 0.0 0.0 15572 2076 ? R+ 00:16 0:00 ps auxw Step 2 Use the exit command or press Ctrl-D to exit the interactive bash session: root@249476631f7d:/opt/webapp# exit Summary docker exec is one of the most usefull docker commands used for troubleshooting containers.","title":"1.11.3 Execute a process in a container"},{"location":"ycit019_Lab_2_Docker_basics/#112-copy-files-tofrom-container","text":"The docker cp command allows you to copy files from the container to the local machine or from the local file system to the container. This command works for a running or stopped container. Step 1 Let\u2019s copy the container\u2019s app.py file to the local machine: docker cp webapp:/opt/webapp/app.py . Step 2 Edit the local app.py file. For example, change the line return 'Hello '+provider+'!' to return 'Hello '+provider+'!!!'. Copy the modified file back and restart the container: docker cp app.py webapp:/opt/webapp/ docker restart webapp Step 3 Check that the modified web application works:: curl http://localhost/ Result `Hello world!!!``","title":"1.12 Copy files to/from container"},{"location":"ycit019_Lab_2_Docker_basics/#112-remove-containers","text":"Now let's clean up the environment and at the same time learn how delete containers. Step 1 First list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 81c4c66baaf9 training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp Step 2 Than try to delete running container using docker rm <container_id> docker rm $container_id Error response from daemon: You cannot remove a running container 81c4c66baaf9. Stop the container before attempting removal or force remove. Failure Docker containers needs to be first stopped or deleted using --force flag. docker rm $container_id -f Alternatively, you can run stop and rm in sequence: docker stop 81c4c66baaf9 docker rm 81c4c66baaf9 Summary We've learned a lot of docker commands which are very handy to know both when using Docker and Kubernetes. We've also learned how to create Docker Images from DOCKERFILE.","title":"1.12 Remove containers"},{"location":"ycit019_Lab_3_Advanced_Docker/","text":"Lab 3 Docker Networking, Persistence, Monitoring and Logging Objective: Networks Docker basics User-defined private Networks Persistence Data Volumes 1 Docker Networking \u00b6 1.1 Docker Networking Basics \u00b6 Step 1: The Docker Network Command The docker network command is the main command for configuring and managing container networks. Run the docker network command from the first terminal. docker network Usage: docker network COMMAND Manage networks Options: --help Print usage Commands: connect Connect a container to a network create Create a network disconnect Disconnect a container from a network inspect Display detailed information on one or more networks ls List networks prune Remove all unused networks rm Remove one or more networks Run 'docker network COMMAND --help' for more information on a command. The command output shows how to use the command as well as all of the docker network sub-commands. As you can see from the output, the docker network command allows you to create new networks, list existing networks, inspect networks, and remove networks. It also allows you to connect and disconnect containers from networks. Step 2 Run a docker network ls command to view existing container networks on the current Docker host. docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local The output above shows the container networks that are created as part of a standard installation of Docker. New networks that you create will also show up in the output of the docker network ls command. You can see that each network gets a unique ID and NAME . Each network is also associated with a single driver. Notice that the \"bridge\" network and the \"host\" network have the same name as their respective drivers. Step 3: The docker network inspect command is used to view network configuration details. These details include; name, ID, driver, IPAM driver, subnet info, connected containers, and more. Use docker network inspect <network> to view configuration details of the container networks on your Docker host. The command below shows the details of the network called bridge . docker network inspect bridge [ { \"Name\": \"bridge\", \"Id\": \"3430ad6f20bf1486df2e5f64ddc93cc4ff95d81f59b6baea8a510ad500df2e57\", \"Created\": \"2017-04-03T16:49:58.6536278Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Containers\": {}, \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" }, \"Labels\": {} } ] Note The syntax of the docker network inspect command is docker network inspect <network> , where <network> can be either network name or network ID. In the example above we are showing the configuration details for the network called \"bridge\". Do not confuse this with the \"bridge\" driver. Step 4 Now, list Docker supported network driver plugins. For that run docker info command, that shows a lot of interesting information about a Docker installation. Run the docker info command and locate the list of network plugins. docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ee-3 Storage Driver: aufs <Snip> Plugins: Volume: local Network: bridge host macvlan null overlay Swarm: inactive Runtimes: runc <Snip> The output above shows the bridge , host , macvlan , null , and overlay drivers. Summary We've quickly reviewed available docker networking commands as well as found what drivers current docker setup supports. 1.2 Default bridge network \u00b6 Every clean installation of Docker comes with a pre-built network called Default bridge network . Let's explore in more details how it works. Step 1 Verify this with the docker network ls . docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local Result The output above shows that the bridge network is associated with the bridge driver. It's important to note that the network and the driver are connected, but they are not the same. In this example the network and the driver have the same name - but they are not the same thing! The output above also shows that the bridge network is scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the bridge driver - the bridge driver provides single-host networking. All networks created with the bridge driver are based on a Linux bridge (a.k.a. a virtual switch). Step 5 Start webapp in Default bridge network docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 6 Check that the webapp and db containers are running: Command: docker ps 1.3 User-defined Private Networks \u00b6 So far we\u2019ve learned how Docker networking works with Docker default bridge network . With the introduction of user-defined networking in Docker 1.9, it is now possible to create multiple Docker bridges to allow network segregation within the same host or multi-host networking to allow communicate Docker containers between hosts. The commands are available through the Docker Engine CLI are: docker network create docker network connect docker network ls docker network rm docker network disconnect docker network inspect Let's demonstrate how to create a custom bridge network. Step 1 By default, Docker runs containers in the bridge network. You may want to isolate one or more containers in a separate network. Let\u2019s create a new network: docker network create my-network \\ -d bridge \\ --subnet 172.19.0.0/16 The -d bridge command line argument specifies the bridge network driver and the --subnet command line argument specifies the network segment in CIDR format. If you do not specify a subnet when creating a network, then Docker assigns a subnet automatically, so it is a good idea to specify a subnet to avoid potential conflicts with the existing networks. Below are some other options that are available with the bridge Driver: com.docker.network.bridge.enable_ip_masquerade: This instructs the Docker host to hide or masquerade all containers in this network behind the Docker host's interfaces if the container attempts to route off the local host . com.docker.network.bridge.name: This is the name you wish to give to the bridge. com.docker.network.bridge.enable_icc: This turns on or off Inter-Container Connectivity (ICC) mode for the bridge. com.docker.network.bridge.host_binding_ipv4: This defines the host interface that should be used for port binding. com.docker.network.driver.mtu: This sets MTU for containers attached to this bridge. Step 2 To check that the new network is created, execute docker network ls: docker network ls NETWORK ID NAME DRIVER SCOPE d428e49e4869 bridge bridge local 0d1f78528cc5 host host local 56ef0481820d my-network bridge local 4a07cef84617 none null local Step 3 Let\u2019s inspect the new network: docker network inspect my-network [ { \"Name\": \"my-network\", \"Id\": \"56ef0481820d...\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"172.19.0.0/16\" } ] }, \"Internal\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ] Step 4 As expected, there are no containers connected to the my-network. Let\u2019s recreate the db container in the my-network: docker rm -f db docker run -d --network=my-network --name db training/postgres Step 5 Inspect the my-network again: docker network inspect my-network Output: \"Containers\": { \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... As you see, the db container is connected to the my-network and has 172.19.0.2 address. Step 6 Let\u2019s start an interactive session in the db container and ping the IP address of the webapp again: Note Quick reminder how to locate webapp ip: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp docker exec -it db bash Once inside of container run: root@c3afff20019a:/# ping -c 1 172.17.0.3 PING 172.17.0.3 (172.17.0.3) 56(84) bytes of data. --- 172.17.0.3 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms As expected, the webapp container is no longer accessible from the db container, because they are connected to different networks. Summary Using Multi-host networking provides network isolation within a Docker host via network namepsaces. This is can be used if you want to deploy different applications on same host for isolation or resource duplicate prevention. Step 7 Let\u2019s connect the webapp container to the my-network: docker network connect my-network webapp Step 8 Check that the webapp container now is connected to the my-network: docker network inspect my-network Output: ... \"Containers\": { \"62ed4a627356...\": { \"Name\": \"webapp\", \"EndpointID\": \"ae95b0103bbc...\", \"MacAddress\": \"02:42:ac:12:00:03\", \"IPv4Address\": \"172.19.0.3/16\", \"IPv6Address\": \"\" }, \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... The output shows that two containers are connected to the my-network and the webapp container has 172.19.0.3 address in that network. Step 9 Check that the webapp container is accessible from the db container using its new IP address: docker exec -it db bash root@c3afff20019a:/# ping -c 1 172.19.0.3 PING 172.19.0.3 (172.19.0.3) 56(84) bytes of data. 64 bytes from 172.19.0.3: icmp_seq=1 ttl=64 time=0.136 ms --- 172.19.0.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms Success As expected containers can communicate with each other. Step 10 You can now remove the existing container. You should stop the container before removing it. Alternatively you can use the -f command line argument: docker rm -f webapp docker rm -f db docker network rm my-network Hint Use below command to delete running containers in bulk : docker rm -f $(docker ps -q) Summary It is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses 1.4 Access containers from outside \u00b6 External Access to the Containers can be configured via publishing mechanism. Docker provides 2 options to publish ports: -P flag publishes all exposed ports -p flag allows you to specify specific ports and interfaces to use when mapping ports. The -p flag can take several different forms with the syntax looking like this: Specify the host port and container port: \u2013p <host port>:<container port> Specify the host interface, host port, and container port: \u2013p <host IP interface>:<host port>:<container port> Specify the host interface, have Docker choose a random host port, and specify the container port: \u2013p <host IP interface>::<container port> Specify only a container port and have Docker use a random host port: \u2013p <container port> Let's test exposing containers. For that let's start a new NGINX container and map port 8080 on the Docker host to port 80 inside of the container. This means that traffic that hits the Docker host on port 8080 will be passed on to port 80 inside the container. Note If you start a new container from the official NGINX image without specifying a command to run, the container will run a basic web server on port 80. Step 1 Start a new container based off the official NGINX image by running docker run --name web1 -d -p 8080:80 nginx . docker run --name web1 -d -p 8080:80 nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 6d827a3ef358: Pull complete b556b18c7952: Pull complete 03558b976e24: Pull complete 9abee7e1ef9d: Pull complete Digest: sha256:52f84ace6ea43f2f58937e5f9fc562e99ad6876e82b99d171916c1ece587c188 Status: Downloaded newer image for nginx:latest 4e0da45b0f169f18b0e1ee9bf779500cb0f756402c0a0821d55565f162741b3e Step 2 Review the container status and port mappings by running docker ps . docker ps CONTAINER ID IMAGE COMMAND PORTS NAMES 4e0da45b0f16 nginx \"nginx -g 'daemon ...\" 443/tcp, 0.0.0.0:8080->80/tcp web1 Result The top line shows the new web1 container running NGINX. Take note of the command the container is running as well as the port mapping - 0.0.0.0:8080->80/tcp maps port 8080 on all host interfaces to port 80 inside the web1 container. This port mapping is what effectively makes the containers web service accessible from external sources (via the Docker hosts IP address on port 8080). Step 3 Test connectivity to the NGINX web server, by pasting <Public_IP:8080> of VM to the browser. Note In order to locate Public IP see the list of VMs. Alternatively from inside of VM run curl 127.0.0.1:8080 command. curl 127.0.0.1:8080 <!DOCTYPE html> <html> <Snip> <head> <title>Welcome to nginx!</title> <Snip> <p><em>Thank you for using nginx.</em></p> </body> </html> Success Both CLI and UI method works! If you try and curl the IP address on a different port number it will fail. Summary Docker provides easy way to expose containers outside of the Docker Node. This can ber used for connecting containers between each other: Between networks on the same host Between networks on different host Accessing containers from outside (e.g web site) However, port mapping is implemented via port address translation (PAT) unlike in Kubernetes which we learn soon, exposes applications via service IPs and communicates via POD IPs using (NAT) Step 4 Cleanup environment docker rm -f $(docker ps -q) 2 Persistant Volumes \u00b6 2.1 Storage driver \u00b6 We've discussed several Storage drivers (graphdrivers) during the class. Let's find out what graphdriver is running in our Lab environment. docker info | grep Storage WARNING: No swap limit support Storage Driver: aufs Result Our Classroom is running aufs storage driver. Not a suprise as we running our Lab on Ubuntu VM. Summary Systems runnng Ubuntu or Debian ,going to run aufs graphdriver by default and will most likely meet the majority of your needs. In future overlay2 may replace aufs stay tunned! 2.2 Persisting Data Using Volumes \u00b6 Docker Volumes are created and assigned when containers are started. Data Volumes allow you to map a host directory to a container for sharing data. This mapping is bi-directional. It allows data stored on the host to be accessed from within the container. It also means data saved by the process inside the container is persisted on the host. 2.2.1 Create and manage volumes \u00b6 Step 1 Create a volume: docker volume create --name my-vol Step 2 List volumes: docker volume ls Output: local my-vol Step 3 Inspect a volume: docker volume inspect my-vol [ { \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\", \"Name\": \"my-vol\", \"Options\": {}, \"Scope\": \"local\" } ] Step 3 Add some data to the Mountpoint of the volume: sudo touch /var/lib/docker/volumes/my-vol/_data/test_vol sudo ls /var/lib/docker/volumes/my-vol/_data/ Step 4 Create a container busybox alpine image and attach created my-vol volume in to it: docker run -it -v my-vol:/world busybox / # ls /world test_vol / # Result Volume is mounted and test_vol file is under /world folder as expected Step 5 Try to delete the volume: docker volume rm my-vol Error response from daemon: unable to remove volume: remove my-vol: volume is in use - [6ef3055b516b306847150af8fcea796c02cd90578967802ac29c39d3a2c90102] Failure Deleting container that is attached is not permited. However you can delete with -f option Step 5 Busybox container stopped, howerver it is not deleted. Let's locate stopped busybox container and delete it: docker ps -a | grep busybox docker rm $docker_id Step 6 You can now delete my-vol Note Volume is still avaiable if needed to be reattached any time docker volume ls docker volume rm my-vol docker volume ls Summary Volumes can be craeted and managed separately from containers. 2.2.2 Start a container with a volume \u00b6 If you start a container with a volume that does not yet exist, Docker creates the volume for you. Step 1 Add a data volume to a container: docker run -d -P --name webapp -v /webapp training/webapp python app.py Result Command started a new container and created a new volume inside the container at /webapp. Step 2 Locate the volume on the host using the docker inspect command: docker inspect webapp | grep -A9 Mounts ``` **Output:** ``` \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\", \"Source\": \"/var/lib/docker/volumes/39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d/_data\", \"Destination\": \"/webapp\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" Step 3 List container docker volume ls Output: DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Step 5 Alternatively, you can specify a host directory you want to use as a data volume: mkdir db docker run -d --name db -v ~/db:/db training/postgres Step 2 Start an interactive session in the db container and create a new file in the /db directory: docker exec -it db bash Type inside docker containers console: root@9a7a4fbcc929:/# cd /db root@9a7a4fbcc929:/db# touch hello_from_db_container root@9a7a4fbcc929:/db# exit Step 4 Check that the local db directory contains the new file: ls db hello_from_db_container Step 5 Check that the data volume is persistent. Remove the db container: docker rm -f db Step 6 Create the db container again: docker run -d --name db -v ~/db:/db training/postgres Step 7 Check that its /db directory contains the hello_from_db_container file: docker exec -it db bash Run commands inside container: root@47a60c01590e:/# ls /db hello_from_db_container root@47a60c01590e:/# exit 2.2.3 Use a read-only volume \u00b6 Step 1 Mounting Volumes gives the container full read and write access to the directory. You can specify read-only permissions on the directory by adding the permissions :ro to the mount. If the container attempts to modify data within the directory it will error. docker run -d --name db1 -v ~/db:/db:ro training/postgres docker exec -it db1 bash cd db touch test Result touch: cannot touch 'test': Read-only file system $ exit Step 2 Clean up containers and volumes: docker rm -f $(docker ps -q) docker volume ls Output DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d docker volume rm 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Summary We've learned how to manage volumes with containers Hint If you Docker host has several Storage plugins configured (e.g. ceph, gluster) you can specify via --opt type=btrfs, nfs or --driver=glusterfs during docker volume creation.","title":"ycit019 Lab 3 Advanced Docker"},{"location":"ycit019_Lab_3_Advanced_Docker/#1-docker-networking","text":"","title":"1 Docker Networking"},{"location":"ycit019_Lab_3_Advanced_Docker/#11-docker-networking-basics","text":"Step 1: The Docker Network Command The docker network command is the main command for configuring and managing container networks. Run the docker network command from the first terminal. docker network Usage: docker network COMMAND Manage networks Options: --help Print usage Commands: connect Connect a container to a network create Create a network disconnect Disconnect a container from a network inspect Display detailed information on one or more networks ls List networks prune Remove all unused networks rm Remove one or more networks Run 'docker network COMMAND --help' for more information on a command. The command output shows how to use the command as well as all of the docker network sub-commands. As you can see from the output, the docker network command allows you to create new networks, list existing networks, inspect networks, and remove networks. It also allows you to connect and disconnect containers from networks. Step 2 Run a docker network ls command to view existing container networks on the current Docker host. docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local The output above shows the container networks that are created as part of a standard installation of Docker. New networks that you create will also show up in the output of the docker network ls command. You can see that each network gets a unique ID and NAME . Each network is also associated with a single driver. Notice that the \"bridge\" network and the \"host\" network have the same name as their respective drivers. Step 3: The docker network inspect command is used to view network configuration details. These details include; name, ID, driver, IPAM driver, subnet info, connected containers, and more. Use docker network inspect <network> to view configuration details of the container networks on your Docker host. The command below shows the details of the network called bridge . docker network inspect bridge [ { \"Name\": \"bridge\", \"Id\": \"3430ad6f20bf1486df2e5f64ddc93cc4ff95d81f59b6baea8a510ad500df2e57\", \"Created\": \"2017-04-03T16:49:58.6536278Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Containers\": {}, \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" }, \"Labels\": {} } ] Note The syntax of the docker network inspect command is docker network inspect <network> , where <network> can be either network name or network ID. In the example above we are showing the configuration details for the network called \"bridge\". Do not confuse this with the \"bridge\" driver. Step 4 Now, list Docker supported network driver plugins. For that run docker info command, that shows a lot of interesting information about a Docker installation. Run the docker info command and locate the list of network plugins. docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ee-3 Storage Driver: aufs <Snip> Plugins: Volume: local Network: bridge host macvlan null overlay Swarm: inactive Runtimes: runc <Snip> The output above shows the bridge , host , macvlan , null , and overlay drivers. Summary We've quickly reviewed available docker networking commands as well as found what drivers current docker setup supports.","title":"1.1 Docker Networking Basics"},{"location":"ycit019_Lab_3_Advanced_Docker/#12-default-bridge-network","text":"Every clean installation of Docker comes with a pre-built network called Default bridge network . Let's explore in more details how it works. Step 1 Verify this with the docker network ls . docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local Result The output above shows that the bridge network is associated with the bridge driver. It's important to note that the network and the driver are connected, but they are not the same. In this example the network and the driver have the same name - but they are not the same thing! The output above also shows that the bridge network is scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the bridge driver - the bridge driver provides single-host networking. All networks created with the bridge driver are based on a Linux bridge (a.k.a. a virtual switch). Step 5 Start webapp in Default bridge network docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 6 Check that the webapp and db containers are running: Command: docker ps","title":"1.2 Default bridge network"},{"location":"ycit019_Lab_3_Advanced_Docker/#13-user-defined-private-networks","text":"So far we\u2019ve learned how Docker networking works with Docker default bridge network . With the introduction of user-defined networking in Docker 1.9, it is now possible to create multiple Docker bridges to allow network segregation within the same host or multi-host networking to allow communicate Docker containers between hosts. The commands are available through the Docker Engine CLI are: docker network create docker network connect docker network ls docker network rm docker network disconnect docker network inspect Let's demonstrate how to create a custom bridge network. Step 1 By default, Docker runs containers in the bridge network. You may want to isolate one or more containers in a separate network. Let\u2019s create a new network: docker network create my-network \\ -d bridge \\ --subnet 172.19.0.0/16 The -d bridge command line argument specifies the bridge network driver and the --subnet command line argument specifies the network segment in CIDR format. If you do not specify a subnet when creating a network, then Docker assigns a subnet automatically, so it is a good idea to specify a subnet to avoid potential conflicts with the existing networks. Below are some other options that are available with the bridge Driver: com.docker.network.bridge.enable_ip_masquerade: This instructs the Docker host to hide or masquerade all containers in this network behind the Docker host's interfaces if the container attempts to route off the local host . com.docker.network.bridge.name: This is the name you wish to give to the bridge. com.docker.network.bridge.enable_icc: This turns on or off Inter-Container Connectivity (ICC) mode for the bridge. com.docker.network.bridge.host_binding_ipv4: This defines the host interface that should be used for port binding. com.docker.network.driver.mtu: This sets MTU for containers attached to this bridge. Step 2 To check that the new network is created, execute docker network ls: docker network ls NETWORK ID NAME DRIVER SCOPE d428e49e4869 bridge bridge local 0d1f78528cc5 host host local 56ef0481820d my-network bridge local 4a07cef84617 none null local Step 3 Let\u2019s inspect the new network: docker network inspect my-network [ { \"Name\": \"my-network\", \"Id\": \"56ef0481820d...\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"172.19.0.0/16\" } ] }, \"Internal\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ] Step 4 As expected, there are no containers connected to the my-network. Let\u2019s recreate the db container in the my-network: docker rm -f db docker run -d --network=my-network --name db training/postgres Step 5 Inspect the my-network again: docker network inspect my-network Output: \"Containers\": { \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... As you see, the db container is connected to the my-network and has 172.19.0.2 address. Step 6 Let\u2019s start an interactive session in the db container and ping the IP address of the webapp again: Note Quick reminder how to locate webapp ip: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp docker exec -it db bash Once inside of container run: root@c3afff20019a:/# ping -c 1 172.17.0.3 PING 172.17.0.3 (172.17.0.3) 56(84) bytes of data. --- 172.17.0.3 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms As expected, the webapp container is no longer accessible from the db container, because they are connected to different networks. Summary Using Multi-host networking provides network isolation within a Docker host via network namepsaces. This is can be used if you want to deploy different applications on same host for isolation or resource duplicate prevention. Step 7 Let\u2019s connect the webapp container to the my-network: docker network connect my-network webapp Step 8 Check that the webapp container now is connected to the my-network: docker network inspect my-network Output: ... \"Containers\": { \"62ed4a627356...\": { \"Name\": \"webapp\", \"EndpointID\": \"ae95b0103bbc...\", \"MacAddress\": \"02:42:ac:12:00:03\", \"IPv4Address\": \"172.19.0.3/16\", \"IPv6Address\": \"\" }, \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... The output shows that two containers are connected to the my-network and the webapp container has 172.19.0.3 address in that network. Step 9 Check that the webapp container is accessible from the db container using its new IP address: docker exec -it db bash root@c3afff20019a:/# ping -c 1 172.19.0.3 PING 172.19.0.3 (172.19.0.3) 56(84) bytes of data. 64 bytes from 172.19.0.3: icmp_seq=1 ttl=64 time=0.136 ms --- 172.19.0.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms Success As expected containers can communicate with each other. Step 10 You can now remove the existing container. You should stop the container before removing it. Alternatively you can use the -f command line argument: docker rm -f webapp docker rm -f db docker network rm my-network Hint Use below command to delete running containers in bulk : docker rm -f $(docker ps -q) Summary It is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses","title":"1.3 User-defined Private Networks"},{"location":"ycit019_Lab_3_Advanced_Docker/#14-access-containers-from-outside","text":"External Access to the Containers can be configured via publishing mechanism. Docker provides 2 options to publish ports: -P flag publishes all exposed ports -p flag allows you to specify specific ports and interfaces to use when mapping ports. The -p flag can take several different forms with the syntax looking like this: Specify the host port and container port: \u2013p <host port>:<container port> Specify the host interface, host port, and container port: \u2013p <host IP interface>:<host port>:<container port> Specify the host interface, have Docker choose a random host port, and specify the container port: \u2013p <host IP interface>::<container port> Specify only a container port and have Docker use a random host port: \u2013p <container port> Let's test exposing containers. For that let's start a new NGINX container and map port 8080 on the Docker host to port 80 inside of the container. This means that traffic that hits the Docker host on port 8080 will be passed on to port 80 inside the container. Note If you start a new container from the official NGINX image without specifying a command to run, the container will run a basic web server on port 80. Step 1 Start a new container based off the official NGINX image by running docker run --name web1 -d -p 8080:80 nginx . docker run --name web1 -d -p 8080:80 nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 6d827a3ef358: Pull complete b556b18c7952: Pull complete 03558b976e24: Pull complete 9abee7e1ef9d: Pull complete Digest: sha256:52f84ace6ea43f2f58937e5f9fc562e99ad6876e82b99d171916c1ece587c188 Status: Downloaded newer image for nginx:latest 4e0da45b0f169f18b0e1ee9bf779500cb0f756402c0a0821d55565f162741b3e Step 2 Review the container status and port mappings by running docker ps . docker ps CONTAINER ID IMAGE COMMAND PORTS NAMES 4e0da45b0f16 nginx \"nginx -g 'daemon ...\" 443/tcp, 0.0.0.0:8080->80/tcp web1 Result The top line shows the new web1 container running NGINX. Take note of the command the container is running as well as the port mapping - 0.0.0.0:8080->80/tcp maps port 8080 on all host interfaces to port 80 inside the web1 container. This port mapping is what effectively makes the containers web service accessible from external sources (via the Docker hosts IP address on port 8080). Step 3 Test connectivity to the NGINX web server, by pasting <Public_IP:8080> of VM to the browser. Note In order to locate Public IP see the list of VMs. Alternatively from inside of VM run curl 127.0.0.1:8080 command. curl 127.0.0.1:8080 <!DOCTYPE html> <html> <Snip> <head> <title>Welcome to nginx!</title> <Snip> <p><em>Thank you for using nginx.</em></p> </body> </html> Success Both CLI and UI method works! If you try and curl the IP address on a different port number it will fail. Summary Docker provides easy way to expose containers outside of the Docker Node. This can ber used for connecting containers between each other: Between networks on the same host Between networks on different host Accessing containers from outside (e.g web site) However, port mapping is implemented via port address translation (PAT) unlike in Kubernetes which we learn soon, exposes applications via service IPs and communicates via POD IPs using (NAT) Step 4 Cleanup environment docker rm -f $(docker ps -q)","title":"1.4 Access containers from outside"},{"location":"ycit019_Lab_3_Advanced_Docker/#2-persistant-volumes","text":"","title":"2 Persistant Volumes"},{"location":"ycit019_Lab_3_Advanced_Docker/#21-storage-driver","text":"We've discussed several Storage drivers (graphdrivers) during the class. Let's find out what graphdriver is running in our Lab environment. docker info | grep Storage WARNING: No swap limit support Storage Driver: aufs Result Our Classroom is running aufs storage driver. Not a suprise as we running our Lab on Ubuntu VM. Summary Systems runnng Ubuntu or Debian ,going to run aufs graphdriver by default and will most likely meet the majority of your needs. In future overlay2 may replace aufs stay tunned!","title":"2.1 Storage driver"},{"location":"ycit019_Lab_3_Advanced_Docker/#22-persisting-data-using-volumes","text":"Docker Volumes are created and assigned when containers are started. Data Volumes allow you to map a host directory to a container for sharing data. This mapping is bi-directional. It allows data stored on the host to be accessed from within the container. It also means data saved by the process inside the container is persisted on the host.","title":"2.2 Persisting Data Using Volumes"},{"location":"ycit019_Lab_3_Advanced_Docker/#221-create-and-manage-volumes","text":"Step 1 Create a volume: docker volume create --name my-vol Step 2 List volumes: docker volume ls Output: local my-vol Step 3 Inspect a volume: docker volume inspect my-vol [ { \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\", \"Name\": \"my-vol\", \"Options\": {}, \"Scope\": \"local\" } ] Step 3 Add some data to the Mountpoint of the volume: sudo touch /var/lib/docker/volumes/my-vol/_data/test_vol sudo ls /var/lib/docker/volumes/my-vol/_data/ Step 4 Create a container busybox alpine image and attach created my-vol volume in to it: docker run -it -v my-vol:/world busybox / # ls /world test_vol / # Result Volume is mounted and test_vol file is under /world folder as expected Step 5 Try to delete the volume: docker volume rm my-vol Error response from daemon: unable to remove volume: remove my-vol: volume is in use - [6ef3055b516b306847150af8fcea796c02cd90578967802ac29c39d3a2c90102] Failure Deleting container that is attached is not permited. However you can delete with -f option Step 5 Busybox container stopped, howerver it is not deleted. Let's locate stopped busybox container and delete it: docker ps -a | grep busybox docker rm $docker_id Step 6 You can now delete my-vol Note Volume is still avaiable if needed to be reattached any time docker volume ls docker volume rm my-vol docker volume ls Summary Volumes can be craeted and managed separately from containers.","title":"2.2.1  Create and manage volumes"},{"location":"ycit019_Lab_3_Advanced_Docker/#222-start-a-container-with-a-volume","text":"If you start a container with a volume that does not yet exist, Docker creates the volume for you. Step 1 Add a data volume to a container: docker run -d -P --name webapp -v /webapp training/webapp python app.py Result Command started a new container and created a new volume inside the container at /webapp. Step 2 Locate the volume on the host using the docker inspect command: docker inspect webapp | grep -A9 Mounts ``` **Output:** ``` \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\", \"Source\": \"/var/lib/docker/volumes/39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d/_data\", \"Destination\": \"/webapp\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" Step 3 List container docker volume ls Output: DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Step 5 Alternatively, you can specify a host directory you want to use as a data volume: mkdir db docker run -d --name db -v ~/db:/db training/postgres Step 2 Start an interactive session in the db container and create a new file in the /db directory: docker exec -it db bash Type inside docker containers console: root@9a7a4fbcc929:/# cd /db root@9a7a4fbcc929:/db# touch hello_from_db_container root@9a7a4fbcc929:/db# exit Step 4 Check that the local db directory contains the new file: ls db hello_from_db_container Step 5 Check that the data volume is persistent. Remove the db container: docker rm -f db Step 6 Create the db container again: docker run -d --name db -v ~/db:/db training/postgres Step 7 Check that its /db directory contains the hello_from_db_container file: docker exec -it db bash Run commands inside container: root@47a60c01590e:/# ls /db hello_from_db_container root@47a60c01590e:/# exit","title":"2.2.2 Start a container with a volume"},{"location":"ycit019_Lab_3_Advanced_Docker/#223-use-a-read-only-volume","text":"Step 1 Mounting Volumes gives the container full read and write access to the directory. You can specify read-only permissions on the directory by adding the permissions :ro to the mount. If the container attempts to modify data within the directory it will error. docker run -d --name db1 -v ~/db:/db:ro training/postgres docker exec -it db1 bash cd db touch test Result touch: cannot touch 'test': Read-only file system $ exit Step 2 Clean up containers and volumes: docker rm -f $(docker ps -q) docker volume ls Output DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d docker volume rm 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Summary We've learned how to manage volumes with containers Hint If you Docker host has several Storage plugins configured (e.g. ceph, gluster) you can specify via --opt type=btrfs, nfs or --driver=glusterfs during docker volume creation.","title":"2.2.3 Use a read-only volume"},{"location":"ycit019_Lab_4_Docker_Images%20copy/","text":"Lab 4 Managing Docker Images Objective: Learn to build docker images using Dockerfiles. Store images in Docker Hub Learn alternative registry solutions (GCR) Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1 Distributing Docker images with Container Registry \u00b6 In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: https://github.com/Cloud-Architects-Program/ycit019/tree/main/Module5/flask-app 1.1 Create DOCKERFILE \u00b6 Step 1 Clone git repo on you laptop: git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Module5/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\" , \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\" , \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\" , \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\" , \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\" , \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\" , \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\" , \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\" , \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\" , \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==2.0.0 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # Official Python Alpine Base image using Simple Tags # Image contains Python 3 and pip pre-installed, so no need to install them FROM python:3.9.5-alpine3.12 # Specify Working directory WORKDIR /usr/src/app # COPY requirements.txt /usr/src/app/ COPY requirements.txt ./ # Install Python Flask used by the Python app RUN pip install --no-cache-dir -r requirements.txt # Copy files required for the app to run COPY app.py ./ COPY templates/index.html ./templates/ # Make a record that the port number the container should be expose is: EXPOSE 5000 # run the application CMD [ \"python\" , \"./app.py\" ] 1.2 Build a Docker image \u00b6 Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step to your own Docker Hub. docker build -t <Docker-hub-user-name>/myfirstapp . Result Image has been buit Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 8080:5000 --name myfirstapp <Docker-hub-user-name>/myfirstapp Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Stop the container and remove it: docker rm -f myfirstapp 1.2.2 Publish Docker Image to Docker Hub \u00b6 One of the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <Docker-hub-user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry Note Image Tag of the created myfirstapp : docker images Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag <Docker-hub-user-name>/myfirstapp:v1 docker push <Docker-hub-user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <Docker-hub-user-name>/myfirstapp:latest docker pull <Docker-hub-user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <Docker-hub-user-name>/myfirstapp:v1 1.2.3 Pushing images to gcr.io \u00b6 In a similar manner we need to tag the image to prepare it to be pushed to gcr.io. We just need to change the registry, which is for gcr.io formatted as gcr.io/PROJECT_ID. Step 1 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 2 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 3 Tag the image: Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag gcr.io/${PROJECT_ID}/myfirstapp:v1 $docker_image_tag Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/myfirstapp:v1 Step 4 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 1.2.3 Pushing images to Local Repository \u00b6 First, we need to spin up a local docker registry. This could be a use case if you want to deploy basic registry On-Prem. This registry will luck security features such as Authentication, SSL, scanning. If you interested to use Enterprise ready solution On-Prem consider: Jfrog Artifactory, RedHa's Clair, Docker Enterprise or open source CNCF project Harbor. Step 1 Deploy local registry docker run -d -p 5000:5000 --name registry registry:2.7.1 Step 2 In order to upload an image to a registry, we need to tag it properly Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag localhost:5000/myfirstapp:v1 Step 3 Now that we have an image tagged correctly, we can push it to our local registry docker push localhost:5000/myfirstapp:v1 Step 4 Let\u2019s now delete the local image, and pull it again from the local registry To delete the image, we need to first remove the container that depends on that image. Run docker ps and get the Container_ID for the container that uses myfirstapp:v1 Kill and delete that container by running the following command, but make sure to replace CONTAINER_ID, with the actual ID. docker rm CONTAINER_ID Result: The command will print back the container ID, which is an indication it was successful. Step 5 Run docker images to validate docker images Step 6 Now we can delete the docker image docker rmi localhost:5000/myfirstapp:v1 Step 7 Although the image is deleted locally, it is still in the registry and we can pull it back, or use it to deploy containers. docker run -dp 8080:5000 --name myfirstapp localhost:5000/myfirstapp:v1 Run docker images again to check how the image is available locally again. docker images Step 8 Cleanup: docker rm -f myfirstapp 2 Follow Docker Best Practices \u00b6 2.1 Inspecting Dockerfiles with dockle \u00b6 Dockle - Container Image Linter for Security, Helping build the Best-Practice Docker Image, Easy to start Dockle helps you: Build Best Practice Docker images Build secure Docker images Checkpoints includes CIS Benchmarks Step 1 Install Dockle $ VERSION=$( curl --silent \"https://api.github.com/repos/goodwithtech/dockle/releases/latest\" | \\ grep '\"tag_name\":' | \\ sed -E 's/.*\"v([^\"]+)\".*/\\1/' \\ ) && curl -L -o dockle.deb https://github.com/goodwithtech/dockle/releases/download/v${VERSION}/dockle_${VERSION}_Linux-64bit.deb $ sudo dpkg -i dockle.deb && rm dockle.deb Step 2 Experiment with existing applications we've created in the class: $ dockle [YOUR_IMAGE_NAME] e.g. dockle archy/myfirstapp output: WARN - CIS-DI-0001: Create a user for the container * Last user should not be root WARN - DKL-DI-0006: Avoid latest tag * Avoid 'latest' tag INFO - CIS-DI-0005: Enable Content trust for Docker * export DOCKER_CONTENT_TRUST=1 before docker pull/build INFO - CIS-DI-0006: Add HEALTHCHECK instruction to the container image * not found HEALTHCHECK statement INFO - DKL-LI-0003: Only put necessary files * Suspicious directory : tmp 2.2 Automated Builds with Google Cloud Build \u00b6 Live Demo: GCR Image scanning Setting Up Docker Image Auto-Build with Google Cloud Build based on Push to Branch Auto Deployment of Image to Cloud Run","title":"ycit019 Lab 4 Docker Images copy"},{"location":"ycit019_Lab_4_Docker_Images%20copy/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ycit019_Lab_4_Docker_Images%20copy/#1-distributing-docker-images-with-container-registry","text":"In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: https://github.com/Cloud-Architects-Program/ycit019/tree/main/Module5/flask-app","title":"1 Distributing Docker images with Container Registry"},{"location":"ycit019_Lab_4_Docker_Images%20copy/#11-create-dockerfile","text":"Step 1 Clone git repo on you laptop: git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Module5/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\" , \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\" , \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\" , \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\" , \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\" , \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\" , \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\" , \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\" , \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\" , \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==2.0.0 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # Official Python Alpine Base image using Simple Tags # Image contains Python 3 and pip pre-installed, so no need to install them FROM python:3.9.5-alpine3.12 # Specify Working directory WORKDIR /usr/src/app # COPY requirements.txt /usr/src/app/ COPY requirements.txt ./ # Install Python Flask used by the Python app RUN pip install --no-cache-dir -r requirements.txt # Copy files required for the app to run COPY app.py ./ COPY templates/index.html ./templates/ # Make a record that the port number the container should be expose is: EXPOSE 5000 # run the application CMD [ \"python\" , \"./app.py\" ]","title":"1.1 Create DOCKERFILE"},{"location":"ycit019_Lab_4_Docker_Images%20copy/#12-build-a-docker-image","text":"Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step to your own Docker Hub. docker build -t <Docker-hub-user-name>/myfirstapp . Result Image has been buit Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 8080:5000 --name myfirstapp <Docker-hub-user-name>/myfirstapp Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Stop the container and remove it: docker rm -f myfirstapp","title":"1.2 Build a Docker image"},{"location":"ycit019_Lab_4_Docker_Images%20copy/#122-publish-docker-image-to-docker-hub","text":"One of the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <Docker-hub-user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry Note Image Tag of the created myfirstapp : docker images Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag <Docker-hub-user-name>/myfirstapp:v1 docker push <Docker-hub-user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <Docker-hub-user-name>/myfirstapp:latest docker pull <Docker-hub-user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <Docker-hub-user-name>/myfirstapp:v1","title":"1.2.2 Publish Docker Image to Docker Hub"},{"location":"ycit019_Lab_4_Docker_Images%20copy/#123-pushing-images-to-gcrio","text":"In a similar manner we need to tag the image to prepare it to be pushed to gcr.io. We just need to change the registry, which is for gcr.io formatted as gcr.io/PROJECT_ID. Step 1 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 2 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 3 Tag the image: Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag gcr.io/${PROJECT_ID}/myfirstapp:v1 $docker_image_tag Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/myfirstapp:v1 Step 4 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"1.2.3 Pushing images to gcr.io"},{"location":"ycit019_Lab_4_Docker_Images%20copy/#123-pushing-images-to-local-repository","text":"First, we need to spin up a local docker registry. This could be a use case if you want to deploy basic registry On-Prem. This registry will luck security features such as Authentication, SSL, scanning. If you interested to use Enterprise ready solution On-Prem consider: Jfrog Artifactory, RedHa's Clair, Docker Enterprise or open source CNCF project Harbor. Step 1 Deploy local registry docker run -d -p 5000:5000 --name registry registry:2.7.1 Step 2 In order to upload an image to a registry, we need to tag it properly Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag localhost:5000/myfirstapp:v1 Step 3 Now that we have an image tagged correctly, we can push it to our local registry docker push localhost:5000/myfirstapp:v1 Step 4 Let\u2019s now delete the local image, and pull it again from the local registry To delete the image, we need to first remove the container that depends on that image. Run docker ps and get the Container_ID for the container that uses myfirstapp:v1 Kill and delete that container by running the following command, but make sure to replace CONTAINER_ID, with the actual ID. docker rm CONTAINER_ID Result: The command will print back the container ID, which is an indication it was successful. Step 5 Run docker images to validate docker images Step 6 Now we can delete the docker image docker rmi localhost:5000/myfirstapp:v1 Step 7 Although the image is deleted locally, it is still in the registry and we can pull it back, or use it to deploy containers. docker run -dp 8080:5000 --name myfirstapp localhost:5000/myfirstapp:v1 Run docker images again to check how the image is available locally again. docker images Step 8 Cleanup: docker rm -f myfirstapp","title":"1.2.3 Pushing images to Local Repository"},{"location":"ycit019_Lab_4_Docker_Images%20copy/#2-follow-docker-best-practices","text":"","title":"2 Follow Docker Best Practices"},{"location":"ycit019_Lab_4_Docker_Images%20copy/#21-inspecting-dockerfiles-with-dockle","text":"Dockle - Container Image Linter for Security, Helping build the Best-Practice Docker Image, Easy to start Dockle helps you: Build Best Practice Docker images Build secure Docker images Checkpoints includes CIS Benchmarks Step 1 Install Dockle $ VERSION=$( curl --silent \"https://api.github.com/repos/goodwithtech/dockle/releases/latest\" | \\ grep '\"tag_name\":' | \\ sed -E 's/.*\"v([^\"]+)\".*/\\1/' \\ ) && curl -L -o dockle.deb https://github.com/goodwithtech/dockle/releases/download/v${VERSION}/dockle_${VERSION}_Linux-64bit.deb $ sudo dpkg -i dockle.deb && rm dockle.deb Step 2 Experiment with existing applications we've created in the class: $ dockle [YOUR_IMAGE_NAME] e.g. dockle archy/myfirstapp output: WARN - CIS-DI-0001: Create a user for the container * Last user should not be root WARN - DKL-DI-0006: Avoid latest tag * Avoid 'latest' tag INFO - CIS-DI-0005: Enable Content trust for Docker * export DOCKER_CONTENT_TRUST=1 before docker pull/build INFO - CIS-DI-0006: Add HEALTHCHECK instruction to the container image * not found HEALTHCHECK statement INFO - DKL-LI-0003: Only put necessary files * Suspicious directory : tmp","title":"2.1 Inspecting Dockerfiles with dockle"},{"location":"ycit019_Lab_4_Docker_Images%20copy/#22-automated-builds-with-google-cloud-build","text":"Live Demo: GCR Image scanning Setting Up Docker Image Auto-Build with Google Cloud Build based on Push to Branch Auto Deployment of Image to Cloud Run","title":"2.2 Automated Builds with Google Cloud Build"},{"location":"ycit019_Lab_5_Docker_Compose/","text":"Lab 5 Docker Compose and Docker Security Objective: Practice to use Docker Compose, 1 Docker Security \u00b6 1.1 Scan images with Trivy \u00b6 Trivy (tri pronounced like trigger, vy pronounced like envy) is a simple and comprehensive vulnerability scanner for containers and other artifacts. A software vulnerability is a glitch, flaw, or weakness present in the software or in an Operating System. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and application dependencies (Bundler, Composer, npm, yarn, etc.). Trivy is easy to use. Just install the binary and you're ready to scan. All you need to do for scanning is to specify a target such as an image name of the container. Step 1 Install Trivy sudo apt-get install wget apt-transport-https gnupg lsb-release wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add - echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list sudo apt-get update sudo apt-get install trivy Step 2 Specify an image name (and a tag). $ trivy image [YOUR_IMAGE_NAME] For example: $ trivy image python:3.4-alpine 2019-05-16T01:20:43.180+0900 INFO Updating vulnerability database... 2019-05-16T01:20:53.029+0900 INFO Detecting Alpine vulnerabilities... python:3.4-alpine3.9 (alpine 3.9.2) =================================== Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0) +---------+------------------+----------+-------------------+---------------+--------------------------------+ | LIBRARY | VULNERABILITY ID | SEVERITY | INSTALLED VERSION | FIXED VERSION | TITLE | +---------+------------------+----------+-------------------+---------------+--------------------------------+ | openssl | CVE-2019-1543 | MEDIUM | 1.1.1a-r1 | 1.1.1b-r1 | openssl: ChaCha20-Poly1305 | | | | | | | with long nonces | +---------+------------------+----------+-------------------+---------------+--------------------------------+ Step 3 Explore local images in your environment. 2 Docker Compose \u00b6 In this module, will guide you through the process of building a multi-container application using docker compose. The application code is available at GitHub: https://github.com/Cloud-Architects-Program/ycit019 2.1 Deploy Guestbook app with Compose \u00b6 Let\u2019s build another application. This time we going to create famous Guestbook application. Guestbook consists of three services. A redis-master node, a set of redis-slave that can be scaled and find the redis-master via its DNS name. And a PHP frontend that exposes itself on port 80. The resulting application allows you to leave short messages which are stored in the redis cluster. Step 1 Change directory to the guestbook cd ~/ycit019/Module5/guestbook/ ls Step 2 Let\u2019s review the docker-guestbook.yml file version: \"2\" services: redis-master: image: gcr.io/google_containers/redis:e2e ports: - \"6379\" redis-slave: image: gcr.io/google_samples/gb-redisslave:v1 ports: - \"6379\" environment: - GET_HOSTS_FROM=dns frontend: image: gcr.io/google-samples/gb-frontend:v4 ports: - \"80:80\" environment: - GET_HOSTS_FROM=dns Step 3 Let\u2019s run docker-guestbook.yml with compose export LD_LIBRARY_PATH=/usr/local/lib docker-compose -f docker-guestbook.yml up -d Creating network \"examples_default\" with the default driver Creating examples_redis-slave_1 Creating examples_frontend_1 Creating examples_redis-master_1 Note -d - Detached mode: Run containers in the background, print new container names. -f - Specify an alternate compose file (default: docker-compose.yml) Step 4 Check that all containers are running: docker ps CONTAINER ID IMAGE COMMAND d1006d1beee5 gcr.io/google-samples/gb-frontend:v4 \"apache2-foreground\" fb3a15fde23f gcr.io/google_containers/redis:e2e \"redis-server /etc...\" 326b94d4cdd7 gcr.io/google_samples/gb-redisslave:v1 \"/entrypoint.sh /b...\" Step 5 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Success Nice you now have compose stuck up and running! Step 6 Cleanup environment: docker-compose -f docker-guestbook.yml down Stopping guestbook_frontend_1 ... done Stopping guestbook_redis-master_1 ... done Stopping guestbook_redis-slave_1 ... done Removing guestbook_frontend_1 ... done Removing guestbook_redis-master_1 ... done Removing guestbook_redis-slave_1 ... done Removing network guestbook_default 2.2 Deploy Voting App using Compose \u00b6 Step 1 Switch to Module5/example-voting-app folder : cd ~/ycit019/Module5/example-voting-app/ Step 2 The existing file docker-compose.yml defines several images: A voting-app container based on a Python image A result-app container based on a Node.js image A Redis container based on a redis image, to temporarily store the data. A worker app based on a dotnet image A Postgres container based on a postgres image App Architecture: Note that three of the containers are built from Dockerfiles, while the other two are images on Docker Hub. Let's review them closely: Step 3 Review files that going to be deployed with tree command. Alternatively view the files in gitrepo page here sudo apt install tree tree Step 5 Let\u2019s change the default port to expose. Edit the docker-compose.yml file and find the following lines: ports: - \"5000:80\" Change 5000 to 8080: ports: - \"8080:80\" Step 4 Verify Docker Compose version: docker-compose version Step 5 Use the docker-compose tool to launch your application: docker-compose up -d Step 6 Check that all containers are running, volumes created. Check compose state and logs : #Docker state docker ps docker volumes #Docker compose state docker-compose ps docker-compose logs Step 7 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 8 Cleanup up. docker-compose down Stopping examplevotingapp_worker_1 ... done Stopping examplevotingapp_redis_1 ... done Stopping examplevotingapp_result_1 ... done Stopping examplevotingapp_db_1 ... done Stopping examplevotingapp_vote_1 ... done Removing examplevotingapp_worker_1 ... done Removing examplevotingapp_redis_1 ... done Removing examplevotingapp_result_1 ... done Removing examplevotingapp_db_1 ... done Removing examplevotingapp_vote_1 ... done Removing network examplevotingapp_default Step 9 You Boss told you that the application has a bug. Update the the app by editing the vote/app.py file and change the following lines near the top of the file: vim vote/app.py Press 'i' option_a = os.getenv('OPTION_A', \"Cats\") option_b = os.getenv('OPTION_B', \"Dogs\") Step 10 Replace \u201cCats\u201d and \u201cDogs\u201d with two options of your choice. For example: option_a = os.getenv('OPTION_A', \"Java\") option_b = os.getenv('OPTION_B', \"Python\") Press 'wq!' Step 11 Use docker-compose tool to launch your Update application: docker-compose up -d Check the UI Bingo Let's see who wins the battle of Orchestrations! Step 8 Cleanup up docker-compose down Congratulations You are now docker expert! We were able to start 2 microservices application with docker compose. First microservice had 3 services. Second microservice had 5 servics written in 3 different languages and able to talk to each other. Summary So far we've learned docker-compose v2. docker-compose v3 is out of scope for this Lab. However you got the idea! Read the Docker-Compose documentation on new syntax. Also example of v3 version of voting-app is here for you reference.","title":"ycit019 Lab 5 Docker Compose"},{"location":"ycit019_Lab_5_Docker_Compose/#1-docker-security","text":"","title":"1 Docker Security"},{"location":"ycit019_Lab_5_Docker_Compose/#11-scan-images-with-trivy","text":"Trivy (tri pronounced like trigger, vy pronounced like envy) is a simple and comprehensive vulnerability scanner for containers and other artifacts. A software vulnerability is a glitch, flaw, or weakness present in the software or in an Operating System. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and application dependencies (Bundler, Composer, npm, yarn, etc.). Trivy is easy to use. Just install the binary and you're ready to scan. All you need to do for scanning is to specify a target such as an image name of the container. Step 1 Install Trivy sudo apt-get install wget apt-transport-https gnupg lsb-release wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add - echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list sudo apt-get update sudo apt-get install trivy Step 2 Specify an image name (and a tag). $ trivy image [YOUR_IMAGE_NAME] For example: $ trivy image python:3.4-alpine 2019-05-16T01:20:43.180+0900 INFO Updating vulnerability database... 2019-05-16T01:20:53.029+0900 INFO Detecting Alpine vulnerabilities... python:3.4-alpine3.9 (alpine 3.9.2) =================================== Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0) +---------+------------------+----------+-------------------+---------------+--------------------------------+ | LIBRARY | VULNERABILITY ID | SEVERITY | INSTALLED VERSION | FIXED VERSION | TITLE | +---------+------------------+----------+-------------------+---------------+--------------------------------+ | openssl | CVE-2019-1543 | MEDIUM | 1.1.1a-r1 | 1.1.1b-r1 | openssl: ChaCha20-Poly1305 | | | | | | | with long nonces | +---------+------------------+----------+-------------------+---------------+--------------------------------+ Step 3 Explore local images in your environment.","title":"1.1 Scan images with Trivy"},{"location":"ycit019_Lab_5_Docker_Compose/#2-docker-compose","text":"In this module, will guide you through the process of building a multi-container application using docker compose. The application code is available at GitHub: https://github.com/Cloud-Architects-Program/ycit019","title":"2 Docker Compose"},{"location":"ycit019_Lab_5_Docker_Compose/#21-deploy-guestbook-app-with-compose","text":"Let\u2019s build another application. This time we going to create famous Guestbook application. Guestbook consists of three services. A redis-master node, a set of redis-slave that can be scaled and find the redis-master via its DNS name. And a PHP frontend that exposes itself on port 80. The resulting application allows you to leave short messages which are stored in the redis cluster. Step 1 Change directory to the guestbook cd ~/ycit019/Module5/guestbook/ ls Step 2 Let\u2019s review the docker-guestbook.yml file version: \"2\" services: redis-master: image: gcr.io/google_containers/redis:e2e ports: - \"6379\" redis-slave: image: gcr.io/google_samples/gb-redisslave:v1 ports: - \"6379\" environment: - GET_HOSTS_FROM=dns frontend: image: gcr.io/google-samples/gb-frontend:v4 ports: - \"80:80\" environment: - GET_HOSTS_FROM=dns Step 3 Let\u2019s run docker-guestbook.yml with compose export LD_LIBRARY_PATH=/usr/local/lib docker-compose -f docker-guestbook.yml up -d Creating network \"examples_default\" with the default driver Creating examples_redis-slave_1 Creating examples_frontend_1 Creating examples_redis-master_1 Note -d - Detached mode: Run containers in the background, print new container names. -f - Specify an alternate compose file (default: docker-compose.yml) Step 4 Check that all containers are running: docker ps CONTAINER ID IMAGE COMMAND d1006d1beee5 gcr.io/google-samples/gb-frontend:v4 \"apache2-foreground\" fb3a15fde23f gcr.io/google_containers/redis:e2e \"redis-server /etc...\" 326b94d4cdd7 gcr.io/google_samples/gb-redisslave:v1 \"/entrypoint.sh /b...\" Step 5 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Success Nice you now have compose stuck up and running! Step 6 Cleanup environment: docker-compose -f docker-guestbook.yml down Stopping guestbook_frontend_1 ... done Stopping guestbook_redis-master_1 ... done Stopping guestbook_redis-slave_1 ... done Removing guestbook_frontend_1 ... done Removing guestbook_redis-master_1 ... done Removing guestbook_redis-slave_1 ... done Removing network guestbook_default","title":"2.1 Deploy Guestbook app with Compose"},{"location":"ycit019_Lab_5_Docker_Compose/#22-deploy-voting-app-using-compose","text":"Step 1 Switch to Module5/example-voting-app folder : cd ~/ycit019/Module5/example-voting-app/ Step 2 The existing file docker-compose.yml defines several images: A voting-app container based on a Python image A result-app container based on a Node.js image A Redis container based on a redis image, to temporarily store the data. A worker app based on a dotnet image A Postgres container based on a postgres image App Architecture: Note that three of the containers are built from Dockerfiles, while the other two are images on Docker Hub. Let's review them closely: Step 3 Review files that going to be deployed with tree command. Alternatively view the files in gitrepo page here sudo apt install tree tree Step 5 Let\u2019s change the default port to expose. Edit the docker-compose.yml file and find the following lines: ports: - \"5000:80\" Change 5000 to 8080: ports: - \"8080:80\" Step 4 Verify Docker Compose version: docker-compose version Step 5 Use the docker-compose tool to launch your application: docker-compose up -d Step 6 Check that all containers are running, volumes created. Check compose state and logs : #Docker state docker ps docker volumes #Docker compose state docker-compose ps docker-compose logs Step 7 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 8 Cleanup up. docker-compose down Stopping examplevotingapp_worker_1 ... done Stopping examplevotingapp_redis_1 ... done Stopping examplevotingapp_result_1 ... done Stopping examplevotingapp_db_1 ... done Stopping examplevotingapp_vote_1 ... done Removing examplevotingapp_worker_1 ... done Removing examplevotingapp_redis_1 ... done Removing examplevotingapp_result_1 ... done Removing examplevotingapp_db_1 ... done Removing examplevotingapp_vote_1 ... done Removing network examplevotingapp_default Step 9 You Boss told you that the application has a bug. Update the the app by editing the vote/app.py file and change the following lines near the top of the file: vim vote/app.py Press 'i' option_a = os.getenv('OPTION_A', \"Cats\") option_b = os.getenv('OPTION_B', \"Dogs\") Step 10 Replace \u201cCats\u201d and \u201cDogs\u201d with two options of your choice. For example: option_a = os.getenv('OPTION_A', \"Java\") option_b = os.getenv('OPTION_B', \"Python\") Press 'wq!' Step 11 Use docker-compose tool to launch your Update application: docker-compose up -d Check the UI Bingo Let's see who wins the battle of Orchestrations! Step 8 Cleanup up docker-compose down Congratulations You are now docker expert! We were able to start 2 microservices application with docker compose. First microservice had 3 services. Second microservice had 5 servics written in 3 different languages and able to talk to each other. Summary So far we've learned docker-compose v2. docker-compose v3 is out of scope for this Lab. However you got the idea! Read the Docker-Compose documentation on new syntax. Also example of v3 version of voting-app is here for you reference.","title":"2.2 Deploy Voting App using Compose"},{"location":"ycit019_Module_5_Lab_Docker_basics/","text":"Module 5 Lab Docker basics \u00b6 Objective: Practice to run Docker containers Learn to build docker images using Dockerfiles Learn Docker Networking 1 Setup Environment and Install Docker \u00b6 1.1 Create an instance with gcloud \u00b6 Rather than using the Google Cloud Console to create a virtual machine instance, you can use the command line tool gcloud , which is pre-installed in Google Cloud Shell . Cloud Shell is a Debian-based virtual machine loaded with all the development tools you\u2019ll need (gcloud, git, and others) and offers a persistent 5GB home directory. Note If you want to try this on your own machine in the future, read the gcloud command line tool guide. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can create a new virtual machine instance from the command line by using gcloud (feel free to use another zone closer to you): Step 1 Create a new virtual machine instance on GCE gcloud config set project <set_you_project_id> gcloud compute instances create docker-nginx --zone us-central1-c Output: Created [...docker-nginx]. NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS docker-nginx us-central1-c n1-standard-1 10.240.X.X X.X.X.X RUNNING The instance created has these default values: - The latest Debian 10 (buster) image. - The n1-standard-1 machine type. - A root persistent disk with the same name as the instance; the disk is automatically attached to the instance. Note You can set the default region and zones that gcloud uses if you are always working within one region/zone and you don\u2019t want to append the -- zone flag every time. Do this by running these commands: gcloud config set compute/zone ... gcloud config set compute/region ... Step 2 SSH into your instance using gcloud as well. gcloud compute ssh docker-nginx --zone us-central1-c Output: WARNING: The public SSH key file for gcloud does not exist. WARNING: The private SSH key file for gcloud does not exist. WARNING: You do not have an SSH key for gcloud. WARNING: [/usr/bin/ssh-keygen] will be executed to generate a key. This tool needs to create the directory [/home/gcpstaging306_student/.ssh] before being able to generate SSH Keys. Now you\u2019ll type Y to continue. Do you want to continue? (Y/n) Enter through the passphrase section to leave the passphrase empty. Generating public/private rsa key pair. Enter passphrase (empty for no passphrase) 1.2 Install Docker Engine \u00b6 We need to first set up the Docker repository. To set up the repository: Step 1 Update the apt package index: sudo apt-get update Step 2 Then install packages to allow \u201capt\u201d to use a repository over HTTPS: sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Step 3 Add Docker\u2019s official GPG key: curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Step 4 Set up the stable repository echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Step 5 Install Docker Engine: sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io Step 6 Verify Docker Version has been deployed: docker version Verify what is the latest Docker release Step 6 Confirm that your installation was successful by running Hello World! Container. sudo docker run hello-world Step 7 If it is successful, you will see the following output: Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world b8dfde127a29: Pull complete Digest: sha256:f2266cbfc127c960fd30e76b7c792dc23b588c0db76233517e1891a4e357d519 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. Step 8 Exit from VM exit Step 9 Delete VM Instance to avoid extra cost: gcloud compute instances delete docker-nginx --zone us-central1-c Now you\u2019ll type Y to continue. Do you want to continue? (Y/n) Summary Now you know how to deploy Docker on Linux VM. Task Find out how to deploy Docker on Mac or Windows? 2 Working with Docker CLI \u00b6 2.1 Show running containers \u00b6 Going forward we going to run Docker commands directly from Google Cloud Console. This is possible because Docker is installed on Cloud Console VM for you convenience. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. Step 1 Create a docker container hello-world : docker run hello-world Step 2 Run docker ps to show running containers: docker ps Result The output shows that there are no running containers at the moment. Step 2 Use the command docker ps -a to list all containers including the ones has been stopped: docker ps -a Output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e6db2a24a8e hello-world \"/hello\" 15 minutes ago Exited (0) 15 min dreamy_nobel Review the collumns CONTAINER ID , STATUS , COMMAND , PORTS , NAMES . In the previous section we started one container and the command docker ps -a shows it as Exited . Note You can name your own containers with --name when you use docker run. If you do not provide a name, Docker will generate a random one like the one you have. Question Why Docker names are random? How docker containers named? Step 3 Let\u2019s run the command docker images to show all the images on your local system: docker images As you see, there is only one image that was downloaded from the Docker Hub. 2.3 Specify a container main process \u00b6 Step 1 Let\u2019s run our own \"hello world\" container. For that we will use the official Ubuntu image : docker run ubuntu /bin/echo 'Hello world' Output: Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:latest Hello world As you see, Docker downloaded the image ubuntu because it was not on the local machine. Step 2 Let\u2019s run the command docker images again: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Step 3 If you run the same \"hello world\" container again, Docker will use a local copy of the image: docker run ubuntu /bin/echo 'Hello world' Output: Hello world Question Compare Ubuntu Docker image with ISO image or with Cloud VM image. Why the size is so different ? Summary Pulling docker images from Docker Hub takes sometime. This time depends on: How large is the image? How fast is the network to Internet ? However, it is still much faster than booting traditional OS with Ubuntu on VM. If image already pulled on local host it takes fraction of a second to start a container. Running application in docker containers considered as a best practice for running CI/CD pipelines as it considerably faster than using VMs and reduce time for deploying a test environments. 2.3 Specify an image version \u00b6 Step 1 As you see, Docker has downloaded the ubuntu:latest image. You can see Ubuntu version by running the following command: docker run ubuntu /bin/cat /etc/issue.net Output: Ubuntu 22.04 LTS Let\u2019s say you need a previous Ubuntu LTS release. In this case, you can specify the version you need: docker run ubuntu:14.04 /bin/cat /etc/issue.net Output: Unable to find image 'ubuntu:14.04' locally 14.04: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:14.04 Ubuntu 14.04.4 LTS Step 2 The docker images command should show that we have 3 Ubuntu images downloaded locally: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB ubuntu 14.04 0ccb13bf1954 11 days ago 188 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Tip Running CI/CD pipeline with Docker using latest tag considered as a Bad Practice. Instead consider using: Versioning SHA tagging. 2.4 Run an interactive container \u00b6 Step 1 Let\u2019s use the ubuntu image to run an interactive bash session and inspect what is running inside our docker image. To achive that we going to use -i and -t flags. The -i is shorthand for --interactive , which instructs Docker to keep stdin open so that we can send commands to the sprocess. The -t flag is short for --tty and allocates a pseudo-TTY or terminal inside of the session. docker run -it ubuntu /bin/bash root@17d8bdeda98e:/# Result We get a bash shell prompt inside of the container. Note Bash prompt is not availabe for all docker images. Step 2 Let's print the system information of the latest Ubuntu image: root@17d8bdeda98e:/# uname -a Linux 17d8bdeda98e 3.19.0-31-generic ... Step 3 Let's verify what Ubuntu version is run by latest image of ubuntu: root@17d8bdeda98e:/# lsb_release -a bash: lsb_release: command not found Failure Why the standard Ubuntu command that checks version of OS is not working as expeced ? Step 4 Let's verify Ubuntu version using alternative way by checking /etc/lsb-release file. root@8cbcbd0fe8d2:/# cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.3 LTS\" Step 5 Let's compare the number of executable binaries availabe inside of the docker image versus Cloud VM that we running our class environment. First, run ls command on /bin and /usr/bin directories inside of the running ubuntu container as well as dpkg --list command that shows total number of installed packages: root@8cbcbd0fe8d2:/# ls /bin | wc -l 294 root@8cbcbd0fe8d2:/# ls /usr/bin | wc -l xx root@eb11cd0b4106:/# dpkg --list | wc -l xx Step 6 Use the exit command or press Ctrl-D to exit the interactive bash session back to Cloud VM. root@eb11cd0b4106:/# exit Step 7 Now run ls command on /bin and /usr/bin directories on Cloud VM that we using as our class environment: cca-user@userx-docker-vm:~$ ls /bin | wc -l 1173 cca-user@userx-docker-vm:~$ ls /usr/bin | wc -l 660 cca-user@userx-docker-vm:~$ dpkg --list | wc -l 463 Result Official Docker container has much less binaries and packages installed vs Ubuntu Cloud Image. Summary Some of the use cases running docker containers in interactive mode are: Troubleshooting containerized applications Deploying and running containerized application on the existing production systems without affecting it. We've also learned that an official Docker \"minimal\" ubuntu image, does not include lsb_release command, as well as many other commands and packages that can be found in Official Ubuntu ISO image . The docker images are ment to contain only required core system commands and functions to make Images as light as possible. That say you can still install required packages using apt-get install , however this may increase size of docker image considerably. Hint While Docker Ubuntu image we used so far or Docker Centos image are very familiar to users and can be good starting point for learning docker containers. Using them in production or development considered as a Bad Practice. This is due those images still considered as heavy and potentially contain a lot more valnurabilities compare to specialized images. To reduce image pull time from docker hub and follow the best secuirity practices consider using specialized images that works well with you underlining code (Node image for NodeJS applications and etc.). Examples of specialized images are: Alpine Linux Node Atomic In fact, not so long ago all the official Docker Images in Docker-Hub has been moved to use Alpine Image . Step 8 Finally let\u2019s check that when the shell process has finished, the container stops: docker ps 2.5 Run a container in a background \u00b6 Now we know how to connect to running container and execute commands in it. However in most cases you just want run a container in a background so it can do a specific action. Step 1 Run a container in a background using the -d command line argument: docker run -d ubuntu /bin/sh -c \"while true; do date; echo hello world; sleep 1; done\" Result Command should return the container ID. Step 2 Let\u2019s use the docker ps command to see running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac231579e57f ubuntu \"/bin/sh -c 'while tr\" 1 minute ago Up 11 minute evil_golick Note Container id is going to be different in your case Hint Instead of using full container-id when building commands, it is possible simply type first few characters of container-id, to make things nice and easy. Step 3 Let\u2019s use container-id to show the container standard output: docker logs <container-id> Thu Jan 26 00:23:45 UTC 2017 hello world Thu Jan 26 00:23:46 UTC 2017 hello world Thu Jan 26 00:23:47 UTC 2017 hello world ... As you can see, in the docker ps command output, the auto generated container name is evil_golick (your container can have a different name). Step 4 Now, instead of using docker contaier-id use container name to show the container standard output: docker logs <name> Thu Jan 26 00:23:51 UTC 2017 hello world Thu Jan 26 00:23:52 UTC 2017 hello world Thu Jan 26 00:23:53 UTC 2017 hello world ... Step 5 Finally, let\u2019s stop our container: docker stop <name> Step 6 Check, that there are no running containers: docker ps Summary docker logs is a very usefull command to troubleshoot containers, and going to be used very often both for Docker and Kubernertes troubleshooting. 2.6 Accessing Containers from the Internet \u00b6 Step 1 Let\u2019s run a simple web application. We will use the existing image training/webapp, which contains a Python Flask application: docker run -d -P training/webapp python app.py ... Status: Downloaded newer image for training/webapp:latest 6e88f42d3d853762edcbfe1fe73fdc5c48865275bc6df759b83b0939d5bd2456 In the command above we specified the main process (python app.py), the -d command line argument, which tells Docker to run the container in the background. The -P command line argument tells Docker to map any required network ports inside our container to our host. This allows us to access the web application in the container. Step 2 Use the docker ps command to list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e88f42d3d85 training/webapp \"python app.py\" 3 minutes ago Up 3 minutes 0.0.0.0:32768->5000/tcp determined_torvalds The PORTS column contains the mapped ports. In our case, Docker has exposed port 5000 (the default Python Flask port) on port 32768 (can be different in your case). Step 3 The docker port command shows the exposed port. We will use the container name (determined_torvalds in the example above, it can be different in your case): docker port <name> 5000 0.0.0.0:32768 Step 4 Let\u2019s check that we can access the web application exposed port: curl http://localhost:<port>/ Result Hello world! Step 5 Let\u2019s stop our web application for now: docker stop <name> Step 6 We want to manually specify the local port to expose (-p argument). Let\u2019s use the standard HTTP port 8080. We also want to specify the container name (--name argument): docker run -d -p 8080:5000 --name webapp training/webapp python app.py Step 7 Let\u2019s check that the port 8080 is exposed: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp curl http://localhost/ Result Hello world! Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Result Our web-app can be accessed from Internet! 2.7 Restart a container \u00b6 Step 1 Let\u2019s stop the container with web application: docker stop webapp The main process inside of the container will receive SIGTERM, and after a grace period, SIGKILL. Step 2 You can start the container later using the docker start command: docker start webapp Step 3 Check that the web application works: Step 4 You also can restart the running container using the docker restart command. docker restart webapp Step 4 Run docker ps command and check STATUS field: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS 6e400179070f training/webapp \"python app.py\" 25 minutes ago Up 3 seconds 2.8 Ensuring Container Uptime \u00b6 Docker considers any containers to exit with a non-zero exit code to have crashed. By default a crashed container will remain stopped. Step 1 Start the container that outputs a message and then exits with code 1 to simulate a crash. docker run -d --name restart-default scrapbook/docker-restart-example docker ps -a | grep restart-default CONTAINER ID IMAGE CREATED STATUS NAMES c854289d2f39 scrapbook/docker-restart-example 5 seconds ago Exited 3 sec ago restart-default $ docker logs restart-default Sun Sep 17 20:34:55 UTC 2017 Booting up... Result Container crushed and exited. However, there are several ways to ensure that you container up and running even if it\u2019s restarts. Step 2 The option --restart=on-failure : allows you to say how many times Docker should try again: docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example docker logs restart-3 Thu Apr 20 14:01:27 UTC 2017 Booting up... Thu Apr 20 14:01:28 UTC 2017 Booting up... Thu Apr 20 14:01:29 UTC 2017 Booting up... Thu Apr 20 14:01:31 UTC 2017 Booting up... Step 3 Finally, Docker can always restart a failed container. In this case, Docker will keep trying until the container is explicitly told to stop. docker run -d --name restart-always --restart=always scrapbook/docker-restart-example docker logs restart-always Step 4 After sometime stop running docker container, as it will be keep failing and starting again: docker stop restart-always 2.9 Inspect a container \u00b6 Step 1 You can use the docker inspect command to see the configuration and status information for the specified container: docker inspect webapp [ { \"Id\": \"249476631f7d...\", \"Created\": \"2016-08-02T23:42:56.932135327Z\", \"Path\": \"python\", \"Args\": [ \"app.py\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 16055, \"ExitCode\": 0, \"Error\": \"\", ... Step 2 You can specify a filter (-f command line argument) to show only specific elements. For example: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp 172.17.0.2 The command returns the IP address of the container. 2.10 Interacting with containers \u00b6 In some cases using docker log is not enough to undertand issues and you want to login inside of running VM. Also sometimes you package you applicaiton and in order to run it you need to login inside of container and execute and leave it running in background. Below provded few ways to interacting with containers that can help to achive descrined use cases. 2.10.1 Detach from Interactive container \u00b6 In Module, 1.4 Run an interactive container we run an Ubuntu container with -it flag and able directly login inside of the container to interact with it, however after we exited contianer using Ctrl-D or exit command container stopped. However you can exit from Interactive mode without stoping a container. Let's demonstrate how this works: Step 1 Start Ubunu container in interactive mode: docker run -it ubuntu /bin/bash Step 2 Run watch date command inside running container in order to exit date command every 2 seconds. root@1d688a9f4ed4:/# watch date Step 3 Detach from a container and leave it running using the CTRL-p CTRL-q key sequence. Step 4 Verify that Ubuntu container is still running: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS NAMES 1d688a9f4ed4 ubuntu \"/bin/bash\" 1 minutes ago Up 1 minutes admiring_lovelace Result Great you were able to detach from Docker container without stopping it, while it is executing a process in it. What about attaching back to container ? Important CTRL-p CTRL-q sequence key only works if docker contaienr started with -it command! 2.11.2 Attach to a container \u00b6 Now let's get back and attach to our running Ubuntu image. For that docker provides docker attach command. docker attach <container name> Every 2.0s: date Mon Sep 18 00:08:57 2017 Summary docker attach attaches your contairs terminal\u2019s standard input, output, and error (or any combination of the 3) to a running container. This allows you to view its ongoing output or to control it interactively, as though the commands were running directly in your terminal. 2.11.3 Execute a process in a container \u00b6 Step 1 Let verify if webapp container is still running docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp If not running start it with following command: $ docker run -d -p 80:5000 --name webapp training/webapp python app.py other wise skip to next step . Step 2 Use the docker exec command to execute a command in the running container. For example: docker exec webapp ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 26 0.0 0.0 15572 2104 ? Rs 00:12 0:00 ps aux The same command with the -it command line argument can be used to run an interactive session in the container: docker exec -it webapp bash root@249476631f7d:/opt/webapp# ps auxw ps auxw USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 32 0.0 0.0 18144 3064 ? Ss 00:14 0:00 bash root 47 0.0 0.0 15572 2076 ? R+ 00:16 0:00 ps auxw Step 2 Use the exit command or press Ctrl-D to exit the interactive bash session: root@249476631f7d:/opt/webapp# exit Summary docker exec is one of the most usefull docker commands used for troubleshooting containers. 2.12 Copy files to/from container \u00b6 The docker cp command allows you to copy files from the container to the local machine or from the local file system to the container. This command works for a running or stopped container. Step 1 Let\u2019s copy the container\u2019s app.py file to the local machine: docker cp webapp:/opt/webapp/app.py . Step 2 Edit the local app.py file. For example, change the line return 'Hello '+provider+'!' to return 'Hello '+provider+'!!!'. Copy the modified file back and restart the container: docker cp app.py webapp:/opt/webapp/ docker restart webapp Step 3 Check that the modified web application works:: curl http://localhost/ Result `Hello world!!!`` 2.12 Remove containers \u00b6 Now let's clean up the environment and at the same time learn how delete containers. Step 1 First list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 81c4c66baaf9 training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp Step 2 Than try to delete running container using docker rm <container_id> docker rm $container_id Error response from daemon: You cannot remove a running container 81c4c66baaf9. Stop the container before attempting removal or force remove. Failure Docker containers needs to be first stopped or deleted using --force flag. docker rm $container_id -f Alternatively, you can run stop and rm in sequence: docker stop 81c4c66baaf9 docker rm 81c4c66baaf9 3 Building Images with DockerFile \u00b6 In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. The code for this application is also available in GitHub: https://github.com/Cloud-Architects-Program/ycit019_2022/tree/main/Module5/flask-app 3.1 Overview DOCKERFILE Creation \u00b6 Step 1 Clone git repo on you laptop: git clone https://github.com/Cloud-Architects-Program/ycit019_2022 cd ~/ycit019/Module5/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\" , \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\" , \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\" , \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\" , \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\" , \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\" , \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\" , \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\" , \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\" , \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==2.0.0 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # Official Python Alpine Base image using Simple Tags # Image contains Python 3 and pip pre-installed, so no need to install them FROM python:3.9.5-alpine3.12 # Specify Working directory WORKDIR /usr/src/app # COPY requirements.txt /usr/src/app/ COPY requirements.txt ./ # Install Python Flask used by the Python app RUN pip install --no-cache-dir -r requirements.txt # Copy files required for the app to run COPY app.py ./ COPY templates/index.html ./templates/ # Make a record that the port number the container should be expose is: EXPOSE 5000 # run the application CMD [ \"python\" , \"./app.py\" ] 3.2 Build a Docker image \u00b6 Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. docker build -t <user-name>/myfirstapp . Result Image has been buit Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 8080:5000 --name myfirstapp <user-name>/myfirstapp Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Stop the container and remove it: docker rm -f myfirstapp Summary We've learned a lot of docker commands which are very handy to know both when using Docker and Kubernetes. Next we will learn how to create Docker networks. 4 Docker Networking \u00b6 4.1 Docker Networking Basics \u00b6 Step 1: The Docker Network Command The docker network command is the main command for configuring and managing container networks. Run the docker network command from the first terminal. docker network Usage: docker network COMMAND Manage networks Options: --help Print usage Commands: connect Connect a container to a network create Create a network disconnect Disconnect a container from a network inspect Display detailed information on one or more networks ls List networks prune Remove all unused networks rm Remove one or more networks Run 'docker network COMMAND --help' for more information on a command. The command output shows how to use the command as well as all of the docker network sub-commands. As you can see from the output, the docker network command allows you to create new networks, list existing networks, inspect networks, and remove networks. It also allows you to connect and disconnect containers from networks. Step 2 Run a docker network ls command to view existing container networks on the current Docker host. docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local The output above shows the container networks that are created as part of a standard installation of Docker. New networks that you create will also show up in the output of the docker network ls command. You can see that each network gets a unique ID and NAME . Each network is also associated with a single driver. Notice that the \"bridge\" network and the \"host\" network have the same name as their respective drivers. Step 3: The docker network inspect command is used to view network configuration details. These details include; name, ID, driver, IPAM driver, subnet info, connected containers, and more. Use docker network inspect <network> to view configuration details of the container networks on your Docker host. The command below shows the details of the network called bridge . docker network inspect bridge [ { \"Name\": \"bridge\", \"Id\": \"3430ad6f20bf1486df2e5f64ddc93cc4ff95d81f59b6baea8a510ad500df2e57\", \"Created\": \"2017-04-03T16:49:58.6536278Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Containers\": {}, \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" }, \"Labels\": {} } ] Note The syntax of the docker network inspect command is docker network inspect <network> , where <network> can be either network name or network ID. In the example above we are showing the configuration details for the network called \"bridge\". Do not confuse this with the \"bridge\" driver. Step 4 Now, list Docker supported network driver plugins. For that run docker info command, that shows a lot of interesting information about a Docker installation. Run the docker info command and locate the list of network plugins. docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ee-3 Storage Driver: aufs <Snip> Plugins: Volume: local Network: bridge host macvlan null overlay Swarm: inactive Runtimes: runc <Snip> The output above shows the bridge , host , macvlan , null , and overlay drivers. Summary We've quickly reviewed available docker networking commands as well as found what drivers current docker setup supports. 4.2 Default bridge network \u00b6 Every clean installation of Docker comes with a pre-built network called Default bridge network . Let's explore in more details how it works. Step 1 Verify this with the docker network ls . docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local Result The output above shows that the bridge network is associated with the bridge driver. It's important to note that the network and the driver are connected, but they are not the same. In this example the network and the driver have the same name - but they are not the same thing! The output above also shows that the bridge network is scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the bridge driver - the bridge driver provides single-host networking. All networks created with the bridge driver are based on a Linux bridge (a.k.a. a virtual switch). Step 5 Start webapp in Default bridge network docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 6 Check that the webapp and db containers are running: Command: docker ps 4.3 User-defined Private Networks \u00b6 So far we\u2019ve learned how Docker networking works with Docker default bridge network . With the introduction of user-defined networking in Docker 1.9, it is now possible to create multiple Docker bridges to allow network segregation within the same host or multi-host networking to allow communicate Docker containers between hosts. The commands are available through the Docker Engine CLI are: docker network create docker network connect docker network ls docker network rm docker network disconnect docker network inspect Let's demonstrate how to create a custom bridge network. Step 1 By default, Docker runs containers in the bridge network. You may want to isolate one or more containers in a separate network. Let\u2019s create a new network: docker network create my-network \\ -d bridge \\ --subnet 172.19.0.0/16 The -d bridge command line argument specifies the bridge network driver and the --subnet command line argument specifies the network segment in CIDR format. If you do not specify a subnet when creating a network, then Docker assigns a subnet automatically, so it is a good idea to specify a subnet to avoid potential conflicts with the existing networks. Below are some other options that are available with the bridge Driver: com.docker.network.bridge.enable_ip_masquerade: This instructs the Docker host to hide or masquerade all containers in this network behind the Docker host's interfaces if the container attempts to route off the local host . com.docker.network.bridge.name: This is the name you wish to give to the bridge. com.docker.network.bridge.enable_icc: This turns on or off Inter-Container Connectivity (ICC) mode for the bridge. com.docker.network.bridge.host_binding_ipv4: This defines the host interface that should be used for port binding. com.docker.network.driver.mtu: This sets MTU for containers attached to this bridge. Step 2 To check that the new network is created, execute docker network ls: docker network ls NETWORK ID NAME DRIVER SCOPE d428e49e4869 bridge bridge local 0d1f78528cc5 host host local 56ef0481820d my-network bridge local 4a07cef84617 none null local Step 3 Let\u2019s inspect the new network: docker network inspect my-network [ { \"Name\": \"my-network\", \"Id\": \"56ef0481820d...\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"172.19.0.0/16\" } ] }, \"Internal\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ] Step 4 As expected, there are no containers connected to the my-network. Let\u2019s recreate the db container in the my-network: docker rm -f db docker run -d --network=my-network --name db training/postgres Step 5 Inspect the my-network again: docker network inspect my-network Output: \"Containers\": { \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... As you see, the db container is connected to the my-network and has 172.19.0.2 address. Step 6 Let\u2019s start an interactive session in the db container and ping the IP address of the webapp again: Note Quick reminder how to locate webapp ip: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp docker exec -it db bash Once inside of container run: root@c3afff20019a:/# ping -c 1 172.17.0.3 PING 172.17.0.3 (172.17.0.3) 56(84) bytes of data. --- 172.17.0.3 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms As expected, the webapp container is no longer accessible from the db container, because they are connected to different networks. Summary Using Multi-host networking provides network isolation within a Docker host via network namepsaces. This is can be used if you want to deploy different applications on same host for isolation or resource duplicate prevention. Step 7 Let\u2019s connect the webapp container to the my-network: docker network connect my-network webapp Step 8 Check that the webapp container now is connected to the my-network: docker network inspect my-network Output: ... \"Containers\": { \"62ed4a627356...\": { \"Name\": \"webapp\", \"EndpointID\": \"ae95b0103bbc...\", \"MacAddress\": \"02:42:ac:12:00:03\", \"IPv4Address\": \"172.19.0.3/16\", \"IPv6Address\": \"\" }, \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... The output shows that two containers are connected to the my-network and the webapp container has 172.19.0.3 address in that network. Step 9 Check that the webapp container is accessible from the db container using its new IP address: docker exec -it db bash root@c3afff20019a:/# ping -c 1 172.19.0.3 PING 172.19.0.3 (172.19.0.3) 56(84) bytes of data. 64 bytes from 172.19.0.3: icmp_seq=1 ttl=64 time=0.136 ms --- 172.19.0.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms Success As expected containers can communicate with each other. Step 10 You can now remove the existing container. You should stop the container before removing it. Alternatively you can use the -f command line argument: docker rm -f webapp docker rm -f db docker network rm my-network Hint Use below command to delete running containers in bulk : docker rm -f $(docker ps -q) Summary It is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses","title":"Module 5 Lab - Docker Basics"},{"location":"ycit019_Module_5_Lab_Docker_basics/#module-5-lab-docker-basics","text":"Objective: Practice to run Docker containers Learn to build docker images using Dockerfiles Learn Docker Networking","title":"Module 5 Lab Docker basics"},{"location":"ycit019_Module_5_Lab_Docker_basics/#1-setup-environment-and-install-docker","text":"","title":"1 Setup Environment and Install Docker"},{"location":"ycit019_Module_5_Lab_Docker_basics/#11-create-an-instance-with-gcloud","text":"Rather than using the Google Cloud Console to create a virtual machine instance, you can use the command line tool gcloud , which is pre-installed in Google Cloud Shell . Cloud Shell is a Debian-based virtual machine loaded with all the development tools you\u2019ll need (gcloud, git, and others) and offers a persistent 5GB home directory. Note If you want to try this on your own machine in the future, read the gcloud command line tool guide. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can create a new virtual machine instance from the command line by using gcloud (feel free to use another zone closer to you): Step 1 Create a new virtual machine instance on GCE gcloud config set project <set_you_project_id> gcloud compute instances create docker-nginx --zone us-central1-c Output: Created [...docker-nginx]. NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS docker-nginx us-central1-c n1-standard-1 10.240.X.X X.X.X.X RUNNING The instance created has these default values: - The latest Debian 10 (buster) image. - The n1-standard-1 machine type. - A root persistent disk with the same name as the instance; the disk is automatically attached to the instance. Note You can set the default region and zones that gcloud uses if you are always working within one region/zone and you don\u2019t want to append the -- zone flag every time. Do this by running these commands: gcloud config set compute/zone ... gcloud config set compute/region ... Step 2 SSH into your instance using gcloud as well. gcloud compute ssh docker-nginx --zone us-central1-c Output: WARNING: The public SSH key file for gcloud does not exist. WARNING: The private SSH key file for gcloud does not exist. WARNING: You do not have an SSH key for gcloud. WARNING: [/usr/bin/ssh-keygen] will be executed to generate a key. This tool needs to create the directory [/home/gcpstaging306_student/.ssh] before being able to generate SSH Keys. Now you\u2019ll type Y to continue. Do you want to continue? (Y/n) Enter through the passphrase section to leave the passphrase empty. Generating public/private rsa key pair. Enter passphrase (empty for no passphrase)","title":"1.1 Create an instance with gcloud"},{"location":"ycit019_Module_5_Lab_Docker_basics/#12-install-docker-engine","text":"We need to first set up the Docker repository. To set up the repository: Step 1 Update the apt package index: sudo apt-get update Step 2 Then install packages to allow \u201capt\u201d to use a repository over HTTPS: sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Step 3 Add Docker\u2019s official GPG key: curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Step 4 Set up the stable repository echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Step 5 Install Docker Engine: sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io Step 6 Verify Docker Version has been deployed: docker version Verify what is the latest Docker release Step 6 Confirm that your installation was successful by running Hello World! Container. sudo docker run hello-world Step 7 If it is successful, you will see the following output: Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world b8dfde127a29: Pull complete Digest: sha256:f2266cbfc127c960fd30e76b7c792dc23b588c0db76233517e1891a4e357d519 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. Step 8 Exit from VM exit Step 9 Delete VM Instance to avoid extra cost: gcloud compute instances delete docker-nginx --zone us-central1-c Now you\u2019ll type Y to continue. Do you want to continue? (Y/n) Summary Now you know how to deploy Docker on Linux VM. Task Find out how to deploy Docker on Mac or Windows?","title":"1.2 Install Docker Engine"},{"location":"ycit019_Module_5_Lab_Docker_basics/#2-working-with-docker-cli","text":"","title":"2 Working with Docker CLI"},{"location":"ycit019_Module_5_Lab_Docker_basics/#21-show-running-containers","text":"Going forward we going to run Docker commands directly from Google Cloud Console. This is possible because Docker is installed on Cloud Console VM for you convenience. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. Step 1 Create a docker container hello-world : docker run hello-world Step 2 Run docker ps to show running containers: docker ps Result The output shows that there are no running containers at the moment. Step 2 Use the command docker ps -a to list all containers including the ones has been stopped: docker ps -a Output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e6db2a24a8e hello-world \"/hello\" 15 minutes ago Exited (0) 15 min dreamy_nobel Review the collumns CONTAINER ID , STATUS , COMMAND , PORTS , NAMES . In the previous section we started one container and the command docker ps -a shows it as Exited . Note You can name your own containers with --name when you use docker run. If you do not provide a name, Docker will generate a random one like the one you have. Question Why Docker names are random? How docker containers named? Step 3 Let\u2019s run the command docker images to show all the images on your local system: docker images As you see, there is only one image that was downloaded from the Docker Hub.","title":"2.1 Show running containers"},{"location":"ycit019_Module_5_Lab_Docker_basics/#23-specify-a-container-main-process","text":"Step 1 Let\u2019s run our own \"hello world\" container. For that we will use the official Ubuntu image : docker run ubuntu /bin/echo 'Hello world' Output: Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:latest Hello world As you see, Docker downloaded the image ubuntu because it was not on the local machine. Step 2 Let\u2019s run the command docker images again: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Step 3 If you run the same \"hello world\" container again, Docker will use a local copy of the image: docker run ubuntu /bin/echo 'Hello world' Output: Hello world Question Compare Ubuntu Docker image with ISO image or with Cloud VM image. Why the size is so different ? Summary Pulling docker images from Docker Hub takes sometime. This time depends on: How large is the image? How fast is the network to Internet ? However, it is still much faster than booting traditional OS with Ubuntu on VM. If image already pulled on local host it takes fraction of a second to start a container. Running application in docker containers considered as a best practice for running CI/CD pipelines as it considerably faster than using VMs and reduce time for deploying a test environments.","title":"2.3 Specify a container main process"},{"location":"ycit019_Module_5_Lab_Docker_basics/#23-specify-an-image-version","text":"Step 1 As you see, Docker has downloaded the ubuntu:latest image. You can see Ubuntu version by running the following command: docker run ubuntu /bin/cat /etc/issue.net Output: Ubuntu 22.04 LTS Let\u2019s say you need a previous Ubuntu LTS release. In this case, you can specify the version you need: docker run ubuntu:14.04 /bin/cat /etc/issue.net Output: Unable to find image 'ubuntu:14.04' locally 14.04: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:14.04 Ubuntu 14.04.4 LTS Step 2 The docker images command should show that we have 3 Ubuntu images downloaded locally: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB ubuntu 14.04 0ccb13bf1954 11 days ago 188 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Tip Running CI/CD pipeline with Docker using latest tag considered as a Bad Practice. Instead consider using: Versioning SHA tagging.","title":"2.3 Specify an image version"},{"location":"ycit019_Module_5_Lab_Docker_basics/#24-run-an-interactive-container","text":"Step 1 Let\u2019s use the ubuntu image to run an interactive bash session and inspect what is running inside our docker image. To achive that we going to use -i and -t flags. The -i is shorthand for --interactive , which instructs Docker to keep stdin open so that we can send commands to the sprocess. The -t flag is short for --tty and allocates a pseudo-TTY or terminal inside of the session. docker run -it ubuntu /bin/bash root@17d8bdeda98e:/# Result We get a bash shell prompt inside of the container. Note Bash prompt is not availabe for all docker images. Step 2 Let's print the system information of the latest Ubuntu image: root@17d8bdeda98e:/# uname -a Linux 17d8bdeda98e 3.19.0-31-generic ... Step 3 Let's verify what Ubuntu version is run by latest image of ubuntu: root@17d8bdeda98e:/# lsb_release -a bash: lsb_release: command not found Failure Why the standard Ubuntu command that checks version of OS is not working as expeced ? Step 4 Let's verify Ubuntu version using alternative way by checking /etc/lsb-release file. root@8cbcbd0fe8d2:/# cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.3 LTS\" Step 5 Let's compare the number of executable binaries availabe inside of the docker image versus Cloud VM that we running our class environment. First, run ls command on /bin and /usr/bin directories inside of the running ubuntu container as well as dpkg --list command that shows total number of installed packages: root@8cbcbd0fe8d2:/# ls /bin | wc -l 294 root@8cbcbd0fe8d2:/# ls /usr/bin | wc -l xx root@eb11cd0b4106:/# dpkg --list | wc -l xx Step 6 Use the exit command or press Ctrl-D to exit the interactive bash session back to Cloud VM. root@eb11cd0b4106:/# exit Step 7 Now run ls command on /bin and /usr/bin directories on Cloud VM that we using as our class environment: cca-user@userx-docker-vm:~$ ls /bin | wc -l 1173 cca-user@userx-docker-vm:~$ ls /usr/bin | wc -l 660 cca-user@userx-docker-vm:~$ dpkg --list | wc -l 463 Result Official Docker container has much less binaries and packages installed vs Ubuntu Cloud Image. Summary Some of the use cases running docker containers in interactive mode are: Troubleshooting containerized applications Deploying and running containerized application on the existing production systems without affecting it. We've also learned that an official Docker \"minimal\" ubuntu image, does not include lsb_release command, as well as many other commands and packages that can be found in Official Ubuntu ISO image . The docker images are ment to contain only required core system commands and functions to make Images as light as possible. That say you can still install required packages using apt-get install , however this may increase size of docker image considerably. Hint While Docker Ubuntu image we used so far or Docker Centos image are very familiar to users and can be good starting point for learning docker containers. Using them in production or development considered as a Bad Practice. This is due those images still considered as heavy and potentially contain a lot more valnurabilities compare to specialized images. To reduce image pull time from docker hub and follow the best secuirity practices consider using specialized images that works well with you underlining code (Node image for NodeJS applications and etc.). Examples of specialized images are: Alpine Linux Node Atomic In fact, not so long ago all the official Docker Images in Docker-Hub has been moved to use Alpine Image . Step 8 Finally let\u2019s check that when the shell process has finished, the container stops: docker ps","title":"2.4 Run an interactive container"},{"location":"ycit019_Module_5_Lab_Docker_basics/#25-run-a-container-in-a-background","text":"Now we know how to connect to running container and execute commands in it. However in most cases you just want run a container in a background so it can do a specific action. Step 1 Run a container in a background using the -d command line argument: docker run -d ubuntu /bin/sh -c \"while true; do date; echo hello world; sleep 1; done\" Result Command should return the container ID. Step 2 Let\u2019s use the docker ps command to see running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac231579e57f ubuntu \"/bin/sh -c 'while tr\" 1 minute ago Up 11 minute evil_golick Note Container id is going to be different in your case Hint Instead of using full container-id when building commands, it is possible simply type first few characters of container-id, to make things nice and easy. Step 3 Let\u2019s use container-id to show the container standard output: docker logs <container-id> Thu Jan 26 00:23:45 UTC 2017 hello world Thu Jan 26 00:23:46 UTC 2017 hello world Thu Jan 26 00:23:47 UTC 2017 hello world ... As you can see, in the docker ps command output, the auto generated container name is evil_golick (your container can have a different name). Step 4 Now, instead of using docker contaier-id use container name to show the container standard output: docker logs <name> Thu Jan 26 00:23:51 UTC 2017 hello world Thu Jan 26 00:23:52 UTC 2017 hello world Thu Jan 26 00:23:53 UTC 2017 hello world ... Step 5 Finally, let\u2019s stop our container: docker stop <name> Step 6 Check, that there are no running containers: docker ps Summary docker logs is a very usefull command to troubleshoot containers, and going to be used very often both for Docker and Kubernertes troubleshooting.","title":"2.5 Run a container in a background"},{"location":"ycit019_Module_5_Lab_Docker_basics/#26-accessing-containers-from-the-internet","text":"Step 1 Let\u2019s run a simple web application. We will use the existing image training/webapp, which contains a Python Flask application: docker run -d -P training/webapp python app.py ... Status: Downloaded newer image for training/webapp:latest 6e88f42d3d853762edcbfe1fe73fdc5c48865275bc6df759b83b0939d5bd2456 In the command above we specified the main process (python app.py), the -d command line argument, which tells Docker to run the container in the background. The -P command line argument tells Docker to map any required network ports inside our container to our host. This allows us to access the web application in the container. Step 2 Use the docker ps command to list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e88f42d3d85 training/webapp \"python app.py\" 3 minutes ago Up 3 minutes 0.0.0.0:32768->5000/tcp determined_torvalds The PORTS column contains the mapped ports. In our case, Docker has exposed port 5000 (the default Python Flask port) on port 32768 (can be different in your case). Step 3 The docker port command shows the exposed port. We will use the container name (determined_torvalds in the example above, it can be different in your case): docker port <name> 5000 0.0.0.0:32768 Step 4 Let\u2019s check that we can access the web application exposed port: curl http://localhost:<port>/ Result Hello world! Step 5 Let\u2019s stop our web application for now: docker stop <name> Step 6 We want to manually specify the local port to expose (-p argument). Let\u2019s use the standard HTTP port 8080. We also want to specify the container name (--name argument): docker run -d -p 8080:5000 --name webapp training/webapp python app.py Step 7 Let\u2019s check that the port 8080 is exposed: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp curl http://localhost/ Result Hello world! Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Result Our web-app can be accessed from Internet!","title":"2.6 Accessing Containers from the Internet"},{"location":"ycit019_Module_5_Lab_Docker_basics/#27-restart-a-container","text":"Step 1 Let\u2019s stop the container with web application: docker stop webapp The main process inside of the container will receive SIGTERM, and after a grace period, SIGKILL. Step 2 You can start the container later using the docker start command: docker start webapp Step 3 Check that the web application works: Step 4 You also can restart the running container using the docker restart command. docker restart webapp Step 4 Run docker ps command and check STATUS field: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS 6e400179070f training/webapp \"python app.py\" 25 minutes ago Up 3 seconds","title":"2.7 Restart a container"},{"location":"ycit019_Module_5_Lab_Docker_basics/#28-ensuring-container-uptime","text":"Docker considers any containers to exit with a non-zero exit code to have crashed. By default a crashed container will remain stopped. Step 1 Start the container that outputs a message and then exits with code 1 to simulate a crash. docker run -d --name restart-default scrapbook/docker-restart-example docker ps -a | grep restart-default CONTAINER ID IMAGE CREATED STATUS NAMES c854289d2f39 scrapbook/docker-restart-example 5 seconds ago Exited 3 sec ago restart-default $ docker logs restart-default Sun Sep 17 20:34:55 UTC 2017 Booting up... Result Container crushed and exited. However, there are several ways to ensure that you container up and running even if it\u2019s restarts. Step 2 The option --restart=on-failure : allows you to say how many times Docker should try again: docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example docker logs restart-3 Thu Apr 20 14:01:27 UTC 2017 Booting up... Thu Apr 20 14:01:28 UTC 2017 Booting up... Thu Apr 20 14:01:29 UTC 2017 Booting up... Thu Apr 20 14:01:31 UTC 2017 Booting up... Step 3 Finally, Docker can always restart a failed container. In this case, Docker will keep trying until the container is explicitly told to stop. docker run -d --name restart-always --restart=always scrapbook/docker-restart-example docker logs restart-always Step 4 After sometime stop running docker container, as it will be keep failing and starting again: docker stop restart-always","title":"2.8 Ensuring Container Uptime"},{"location":"ycit019_Module_5_Lab_Docker_basics/#29-inspect-a-container","text":"Step 1 You can use the docker inspect command to see the configuration and status information for the specified container: docker inspect webapp [ { \"Id\": \"249476631f7d...\", \"Created\": \"2016-08-02T23:42:56.932135327Z\", \"Path\": \"python\", \"Args\": [ \"app.py\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 16055, \"ExitCode\": 0, \"Error\": \"\", ... Step 2 You can specify a filter (-f command line argument) to show only specific elements. For example: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp 172.17.0.2 The command returns the IP address of the container.","title":"2.9 Inspect a container"},{"location":"ycit019_Module_5_Lab_Docker_basics/#210-interacting-with-containers","text":"In some cases using docker log is not enough to undertand issues and you want to login inside of running VM. Also sometimes you package you applicaiton and in order to run it you need to login inside of container and execute and leave it running in background. Below provded few ways to interacting with containers that can help to achive descrined use cases.","title":"2.10 Interacting with containers"},{"location":"ycit019_Module_5_Lab_Docker_basics/#2101-detach-from-interactive-container","text":"In Module, 1.4 Run an interactive container we run an Ubuntu container with -it flag and able directly login inside of the container to interact with it, however after we exited contianer using Ctrl-D or exit command container stopped. However you can exit from Interactive mode without stoping a container. Let's demonstrate how this works: Step 1 Start Ubunu container in interactive mode: docker run -it ubuntu /bin/bash Step 2 Run watch date command inside running container in order to exit date command every 2 seconds. root@1d688a9f4ed4:/# watch date Step 3 Detach from a container and leave it running using the CTRL-p CTRL-q key sequence. Step 4 Verify that Ubuntu container is still running: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS NAMES 1d688a9f4ed4 ubuntu \"/bin/bash\" 1 minutes ago Up 1 minutes admiring_lovelace Result Great you were able to detach from Docker container without stopping it, while it is executing a process in it. What about attaching back to container ? Important CTRL-p CTRL-q sequence key only works if docker contaienr started with -it command!","title":"2.10.1 Detach from Interactive container"},{"location":"ycit019_Module_5_Lab_Docker_basics/#2112-attach-to-a-container","text":"Now let's get back and attach to our running Ubuntu image. For that docker provides docker attach command. docker attach <container name> Every 2.0s: date Mon Sep 18 00:08:57 2017 Summary docker attach attaches your contairs terminal\u2019s standard input, output, and error (or any combination of the 3) to a running container. This allows you to view its ongoing output or to control it interactively, as though the commands were running directly in your terminal.","title":"2.11.2 Attach to a container"},{"location":"ycit019_Module_5_Lab_Docker_basics/#2113-execute-a-process-in-a-container","text":"Step 1 Let verify if webapp container is still running docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp If not running start it with following command: $ docker run -d -p 80:5000 --name webapp training/webapp python app.py other wise skip to next step . Step 2 Use the docker exec command to execute a command in the running container. For example: docker exec webapp ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 26 0.0 0.0 15572 2104 ? Rs 00:12 0:00 ps aux The same command with the -it command line argument can be used to run an interactive session in the container: docker exec -it webapp bash root@249476631f7d:/opt/webapp# ps auxw ps auxw USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 32 0.0 0.0 18144 3064 ? Ss 00:14 0:00 bash root 47 0.0 0.0 15572 2076 ? R+ 00:16 0:00 ps auxw Step 2 Use the exit command or press Ctrl-D to exit the interactive bash session: root@249476631f7d:/opt/webapp# exit Summary docker exec is one of the most usefull docker commands used for troubleshooting containers.","title":"2.11.3 Execute a process in a container"},{"location":"ycit019_Module_5_Lab_Docker_basics/#212-copy-files-tofrom-container","text":"The docker cp command allows you to copy files from the container to the local machine or from the local file system to the container. This command works for a running or stopped container. Step 1 Let\u2019s copy the container\u2019s app.py file to the local machine: docker cp webapp:/opt/webapp/app.py . Step 2 Edit the local app.py file. For example, change the line return 'Hello '+provider+'!' to return 'Hello '+provider+'!!!'. Copy the modified file back and restart the container: docker cp app.py webapp:/opt/webapp/ docker restart webapp Step 3 Check that the modified web application works:: curl http://localhost/ Result `Hello world!!!``","title":"2.12 Copy files to/from container"},{"location":"ycit019_Module_5_Lab_Docker_basics/#212-remove-containers","text":"Now let's clean up the environment and at the same time learn how delete containers. Step 1 First list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 81c4c66baaf9 training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp Step 2 Than try to delete running container using docker rm <container_id> docker rm $container_id Error response from daemon: You cannot remove a running container 81c4c66baaf9. Stop the container before attempting removal or force remove. Failure Docker containers needs to be first stopped or deleted using --force flag. docker rm $container_id -f Alternatively, you can run stop and rm in sequence: docker stop 81c4c66baaf9 docker rm 81c4c66baaf9","title":"2.12 Remove containers"},{"location":"ycit019_Module_5_Lab_Docker_basics/#3-building-images-with-dockerfile","text":"In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. The code for this application is also available in GitHub: https://github.com/Cloud-Architects-Program/ycit019_2022/tree/main/Module5/flask-app","title":"3 Building Images with DockerFile"},{"location":"ycit019_Module_5_Lab_Docker_basics/#31-overview-dockerfile-creation","text":"Step 1 Clone git repo on you laptop: git clone https://github.com/Cloud-Architects-Program/ycit019_2022 cd ~/ycit019/Module5/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\" , \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\" , \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\" , \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\" , \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\" , \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\" , \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\" , \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\" , \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\" , \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==2.0.0 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # Official Python Alpine Base image using Simple Tags # Image contains Python 3 and pip pre-installed, so no need to install them FROM python:3.9.5-alpine3.12 # Specify Working directory WORKDIR /usr/src/app # COPY requirements.txt /usr/src/app/ COPY requirements.txt ./ # Install Python Flask used by the Python app RUN pip install --no-cache-dir -r requirements.txt # Copy files required for the app to run COPY app.py ./ COPY templates/index.html ./templates/ # Make a record that the port number the container should be expose is: EXPOSE 5000 # run the application CMD [ \"python\" , \"./app.py\" ]","title":"3.1 Overview DOCKERFILE Creation"},{"location":"ycit019_Module_5_Lab_Docker_basics/#32-build-a-docker-image","text":"Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. docker build -t <user-name>/myfirstapp . Result Image has been buit Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 8080:5000 --name myfirstapp <user-name>/myfirstapp Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Stop the container and remove it: docker rm -f myfirstapp Summary We've learned a lot of docker commands which are very handy to know both when using Docker and Kubernetes. Next we will learn how to create Docker networks.","title":"3.2 Build a Docker image"},{"location":"ycit019_Module_5_Lab_Docker_basics/#4-docker-networking","text":"","title":"4 Docker Networking"},{"location":"ycit019_Module_5_Lab_Docker_basics/#41-docker-networking-basics","text":"Step 1: The Docker Network Command The docker network command is the main command for configuring and managing container networks. Run the docker network command from the first terminal. docker network Usage: docker network COMMAND Manage networks Options: --help Print usage Commands: connect Connect a container to a network create Create a network disconnect Disconnect a container from a network inspect Display detailed information on one or more networks ls List networks prune Remove all unused networks rm Remove one or more networks Run 'docker network COMMAND --help' for more information on a command. The command output shows how to use the command as well as all of the docker network sub-commands. As you can see from the output, the docker network command allows you to create new networks, list existing networks, inspect networks, and remove networks. It also allows you to connect and disconnect containers from networks. Step 2 Run a docker network ls command to view existing container networks on the current Docker host. docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local The output above shows the container networks that are created as part of a standard installation of Docker. New networks that you create will also show up in the output of the docker network ls command. You can see that each network gets a unique ID and NAME . Each network is also associated with a single driver. Notice that the \"bridge\" network and the \"host\" network have the same name as their respective drivers. Step 3: The docker network inspect command is used to view network configuration details. These details include; name, ID, driver, IPAM driver, subnet info, connected containers, and more. Use docker network inspect <network> to view configuration details of the container networks on your Docker host. The command below shows the details of the network called bridge . docker network inspect bridge [ { \"Name\": \"bridge\", \"Id\": \"3430ad6f20bf1486df2e5f64ddc93cc4ff95d81f59b6baea8a510ad500df2e57\", \"Created\": \"2017-04-03T16:49:58.6536278Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Containers\": {}, \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" }, \"Labels\": {} } ] Note The syntax of the docker network inspect command is docker network inspect <network> , where <network> can be either network name or network ID. In the example above we are showing the configuration details for the network called \"bridge\". Do not confuse this with the \"bridge\" driver. Step 4 Now, list Docker supported network driver plugins. For that run docker info command, that shows a lot of interesting information about a Docker installation. Run the docker info command and locate the list of network plugins. docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ee-3 Storage Driver: aufs <Snip> Plugins: Volume: local Network: bridge host macvlan null overlay Swarm: inactive Runtimes: runc <Snip> The output above shows the bridge , host , macvlan , null , and overlay drivers. Summary We've quickly reviewed available docker networking commands as well as found what drivers current docker setup supports.","title":"4.1 Docker Networking Basics"},{"location":"ycit019_Module_5_Lab_Docker_basics/#42-default-bridge-network","text":"Every clean installation of Docker comes with a pre-built network called Default bridge network . Let's explore in more details how it works. Step 1 Verify this with the docker network ls . docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local Result The output above shows that the bridge network is associated with the bridge driver. It's important to note that the network and the driver are connected, but they are not the same. In this example the network and the driver have the same name - but they are not the same thing! The output above also shows that the bridge network is scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the bridge driver - the bridge driver provides single-host networking. All networks created with the bridge driver are based on a Linux bridge (a.k.a. a virtual switch). Step 5 Start webapp in Default bridge network docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 6 Check that the webapp and db containers are running: Command: docker ps","title":"4.2 Default bridge network"},{"location":"ycit019_Module_5_Lab_Docker_basics/#43-user-defined-private-networks","text":"So far we\u2019ve learned how Docker networking works with Docker default bridge network . With the introduction of user-defined networking in Docker 1.9, it is now possible to create multiple Docker bridges to allow network segregation within the same host or multi-host networking to allow communicate Docker containers between hosts. The commands are available through the Docker Engine CLI are: docker network create docker network connect docker network ls docker network rm docker network disconnect docker network inspect Let's demonstrate how to create a custom bridge network. Step 1 By default, Docker runs containers in the bridge network. You may want to isolate one or more containers in a separate network. Let\u2019s create a new network: docker network create my-network \\ -d bridge \\ --subnet 172.19.0.0/16 The -d bridge command line argument specifies the bridge network driver and the --subnet command line argument specifies the network segment in CIDR format. If you do not specify a subnet when creating a network, then Docker assigns a subnet automatically, so it is a good idea to specify a subnet to avoid potential conflicts with the existing networks. Below are some other options that are available with the bridge Driver: com.docker.network.bridge.enable_ip_masquerade: This instructs the Docker host to hide or masquerade all containers in this network behind the Docker host's interfaces if the container attempts to route off the local host . com.docker.network.bridge.name: This is the name you wish to give to the bridge. com.docker.network.bridge.enable_icc: This turns on or off Inter-Container Connectivity (ICC) mode for the bridge. com.docker.network.bridge.host_binding_ipv4: This defines the host interface that should be used for port binding. com.docker.network.driver.mtu: This sets MTU for containers attached to this bridge. Step 2 To check that the new network is created, execute docker network ls: docker network ls NETWORK ID NAME DRIVER SCOPE d428e49e4869 bridge bridge local 0d1f78528cc5 host host local 56ef0481820d my-network bridge local 4a07cef84617 none null local Step 3 Let\u2019s inspect the new network: docker network inspect my-network [ { \"Name\": \"my-network\", \"Id\": \"56ef0481820d...\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"172.19.0.0/16\" } ] }, \"Internal\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ] Step 4 As expected, there are no containers connected to the my-network. Let\u2019s recreate the db container in the my-network: docker rm -f db docker run -d --network=my-network --name db training/postgres Step 5 Inspect the my-network again: docker network inspect my-network Output: \"Containers\": { \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... As you see, the db container is connected to the my-network and has 172.19.0.2 address. Step 6 Let\u2019s start an interactive session in the db container and ping the IP address of the webapp again: Note Quick reminder how to locate webapp ip: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp docker exec -it db bash Once inside of container run: root@c3afff20019a:/# ping -c 1 172.17.0.3 PING 172.17.0.3 (172.17.0.3) 56(84) bytes of data. --- 172.17.0.3 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms As expected, the webapp container is no longer accessible from the db container, because they are connected to different networks. Summary Using Multi-host networking provides network isolation within a Docker host via network namepsaces. This is can be used if you want to deploy different applications on same host for isolation or resource duplicate prevention. Step 7 Let\u2019s connect the webapp container to the my-network: docker network connect my-network webapp Step 8 Check that the webapp container now is connected to the my-network: docker network inspect my-network Output: ... \"Containers\": { \"62ed4a627356...\": { \"Name\": \"webapp\", \"EndpointID\": \"ae95b0103bbc...\", \"MacAddress\": \"02:42:ac:12:00:03\", \"IPv4Address\": \"172.19.0.3/16\", \"IPv6Address\": \"\" }, \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... The output shows that two containers are connected to the my-network and the webapp container has 172.19.0.3 address in that network. Step 9 Check that the webapp container is accessible from the db container using its new IP address: docker exec -it db bash root@c3afff20019a:/# ping -c 1 172.19.0.3 PING 172.19.0.3 (172.19.0.3) 56(84) bytes of data. 64 bytes from 172.19.0.3: icmp_seq=1 ttl=64 time=0.136 ms --- 172.19.0.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms Success As expected containers can communicate with each other. Step 10 You can now remove the existing container. You should stop the container before removing it. Alternatively you can use the -f command line argument: docker rm -f webapp docker rm -f db docker network rm my-network Hint Use below command to delete running containers in bulk : docker rm -f $(docker ps -q) Summary It is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses","title":"4.3 User-defined Private Networks"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/","text":"Lab 4 Managing Docker Images Objective: Docker Storage Learn to build docker images using Dockerfiles. Store images in Docker Hub Learn alternative registry solutions (GCR) Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1 Persistant Volumes \u00b6 1.1 Storage driver \u00b6 We've discussed several Storage drivers (graphdrivers) during the class. Let's find out what graphdriver is running in our Lab environment. docker info | grep Storage Storage Driver: overlay2 Result Our Classroom is running auoverlay2fs storage driver. Summary Systems runnng Ubuntu or Debian ,going to run overlay2 storage driver by default. Prior to that it was aufs storage driver 1.2 Persisting Data Using Volumes \u00b6 Docker Volumes are created and assigned when containers are started. Data Volumes allow you to map a host directory to a container for sharing data. This mapping is bi-directional. It allows data stored on the host to be accessed from within the container. It also means data saved by the process inside the container is persisted on the host. 1.2.1 Create and manage volumes \u00b6 Step 1 Create a volume: docker volume create --name my-vol Step 2 List volumes: docker volume ls Output: local my-vol Step 3 Inspect a volume: docker volume inspect my-vol [ { \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\", \"Name\": \"my-vol\", \"Options\": {}, \"Scope\": \"local\" } ] Step 3 Add some data to the Mountpoint of the volume: sudo touch /var/lib/docker/volumes/my-vol/_data/test_vol sudo ls /var/lib/docker/volumes/my-vol/_data/ Step 4 Create a container busybox alpine image and attach created my-vol volume in to it: docker run -it -v my-vol:/world busybox / # ls /world test_vol / # Result Volume is mounted and test_vol file is under /world folder as expected Step 5 Try to delete the volume: docker volume rm my-vol Error response from daemon: unable to remove volume: remove my-vol: volume is in use - [6ef3055b516b306847150af8fcea796c02cd90578967802ac29c39d3a2c90102] Failure Deleting container that is attached is not permited. However you can delete with -f option Step 5 Busybox container stopped, howerver it is not deleted. Let's locate stopped busybox container and delete it: docker ps -a | grep busybox docker rm $docker_id Step 6 You can now delete my-vol Note Volume is still avaiable if needed to be reattached any time docker volume ls docker volume rm my-vol docker volume ls Summary Volumes can be craeted and managed separately from containers. 1.2.2 Start a container with a volume \u00b6 If you start a container with a volume that does not yet exist, Docker creates the volume for you. Step 1 Add a data volume to a container: docker run -d -P --name webapp -v /webapp training/webapp python app.py Result Command started a new container and created a new volume inside the container at /webapp. Step 2 Locate the volume on the host using the docker inspect command: docker inspect webapp | grep -A9 Mounts ``` **Output:** ``` \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\", \"Source\": \"/var/lib/docker/volumes/39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d/_data\", \"Destination\": \"/webapp\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" Step 3 List container docker volume ls Output: DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Step 5 Alternatively, you can specify a host directory you want to use as a data volume: mkdir db docker run -d --name db -v ~/db:/db training/postgres Step 2 Start an interactive session in the db container and create a new file in the /db directory: docker exec -it db bash Type inside docker containers console: root@9a7a4fbcc929:/# cd /db root@9a7a4fbcc929:/db# touch hello_from_db_container root@9a7a4fbcc929:/db# exit Step 4 Check that the local db directory contains the new file: ls db hello_from_db_container Step 5 Check that the data volume is persistent. Remove the db container: docker rm -f db Step 6 Create the db container again: docker run -d --name db -v ~/db:/db training/postgres Step 7 Check that its /db directory contains the hello_from_db_container file: docker exec -it db bash Run commands inside container: root@47a60c01590e:/# ls /db hello_from_db_container root@47a60c01590e:/# exit 2 Distributing Docker images with Container Registry \u00b6 In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: https://github.com/Cloud-Architects-Program/ycit019_2022/tree/main/Module5/flask-app 1.1 Create DOCKERFILE \u00b6 Step 1 Clone git repo on you laptop: git clone https://github.com/Cloud-Architects-Program/ycit019_2022 cd ~/ycit019_2022/Module5/flask-app/ git pull Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\" , \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\" , \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\" , \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\" , \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\" , \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\" , \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\" , \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\" , \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\" , \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==2.0.0 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # Official Python Alpine Base image using Simple Tags # Image contains Python 3 and pip pre-installed, so no need to install them FROM python:3.9.5-alpine3.12 # Specify Working directory WORKDIR /usr/src/app # COPY requirements.txt /usr/src/app/ COPY requirements.txt ./ # Install Python Flask used by the Python app RUN pip install --no-cache-dir -r requirements.txt # Copy files required for the app to run COPY app.py ./ COPY templates/index.html ./templates/ # Make a record that the port number the container should be expose is: EXPOSE 5000 # run the application CMD [ \"python\" , \"./app.py\" ] 1.2 Build a Docker image \u00b6 Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step to your own Docker Hub. docker build -t <Docker-hub-user-name>/myfirstapp . Result Image has been buit Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 8080:5000 --name myfirstapp <Docker-hub-user-name>/myfirstapp Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Stop the container and remove it: docker rm -f myfirstapp 1.2.2 Publish Docker Image to Docker Hub \u00b6 One of the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <Docker-hub-user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry Note Image Tag of the created myfirstapp : docker images Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag <Docker-hub-user-name>/myfirstapp:v1 docker push <Docker-hub-user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <Docker-hub-user-name>/myfirstapp:latest docker pull <Docker-hub-user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <Docker-hub-user-name>/myfirstapp:v1 1.2.3 Pushing images to gcr.io \u00b6 In a similar manner we need to tag the image to prepare it to be pushed to gcr.io. We just need to change the registry, which is for gcr.io formatted as gcr.io/PROJECT_ID. Step 1 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 2 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 3 Tag the image: Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag gcr.io/${PROJECT_ID}/myfirstapp:v1 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/myfirstapp:v1 Step 4 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 1.2.3 Pushing images to Local Repository \u00b6 First, we need to spin up a local docker registry. This could be a use case if you want to deploy basic registry On-Prem. This registry will luck security features such as Authentication, SSL, scanning. If you interested to use Enterprise ready solution On-Prem consider: Jfrog Artifactory, RedHa's Clair, Docker Enterprise or open source CNCF project Harbor. Step 1 Deploy local registry docker run -d -p 5000:5000 --name registry registry:2.7.1 Step 2 In order to upload an image to a registry, we need to tag it properly Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag localhost:5000/myfirstapp:v1 Step 3 Now that we have an image tagged correctly, we can push it to our local registry docker push localhost:5000/myfirstapp:v1 Step 4 Let\u2019s now delete the local image, and pull it again from the local registry To delete the image, we need to first remove the container that depends on that image. Run docker ps and get the Container_ID for the container that uses myfirstapp:v1 Kill and delete that container by running the following command, but make sure to replace CONTAINER_ID, with the actual ID. docker rm CONTAINER_ID Result: The command will print back the container ID, which is an indication it was successful. Step 5 Run docker images to validate docker images Step 6 Now we can delete the docker image docker rmi localhost:5000/myfirstapp:v1 Step 7 Although the image is deleted locally, it is still in the registry and we can pull it back, or use it to deploy containers. docker run -dp 8080:5000 --name myfirstapp localhost:5000/myfirstapp:v1 Run docker images again to check how the image is available locally again. docker images Step 8 Cleanup: docker rm -f myfirstapp 3 Follow Docker Best Practices \u00b6 3.1 Inspecting Dockerfiles with dockle \u00b6 Dockle - Container Image Linter for Security, Helping build the Best-Practice Docker Image, Easy to start Dockle helps you: Build Best Practice Docker images Build secure Docker images Checkpoints includes CIS Benchmarks Step 1 Install Dockle $ VERSION=$( curl --silent \"https://api.github.com/repos/goodwithtech/dockle/releases/latest\" | \\ grep '\"tag_name\":' | \\ sed -E 's/.*\"v([^\"]+)\".*/\\1/' \\ ) && curl -L -o dockle.deb https://github.com/goodwithtech/dockle/releases/download/v${VERSION}/dockle_${VERSION}_Linux-64bit.deb $ sudo dpkg -i dockle.deb && rm dockle.deb Step 2 Experiment with existing applications we've created in the class: $ dockle [YOUR_IMAGE_NAME] e.g. dockle archy/myfirstapp output: WARN - CIS-DI-0001: Create a user for the container * Last user should not be root WARN - DKL-DI-0006: Avoid latest tag * Avoid 'latest' tag INFO - CIS-DI-0005: Enable Content trust for Docker * export DOCKER_CONTENT_TRUST=1 before docker pull/build INFO - CIS-DI-0006: Add HEALTHCHECK instruction to the container image * not found HEALTHCHECK statement INFO - DKL-LI-0003: Only put necessary files * Suspicious directory : tmp 3.2 Scan images with Trivy \u00b6 Trivy (tri pronounced like trigger, vy pronounced like envy) is a simple and comprehensive vulnerability scanner for containers and other artifacts. A software vulnerability is a glitch, flaw, or weakness present in the software or in an Operating System. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and application dependencies (Bundler, Composer, npm, yarn, etc.). Trivy is easy to use. Just install the binary and you're ready to scan. All you need to do for scanning is to specify a target such as an image name of the container. Step 1 Install Trivy sudo apt-get install wget apt-transport-https gnupg lsb-release wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add - echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list sudo apt-get update sudo apt-get install trivy Step 2 Specify an image name (and a tag). $ trivy image [YOUR_IMAGE_NAME] For example: $ trivy image python:3.4-alpine 2019-05-16T01:20:43.180+0900 INFO Updating vulnerability database... 2019-05-16T01:20:53.029+0900 INFO Detecting Alpine vulnerabilities... python:3.4-alpine3.9 (alpine 3.9.2) =================================== Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0) +---------+------------------+----------+-------------------+---------------+--------------------------------+ | LIBRARY | VULNERABILITY ID | SEVERITY | INSTALLED VERSION | FIXED VERSION | TITLE | +---------+------------------+----------+-------------------+---------------+--------------------------------+ | openssl | CVE-2019-1543 | MEDIUM | 1.1.1a-r1 | 1.1.1b-r1 | openssl: ChaCha20-Poly1305 | | | | | | | with long nonces | +---------+------------------+----------+-------------------+---------------+--------------------------------+ Step 3 Explore local images in your environment. 4 Docker Compose \u00b6 In this module, will guide you through the process of building a multi-container application using docker compose. The application code is available at GitHub: https://github.com/Cloud-Architects-Program/ycit019_2022 4.1 Deploy Guestbook app with Compose \u00b6 Let\u2019s build another application. This time we going to create famous Guestbook application. Guestbook consists of three services. A redis-master node, a set of redis-slave that can be scaled and find the redis-master via its DNS name. And a PHP frontend that exposes itself on port 80. The resulting application allows you to leave short messages which are stored in the redis cluster. Step 1 Change directory to the guestbook cd ~/ycit019_2022/Module6/guestbook/ ls Step 2 Let\u2019s review the docker-guestbook.yml file version: \"2\" services: redis-master: image: gcr.io/google_containers/redis:e2e ports: - \"6379\" redis-slave: image: gcr.io/google_samples/gb-redisslave:v1 ports: - \"6379\" environment: - GET_HOSTS_FROM=dns frontend: image: gcr.io/google-samples/gb-frontend:v4 ports: - \"80:80\" environment: - GET_HOSTS_FROM=dns Step 3 Let\u2019s run docker-guestbook.yml with compose export LD_LIBRARY_PATH=/usr/local/lib docker-compose -f docker-guestbook.yml up -d Creating network \"examples_default\" with the default driver Creating examples_redis-slave_1 Creating examples_frontend_1 Creating examples_redis-master_1 Note -d - Detached mode: Run containers in the background, print new container names. -f - Specify an alternate compose file (default: docker-compose.yml) Step 4 Check that all containers are running: docker ps CONTAINER ID IMAGE COMMAND d1006d1beee5 gcr.io/google-samples/gb-frontend:v4 \"apache2-foreground\" fb3a15fde23f gcr.io/google_containers/redis:e2e \"redis-server /etc...\" 326b94d4cdd7 gcr.io/google_samples/gb-redisslave:v1 \"/entrypoint.sh /b...\" Step 5 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Success Nice you now have compose stuck up and running! Step 6 Cleanup environment: docker-compose -f docker-guestbook.yml down Stopping guestbook_frontend_1 ... done Stopping guestbook_redis-master_1 ... done Stopping guestbook_redis-slave_1 ... done Removing guestbook_frontend_1 ... done Removing guestbook_redis-master_1 ... done Removing guestbook_redis-slave_1 ... done Removing network guestbook_default Congratulations You are now docker expert! We were able to start microservices application with docker compose. Summary We've learned how to use docker-compose v2. In the assignement you will be using docker-compose v3 Read the Docker-Compose documentation on new syntax.","title":"Module 6 Lab - Advanced Docker"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#1-persistant-volumes","text":"","title":"1 Persistant Volumes"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#11-storage-driver","text":"We've discussed several Storage drivers (graphdrivers) during the class. Let's find out what graphdriver is running in our Lab environment. docker info | grep Storage Storage Driver: overlay2 Result Our Classroom is running auoverlay2fs storage driver. Summary Systems runnng Ubuntu or Debian ,going to run overlay2 storage driver by default. Prior to that it was aufs storage driver","title":"1.1 Storage driver"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#12-persisting-data-using-volumes","text":"Docker Volumes are created and assigned when containers are started. Data Volumes allow you to map a host directory to a container for sharing data. This mapping is bi-directional. It allows data stored on the host to be accessed from within the container. It also means data saved by the process inside the container is persisted on the host.","title":"1.2 Persisting Data Using Volumes"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#121-create-and-manage-volumes","text":"Step 1 Create a volume: docker volume create --name my-vol Step 2 List volumes: docker volume ls Output: local my-vol Step 3 Inspect a volume: docker volume inspect my-vol [ { \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\", \"Name\": \"my-vol\", \"Options\": {}, \"Scope\": \"local\" } ] Step 3 Add some data to the Mountpoint of the volume: sudo touch /var/lib/docker/volumes/my-vol/_data/test_vol sudo ls /var/lib/docker/volumes/my-vol/_data/ Step 4 Create a container busybox alpine image and attach created my-vol volume in to it: docker run -it -v my-vol:/world busybox / # ls /world test_vol / # Result Volume is mounted and test_vol file is under /world folder as expected Step 5 Try to delete the volume: docker volume rm my-vol Error response from daemon: unable to remove volume: remove my-vol: volume is in use - [6ef3055b516b306847150af8fcea796c02cd90578967802ac29c39d3a2c90102] Failure Deleting container that is attached is not permited. However you can delete with -f option Step 5 Busybox container stopped, howerver it is not deleted. Let's locate stopped busybox container and delete it: docker ps -a | grep busybox docker rm $docker_id Step 6 You can now delete my-vol Note Volume is still avaiable if needed to be reattached any time docker volume ls docker volume rm my-vol docker volume ls Summary Volumes can be craeted and managed separately from containers.","title":"1.2.1  Create and manage volumes"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#122-start-a-container-with-a-volume","text":"If you start a container with a volume that does not yet exist, Docker creates the volume for you. Step 1 Add a data volume to a container: docker run -d -P --name webapp -v /webapp training/webapp python app.py Result Command started a new container and created a new volume inside the container at /webapp. Step 2 Locate the volume on the host using the docker inspect command: docker inspect webapp | grep -A9 Mounts ``` **Output:** ``` \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\", \"Source\": \"/var/lib/docker/volumes/39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d/_data\", \"Destination\": \"/webapp\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" Step 3 List container docker volume ls Output: DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Step 5 Alternatively, you can specify a host directory you want to use as a data volume: mkdir db docker run -d --name db -v ~/db:/db training/postgres Step 2 Start an interactive session in the db container and create a new file in the /db directory: docker exec -it db bash Type inside docker containers console: root@9a7a4fbcc929:/# cd /db root@9a7a4fbcc929:/db# touch hello_from_db_container root@9a7a4fbcc929:/db# exit Step 4 Check that the local db directory contains the new file: ls db hello_from_db_container Step 5 Check that the data volume is persistent. Remove the db container: docker rm -f db Step 6 Create the db container again: docker run -d --name db -v ~/db:/db training/postgres Step 7 Check that its /db directory contains the hello_from_db_container file: docker exec -it db bash Run commands inside container: root@47a60c01590e:/# ls /db hello_from_db_container root@47a60c01590e:/# exit","title":"1.2.2 Start a container with a volume"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#2-distributing-docker-images-with-container-registry","text":"In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: https://github.com/Cloud-Architects-Program/ycit019_2022/tree/main/Module5/flask-app","title":"2 Distributing Docker images with Container Registry"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#11-create-dockerfile","text":"Step 1 Clone git repo on you laptop: git clone https://github.com/Cloud-Architects-Program/ycit019_2022 cd ~/ycit019_2022/Module5/flask-app/ git pull Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\" , \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\" , \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\" , \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\" , \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\" , \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\" , \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\" , \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\" , \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\" , \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==2.0.0 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # Official Python Alpine Base image using Simple Tags # Image contains Python 3 and pip pre-installed, so no need to install them FROM python:3.9.5-alpine3.12 # Specify Working directory WORKDIR /usr/src/app # COPY requirements.txt /usr/src/app/ COPY requirements.txt ./ # Install Python Flask used by the Python app RUN pip install --no-cache-dir -r requirements.txt # Copy files required for the app to run COPY app.py ./ COPY templates/index.html ./templates/ # Make a record that the port number the container should be expose is: EXPOSE 5000 # run the application CMD [ \"python\" , \"./app.py\" ]","title":"1.1 Create DOCKERFILE"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#12-build-a-docker-image","text":"Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step to your own Docker Hub. docker build -t <Docker-hub-user-name>/myfirstapp . Result Image has been buit Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 8080:5000 --name myfirstapp <Docker-hub-user-name>/myfirstapp Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Stop the container and remove it: docker rm -f myfirstapp","title":"1.2 Build a Docker image"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#122-publish-docker-image-to-docker-hub","text":"One of the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <Docker-hub-user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry Note Image Tag of the created myfirstapp : docker images Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag <Docker-hub-user-name>/myfirstapp:v1 docker push <Docker-hub-user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <Docker-hub-user-name>/myfirstapp:latest docker pull <Docker-hub-user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <Docker-hub-user-name>/myfirstapp:v1","title":"1.2.2 Publish Docker Image to Docker Hub"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#123-pushing-images-to-gcrio","text":"In a similar manner we need to tag the image to prepare it to be pushed to gcr.io. We just need to change the registry, which is for gcr.io formatted as gcr.io/PROJECT_ID. Step 1 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 2 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 3 Tag the image: Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag gcr.io/${PROJECT_ID}/myfirstapp:v1 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/myfirstapp:v1 Step 4 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"1.2.3 Pushing images to gcr.io"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#123-pushing-images-to-local-repository","text":"First, we need to spin up a local docker registry. This could be a use case if you want to deploy basic registry On-Prem. This registry will luck security features such as Authentication, SSL, scanning. If you interested to use Enterprise ready solution On-Prem consider: Jfrog Artifactory, RedHa's Clair, Docker Enterprise or open source CNCF project Harbor. Step 1 Deploy local registry docker run -d -p 5000:5000 --name registry registry:2.7.1 Step 2 In order to upload an image to a registry, we need to tag it properly Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag localhost:5000/myfirstapp:v1 Step 3 Now that we have an image tagged correctly, we can push it to our local registry docker push localhost:5000/myfirstapp:v1 Step 4 Let\u2019s now delete the local image, and pull it again from the local registry To delete the image, we need to first remove the container that depends on that image. Run docker ps and get the Container_ID for the container that uses myfirstapp:v1 Kill and delete that container by running the following command, but make sure to replace CONTAINER_ID, with the actual ID. docker rm CONTAINER_ID Result: The command will print back the container ID, which is an indication it was successful. Step 5 Run docker images to validate docker images Step 6 Now we can delete the docker image docker rmi localhost:5000/myfirstapp:v1 Step 7 Although the image is deleted locally, it is still in the registry and we can pull it back, or use it to deploy containers. docker run -dp 8080:5000 --name myfirstapp localhost:5000/myfirstapp:v1 Run docker images again to check how the image is available locally again. docker images Step 8 Cleanup: docker rm -f myfirstapp","title":"1.2.3 Pushing images to Local Repository"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#3-follow-docker-best-practices","text":"","title":"3 Follow Docker Best Practices"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#31-inspecting-dockerfiles-with-dockle","text":"Dockle - Container Image Linter for Security, Helping build the Best-Practice Docker Image, Easy to start Dockle helps you: Build Best Practice Docker images Build secure Docker images Checkpoints includes CIS Benchmarks Step 1 Install Dockle $ VERSION=$( curl --silent \"https://api.github.com/repos/goodwithtech/dockle/releases/latest\" | \\ grep '\"tag_name\":' | \\ sed -E 's/.*\"v([^\"]+)\".*/\\1/' \\ ) && curl -L -o dockle.deb https://github.com/goodwithtech/dockle/releases/download/v${VERSION}/dockle_${VERSION}_Linux-64bit.deb $ sudo dpkg -i dockle.deb && rm dockle.deb Step 2 Experiment with existing applications we've created in the class: $ dockle [YOUR_IMAGE_NAME] e.g. dockle archy/myfirstapp output: WARN - CIS-DI-0001: Create a user for the container * Last user should not be root WARN - DKL-DI-0006: Avoid latest tag * Avoid 'latest' tag INFO - CIS-DI-0005: Enable Content trust for Docker * export DOCKER_CONTENT_TRUST=1 before docker pull/build INFO - CIS-DI-0006: Add HEALTHCHECK instruction to the container image * not found HEALTHCHECK statement INFO - DKL-LI-0003: Only put necessary files * Suspicious directory : tmp","title":"3.1 Inspecting Dockerfiles with dockle"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#32-scan-images-with-trivy","text":"Trivy (tri pronounced like trigger, vy pronounced like envy) is a simple and comprehensive vulnerability scanner for containers and other artifacts. A software vulnerability is a glitch, flaw, or weakness present in the software or in an Operating System. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and application dependencies (Bundler, Composer, npm, yarn, etc.). Trivy is easy to use. Just install the binary and you're ready to scan. All you need to do for scanning is to specify a target such as an image name of the container. Step 1 Install Trivy sudo apt-get install wget apt-transport-https gnupg lsb-release wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add - echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list sudo apt-get update sudo apt-get install trivy Step 2 Specify an image name (and a tag). $ trivy image [YOUR_IMAGE_NAME] For example: $ trivy image python:3.4-alpine 2019-05-16T01:20:43.180+0900 INFO Updating vulnerability database... 2019-05-16T01:20:53.029+0900 INFO Detecting Alpine vulnerabilities... python:3.4-alpine3.9 (alpine 3.9.2) =================================== Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0) +---------+------------------+----------+-------------------+---------------+--------------------------------+ | LIBRARY | VULNERABILITY ID | SEVERITY | INSTALLED VERSION | FIXED VERSION | TITLE | +---------+------------------+----------+-------------------+---------------+--------------------------------+ | openssl | CVE-2019-1543 | MEDIUM | 1.1.1a-r1 | 1.1.1b-r1 | openssl: ChaCha20-Poly1305 | | | | | | | with long nonces | +---------+------------------+----------+-------------------+---------------+--------------------------------+ Step 3 Explore local images in your environment.","title":"3.2 Scan images with Trivy"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#4-docker-compose","text":"In this module, will guide you through the process of building a multi-container application using docker compose. The application code is available at GitHub: https://github.com/Cloud-Architects-Program/ycit019_2022","title":"4 Docker Compose"},{"location":"ycit019_Module_6_Lab_Advanced_Docker/#41-deploy-guestbook-app-with-compose","text":"Let\u2019s build another application. This time we going to create famous Guestbook application. Guestbook consists of three services. A redis-master node, a set of redis-slave that can be scaled and find the redis-master via its DNS name. And a PHP frontend that exposes itself on port 80. The resulting application allows you to leave short messages which are stored in the redis cluster. Step 1 Change directory to the guestbook cd ~/ycit019_2022/Module6/guestbook/ ls Step 2 Let\u2019s review the docker-guestbook.yml file version: \"2\" services: redis-master: image: gcr.io/google_containers/redis:e2e ports: - \"6379\" redis-slave: image: gcr.io/google_samples/gb-redisslave:v1 ports: - \"6379\" environment: - GET_HOSTS_FROM=dns frontend: image: gcr.io/google-samples/gb-frontend:v4 ports: - \"80:80\" environment: - GET_HOSTS_FROM=dns Step 3 Let\u2019s run docker-guestbook.yml with compose export LD_LIBRARY_PATH=/usr/local/lib docker-compose -f docker-guestbook.yml up -d Creating network \"examples_default\" with the default driver Creating examples_redis-slave_1 Creating examples_frontend_1 Creating examples_redis-master_1 Note -d - Detached mode: Run containers in the background, print new container names. -f - Specify an alternate compose file (default: docker-compose.yml) Step 4 Check that all containers are running: docker ps CONTAINER ID IMAGE COMMAND d1006d1beee5 gcr.io/google-samples/gb-frontend:v4 \"apache2-foreground\" fb3a15fde23f gcr.io/google_containers/redis:e2e \"redis-server /etc...\" 326b94d4cdd7 gcr.io/google_samples/gb-redisslave:v1 \"/entrypoint.sh /b...\" Step 5 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Success Nice you now have compose stuck up and running! Step 6 Cleanup environment: docker-compose -f docker-guestbook.yml down Stopping guestbook_frontend_1 ... done Stopping guestbook_redis-master_1 ... done Stopping guestbook_redis-slave_1 ... done Removing guestbook_frontend_1 ... done Removing guestbook_redis-master_1 ... done Removing guestbook_redis-slave_1 ... done Removing network guestbook_default Congratulations You are now docker expert! We were able to start microservices application with docker compose. Summary We've learned how to use docker-compose v2. In the assignement you will be using docker-compose v3 Read the Docker-Compose documentation on new syntax.","title":"4.1 Deploy Guestbook app with Compose"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/","text":"Deploy Kubernetes \u00b6 In this Lab, we are going to: Deploy Single Node Kubernetes cluster using kubeadm on a Google Compute Engine node Deploy an application to Kubernetes 1. Deploy Kubernetes and Calico with Kubeadm \u00b6 In general to deploy Kubernetes with kubeadm it is required to use following official Kubernetes documentation . Step 1 Set the Project ID in Environment Variable: export PROJECT_ID=<project_id> Here is how you can find you project_ID: Set the project ID as default gcloud config set project $PROJECT_ID 1.1 Create a VM, and ssh into it \u00b6 Step 1 Create the Compute instance The compute instances in this lab will be provisioned using Ubuntu Server 20.04, which has good support for the containerd container runtime. gcloud compute instances create k8s-cluster \\ --zone us-central1-c \\ --machine-type=e2-standard-4 \\ --can-ip-forward \\ --image-family ubuntu-2004-lts \\ --image-project ubuntu-os-cloud Step 2 SSH in to VM where we going to install Kubernetes: gcloud compute ssh --zone \"us-central1-c\" \"k8s-cluster\" 1.2 Install Containerd \u00b6 Docker has been deprecated from Kubernetes starting K8s 1.24, so we will need to intall containerd instaed Step 1 Update the apt package index and install containerd: sudo su apt update apt install containerd Pres Y, to install packages. Step 2 Enable systemd to start on reboot: systemctl enable containerd systemctl start containerd systemctl status containerd Output: \u25cf containerd.service - containerd container runtime Loaded: loaded (/lib/systemd/system/containerd.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2022-05-18 21:33:39 UTC; 8s ago Docs: https://containerd.io Main PID: 2106 (containerd) Tasks: 15 Memory: 23.3M CGroup: /system.slice/containerd.service \u2514\u25002106 /usr/bin/containerd Step 3 To interact with containerd Install nerdctl: wget https://github.com/containerd/nerdctl/releases/download/v0.18.0/nerdctl-0.18.0-linux-amd64.tar.gz tar zxvf nerdctl-0.18.0-linux-amd64.tar.gz nerdctl mv nerdctl /usr/local/bin Step 4 Install CNI Plugins to test containerd can start containers: wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz mkdir -p /opt/cni/bin/ tar zxvf cni-plugins-linux-amd64-v1.1.1.tgz -C /opt/cni/bin/ Step 5 Run a Docker like command with Nerdctl to start test ubuntu container: See Nerdctl CLI reference here nerdctl run -d ubuntu bash nerdctl ps -a Sucess We can see our docker container been started few seconds ago. 1.3 Let iptables see bridged traffic \u00b6 cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # sysctl params required by setup, params persist across reboots cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # Apply sysctl params without reboot sudo sysctl --system 1.4 Install kubeadm and prerequisite packages on each node \u00b6 The next step is to install kubeadm and prerequisite packages as showed here. Step 1 Deploy kubeadm and prerequisite packages Update the apt package index and install packages needed to use the Kubernetes apt repository: apt-get update && apt-get install -y apt-transport-https ca-certificates curl Download the Google Cloud public signing key: sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg Add the Kubernetes apt repository: echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list Update apt package index, install kubelet, kubeadm and kubectl, and pin their version: sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl Step 2 Verify kubeadm version kubeadm version Output: kubeadm version: &version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.0\", GitCommit:\"4ce5a8954017644c5420bae81d72b09b735c21f0\", GitTreeState:\"clean\", BuildDate:\"2022-05-03T13:44:24Z\", GoVersion:\"go1.18.1\", Compiler:\"gc\", Platform:\"linux/amd64\"} Result Latest available version of Kubernetes/kubeadm has been installed from GitHub Kubernetes repo release page. 1.5 'Kubeadm init' the Master \u00b6 Run On the Master node only: Step 1: Build kubeadm Custom Config cat <<EOF > kubeadm-config.yaml kind: ClusterConfiguration apiVersion: kubeadm.k8s.io/v1beta3 kubernetesVersion: v1.24.0 --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd EOF Make sure the IP address was updated: cat kubeadm-config.yaml Note To expose custom Config you can create a kubeadm.conf and specify during kubecadm init execution. For instance: * ControllerManager configs * Custom Subnet * Custom version * Apiserver configs such as authentication, authorization and etc. Step 2: Create a cluster kubeadm init --config=kubeadm-config.yaml Result Once the command completes, configure the KUBECONFIG env variable with the path to admin.conf (recommend adding it to your .bashrc): mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config export KUBECONFIG=$HOME/.kube/config Let's validate that the installation was successful. You should now be able to run kubectl commands and see that all cluster Pods are running (except DNS one): watch kubectl get pods --all-namespaces -o wide To exit back to the terminal, press ctrl+c 1.6 Deploy Cilium CNI \u00b6 Step 1 Download Helm Package manager: wget https://get.helm.sh/helm-v3.8.2-linux-amd64.tar.gz tar -zxvf helm-v3.8.2-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin/helm Step 1 Setup Helm Cilium repository: helm repo add cilium https://helm.cilium.io/ Step 3 Now lets deploy Cilium Networking helm install cilium cilium/cilium --version 1.9.16 --namespace kube-system Watch the Cilium/node pod for the master get created (hopefully successfully) watch kubectl get pods --all-namespaces -o wide 1.7 Join worker node \u00b6 If you have other nodes around you can run the 'kubeadm join ...' command from the output of kubeadm init on each worker node (incl token). Watch the calico/node pods get created for each worker node automatically. e.g. kubeadm join --token **** For this lab, we are creating a one node kubernetes clusters, so in order to be able to deploy applications on the same node as the control plane, we need to remove the taint that prevent such deployment. kubectl taint nodes --all node-role.kubernetes.io/master:NoSchedule- 1.8 Now lets create a test deployment with 2 replicas \u00b6 kubectl create deployment nginx --replicas=2 --image=nginx --port=8080 Lets get some more detail about the deployment: kubectl describe deployment nginx kubectl get deployment nginx And pods that has been created by nginx deployment: kubectl get pods Congrats. Now you have a working Kubernetes+Calico cluster. 2.1 Verify Kubernetes components deployed by kubeadm \u00b6 2.1.1 Check Kubernetes version \u00b6 Step 1 Verify that Kubernetes is deployed and working. kubectl get nodes Result Kubernetes has single node for workload scheduling. Kubernetes running version 1.24.0 Note At Kubernetes community, we define 3 types of Kubernetes releases: Major (x.0.0) Minor (x.x.0) Patch (x.x.x) Note At a single point of time, we develop the new \"Major\"/\"Minor\" version of Kubernetes (today - Kubernetes 1.21), and we support three existing releases as the \"Patch\" releases (today - 1.19.x, 1.20.x and 1.21.x). 2.1.2 Verify Cluster default namespaces. \u00b6 Step 1 Verify namespaces created in K8s systems $ kubectl get ns NAME STATUS AGE default Active 5h50m kube-node-lease Active 5h50m kube-public Active 5h50m kube-system Active 5h50m Info Namespaces are intendent to isolate groups/teams and give them access to a set of resources. They avoid name collisions between resources. Namespaces provides with a soft Multitenancy, meaning they not provide full isolation. Result By default Kubernetes deployed by kubeadm starts with 4 namespaces: default The default namespace for objects with no other namespace. When listing resources with the kubectl get command, we\u2019ve never specified the namespace explicitly, so kubectl always defaulted to the default namespace, showing us just the objects inside that namespace. kube-system The namespace for objects created by the Kubernetes system kube-public Readable by all users, and mostly reserved for cluster usage. kube-node-lease This namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales. 2.1.3 Verify kubelet \u00b6 Step 1 Verify that kubelet installed in K8s Cluster: systemctl -l | grep kubelet systemctl status kubelet Note Service and its config file can be found in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Step 2 Find manifests file for other master Node components: Once kubelet is deployed, all the rest master node components are deployed as a static pods on Kubernetes Master node. Setting --pod-manifest-path= specifies from where to read Static Pod manifests used for spinning up the control plane. Step 3 List K8s components manifest files that is going to be used for cluster deployment and run as Static Pods by kubelet : sudo ls /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml Result We see etcd, api-server, controller-manager and scheduler that has been used to deploy on this cluster and managed by kubelet . Step 4 Verify K8s Components deployed as containers on K8s: kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-6d7b4db76c-h242g 1/1 Running 0 27m calico-node-gwnng 1/1 Running 0 27m coredns-74ff55c5b-5s7rp 1/1 Running 0 5h59m coredns-74ff55c5b-l6hd4 1/1 Running 0 5h59m etcd-k8s-cluster 1/1 Running 0 5h59m kube-apiserver-k8s-cluster 1/1 Running 0 5h59m kube-controller-manager-k8s-cluster 1/1 Running 0 5h59m kube-proxy-f8647 1/1 Running 0 5h59m kube-scheduler-k8s-cluster 1/1 Running 0 5h59m Result We can see that Kubernetes components: etcd, api-server, controller-manager and scheduler deployed on K8s cluster via kubelet. Calico Networking including calico-etcd, calico-node, calico-policy-controller has been deployed as a last step of kubeadm installation Or Cilium Networking containers 2.1.4 Verify etcd database deployment. \u00b6 Step 1 Verify etcd config file sudo cat /etc/kubernetes/manifests/etcd.yaml Step 2 Overview etcd pod deployed on K8s cluster: kubectl get pods -n kube-system | grep etcd kubectl describe pods/etcd-k8s-cluste -n kube-system Result etcd has been deployed as a static pod. Annotation Priority Class Name: system-node-critical tells to K8s that this Pod is critical and will have highest QOS . Step 3 Check the location of etcd db and snapshot dumps. sudo ls /var/lib/etcd/member Result The data directory has two sub-directories in it: wal: write ahead log files are stored here. snap: log snapshots are stored here. When first started, etcd stores its configuration into a data directory specified by the data-dir configuration parameter. Configuration is stored in the write ahead log and includes: the local member ID, cluster ID, and initial cluster configuration. The write ahead log and snapshot files are used during member operation and to recover after a restart. 2.1.5 Verify api-server deployment on the K8s cluster. \u00b6 Step 1 Review configuration file: sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml Step 2 Overview api-server pod and its parameters. kubectl describe pods/kube-apiserver-k8s-cluster -n kube-system 2.1.6 Verify Controller-manager and scheduler deployment. \u00b6 Step 1 Controller-manager and scheduler deployed on K8s cluster via kubelet the same way api-server . Verify both configuration files and pods running on K8s Cluster. Summary K8s is an orchestration system for containers. Since most of the k8s components are the go binaries that can be containerized, K8s has been designed to run itself. This makes system itself HA, easily deployable, scaleable and upgradable.","title":"Module 7 Lab - Deploy Kubernetes Cluster with Kubeadm"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#deploy-kubernetes","text":"In this Lab, we are going to: Deploy Single Node Kubernetes cluster using kubeadm on a Google Compute Engine node Deploy an application to Kubernetes","title":"Deploy Kubernetes"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#1-deploy-kubernetes-and-calico-with-kubeadm","text":"In general to deploy Kubernetes with kubeadm it is required to use following official Kubernetes documentation . Step 1 Set the Project ID in Environment Variable: export PROJECT_ID=<project_id> Here is how you can find you project_ID: Set the project ID as default gcloud config set project $PROJECT_ID","title":"1. Deploy Kubernetes and Calico with Kubeadm"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#11-create-a-vm-and-ssh-into-it","text":"Step 1 Create the Compute instance The compute instances in this lab will be provisioned using Ubuntu Server 20.04, which has good support for the containerd container runtime. gcloud compute instances create k8s-cluster \\ --zone us-central1-c \\ --machine-type=e2-standard-4 \\ --can-ip-forward \\ --image-family ubuntu-2004-lts \\ --image-project ubuntu-os-cloud Step 2 SSH in to VM where we going to install Kubernetes: gcloud compute ssh --zone \"us-central1-c\" \"k8s-cluster\"","title":"1.1 Create a VM, and ssh into it"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#12-install-containerd","text":"Docker has been deprecated from Kubernetes starting K8s 1.24, so we will need to intall containerd instaed Step 1 Update the apt package index and install containerd: sudo su apt update apt install containerd Pres Y, to install packages. Step 2 Enable systemd to start on reboot: systemctl enable containerd systemctl start containerd systemctl status containerd Output: \u25cf containerd.service - containerd container runtime Loaded: loaded (/lib/systemd/system/containerd.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2022-05-18 21:33:39 UTC; 8s ago Docs: https://containerd.io Main PID: 2106 (containerd) Tasks: 15 Memory: 23.3M CGroup: /system.slice/containerd.service \u2514\u25002106 /usr/bin/containerd Step 3 To interact with containerd Install nerdctl: wget https://github.com/containerd/nerdctl/releases/download/v0.18.0/nerdctl-0.18.0-linux-amd64.tar.gz tar zxvf nerdctl-0.18.0-linux-amd64.tar.gz nerdctl mv nerdctl /usr/local/bin Step 4 Install CNI Plugins to test containerd can start containers: wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz mkdir -p /opt/cni/bin/ tar zxvf cni-plugins-linux-amd64-v1.1.1.tgz -C /opt/cni/bin/ Step 5 Run a Docker like command with Nerdctl to start test ubuntu container: See Nerdctl CLI reference here nerdctl run -d ubuntu bash nerdctl ps -a Sucess We can see our docker container been started few seconds ago.","title":"1.2 Install Containerd"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#13-let-iptables-see-bridged-traffic","text":"cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # sysctl params required by setup, params persist across reboots cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # Apply sysctl params without reboot sudo sysctl --system","title":"1.3 Let iptables see bridged traffic"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#14-install-kubeadm-and-prerequisite-packages-on-each-node","text":"The next step is to install kubeadm and prerequisite packages as showed here. Step 1 Deploy kubeadm and prerequisite packages Update the apt package index and install packages needed to use the Kubernetes apt repository: apt-get update && apt-get install -y apt-transport-https ca-certificates curl Download the Google Cloud public signing key: sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg Add the Kubernetes apt repository: echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list Update apt package index, install kubelet, kubeadm and kubectl, and pin their version: sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl Step 2 Verify kubeadm version kubeadm version Output: kubeadm version: &version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.0\", GitCommit:\"4ce5a8954017644c5420bae81d72b09b735c21f0\", GitTreeState:\"clean\", BuildDate:\"2022-05-03T13:44:24Z\", GoVersion:\"go1.18.1\", Compiler:\"gc\", Platform:\"linux/amd64\"} Result Latest available version of Kubernetes/kubeadm has been installed from GitHub Kubernetes repo release page.","title":"1.4 Install kubeadm and prerequisite packages on each node"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#15-kubeadm-init-the-master","text":"Run On the Master node only: Step 1: Build kubeadm Custom Config cat <<EOF > kubeadm-config.yaml kind: ClusterConfiguration apiVersion: kubeadm.k8s.io/v1beta3 kubernetesVersion: v1.24.0 --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd EOF Make sure the IP address was updated: cat kubeadm-config.yaml Note To expose custom Config you can create a kubeadm.conf and specify during kubecadm init execution. For instance: * ControllerManager configs * Custom Subnet * Custom version * Apiserver configs such as authentication, authorization and etc. Step 2: Create a cluster kubeadm init --config=kubeadm-config.yaml Result Once the command completes, configure the KUBECONFIG env variable with the path to admin.conf (recommend adding it to your .bashrc): mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config export KUBECONFIG=$HOME/.kube/config Let's validate that the installation was successful. You should now be able to run kubectl commands and see that all cluster Pods are running (except DNS one): watch kubectl get pods --all-namespaces -o wide To exit back to the terminal, press ctrl+c","title":"1.5 'Kubeadm init' the Master"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#16-deploy-cilium-cni","text":"Step 1 Download Helm Package manager: wget https://get.helm.sh/helm-v3.8.2-linux-amd64.tar.gz tar -zxvf helm-v3.8.2-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin/helm Step 1 Setup Helm Cilium repository: helm repo add cilium https://helm.cilium.io/ Step 3 Now lets deploy Cilium Networking helm install cilium cilium/cilium --version 1.9.16 --namespace kube-system Watch the Cilium/node pod for the master get created (hopefully successfully) watch kubectl get pods --all-namespaces -o wide","title":"1.6 Deploy Cilium CNI"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#17-join-worker-node","text":"If you have other nodes around you can run the 'kubeadm join ...' command from the output of kubeadm init on each worker node (incl token). Watch the calico/node pods get created for each worker node automatically. e.g. kubeadm join --token **** For this lab, we are creating a one node kubernetes clusters, so in order to be able to deploy applications on the same node as the control plane, we need to remove the taint that prevent such deployment. kubectl taint nodes --all node-role.kubernetes.io/master:NoSchedule-","title":"1.7 Join worker node"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#18-now-lets-create-a-test-deployment-with-2-replicas","text":"kubectl create deployment nginx --replicas=2 --image=nginx --port=8080 Lets get some more detail about the deployment: kubectl describe deployment nginx kubectl get deployment nginx And pods that has been created by nginx deployment: kubectl get pods Congrats. Now you have a working Kubernetes+Calico cluster.","title":"1.8 Now lets create a test deployment with 2 replicas"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#21-verify-kubernetes-components-deployed-by-kubeadm","text":"","title":"2.1 Verify Kubernetes components deployed by kubeadm"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#211-check-kubernetes-version","text":"Step 1 Verify that Kubernetes is deployed and working. kubectl get nodes Result Kubernetes has single node for workload scheduling. Kubernetes running version 1.24.0 Note At Kubernetes community, we define 3 types of Kubernetes releases: Major (x.0.0) Minor (x.x.0) Patch (x.x.x) Note At a single point of time, we develop the new \"Major\"/\"Minor\" version of Kubernetes (today - Kubernetes 1.21), and we support three existing releases as the \"Patch\" releases (today - 1.19.x, 1.20.x and 1.21.x).","title":"2.1.1 Check Kubernetes version"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#212-verify-cluster-default-namespaces","text":"Step 1 Verify namespaces created in K8s systems $ kubectl get ns NAME STATUS AGE default Active 5h50m kube-node-lease Active 5h50m kube-public Active 5h50m kube-system Active 5h50m Info Namespaces are intendent to isolate groups/teams and give them access to a set of resources. They avoid name collisions between resources. Namespaces provides with a soft Multitenancy, meaning they not provide full isolation. Result By default Kubernetes deployed by kubeadm starts with 4 namespaces: default The default namespace for objects with no other namespace. When listing resources with the kubectl get command, we\u2019ve never specified the namespace explicitly, so kubectl always defaulted to the default namespace, showing us just the objects inside that namespace. kube-system The namespace for objects created by the Kubernetes system kube-public Readable by all users, and mostly reserved for cluster usage. kube-node-lease This namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales.","title":"2.1.2 Verify Cluster default namespaces."},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#213-verify-kubelet","text":"Step 1 Verify that kubelet installed in K8s Cluster: systemctl -l | grep kubelet systemctl status kubelet Note Service and its config file can be found in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Step 2 Find manifests file for other master Node components: Once kubelet is deployed, all the rest master node components are deployed as a static pods on Kubernetes Master node. Setting --pod-manifest-path= specifies from where to read Static Pod manifests used for spinning up the control plane. Step 3 List K8s components manifest files that is going to be used for cluster deployment and run as Static Pods by kubelet : sudo ls /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml Result We see etcd, api-server, controller-manager and scheduler that has been used to deploy on this cluster and managed by kubelet . Step 4 Verify K8s Components deployed as containers on K8s: kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-6d7b4db76c-h242g 1/1 Running 0 27m calico-node-gwnng 1/1 Running 0 27m coredns-74ff55c5b-5s7rp 1/1 Running 0 5h59m coredns-74ff55c5b-l6hd4 1/1 Running 0 5h59m etcd-k8s-cluster 1/1 Running 0 5h59m kube-apiserver-k8s-cluster 1/1 Running 0 5h59m kube-controller-manager-k8s-cluster 1/1 Running 0 5h59m kube-proxy-f8647 1/1 Running 0 5h59m kube-scheduler-k8s-cluster 1/1 Running 0 5h59m Result We can see that Kubernetes components: etcd, api-server, controller-manager and scheduler deployed on K8s cluster via kubelet. Calico Networking including calico-etcd, calico-node, calico-policy-controller has been deployed as a last step of kubeadm installation Or Cilium Networking containers","title":"2.1.3 Verify kubelet"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#214-verify-etcd-database-deployment","text":"Step 1 Verify etcd config file sudo cat /etc/kubernetes/manifests/etcd.yaml Step 2 Overview etcd pod deployed on K8s cluster: kubectl get pods -n kube-system | grep etcd kubectl describe pods/etcd-k8s-cluste -n kube-system Result etcd has been deployed as a static pod. Annotation Priority Class Name: system-node-critical tells to K8s that this Pod is critical and will have highest QOS . Step 3 Check the location of etcd db and snapshot dumps. sudo ls /var/lib/etcd/member Result The data directory has two sub-directories in it: wal: write ahead log files are stored here. snap: log snapshots are stored here. When first started, etcd stores its configuration into a data directory specified by the data-dir configuration parameter. Configuration is stored in the write ahead log and includes: the local member ID, cluster ID, and initial cluster configuration. The write ahead log and snapshot files are used during member operation and to recover after a restart.","title":"2.1.4 Verify etcd database deployment."},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#215-verify-api-server-deployment-on-the-k8s-cluster","text":"Step 1 Review configuration file: sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml Step 2 Overview api-server pod and its parameters. kubectl describe pods/kube-apiserver-k8s-cluster -n kube-system","title":"2.1.5 Verify api-server deployment on the K8s cluster."},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#216-verify-controller-manager-and-scheduler-deployment","text":"Step 1 Controller-manager and scheduler deployed on K8s cluster via kubelet the same way api-server . Verify both configuration files and pods running on K8s Cluster. Summary K8s is an orchestration system for containers. Since most of the k8s components are the go binaries that can be containerized, K8s has been designed to run itself. This makes system itself HA, easily deployable, scaleable and upgradable.","title":"2.1.6 Verify Controller-manager and scheduler deployment."},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/","text":"Kubernetes Concepts \u00b6 Objective: Learn basic Kubernetes concepts: Create a GKE Cluster Pods Labels, Selectors and Annotations Create Deployments Create Services namespaces 0 Create GKE Cluster \u00b6 Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c 1 Pods \u00b6 1.1 Create a Pod with manifest \u00b6 Reference: Pod Overview Step 1 Printout explanation of the object and lists of attributes: kubectl explain pods See all possible fields available for the pods: kubectl explain pods.spec --recursive Note It's not require to provide all possible fields for the Pods or any other resources. Most of the fields will be added by default if not specified. For the Pods at minimum it is required to specify image , name , ports inside of spec.containers. Step 2 Define a new pod in the file echoserver-pod.yaml : cat <<EOF > echoserver-pod.yaml apiVersion: v1 kind: Pod metadata: name: echoserver labels: app: echoserver spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.10 ports: - containerPort: 8080 EOF Here, we use the existing image echoserver . This is a simple server that responds with the http headers it received. It runs on nginx server and implemented using lua in the nginx configuration: https://github.com/kubernetes/contrib/tree/master/ingress/echoheaders Step 3 Create the echoserver pod: kubectl apply -f echoserver-pod.yaml Step 4 Use kubectl get pods to watch the pod get created: kubectl get pods Result: NAME READY STATUS RESTARTS AGE echoserver 1/1 Running 0 5s Step 5 Use kubectl describe pods/podname to watch the details about scheduled pod: kubectl describe pods/echoserver Note Review and discuss the following fields: Namespace Status Containers QoS Class Events Step 6 Now let\u2019s get the pod definition back from Kubernetes: kubectl get pods echoserver -o yaml > echoserver-pod-created.yaml cat echoserver-pod-created.yaml Compare echoserver-pod.yaml and echoserver-pod-created.yaml to see additional properties that have been added to the original pod definition. 2 Labels & Selectors \u00b6 Organizing pods and other resources with labels. 2.1 Label and Select Pods \u00b6 Reference: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ Step 1 Label Pod hello-world with label app=hello and env=test kubectl label pods echoserver dep=sales kubectl label pods echoserver env=test Step 2 See all Pods and all their Labels. kubectl get pods --show-labels Step 3 Select all Pods with labels env=test kubectl get pods -l env=test 2.2 Label Nodes \u00b6 Reference: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ Step 1 List available nodes kubectl get nodes Step 2 List a detailed view of nodes kubectl get nodes -o wide Step 3 List Nodes and their labels kubectl get nodes --show-labels Step 4 Label the node as size: small . Make sure to replace YOUR_NODE_NAME with one of the nodes you have. kubectl label node YOUR_NODE_NAME size=small Step 5 Check the labels for this node kubectl get node YOUR_NODE_NAME --show-labels | grep size Note In the upcoming classes we will use node labels to make sure our applications run on eligible nodes only. 3 Services \u00b6 3.1 Create a Service \u00b6 We have three running echoserver pods, but we cannot access them yet, because the container ports are not accessible. Let\u2019s define a new service that will expose echoserver ports and make them accessible. Step 1 Create a new file echoserver-service.yaml with the following content: cat <<EOF > echoserver-service.yaml apiVersion: v1 kind: Service metadata: name: echoserver spec: selector: app: echoserver type: \"NodePort\" ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: echoserver EOF Step 2 Create a new service: kubectl create -f echoserver-service.yaml Step 3 Check the service details: kubectl describe services/echoserver Output: Name: echoserver Namespace: default Labels: <none> Selector: app=echoserver Type: NodePort IP: ... Port: <unset> 8080/TCP NodePort: <unset> 30366/TCP Endpoints: ...:8080,...:8080,..:8080 Session Affinity: None No events. Note The above output contains one endpoint and a node port, 30366, but it can be different in your case. Remember this port to use it in the next step. Step 4 We need to open the node port on one of the cluster nodes to be able to access the service externally. Let's first find the exteran IP address of one of the nodes. kubectl get nodes -o wide Output: NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gke-k8s-concepts-default-pool-ad96fd50-1rf1 Ready <none> 20m v1.19.9-gke.1400 10.128.0.32 34.136.1.22 gke-k8s-concepts-default-pool-ad96fd50-jpd2 Ready <none> 20m v1.19.9-gke.1400 10.128.0.31 34.69.114.67 Step 5 Create a firewall rule to allow TCP traffic on your node port. Make sure to replace YOUR_NODE_PORT. gcloud compute firewall-rules create echoserver-node-port --allow tcp:YOUR_NODE_PORT Step 6 To access a service exposed via a node port, specify the node port from the previous step and use one of the IP addresses of the cluster nodes. Make sure to replace both NODE_IP and YOUR_NODE_PORT curl http://NODE_IP:YOUR_NODE_PORT 3.2 Cleanup Services and Pods \u00b6 Step 1 Before diving into Kubernetes deployment, let\u2019s delete our service and pods. To delete the service execute the following command: kubectl delete service echoserver Step 2 delete the pod kubectl delete pod echoserver Step 3 Check that there are no running pods: kubectl get pods 4 Deployments \u00b6 4.1 Deploy hello-app on Kubernetes using Deployments \u00b6 4.1.1 Create a Deployment \u00b6 Step 1 The simplest way to create a new deployment for a single-container pod is to use kubectl run : kubectl create deployment hello-app \\ --image=gcr.io/google-samples/hello-app:1.0 \\ --port=8080 \\ --replicas=2 Note --port Deployment opens port 8080 for use by the Pods. --replicas number of replicas. Step 2 Check pods: kubectl get pods Step 3 To access the hello-app deployment, create a new service of type LoadBalancer this time using kubectl expose deployment : kubectl expose deployment hello-app --type=LoadBalancer To get the external IP for the loadbalancer that got created: kubectl get services/hello-app The Loadbalancer might take few minutes to get created, and it'll show pending status. Step 4 Check that the hello-app is accessible: Make sure to replace the LB_IP. curl http://LB_IP:8080 Output: Hello, world! Version: 1.0.0 Hostname: hello-app-76f778987d-rdhr7 Step 5 You can open the app in the browser by navigating to LB_IP:8080 Summary We learned how to create a deployment and expose our container. 4.1.2 Scale a Deployment \u00b6 Now, let's scale our application as our website get popular. Step 1 Deployments using replica set (RS) to scale the containers. Let's check how replica set (RS) looks like: kubectl get rs,deploy NAME DESIRED CURRENT READY AGE replicaset.apps/hello-app-76f778987d 2 2 2 5m12s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hello-app 2/2 2 2 5m12s Step 2 Let\u2019s scale number of pods in replica for the deployment. Use going to use kubectl scale to change the number of replicas to 5: kubectl scale deployment hello-app --replicas=5 Step 3 View the deployment details: kubectl describe deployment hello-app Step 4 Check that there are 5 running pods: kubectl get pods 4.1.3 Rolling Update of Containers \u00b6 To perform rolling upgrade we need a new version of our application and then perform Rolling Upgrade using deployments Step 4 Use kubectl rollout history deployment to see revisions of the deployment: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> Result Since we've just deployed there is only 1 revision that currenly running. Step 5 Now we want to replace our hello-app with a new implementation. We want to use a new version of hello-app image. We are going to use kubectl set command this time around. Hint kubectl set used only to change image name/version. You can use this for command for CI/CD pipeline. Suppose that we want to update the webk8sbirthday Pods to use the hello-app:2.0 image instead of the hello-app:1.0 image. kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record kubectl get pods Note It is a good practice to paste --record at the end of the rolling upgrade command as it will record the action in the rollout history Result: We can see that the Rolling Upgraded was recorded: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> 2 kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record=true Step 6 Refresh browser and see new version of app deployed http://LB_IP:8080 Step 7 Let's assume there was something wrong with this new version and we need to rollback with kubectl rollout undo our deployment: kubectl rollout undo deployment/hello-app Refresh the browser again to see how we rolledback to version 1.0.0 We have successfully rolled back the deployment and our pods are based on the hello-app:1.0.0 image again. Step 8 Let's delete the deployment and the service: kubectl delete deployment hello-app kubectl delete services/hello-app Success You are now up to speed with Kubernetes Concepts such as Pods, Services and Deployments. Let's move on to Kubernetes Features to learn what else Kubernetes is capable of! 5 NameSpace \u00b6 Namespace can be used for: Splitting complex systems with several components into smaller groups Separating resources in a multi-tenant env: production, development and QA environments Separating resources per production Separating per-user or per-department or any other logical group Some other rules and regulations: Resource names only need to be unique within a namespace. Two different namespaces can contain resources of the same name. Most of the Kubernetes resources (e.g. pods, svc, rcs, and others) are namespaced. However, some resource can be cluster-wide e.g nodes, persistentVolumes and PodSecurityPolicy. 5.1 Viewing namespaces \u00b6 Step 1 List the current namespaces in a cluster using: kubectl get ns Output: NAME STATUS AGE default Active 71m kube-node-lease Active 71m kube-public Active 71m kube-system Active 71m Step 2 You can also get the summary of a specific namespace using: kubectl get namespaces <name> Or you can get detailed information with: kubectl describe namespaces <name> A namespace can be in one of two phases: Active the namespace is in use Terminating the namespace is being deleted, and can not be used for new objects Note These details show both resource quota (if present) as well as resource and limit ranges. Resource quota tracks aggregate usage of resources in the Namespace and allows cluster operators to define Hard resource usage limits that a Namespace may consume. A limit range defines min/max constraints on the amount of resources a single entity can consume in a Namespace . Step 3 Let\u2019s have a look at the pods that belong to the kube-system namespace, by telling kubectl to list pods in that namespace: kubectl get po --namespace kube-system Output: NAME READY STATUS RESTARTS AGE event-exporter-gke-67986489c8-5fsdv 2/2 Running 0 71m fluentbit-gke-fqcsx 2/2 Running 0 71m fluentbit-gke-ppb9j 2/2 Running 0 71m gke-metrics-agent-5vl7t 1/1 Running 0 71m gke-metrics-agent-bxt2r 1/1 Running 0 71m kube-dns-5d54b45645-9srx6 4/4 Running 0 71m kube-dns-5d54b45645-b7njm 4/4 Running 0 71m kube-dns-autoscaler-58cbd4f75c-2scrv 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-1rf1 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-jpd2 1/1 Running 0 71m l7-default-backend-66579f5d7-dsbdt 1/1 Running 0 71m metrics-server-v0.3.6-6c47ffd7d7-mtls4 2/2 Running 0 71m pdcsi-node-knlqp 2/2 Running 0 71m pdcsi-node-vh4tx 2/2 Running 0 71m stackdriver-metadata-agent-cluster-level-6f7d66dc98-zcd25 2/2 Running 0 71m Tip You can also use -n instead of --namespace Yot may already know some of the pods, the rest we will cover later. It\u2019s clear from the name of the namespace, that resources inside kube-system related to the Kubernetes system itself. By having them in this separate namespace, it keeps everything nicely organized. If they were all in the default namespace, mixed in with the resources we create ourselves, we\u2019d have a hard time seeing what belongs where and we might inadvertently delete some system resources. Step 4 Now you know how to view resources in specific namespaces. Additionally, it is also possible to view list all resources in all namespaces. For example below is example to list all pods in all namespaces: kubectl get pods --all-namespaces 5.2 Creating Namespaces \u00b6 A namespace is a Kubernetes resource, therefore it is possible to create it by posting a YAML file to the Kubernetes API server or using kubectl create ns . Step 1 First, create a custom-namespace.yaml file with the following content: cat <<EOF > custom-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: custom-namespace EOF Step 2 Than, use kubectl to post the file to the Kubernetes API server: kubectl create -f custom-namespace.yaml Step 3 A much easier and faster way to create a namespaces using kubectl create ns command, as shown below: kubectl create namespace custom-namespace2 5.3 Setting the namespace preference. \u00b6 By default, a Kubernetes cluster will instantiate a default namespace when provisioning the cluster to hold the default set of Pods, Services, and Deployments used by the cluster. So by default all kubectl calls such as list or create resources will end up in default namespace . However sometimes you want to list or create resources in other namespaces than default namespace . As we discussed in previous exercise this can be done by specifying -n or --namespace to point in which namespaces action has to be done. However it is not convenient to do this action every time. Below example will show how to create 2 namespaces dev and prod and switch between each other. Step 1 kubectl API uses so called kubeconfig context where you can controls which namespace, user or cluster needs to be accessed. In order to display which context is currently in use run: KUBECONFIG=~/.kube/config kubectl config current-context Result We running in kubernetes-admin@kubernetes context, which is default for lab environment. Step 3 To see full view of the kubeconfig context run: kubectl config view Result Our context named as kubernetes-admin@kubernetes uses cluster cluster , and user kubernetes-admin Step 4 The result of the above command comes from kubeconfig file, in our case we defined it under ~/.kube/config . echo $KUBECONFIG Result KUBECONFIG is configured to use following ~/.kube/config file cat ~/.kube/config Note The KUBECONFIG environment variable is a list of paths to configuration files. The list is colon-delimited for Linux and Mac, and semicolon-delimited for Windows. We already set KUBECONFIG environment variable in a first step of exercise to be ~/.kube/config Tip You can use use multiple kubeconfig files at the same time and view merged config: $ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view Step 5 Create two new namespaces dev : kubectl create namespace dev And prod namespace: kubectl create namespace prod Step 7 Let\u2019s switch to operate in the development namespace: kubectl config set-context --current --namespace=dev We can see now that our current context is switched to dev: kubectl config view | grep namespace Output: namespace: dev Result At this point, all requests we make to the Kubernetes cluster from the command line are scoped to the development namespace. Step 8 Let's test that all resources going to be created in dev namespace. kubectl run devsnowflake --image=nginx Step 9 Verify result of creation: kubectl get pods Success Developers are able to do what they want, and they do not have to worry about affecting content in the production namespace. Step 10 Now switch to the production namespace and show how resources in one namespace are hidden from the other. kubectl config set-context --current --namespace=prod The production namespace should be empty, and the following commands should return nothing. kubectl get pods Step 11 Let's create some production workloads: kubectl run prodapp --image=nginx kubectl get pods -n prod Summary At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace. 6.4 Deleting Namespaces \u00b6 Step 1 Delete a namespace with kubectl delete namespaces custom-namespace kubectl delete namespaces dev kubectl delete namespaces prod Warning Unlike with OpenStack where when you delete a project/tenant, underlining resources will still exist as zombies and not deleted. In Kubernetes when you delete namespace it deletes everything under it (pods, svc, rc, and etc.)! This is called resource garbage collection in Kubernetes. Delete process is asynchronous, so you may see Terminating state for some time. 6.5 Create a pod in a different namespace \u00b6 Create test namespace: kubectl create ns test Create a pod in this namespaces: cat <<EOF > echoserver-pod_ns.yaml apiVersion: v1 kind: Pod metadata: name: echoserverns spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.4 ports: - containerPort: 8080 EOF Create Pod in namespaces: kubectl create -f echoserver-pod_ns.yaml -n test Verify Pods created in specified namespaces: kubectl get pods -n test 6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts Step 2 Delete the firewall rule gcloud compute firewall-rules delete echoserver-node-port","title":"Module 8 Lab - Kuberenetes Concepts"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#kubernetes-concepts","text":"Objective: Learn basic Kubernetes concepts: Create a GKE Cluster Pods Labels, Selectors and Annotations Create Deployments Create Services namespaces","title":"Kubernetes Concepts"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#0-create-gke-cluster","text":"Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c","title":"0 Create GKE Cluster"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#1-pods","text":"","title":"1 Pods"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#11-create-a-pod-with-manifest","text":"Reference: Pod Overview Step 1 Printout explanation of the object and lists of attributes: kubectl explain pods See all possible fields available for the pods: kubectl explain pods.spec --recursive Note It's not require to provide all possible fields for the Pods or any other resources. Most of the fields will be added by default if not specified. For the Pods at minimum it is required to specify image , name , ports inside of spec.containers. Step 2 Define a new pod in the file echoserver-pod.yaml : cat <<EOF > echoserver-pod.yaml apiVersion: v1 kind: Pod metadata: name: echoserver labels: app: echoserver spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.10 ports: - containerPort: 8080 EOF Here, we use the existing image echoserver . This is a simple server that responds with the http headers it received. It runs on nginx server and implemented using lua in the nginx configuration: https://github.com/kubernetes/contrib/tree/master/ingress/echoheaders Step 3 Create the echoserver pod: kubectl apply -f echoserver-pod.yaml Step 4 Use kubectl get pods to watch the pod get created: kubectl get pods Result: NAME READY STATUS RESTARTS AGE echoserver 1/1 Running 0 5s Step 5 Use kubectl describe pods/podname to watch the details about scheduled pod: kubectl describe pods/echoserver Note Review and discuss the following fields: Namespace Status Containers QoS Class Events Step 6 Now let\u2019s get the pod definition back from Kubernetes: kubectl get pods echoserver -o yaml > echoserver-pod-created.yaml cat echoserver-pod-created.yaml Compare echoserver-pod.yaml and echoserver-pod-created.yaml to see additional properties that have been added to the original pod definition.","title":"1.1 Create a Pod with manifest"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#2-labels-selectors","text":"Organizing pods and other resources with labels.","title":"2 Labels &amp; Selectors"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#21-label-and-select-pods","text":"Reference: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ Step 1 Label Pod hello-world with label app=hello and env=test kubectl label pods echoserver dep=sales kubectl label pods echoserver env=test Step 2 See all Pods and all their Labels. kubectl get pods --show-labels Step 3 Select all Pods with labels env=test kubectl get pods -l env=test","title":"2.1 Label and Select Pods"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#22-label-nodes","text":"Reference: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ Step 1 List available nodes kubectl get nodes Step 2 List a detailed view of nodes kubectl get nodes -o wide Step 3 List Nodes and their labels kubectl get nodes --show-labels Step 4 Label the node as size: small . Make sure to replace YOUR_NODE_NAME with one of the nodes you have. kubectl label node YOUR_NODE_NAME size=small Step 5 Check the labels for this node kubectl get node YOUR_NODE_NAME --show-labels | grep size Note In the upcoming classes we will use node labels to make sure our applications run on eligible nodes only.","title":"2.2 Label Nodes"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#3-services","text":"","title":"3 Services"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#31-create-a-service","text":"We have three running echoserver pods, but we cannot access them yet, because the container ports are not accessible. Let\u2019s define a new service that will expose echoserver ports and make them accessible. Step 1 Create a new file echoserver-service.yaml with the following content: cat <<EOF > echoserver-service.yaml apiVersion: v1 kind: Service metadata: name: echoserver spec: selector: app: echoserver type: \"NodePort\" ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: echoserver EOF Step 2 Create a new service: kubectl create -f echoserver-service.yaml Step 3 Check the service details: kubectl describe services/echoserver Output: Name: echoserver Namespace: default Labels: <none> Selector: app=echoserver Type: NodePort IP: ... Port: <unset> 8080/TCP NodePort: <unset> 30366/TCP Endpoints: ...:8080,...:8080,..:8080 Session Affinity: None No events. Note The above output contains one endpoint and a node port, 30366, but it can be different in your case. Remember this port to use it in the next step. Step 4 We need to open the node port on one of the cluster nodes to be able to access the service externally. Let's first find the exteran IP address of one of the nodes. kubectl get nodes -o wide Output: NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gke-k8s-concepts-default-pool-ad96fd50-1rf1 Ready <none> 20m v1.19.9-gke.1400 10.128.0.32 34.136.1.22 gke-k8s-concepts-default-pool-ad96fd50-jpd2 Ready <none> 20m v1.19.9-gke.1400 10.128.0.31 34.69.114.67 Step 5 Create a firewall rule to allow TCP traffic on your node port. Make sure to replace YOUR_NODE_PORT. gcloud compute firewall-rules create echoserver-node-port --allow tcp:YOUR_NODE_PORT Step 6 To access a service exposed via a node port, specify the node port from the previous step and use one of the IP addresses of the cluster nodes. Make sure to replace both NODE_IP and YOUR_NODE_PORT curl http://NODE_IP:YOUR_NODE_PORT","title":"3.1 Create a Service"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#32-cleanup-services-and-pods","text":"Step 1 Before diving into Kubernetes deployment, let\u2019s delete our service and pods. To delete the service execute the following command: kubectl delete service echoserver Step 2 delete the pod kubectl delete pod echoserver Step 3 Check that there are no running pods: kubectl get pods","title":"3.2 Cleanup Services and Pods"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#4-deployments","text":"","title":"4 Deployments"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#41-deploy-hello-app-on-kubernetes-using-deployments","text":"","title":"4.1 Deploy hello-app on Kubernetes using Deployments"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#411-create-a-deployment","text":"Step 1 The simplest way to create a new deployment for a single-container pod is to use kubectl run : kubectl create deployment hello-app \\ --image=gcr.io/google-samples/hello-app:1.0 \\ --port=8080 \\ --replicas=2 Note --port Deployment opens port 8080 for use by the Pods. --replicas number of replicas. Step 2 Check pods: kubectl get pods Step 3 To access the hello-app deployment, create a new service of type LoadBalancer this time using kubectl expose deployment : kubectl expose deployment hello-app --type=LoadBalancer To get the external IP for the loadbalancer that got created: kubectl get services/hello-app The Loadbalancer might take few minutes to get created, and it'll show pending status. Step 4 Check that the hello-app is accessible: Make sure to replace the LB_IP. curl http://LB_IP:8080 Output: Hello, world! Version: 1.0.0 Hostname: hello-app-76f778987d-rdhr7 Step 5 You can open the app in the browser by navigating to LB_IP:8080 Summary We learned how to create a deployment and expose our container.","title":"4.1.1 Create a Deployment"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#412-scale-a-deployment","text":"Now, let's scale our application as our website get popular. Step 1 Deployments using replica set (RS) to scale the containers. Let's check how replica set (RS) looks like: kubectl get rs,deploy NAME DESIRED CURRENT READY AGE replicaset.apps/hello-app-76f778987d 2 2 2 5m12s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hello-app 2/2 2 2 5m12s Step 2 Let\u2019s scale number of pods in replica for the deployment. Use going to use kubectl scale to change the number of replicas to 5: kubectl scale deployment hello-app --replicas=5 Step 3 View the deployment details: kubectl describe deployment hello-app Step 4 Check that there are 5 running pods: kubectl get pods","title":"4.1.2 Scale a Deployment"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#413-rolling-update-of-containers","text":"To perform rolling upgrade we need a new version of our application and then perform Rolling Upgrade using deployments Step 4 Use kubectl rollout history deployment to see revisions of the deployment: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> Result Since we've just deployed there is only 1 revision that currenly running. Step 5 Now we want to replace our hello-app with a new implementation. We want to use a new version of hello-app image. We are going to use kubectl set command this time around. Hint kubectl set used only to change image name/version. You can use this for command for CI/CD pipeline. Suppose that we want to update the webk8sbirthday Pods to use the hello-app:2.0 image instead of the hello-app:1.0 image. kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record kubectl get pods Note It is a good practice to paste --record at the end of the rolling upgrade command as it will record the action in the rollout history Result: We can see that the Rolling Upgraded was recorded: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> 2 kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record=true Step 6 Refresh browser and see new version of app deployed http://LB_IP:8080 Step 7 Let's assume there was something wrong with this new version and we need to rollback with kubectl rollout undo our deployment: kubectl rollout undo deployment/hello-app Refresh the browser again to see how we rolledback to version 1.0.0 We have successfully rolled back the deployment and our pods are based on the hello-app:1.0.0 image again. Step 8 Let's delete the deployment and the service: kubectl delete deployment hello-app kubectl delete services/hello-app Success You are now up to speed with Kubernetes Concepts such as Pods, Services and Deployments. Let's move on to Kubernetes Features to learn what else Kubernetes is capable of!","title":"4.1.3 Rolling Update of Containers"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#5-namespace","text":"Namespace can be used for: Splitting complex systems with several components into smaller groups Separating resources in a multi-tenant env: production, development and QA environments Separating resources per production Separating per-user or per-department or any other logical group Some other rules and regulations: Resource names only need to be unique within a namespace. Two different namespaces can contain resources of the same name. Most of the Kubernetes resources (e.g. pods, svc, rcs, and others) are namespaced. However, some resource can be cluster-wide e.g nodes, persistentVolumes and PodSecurityPolicy.","title":"5 NameSpace"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#51-viewing-namespaces","text":"Step 1 List the current namespaces in a cluster using: kubectl get ns Output: NAME STATUS AGE default Active 71m kube-node-lease Active 71m kube-public Active 71m kube-system Active 71m Step 2 You can also get the summary of a specific namespace using: kubectl get namespaces <name> Or you can get detailed information with: kubectl describe namespaces <name> A namespace can be in one of two phases: Active the namespace is in use Terminating the namespace is being deleted, and can not be used for new objects Note These details show both resource quota (if present) as well as resource and limit ranges. Resource quota tracks aggregate usage of resources in the Namespace and allows cluster operators to define Hard resource usage limits that a Namespace may consume. A limit range defines min/max constraints on the amount of resources a single entity can consume in a Namespace . Step 3 Let\u2019s have a look at the pods that belong to the kube-system namespace, by telling kubectl to list pods in that namespace: kubectl get po --namespace kube-system Output: NAME READY STATUS RESTARTS AGE event-exporter-gke-67986489c8-5fsdv 2/2 Running 0 71m fluentbit-gke-fqcsx 2/2 Running 0 71m fluentbit-gke-ppb9j 2/2 Running 0 71m gke-metrics-agent-5vl7t 1/1 Running 0 71m gke-metrics-agent-bxt2r 1/1 Running 0 71m kube-dns-5d54b45645-9srx6 4/4 Running 0 71m kube-dns-5d54b45645-b7njm 4/4 Running 0 71m kube-dns-autoscaler-58cbd4f75c-2scrv 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-1rf1 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-jpd2 1/1 Running 0 71m l7-default-backend-66579f5d7-dsbdt 1/1 Running 0 71m metrics-server-v0.3.6-6c47ffd7d7-mtls4 2/2 Running 0 71m pdcsi-node-knlqp 2/2 Running 0 71m pdcsi-node-vh4tx 2/2 Running 0 71m stackdriver-metadata-agent-cluster-level-6f7d66dc98-zcd25 2/2 Running 0 71m Tip You can also use -n instead of --namespace Yot may already know some of the pods, the rest we will cover later. It\u2019s clear from the name of the namespace, that resources inside kube-system related to the Kubernetes system itself. By having them in this separate namespace, it keeps everything nicely organized. If they were all in the default namespace, mixed in with the resources we create ourselves, we\u2019d have a hard time seeing what belongs where and we might inadvertently delete some system resources. Step 4 Now you know how to view resources in specific namespaces. Additionally, it is also possible to view list all resources in all namespaces. For example below is example to list all pods in all namespaces: kubectl get pods --all-namespaces","title":"5.1 Viewing namespaces"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#52-creating-namespaces","text":"A namespace is a Kubernetes resource, therefore it is possible to create it by posting a YAML file to the Kubernetes API server or using kubectl create ns . Step 1 First, create a custom-namespace.yaml file with the following content: cat <<EOF > custom-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: custom-namespace EOF Step 2 Than, use kubectl to post the file to the Kubernetes API server: kubectl create -f custom-namespace.yaml Step 3 A much easier and faster way to create a namespaces using kubectl create ns command, as shown below: kubectl create namespace custom-namespace2","title":"5.2 Creating Namespaces"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#53-setting-the-namespace-preference","text":"By default, a Kubernetes cluster will instantiate a default namespace when provisioning the cluster to hold the default set of Pods, Services, and Deployments used by the cluster. So by default all kubectl calls such as list or create resources will end up in default namespace . However sometimes you want to list or create resources in other namespaces than default namespace . As we discussed in previous exercise this can be done by specifying -n or --namespace to point in which namespaces action has to be done. However it is not convenient to do this action every time. Below example will show how to create 2 namespaces dev and prod and switch between each other. Step 1 kubectl API uses so called kubeconfig context where you can controls which namespace, user or cluster needs to be accessed. In order to display which context is currently in use run: KUBECONFIG=~/.kube/config kubectl config current-context Result We running in kubernetes-admin@kubernetes context, which is default for lab environment. Step 3 To see full view of the kubeconfig context run: kubectl config view Result Our context named as kubernetes-admin@kubernetes uses cluster cluster , and user kubernetes-admin Step 4 The result of the above command comes from kubeconfig file, in our case we defined it under ~/.kube/config . echo $KUBECONFIG Result KUBECONFIG is configured to use following ~/.kube/config file cat ~/.kube/config Note The KUBECONFIG environment variable is a list of paths to configuration files. The list is colon-delimited for Linux and Mac, and semicolon-delimited for Windows. We already set KUBECONFIG environment variable in a first step of exercise to be ~/.kube/config Tip You can use use multiple kubeconfig files at the same time and view merged config: $ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view Step 5 Create two new namespaces dev : kubectl create namespace dev And prod namespace: kubectl create namespace prod Step 7 Let\u2019s switch to operate in the development namespace: kubectl config set-context --current --namespace=dev We can see now that our current context is switched to dev: kubectl config view | grep namespace Output: namespace: dev Result At this point, all requests we make to the Kubernetes cluster from the command line are scoped to the development namespace. Step 8 Let's test that all resources going to be created in dev namespace. kubectl run devsnowflake --image=nginx Step 9 Verify result of creation: kubectl get pods Success Developers are able to do what they want, and they do not have to worry about affecting content in the production namespace. Step 10 Now switch to the production namespace and show how resources in one namespace are hidden from the other. kubectl config set-context --current --namespace=prod The production namespace should be empty, and the following commands should return nothing. kubectl get pods Step 11 Let's create some production workloads: kubectl run prodapp --image=nginx kubectl get pods -n prod Summary At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace.","title":"5.3 Setting the namespace preference."},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#64-deleting-namespaces","text":"Step 1 Delete a namespace with kubectl delete namespaces custom-namespace kubectl delete namespaces dev kubectl delete namespaces prod Warning Unlike with OpenStack where when you delete a project/tenant, underlining resources will still exist as zombies and not deleted. In Kubernetes when you delete namespace it deletes everything under it (pods, svc, rc, and etc.)! This is called resource garbage collection in Kubernetes. Delete process is asynchronous, so you may see Terminating state for some time.","title":"6.4 Deleting Namespaces"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#65-create-a-pod-in-a-different-namespace","text":"Create test namespace: kubectl create ns test Create a pod in this namespaces: cat <<EOF > echoserver-pod_ns.yaml apiVersion: v1 kind: Pod metadata: name: echoserverns spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.4 ports: - containerPort: 8080 EOF Create Pod in namespaces: kubectl create -f echoserver-pod_ns.yaml -n test Verify Pods created in specified namespaces: kubectl get pods -n test","title":"6.5 Create a pod in a different namespace"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#6-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts Step 2 Delete the firewall rule gcloud compute firewall-rules delete echoserver-node-port","title":"6 Cleaning Up"},{"location":"ycit019_Module_9_Kubernetes_Features/","text":"Lab 8 Kubernetes Features Objective Use Liveness Probes to healthcheck you application while it is running Learn about secrets and configmaps Deploy a Daemonset and jobs 0 Create GKE Cluster \u00b6 Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-features \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-features us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-features --zone us-central1-c 1 Kubernetes Features \u00b6 1.1 Using Liveness Probes \u00b6 Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes p rovides liveness probes to detect and remedy such situations. As we already discussed Kubernetes provides 3 types of Probes to perform Liveness checks: HTTP GET EXEC tcpSocket In below example we are going to use HTTP GET probe for a Pod that runs a container based on the gcr.io/google_containers/liveness image. Step 1 Create http-liveness.yaml manifest with below content: cat <<EOF > http-liveness.yaml apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: gcr.io/google_containers/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 EOF Based on the manifest, we can see that the Pod has a single Container. The periodSeconds field specifies that the kubelet should perform a liveness probe every 3 seconds. The initialDelaySeconds field tells the kubelet that it should wait 3 seconds before performing the first probe. To perform a probe, the kubelet sends an HTTP GET request to the server that is running in the Container and listening on port 8080. If the handler for the server's /healthz path returns a success code, the kubelet considers the Container to be alive and healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it. Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure. Full code for for a reference in server.go . For the first 10 seconds that the Container is alive, the /healthz handler returns a status of 200. After that, the handler returns a status of 500. http . HandleFunc ( \"/healthz\" , func ( w http . ResponseWriter , r * http . Request ) { duration := time . Now (). Sub ( started ) if duration . Seconds () > 10 { w . WriteHeader ( 500 ) w . Write ([] byte ( fmt . Sprintf ( \"error: %v\" , duration . Seconds ()))) } else { w . WriteHeader ( 200 ) w . Write ([] byte ( \"ok\" )) } }) The kubelet starts performing health checks 3 seconds after the Container starts. So the first couple of health checks will succeed. But after 10 seconds, the health checks will fail, and the kubelet will kill and restart the Container. Step 2 Let's create a Pod and see how HTTP liveness check works: kubectl create -f http-liveness.yaml Step 3 Monitor the Pod watch kubectl get pod Result After 10 seconds Pods has beed restarted. Exit the shell session by using ctrl+c Step 4 Verify status of the Pod and review the Events happened after restart kubectl describe pod liveness-http Result Pod events shows that liveness probes have failed and the Container has been restarted. Step 5 Clean up kubectl delete -f http-liveness.yaml 1.2 Using ConfigMaps \u00b6 In Kubernetes ConfigMaps could be use in several cases: Storing configuration values as key-values in ConfigMap and referencing them in a Pod as environment variables Storing configurations as a file inside of ConfigMap and referencing it in a Pod as a Volume Let's try second option and deploy nginx pod while storing its config in a ConfigMap. Step 1 Create nginx my-nginx-config.conf config file as below: cat <<EOF > my-nginx-config.conf server { listen 80; server_name www.cloudnative.tech; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } EOF Step 2 Create ConfigMap from this file kubectl create configmap nginxconfig --from-file=my-nginx-config.conf Step 3 Review the ConfigMap kubectl describe cm nginxconfig Result: ``` Name: nginxconfig Namespace: default Labels: <none> Annotations: <none> Data ==== my-nginx-config.conf: ---- server { listen 80; server_name _; gzip off; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } Events: <none> ``` Step 4 Create Nginx Pod website.yaml file, where ConfigMap referenced as a Volume cat <<EOF > website.yaml apiVersion: v1 kind: Pod metadata: name: website spec: containers: - image: nginx:alpine name: website volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: false - name: config mountPath: /etc/nginx/conf.d readOnly: true volumes: - name: html emptyDir: {} - name: config configMap: name: nginxconfig EOF Step 5 Deploy Nginx Pod website.yaml kubectl apply -f website.yaml You can expose a service as we usually do with NodePort or LoadBalancer or by using the port-forward technicque in the next step Step 6 Open second SSH terminal by pressing \"+\" icon in the cloud shell and run following command kubectl port-forward website 8080:80 Result This opens a tunnel and expose our application on port '80' to localhost:8080. Step 7 Test that website is running Navigate back to the first terminal tab and run the following curl localhost:8080 Result: ``` <html> <head><title>403 Forbidden</title></head> <body> <center><h1>403 Forbidden</h1></center> <hr><center>nginx/1.21.0</center> </body> </html> ``` Success You've just learned how to use ConfigMaps. You can also use the preview feature with the console Step 8 start a shell in the pod by using exec kubectl exec website -it -- sh Your terminal now should have /# Step 9 view the content of the folder where we added the volume ls /etc/nginx/conf.d Step 10 check the content of the file in that folder and notice that it's the same as the config file for nginx we mounted. cat /etc/nginx/conf.d/my-nginx-config.conf Step 11 Exit the shell exit Make sure you're back to the cloud shell terminal. 1.3 Using Secrets \u00b6 1.3.1 Kubernetes Secrets \u00b6 Kubernetes secrets allow users to define sensitive information outside of containers and expose that information to containers through environment variables as well as files within Pods. In this section we will declare and create secrets to hold our database connection information that will be used by Wordpress to connect to its backend database. Step 1 Open up two terminal windows. We will use one window to generate encoded strings that will contain our sensitive data. The other window will be used to create the secrets YAML declaration. Step 2 In the first terminal window, execute the following commands to encode our strings: echo -n \"admin\" | base64 echo -n \"t0p-Secret\" | base64 Step 3 create the secret. For this lab, we added the encoded values for you. Feel free to change the username and password, and replace the values in the file below: cat <<EOF > app-secrets.yaml apiVersion: v1 kind: Secret metadata: name: app-secret type: Opaque data: username: YWRtaW4= password: dDBwLVNlY3JldA== EOF Step 4 Create the secret: kubectl create -f app-secrets.yaml Step 5 Verify secret creation and get details: kubectl get secrets Step 6 Get details of this secret: kubectl describe secrets/app-secrets Result: Name: app-secret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== password: 10 bytes username: 5 bytes Step 7 Create a pod that will reference the secret as a volume. Use either nano or vim to create secret-pod.yaml, and copy the following to it apiVersion: v1 kind: Pod metadata: name: secret-test-pod spec: containers: - name: test-container image: alpine command: [\"/bin/sh\", \"-ec\", \"export LOGIN=$(cat /etc/secret-volume/username);export PWD=$(cat /etc/secret-volume/password);while true; do echo hello $LOGIN your password is $PWD; sleep 10; done\"] volumeMounts: - name: secret-volume mountPath: /etc/secret-volume volumes: - name: secret-volume secret: secretName: app-secret restartPolicy: Never Step 8 View the logs of the pod. kubectl logs secret-test-pod Notice how we were able to pull the content of the secret using a command. Not very secure, right? Step 9 Delete both the pod and the secret kubectl delete pod secret-test-pod kubectl delete secret app-secret Summary Secrets has been created and they not visiable when you view them via kubectl describe This does not make Kubernetes secrets secure as we have experienced. Additonal measures need to be in place to protect secrets. 1.4 Jobs \u00b6 A Job creates one or more pods and ensures that a specified number of them successfully complete. A job keeps track of successful completion of a pod. When the specified number of pods have successfully completed, the job itself is complete. The job will start a new pod if the pod fails or is deleted due to hardware failure. A successful completion of the specified number of pods means the job is complete. This is different from a replica set or a deployment which ensures that a certain number of pods are always running. So if a pod in a replica set or deployment terminates, then it is restarted again. This makes replica set or deployment as long-running processes. This is well suited for a web server, such as NGINX. But a job is completed if the specified number of pods successfully completes. This is well suited for tasks that need to run only once. For example, a job may convert an image format from one to another. Restarting this pod in replication controller would not only cause redundant work but may be harmful in certain cases. Jobs are complementary to Replica Set. A Replica Set manages pods which are not expected to terminate (e.g. web servers), and a Job manages pods that are expected to terminate (e.g. batch jobs). 1.4.1 Non-parallel Job \u00b6 Only one pod per job is started, unless the pod fails. Job is complete as soon as the pod terminates successfully. Use the image \"busybox\" and have it sleep for 10 seconds and then complete. Run your job to be sure it works. Step 1 Create a Job Manifest cat <<EOF > busybox.yaml apiVersion: batch/v1 kind: Job metadata: name: busybox spec: template: spec: containers: - name: busybox image: busybox command: [\"sleep\", \"20\"] restartPolicy: Never EOF Step 2 Run a Job kubectl create -f busybox.yaml Step 3 Look at the job: kubectl get jobs Output: NAME DESIRED SUCCESSFUL AGE busybox 1 0 0s Result The output shows that the job is not successful yet. Step 4 Watch the pod status kubectl get -w pods Output: NAME READY STATUS RESTARTS AGE busybox-lk49x 1/1 Running 0 7s busybox-lk49x 0/1 Completed 0 24s Result It starts with pod for the job is Running . Then pod successfully exits after a few seconds and shows the Completed status. Step 5 Watch the job status again: kubectl get jobs Output: NAME COMPLETIONS DURATION AGE busybox 1/1 21s 1m Step 6 Delete a Job kubectl delete -f busybox.yaml 1.4.2 Parallel Job \u00b6 Non-parallel jobs run only one pod per job. This API is used to run multiple pods in parallel for the job. The number of pods to complete is defined by .spec.completions attribute in the configuration file. The number of pods to run in parallel is defined by .spec.parallelism attribute in the configuration file. The default value for both of these attributes is 1. The job is complete when there is one successful pod for each value in the range in 1 to .spec.completions . For that reason, it is also called as fixed completion count job. Step 1 Create a Job Manifest cat <<EOF > job-parallel.yaml apiVersion: batch/v1 kind: Job metadata: name: wait spec: completions: 6 parallelism: 2 template: metadata: name: wait spec: containers: - name: wait image: ubuntu command: [\"sleep\", \"10\"] restartPolicy: Never EOF Note This job specification is similar to the non-parallel job specification above. However it has two new attributes added: .spec.completions and .spec.parallelism . This means the job will be complete when six pods have successfully completed. A maximum of two pods will run in parallel at a given time. Step 2 Create a parallel job using the command: kubectl apply -f job-parallel.yaml Step 3 Watch the status of the job as shown: kubectl get -w jobs Output: NAME COMPLETIONS DURATION AGE wait 0/6 6s 6s wait 1/6 12s 12s wait 2/6 12s 12s wait 3/6 24s 24s wait 4/6 24s 24s wait 5/6 36s 36s wait 6/6 36s 36s Results The output shows that 2 pods are created about every 12 seconds. Step 4 In another terminal window, watch the status of pods created: kubectl get -w pods -l job-name=wait Output: NAME READY STATUS RESTARTS AGE wait-5blwm 0/1 Completed 0 17s wait-stmk4 0/1 Completed 0 17s wait-ts6xt 1/1 Running 0 5s wait-xlhl6 1/1 Running 0 5s wait-xlhl6 0/1 Completed 0 12s wait-rq6z5 0/1 Pending 0 0s wait-ts6xt 0/1 Completed 0 12s wait-rq6z5 0/1 Pending 0 0s wait-rq6z5 0/1 ContainerCreating 0 0s wait-f85bj 0/1 Pending 0 0s wait-f85bj 0/1 Pending 0 0s wait-f85bj 0/1 ContainerCreating 0 0s wait-rq6z5 1/1 Running 0 2s wait-f85bj 1/1 Running 0 2s wait-f85bj 0/1 Completed 0 12s wait-rq6z5 0/1 Completed 0 12s Step 6 Once the job is completed, you can get the list of completed pods kubectl get pods -l job-name=wait Result: NAME READY STATUS RESTARTS AGE wait-5blwm 0/1 Completed 0 2m55s wait-f85bj 0/1 Completed 0 2m31s wait-rq6z5 0/1 Completed 0 2m31s wait-stmk4 0/1 Completed 0 2m55s wait-ts6xt 0/1 Completed 0 2m43s wait-xlhl6 0/1 Completed 0 2m43s Step 5 Similarly, kubectl get jobs shows the status of the job after it has completed: kubectl get jobs Result: NAME COMPLETIONS DURATION AGE wait 6/6 36s 3m54s Step 6 Deleting a job deletes all the pods as well. Delete the job as: kubectl delete -f job-parallel.yaml 1.5 Cron Jobs \u00b6 A Cron Job is a job that runs on a given schedule, written in Cron format. There are two primary use cases: Run jobs once at a specified point in time Repeatedly at a specified point in time 1.5.1 Create Cron Job \u00b6 Step 1 Create CronJob manifest that prints the current timestamp and the message \" Hello World \" every minute. cat <<EOF > cronjob.yaml apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: metadata: labels: app: hello-cronpod spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello World! restartPolicy: OnFailure EOF Step 2 Create the Cron Job as shown in the command: kubectl create -f cronjob.yaml Step 3 Watch the status of the job as shown: kubectl get -w cronjobs Output : NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hello */1 * * * * False 0 <none> 6s hello */1 * * * * False 1 5s 39s hello */1 * * * * False 0 15s 49s hello */1 * * * * False 1 5s 99s hello */1 * * * * False 0 15s 109s hello */1 * * * * False 1 6s 2m40s hello */1 * * * * False 0 16s 2m50s Step 4 In another terminal window, watch the status of pods created: kubectl get -w pods -l app=hello-cronpod Output : NAME READY STATUS RESTARTS AGE hello-1622584020-kc46c 0/1 Completed 0 118s hello-1622584080-c2pcq 0/1 Completed 0 58s hello-1622584140-hxnv2 0/1 Pending 0 0s hello-1622584140-hxnv2 0/1 Pending 0 0s hello-1622584140-hxnv2 0/1 ContainerCreating 0 0s hello-1622584140-hxnv2 1/1 Running 0 1s hello-1622584140-hxnv2 0/1 Completed 0 2s Step 5 Get logs from one of the pods: kubectl logs hello-1622584140-hxnv2 Output : Tue Jun 1 21:49:07 UTC 2021 Hello World! Step 6 Delete Cron Job kubectl delete -f cronjob.yaml 1.6 Daemon Set \u00b6 Daemon Set ensures that a copy of the pod runs on a selected set of nodes. By default, all nodes in the cluster are selected. A selection critieria may be specified to select a limited number of nodes. As new nodes are added to the cluster, pods are started on them. As nodes are removed, pods are removed through garbage collection. The following is an example DaemonSet that runs a Prometheus exporter container that used for collecting machine metrics from each node. Step 1 Create a DaemonSet manifest cat <<EOF > daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: prometheus-daemonset spec: selector: matchLabels: tier: monitoring name: prometheus-exporter template: metadata: labels: tier: monitoring name: prometheus-exporter spec: containers: - name: prometheus image: prom/node-exporter ports: - containerPort: 80 EOF Step 2 Run the following command to create the ReplicaSet and pods: kubectl create -f daemonset.yaml --record Note The --record flag will track changes made through each revision. Step 3 Get basic details about the DaemonSet: kubectl get daemonsets/prometheus-daemonset Output: NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE-SELECTOR AGE prometheus-daemonset 1 1 1 1 1 <none> 5s Step 4 Get more details about the DaemonSet: kubectl describe daemonset/prometheus-daemonset Output: Name: prometheus-daemonset Selector: name=prometheus-exporter,tier=monitoring Node-Selector: <none> Labels: <none> Annotations: deprecated.daemonset.template.generation: 1 kubernetes.io/change-cause: kubectl create --filename=daemonset.yaml --record=true Desired Number of Nodes Scheduled: 2 Current Number of Nodes Scheduled: 2 Number of Nodes Scheduled with Up-to-date Pods: 2 Number of Nodes Scheduled with Available Pods: 2 Number of Nodes Misscheduled: 0 Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: name=prometheus-exporter tier=monitoring Containers: prometheus: Image: prom/node-exporter Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 9m daemonset-controller Created pod: prometheus-daemonset-f4xl2 Normal SuccessfulCreate 2m18s daemonset-controller Created pod: prometheus-daemonset-w596z Step 5 Get pods in the DaemonSet: kubectl get pods -l name=prometheus-exporter Output: NAME READY STATUS RESTARTS AGE prometheus-daemonset-f4xl2 1/1 Running 0 8m27s prometheus-daemonset-w596z 1/1 Running 0 105s Step 6 Verify that the Prometheus pod was successfully deployed to the cluster nodes: kubectl get pods -o wide Output: NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES prometheus-daemonset-f4xl2 1/1 Running 0 6m51s 10.0.0.19 gke-k8s-features-default-pool-73e09df7-m2lf <none> <none> prometheus-daemonset-w596z 1/1 Running 0 9s 10.0.1.2 gke-k8s-features-default-pool-73e09df7-k7hj <none> <none> Notes It is possible to Limit DaemonSets to specific nodes by changing the spec.template.spec to include a nodeSelector to matche node label. Step 7 Delete a DaemonSet kubectl delete -f daemonset.yaml 1.7 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"Lab 9 Kubernetes Features"},{"location":"ycit019_Module_9_Kubernetes_Features/#0-create-gke-cluster","text":"Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-features \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-features us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-features --zone us-central1-c","title":"0 Create GKE Cluster"},{"location":"ycit019_Module_9_Kubernetes_Features/#1-kubernetes-features","text":"","title":"1 Kubernetes Features"},{"location":"ycit019_Module_9_Kubernetes_Features/#11-using-liveness-probes","text":"Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes p rovides liveness probes to detect and remedy such situations. As we already discussed Kubernetes provides 3 types of Probes to perform Liveness checks: HTTP GET EXEC tcpSocket In below example we are going to use HTTP GET probe for a Pod that runs a container based on the gcr.io/google_containers/liveness image. Step 1 Create http-liveness.yaml manifest with below content: cat <<EOF > http-liveness.yaml apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: gcr.io/google_containers/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 EOF Based on the manifest, we can see that the Pod has a single Container. The periodSeconds field specifies that the kubelet should perform a liveness probe every 3 seconds. The initialDelaySeconds field tells the kubelet that it should wait 3 seconds before performing the first probe. To perform a probe, the kubelet sends an HTTP GET request to the server that is running in the Container and listening on port 8080. If the handler for the server's /healthz path returns a success code, the kubelet considers the Container to be alive and healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it. Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure. Full code for for a reference in server.go . For the first 10 seconds that the Container is alive, the /healthz handler returns a status of 200. After that, the handler returns a status of 500. http . HandleFunc ( \"/healthz\" , func ( w http . ResponseWriter , r * http . Request ) { duration := time . Now (). Sub ( started ) if duration . Seconds () > 10 { w . WriteHeader ( 500 ) w . Write ([] byte ( fmt . Sprintf ( \"error: %v\" , duration . Seconds ()))) } else { w . WriteHeader ( 200 ) w . Write ([] byte ( \"ok\" )) } }) The kubelet starts performing health checks 3 seconds after the Container starts. So the first couple of health checks will succeed. But after 10 seconds, the health checks will fail, and the kubelet will kill and restart the Container. Step 2 Let's create a Pod and see how HTTP liveness check works: kubectl create -f http-liveness.yaml Step 3 Monitor the Pod watch kubectl get pod Result After 10 seconds Pods has beed restarted. Exit the shell session by using ctrl+c Step 4 Verify status of the Pod and review the Events happened after restart kubectl describe pod liveness-http Result Pod events shows that liveness probes have failed and the Container has been restarted. Step 5 Clean up kubectl delete -f http-liveness.yaml","title":"1.1 Using Liveness Probes"},{"location":"ycit019_Module_9_Kubernetes_Features/#12-using-configmaps","text":"In Kubernetes ConfigMaps could be use in several cases: Storing configuration values as key-values in ConfigMap and referencing them in a Pod as environment variables Storing configurations as a file inside of ConfigMap and referencing it in a Pod as a Volume Let's try second option and deploy nginx pod while storing its config in a ConfigMap. Step 1 Create nginx my-nginx-config.conf config file as below: cat <<EOF > my-nginx-config.conf server { listen 80; server_name www.cloudnative.tech; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } EOF Step 2 Create ConfigMap from this file kubectl create configmap nginxconfig --from-file=my-nginx-config.conf Step 3 Review the ConfigMap kubectl describe cm nginxconfig Result: ``` Name: nginxconfig Namespace: default Labels: <none> Annotations: <none> Data ==== my-nginx-config.conf: ---- server { listen 80; server_name _; gzip off; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } Events: <none> ``` Step 4 Create Nginx Pod website.yaml file, where ConfigMap referenced as a Volume cat <<EOF > website.yaml apiVersion: v1 kind: Pod metadata: name: website spec: containers: - image: nginx:alpine name: website volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: false - name: config mountPath: /etc/nginx/conf.d readOnly: true volumes: - name: html emptyDir: {} - name: config configMap: name: nginxconfig EOF Step 5 Deploy Nginx Pod website.yaml kubectl apply -f website.yaml You can expose a service as we usually do with NodePort or LoadBalancer or by using the port-forward technicque in the next step Step 6 Open second SSH terminal by pressing \"+\" icon in the cloud shell and run following command kubectl port-forward website 8080:80 Result This opens a tunnel and expose our application on port '80' to localhost:8080. Step 7 Test that website is running Navigate back to the first terminal tab and run the following curl localhost:8080 Result: ``` <html> <head><title>403 Forbidden</title></head> <body> <center><h1>403 Forbidden</h1></center> <hr><center>nginx/1.21.0</center> </body> </html> ``` Success You've just learned how to use ConfigMaps. You can also use the preview feature with the console Step 8 start a shell in the pod by using exec kubectl exec website -it -- sh Your terminal now should have /# Step 9 view the content of the folder where we added the volume ls /etc/nginx/conf.d Step 10 check the content of the file in that folder and notice that it's the same as the config file for nginx we mounted. cat /etc/nginx/conf.d/my-nginx-config.conf Step 11 Exit the shell exit Make sure you're back to the cloud shell terminal.","title":"1.2 Using ConfigMaps"},{"location":"ycit019_Module_9_Kubernetes_Features/#13-using-secrets","text":"","title":"1.3 Using Secrets"},{"location":"ycit019_Module_9_Kubernetes_Features/#131-kubernetes-secrets","text":"Kubernetes secrets allow users to define sensitive information outside of containers and expose that information to containers through environment variables as well as files within Pods. In this section we will declare and create secrets to hold our database connection information that will be used by Wordpress to connect to its backend database. Step 1 Open up two terminal windows. We will use one window to generate encoded strings that will contain our sensitive data. The other window will be used to create the secrets YAML declaration. Step 2 In the first terminal window, execute the following commands to encode our strings: echo -n \"admin\" | base64 echo -n \"t0p-Secret\" | base64 Step 3 create the secret. For this lab, we added the encoded values for you. Feel free to change the username and password, and replace the values in the file below: cat <<EOF > app-secrets.yaml apiVersion: v1 kind: Secret metadata: name: app-secret type: Opaque data: username: YWRtaW4= password: dDBwLVNlY3JldA== EOF Step 4 Create the secret: kubectl create -f app-secrets.yaml Step 5 Verify secret creation and get details: kubectl get secrets Step 6 Get details of this secret: kubectl describe secrets/app-secrets Result: Name: app-secret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== password: 10 bytes username: 5 bytes Step 7 Create a pod that will reference the secret as a volume. Use either nano or vim to create secret-pod.yaml, and copy the following to it apiVersion: v1 kind: Pod metadata: name: secret-test-pod spec: containers: - name: test-container image: alpine command: [\"/bin/sh\", \"-ec\", \"export LOGIN=$(cat /etc/secret-volume/username);export PWD=$(cat /etc/secret-volume/password);while true; do echo hello $LOGIN your password is $PWD; sleep 10; done\"] volumeMounts: - name: secret-volume mountPath: /etc/secret-volume volumes: - name: secret-volume secret: secretName: app-secret restartPolicy: Never Step 8 View the logs of the pod. kubectl logs secret-test-pod Notice how we were able to pull the content of the secret using a command. Not very secure, right? Step 9 Delete both the pod and the secret kubectl delete pod secret-test-pod kubectl delete secret app-secret Summary Secrets has been created and they not visiable when you view them via kubectl describe This does not make Kubernetes secrets secure as we have experienced. Additonal measures need to be in place to protect secrets.","title":"1.3.1 Kubernetes Secrets"},{"location":"ycit019_Module_9_Kubernetes_Features/#14-jobs","text":"A Job creates one or more pods and ensures that a specified number of them successfully complete. A job keeps track of successful completion of a pod. When the specified number of pods have successfully completed, the job itself is complete. The job will start a new pod if the pod fails or is deleted due to hardware failure. A successful completion of the specified number of pods means the job is complete. This is different from a replica set or a deployment which ensures that a certain number of pods are always running. So if a pod in a replica set or deployment terminates, then it is restarted again. This makes replica set or deployment as long-running processes. This is well suited for a web server, such as NGINX. But a job is completed if the specified number of pods successfully completes. This is well suited for tasks that need to run only once. For example, a job may convert an image format from one to another. Restarting this pod in replication controller would not only cause redundant work but may be harmful in certain cases. Jobs are complementary to Replica Set. A Replica Set manages pods which are not expected to terminate (e.g. web servers), and a Job manages pods that are expected to terminate (e.g. batch jobs).","title":"1.4 Jobs"},{"location":"ycit019_Module_9_Kubernetes_Features/#141-non-parallel-job","text":"Only one pod per job is started, unless the pod fails. Job is complete as soon as the pod terminates successfully. Use the image \"busybox\" and have it sleep for 10 seconds and then complete. Run your job to be sure it works. Step 1 Create a Job Manifest cat <<EOF > busybox.yaml apiVersion: batch/v1 kind: Job metadata: name: busybox spec: template: spec: containers: - name: busybox image: busybox command: [\"sleep\", \"20\"] restartPolicy: Never EOF Step 2 Run a Job kubectl create -f busybox.yaml Step 3 Look at the job: kubectl get jobs Output: NAME DESIRED SUCCESSFUL AGE busybox 1 0 0s Result The output shows that the job is not successful yet. Step 4 Watch the pod status kubectl get -w pods Output: NAME READY STATUS RESTARTS AGE busybox-lk49x 1/1 Running 0 7s busybox-lk49x 0/1 Completed 0 24s Result It starts with pod for the job is Running . Then pod successfully exits after a few seconds and shows the Completed status. Step 5 Watch the job status again: kubectl get jobs Output: NAME COMPLETIONS DURATION AGE busybox 1/1 21s 1m Step 6 Delete a Job kubectl delete -f busybox.yaml","title":"1.4.1 Non-parallel Job"},{"location":"ycit019_Module_9_Kubernetes_Features/#142-parallel-job","text":"Non-parallel jobs run only one pod per job. This API is used to run multiple pods in parallel for the job. The number of pods to complete is defined by .spec.completions attribute in the configuration file. The number of pods to run in parallel is defined by .spec.parallelism attribute in the configuration file. The default value for both of these attributes is 1. The job is complete when there is one successful pod for each value in the range in 1 to .spec.completions . For that reason, it is also called as fixed completion count job. Step 1 Create a Job Manifest cat <<EOF > job-parallel.yaml apiVersion: batch/v1 kind: Job metadata: name: wait spec: completions: 6 parallelism: 2 template: metadata: name: wait spec: containers: - name: wait image: ubuntu command: [\"sleep\", \"10\"] restartPolicy: Never EOF Note This job specification is similar to the non-parallel job specification above. However it has two new attributes added: .spec.completions and .spec.parallelism . This means the job will be complete when six pods have successfully completed. A maximum of two pods will run in parallel at a given time. Step 2 Create a parallel job using the command: kubectl apply -f job-parallel.yaml Step 3 Watch the status of the job as shown: kubectl get -w jobs Output: NAME COMPLETIONS DURATION AGE wait 0/6 6s 6s wait 1/6 12s 12s wait 2/6 12s 12s wait 3/6 24s 24s wait 4/6 24s 24s wait 5/6 36s 36s wait 6/6 36s 36s Results The output shows that 2 pods are created about every 12 seconds. Step 4 In another terminal window, watch the status of pods created: kubectl get -w pods -l job-name=wait Output: NAME READY STATUS RESTARTS AGE wait-5blwm 0/1 Completed 0 17s wait-stmk4 0/1 Completed 0 17s wait-ts6xt 1/1 Running 0 5s wait-xlhl6 1/1 Running 0 5s wait-xlhl6 0/1 Completed 0 12s wait-rq6z5 0/1 Pending 0 0s wait-ts6xt 0/1 Completed 0 12s wait-rq6z5 0/1 Pending 0 0s wait-rq6z5 0/1 ContainerCreating 0 0s wait-f85bj 0/1 Pending 0 0s wait-f85bj 0/1 Pending 0 0s wait-f85bj 0/1 ContainerCreating 0 0s wait-rq6z5 1/1 Running 0 2s wait-f85bj 1/1 Running 0 2s wait-f85bj 0/1 Completed 0 12s wait-rq6z5 0/1 Completed 0 12s Step 6 Once the job is completed, you can get the list of completed pods kubectl get pods -l job-name=wait Result: NAME READY STATUS RESTARTS AGE wait-5blwm 0/1 Completed 0 2m55s wait-f85bj 0/1 Completed 0 2m31s wait-rq6z5 0/1 Completed 0 2m31s wait-stmk4 0/1 Completed 0 2m55s wait-ts6xt 0/1 Completed 0 2m43s wait-xlhl6 0/1 Completed 0 2m43s Step 5 Similarly, kubectl get jobs shows the status of the job after it has completed: kubectl get jobs Result: NAME COMPLETIONS DURATION AGE wait 6/6 36s 3m54s Step 6 Deleting a job deletes all the pods as well. Delete the job as: kubectl delete -f job-parallel.yaml","title":"1.4.2 Parallel Job"},{"location":"ycit019_Module_9_Kubernetes_Features/#15-cron-jobs","text":"A Cron Job is a job that runs on a given schedule, written in Cron format. There are two primary use cases: Run jobs once at a specified point in time Repeatedly at a specified point in time","title":"1.5 Cron Jobs"},{"location":"ycit019_Module_9_Kubernetes_Features/#151-create-cron-job","text":"Step 1 Create CronJob manifest that prints the current timestamp and the message \" Hello World \" every minute. cat <<EOF > cronjob.yaml apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: metadata: labels: app: hello-cronpod spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello World! restartPolicy: OnFailure EOF Step 2 Create the Cron Job as shown in the command: kubectl create -f cronjob.yaml Step 3 Watch the status of the job as shown: kubectl get -w cronjobs Output : NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hello */1 * * * * False 0 <none> 6s hello */1 * * * * False 1 5s 39s hello */1 * * * * False 0 15s 49s hello */1 * * * * False 1 5s 99s hello */1 * * * * False 0 15s 109s hello */1 * * * * False 1 6s 2m40s hello */1 * * * * False 0 16s 2m50s Step 4 In another terminal window, watch the status of pods created: kubectl get -w pods -l app=hello-cronpod Output : NAME READY STATUS RESTARTS AGE hello-1622584020-kc46c 0/1 Completed 0 118s hello-1622584080-c2pcq 0/1 Completed 0 58s hello-1622584140-hxnv2 0/1 Pending 0 0s hello-1622584140-hxnv2 0/1 Pending 0 0s hello-1622584140-hxnv2 0/1 ContainerCreating 0 0s hello-1622584140-hxnv2 1/1 Running 0 1s hello-1622584140-hxnv2 0/1 Completed 0 2s Step 5 Get logs from one of the pods: kubectl logs hello-1622584140-hxnv2 Output : Tue Jun 1 21:49:07 UTC 2021 Hello World! Step 6 Delete Cron Job kubectl delete -f cronjob.yaml","title":"1.5.1 Create Cron Job"},{"location":"ycit019_Module_9_Kubernetes_Features/#16-daemon-set","text":"Daemon Set ensures that a copy of the pod runs on a selected set of nodes. By default, all nodes in the cluster are selected. A selection critieria may be specified to select a limited number of nodes. As new nodes are added to the cluster, pods are started on them. As nodes are removed, pods are removed through garbage collection. The following is an example DaemonSet that runs a Prometheus exporter container that used for collecting machine metrics from each node. Step 1 Create a DaemonSet manifest cat <<EOF > daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: prometheus-daemonset spec: selector: matchLabels: tier: monitoring name: prometheus-exporter template: metadata: labels: tier: monitoring name: prometheus-exporter spec: containers: - name: prometheus image: prom/node-exporter ports: - containerPort: 80 EOF Step 2 Run the following command to create the ReplicaSet and pods: kubectl create -f daemonset.yaml --record Note The --record flag will track changes made through each revision. Step 3 Get basic details about the DaemonSet: kubectl get daemonsets/prometheus-daemonset Output: NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE-SELECTOR AGE prometheus-daemonset 1 1 1 1 1 <none> 5s Step 4 Get more details about the DaemonSet: kubectl describe daemonset/prometheus-daemonset Output: Name: prometheus-daemonset Selector: name=prometheus-exporter,tier=monitoring Node-Selector: <none> Labels: <none> Annotations: deprecated.daemonset.template.generation: 1 kubernetes.io/change-cause: kubectl create --filename=daemonset.yaml --record=true Desired Number of Nodes Scheduled: 2 Current Number of Nodes Scheduled: 2 Number of Nodes Scheduled with Up-to-date Pods: 2 Number of Nodes Scheduled with Available Pods: 2 Number of Nodes Misscheduled: 0 Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: name=prometheus-exporter tier=monitoring Containers: prometheus: Image: prom/node-exporter Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 9m daemonset-controller Created pod: prometheus-daemonset-f4xl2 Normal SuccessfulCreate 2m18s daemonset-controller Created pod: prometheus-daemonset-w596z Step 5 Get pods in the DaemonSet: kubectl get pods -l name=prometheus-exporter Output: NAME READY STATUS RESTARTS AGE prometheus-daemonset-f4xl2 1/1 Running 0 8m27s prometheus-daemonset-w596z 1/1 Running 0 105s Step 6 Verify that the Prometheus pod was successfully deployed to the cluster nodes: kubectl get pods -o wide Output: NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES prometheus-daemonset-f4xl2 1/1 Running 0 6m51s 10.0.0.19 gke-k8s-features-default-pool-73e09df7-m2lf <none> <none> prometheus-daemonset-w596z 1/1 Running 0 9s 10.0.1.2 gke-k8s-features-default-pool-73e09df7-k7hj <none> <none> Notes It is possible to Limit DaemonSets to specific nodes by changing the spec.template.spec to include a nodeSelector to matche node label. Step 7 Delete a DaemonSet kubectl delete -f daemonset.yaml","title":"1.6 Daemon Set"},{"location":"ycit019_Module_9_Kubernetes_Features/#17-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"1.7 Cleaning Up"},{"location":"ycit019_ass1/","text":"1 Containerize Applications \u00b6 Objective: Use GCP Cloud Source Repositories to commit code Review process of containerizing of applications Review creation of Docker Images Review build image process Review process to launch containers with docker cli Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1 Configure Cloud Source Repository \u00b6 Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console. Note Google Cloud Source Repository is similar tool to GitHub, so if you know Github CLI it is same experience, apart from that UI is different than GITHUB. 1.1 Create a personal repository within Google Cloud Source Repository \u00b6 Step 1 Run the following command to create a new Cloud Source Repository named $student_name-notepad, where $student_name - is you mcgill student-ID: Setup Environment Variable: export PROJECT_ID=<project_id> Here is how you can find you project_ID: export student_name=<write_your_name_here_and_remove_brakets> Important Replace above with your project_id student_name gcloud config set project $PROJECT_ID gcloud source repos create $student_name-notepad Press Y API [sourcerepo.googleapis.com] not enabled on project [686694291909]. Would you like to enable and retry (this will take a few minutes)? (y/N)? You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: gcloud source repos clone $student_name-notepad The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: ls Observe that repository has been cloned Result You've created a Personal Repository, this is the locaton where you going to submit you assignments going forward 1.2 Locate Module 5 Assignment \u00b6 Step 1 Locate directory where Dockerfile and Readme.md are stored. cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019_2022 cd ~/ycit019_2022/Mod5_assignment/ ls Result You can see Readme.md where you will document docker commands given in the assignment ls gowebapp-mysql ls gowebapp Result You can see Dockerfiles for gowebapp and gowebapp-mysql that you will be working with in this Assignment Step 2 Go into your personal Google Cloud Source Repository: cd ~ cd ~/$student_name-notepad Step 3 Copy Mod 5 Assignment Mod5_assignment folder to your personal repo: cp -r ~/ycit019_2022/Mod5_assignment . Step 4 Configure Git Parametres git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" Step 5 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding Dockerfiles and Readme for Module 5 Assignement\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 1.3 Review Cloud Source Repositories \u00b6 Use the Google Cloud Source Repositories code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Step 1 Browse the Mod5_assignment files you pushed to the repository by opening the Navigation menu and selecting Source Repositories: Click Menu -> Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. Step 2 View a file in the Google Cloud repository Click $student_name-notepad > gowebapp/Dockerfile to view content of the Dockerfile for gowebapp Click $student_name-notepad > gowebapp/Dockerfile to view content of the Dockerfile for gowebapp-mysql Click $student_name-notepad > Readme.md to view content of the Readme 1.4 Grant viewing permissions for a repository to Instructors/Teachers \u00b6 Reference document Step 1 This step will grant view access for Instructor to check you assignments In your Cloud Terminal: gcloud projects add-iam-policy-binding $PROJECT_ID --member='user:ayrat.khayretdinov@gmail.com' --role=roles/viewer Result Your instructor will be able to review you code and grade it. 1.5 Browse and edit files in Cloud Shell Editor \u00b6 Step 1 Browse files in the Google Cloud Source repository edit ~/$student_name-notepad Result Editor opens and you can easily modify you code and save it as you go. 2 Build and Deploy gowebapp application \u00b6 2.1 Overview of the Sample Application \u00b6 This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application. 2.3 Build Dockers image for frontend application \u00b6 Step 1 Locate and review the go source code: cd ~/$student_name-notepad/Mod5_assignment/ Result Two folders with go app and mysql config has been reviewed. Step 2 Write Dockerfile for your frontend application cd ~/$student_name-notepad/Mod5_assignment/gowebapp Modify a file named Dockerfile in this directory for the frontend Go app. Use Cloud Editor or editor of you choice edit Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1 2.4 Build Docker image for backend application \u00b6 Step 1 Locate folder with mysql config cd ~/$student_name-notepad/Mod5_assignment/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use Cloud Editor or editor of you choice. edit Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally 2.5 Test application by running with Docker Engine. \u00b6 Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Note Default bridge only allows connecting container by IP addresses which is not viable solution as IP address of docker change at startup. There for we creating a user defined bridge network gowebapp Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Container needs to run on network: `gowebapp` #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Container needs to run on network: `gowebapp` #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit; 2.6 Cleanup running applications \u00b6 ### TODO docker xxx 3 Submit Assignment to Instructor \u00b6 3.1 Commit DOCKERFILEs and README.md to repository and share it with Instructor/Teacher \u00b6 Step 1 Edit Readme.md files with command you've created a working gowebapp application edit ~/$student_name-notepad/Mod5_assignment/Readme.md Step 2 Commit gowebapp and gowebapp-mysql folders and Readme.md using the following Git commands: cd ~/$student_name-notepad/ git add . git commit -m \"adding DOCKREFILEs and Readme\" Step 2 Push commit to the Cloud Source Repositories: git push origin master Step 3 Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"Module 5 Assignment"},{"location":"ycit019_ass1/#1-containerize-applications","text":"Objective: Use GCP Cloud Source Repositories to commit code Review process of containerizing of applications Review creation of Docker Images Review build image process Review process to launch containers with docker cli","title":"1 Containerize Applications"},{"location":"ycit019_ass1/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ycit019_ass1/#1-configure-cloud-source-repository","text":"Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console. Note Google Cloud Source Repository is similar tool to GitHub, so if you know Github CLI it is same experience, apart from that UI is different than GITHUB.","title":"1 Configure Cloud Source Repository"},{"location":"ycit019_ass1/#11-create-a-personal-repository-within-google-cloud-source-repository","text":"Step 1 Run the following command to create a new Cloud Source Repository named $student_name-notepad, where $student_name - is you mcgill student-ID: Setup Environment Variable: export PROJECT_ID=<project_id> Here is how you can find you project_ID: export student_name=<write_your_name_here_and_remove_brakets> Important Replace above with your project_id student_name gcloud config set project $PROJECT_ID gcloud source repos create $student_name-notepad Press Y API [sourcerepo.googleapis.com] not enabled on project [686694291909]. Would you like to enable and retry (this will take a few minutes)? (y/N)? You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: gcloud source repos clone $student_name-notepad The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: ls Observe that repository has been cloned Result You've created a Personal Repository, this is the locaton where you going to submit you assignments going forward","title":"1.1 Create a personal repository within Google Cloud Source Repository"},{"location":"ycit019_ass1/#12-locate-module-5-assignment","text":"Step 1 Locate directory where Dockerfile and Readme.md are stored. cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019_2022 cd ~/ycit019_2022/Mod5_assignment/ ls Result You can see Readme.md where you will document docker commands given in the assignment ls gowebapp-mysql ls gowebapp Result You can see Dockerfiles for gowebapp and gowebapp-mysql that you will be working with in this Assignment Step 2 Go into your personal Google Cloud Source Repository: cd ~ cd ~/$student_name-notepad Step 3 Copy Mod 5 Assignment Mod5_assignment folder to your personal repo: cp -r ~/ycit019_2022/Mod5_assignment . Step 4 Configure Git Parametres git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" Step 5 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding Dockerfiles and Readme for Module 5 Assignement\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"1.2 Locate Module 5 Assignment"},{"location":"ycit019_ass1/#13-review-cloud-source-repositories","text":"Use the Google Cloud Source Repositories code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Step 1 Browse the Mod5_assignment files you pushed to the repository by opening the Navigation menu and selecting Source Repositories: Click Menu -> Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. Step 2 View a file in the Google Cloud repository Click $student_name-notepad > gowebapp/Dockerfile to view content of the Dockerfile for gowebapp Click $student_name-notepad > gowebapp/Dockerfile to view content of the Dockerfile for gowebapp-mysql Click $student_name-notepad > Readme.md to view content of the Readme","title":"1.3 Review Cloud Source Repositories"},{"location":"ycit019_ass1/#14-grant-viewing-permissions-for-a-repository-to-instructorsteachers","text":"Reference document Step 1 This step will grant view access for Instructor to check you assignments In your Cloud Terminal: gcloud projects add-iam-policy-binding $PROJECT_ID --member='user:ayrat.khayretdinov@gmail.com' --role=roles/viewer Result Your instructor will be able to review you code and grade it.","title":"1.4 Grant viewing permissions for a repository to Instructors/Teachers"},{"location":"ycit019_ass1/#15-browse-and-edit-files-in-cloud-shell-editor","text":"Step 1 Browse files in the Google Cloud Source repository edit ~/$student_name-notepad Result Editor opens and you can easily modify you code and save it as you go.","title":"1.5 Browse and edit files in Cloud Shell Editor"},{"location":"ycit019_ass1/#2-build-and-deploy-gowebapp-application","text":"","title":"2 Build and Deploy gowebapp application"},{"location":"ycit019_ass1/#21-overview-of-the-sample-application","text":"This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application.","title":"2.1 Overview of the Sample Application"},{"location":"ycit019_ass1/#23-build-dockers-image-for-frontend-application","text":"Step 1 Locate and review the go source code: cd ~/$student_name-notepad/Mod5_assignment/ Result Two folders with go app and mysql config has been reviewed. Step 2 Write Dockerfile for your frontend application cd ~/$student_name-notepad/Mod5_assignment/gowebapp Modify a file named Dockerfile in this directory for the frontend Go app. Use Cloud Editor or editor of you choice edit Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1","title":"2.3 Build Dockers image for frontend application"},{"location":"ycit019_ass1/#24-build-docker-image-for-backend-application","text":"Step 1 Locate folder with mysql config cd ~/$student_name-notepad/Mod5_assignment/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use Cloud Editor or editor of you choice. edit Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally","title":"2.4 Build Docker image for backend application"},{"location":"ycit019_ass1/#25-test-application-by-running-with-docker-engine","text":"Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Note Default bridge only allows connecting container by IP addresses which is not viable solution as IP address of docker change at startup. There for we creating a user defined bridge network gowebapp Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Container needs to run on network: `gowebapp` #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Container needs to run on network: `gowebapp` #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit;","title":"2.5 Test application by running with Docker Engine."},{"location":"ycit019_ass1/#26-cleanup-running-applications","text":"### TODO docker xxx","title":"2.6 Cleanup running applications"},{"location":"ycit019_ass1/#3-submit-assignment-to-instructor","text":"","title":"3 Submit Assignment to Instructor"},{"location":"ycit019_ass1/#31-commit-dockerfiles-and-readmemd-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Edit Readme.md files with command you've created a working gowebapp application edit ~/$student_name-notepad/Mod5_assignment/Readme.md Step 2 Commit gowebapp and gowebapp-mysql folders and Readme.md using the following Git commands: cd ~/$student_name-notepad/ git add . git commit -m \"adding DOCKREFILEs and Readme\" Step 2 Push commit to the Cloud Source Repositories: git push origin master Step 3 Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"3.1 Commit DOCKERFILEs and README.md to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass1_sol/","text":"1 Containerize Applications \u00b6 Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1.1 Overview of the Sample Application \u00b6 This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application. 1.1 Build Dockers image for frontend application \u00b6 Result Two folders with go app and mysql config has been reviewed. Step 2 Write Dockerfile for your frontend application Create a file named Dockerfile in this directory for the frontend Go application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: cd ~/ycit019_2022/Mod5_assignment/gowebapp vim Dockerfile FROM golang:1.15.11 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 3 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. docker build -t <user-name>/gowebapp:v1 . 1.2 Build Docker image for backend application \u00b6 Step 1 Locate folder with mysql config cd ~/ycit019_2022/Mod5_assignment/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Build gowebapp-mysql Docker image locally docker build -t <user-name>/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally 1.4 Test application by running with Docker Engine. \u00b6 Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd <user-name>/gowebapp-mysql:v1 docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd archyufa/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp <user-name>/gowebapp:v1 docker run -p 8080:80 -d --net gowebapp --name gowebapp \\ --hostname gowebapp archyufa/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: docker exec -it gowebapp-mysql bash Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit; 1.5 Cleanup running applications and unused networks \u00b6 docker rm -f $(docker ps -q)","title":"Module 5 Assignment Solution"},{"location":"ycit019_ass1_sol/#1-containerize-applications","text":"Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process","title":"1 Containerize Applications"},{"location":"ycit019_ass1_sol/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ycit019_ass1_sol/#11-overview-of-the-sample-application","text":"This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application.","title":"1.1 Overview of the Sample Application"},{"location":"ycit019_ass1_sol/#11-build-dockers-image-for-frontend-application","text":"Result Two folders with go app and mysql config has been reviewed. Step 2 Write Dockerfile for your frontend application Create a file named Dockerfile in this directory for the frontend Go application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: cd ~/ycit019_2022/Mod5_assignment/gowebapp vim Dockerfile FROM golang:1.15.11 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 3 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. docker build -t <user-name>/gowebapp:v1 .","title":"1.1 Build Dockers image for frontend application"},{"location":"ycit019_ass1_sol/#12-build-docker-image-for-backend-application","text":"Step 1 Locate folder with mysql config cd ~/ycit019_2022/Mod5_assignment/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Build gowebapp-mysql Docker image locally docker build -t <user-name>/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally","title":"1.2 Build Docker image for backend application"},{"location":"ycit019_ass1_sol/#14-test-application-by-running-with-docker-engine","text":"Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd <user-name>/gowebapp-mysql:v1 docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd archyufa/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp <user-name>/gowebapp:v1 docker run -p 8080:80 -d --net gowebapp --name gowebapp \\ --hostname gowebapp archyufa/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: docker exec -it gowebapp-mysql bash Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit;","title":"1.4 Test application by running with Docker Engine."},{"location":"ycit019_ass1_sol/#15-cleanup-running-applications-and-unused-networks","text":"docker rm -f $(docker ps -q)","title":"1.5 Cleanup running applications and unused networks"},{"location":"ycit019_ass2/","text":"1 Containerize Applications \u00b6 Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose Prepare the Cloud Source Repository Environment with Module 6 Assignment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. Cloud Source Repositories: Qwik Start Step 1 Locate directory where docker-compose manifest going to be stored. cd ~/ycit019_2022/ git pull # Pull latest Mod6_assignment In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019_2022 cd ~/ycit019_2022/Mod6_assignment/ ls Step 2 Go into the local repository you've created: export student_name=<write_your_name_here_and_remove_brakets> Important Replace above with your project_id student_name cd ~/$student_name-notepad Step 3 Copy Mod6_assignment folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019_2022/Mod6_assignment/ . Step 4 Commit Mod6_assignment folder using the following Git commands: git status git add . git commit -m \"adding `Mod6_assignment` with docker-compose manifest\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Review Cloud Source Repositories Use the Google Cloud Source Repositories code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Mod6_assignment files you pushed to the repository by opening the Navigation menu and selecting Source Repositories: Click Menu -> Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. 2 Build and push Docker images to Google Container Registry (GCR) \u00b6 2.1 Build and push gowebapp-mysql Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$student_name-notepad/Mod6_assignment/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Set the Project ID in Environment Variable: export PROJECT_ID=<project_id> Here is how you can find you project_ID: Set the project ID as default gcloud config set project $PROJECT_ID Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 2.1 Build and push gowebapp Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$student_name-notepad/Mod6_assignment/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Note We've updated our application to support golang version 1.16. Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry Step 7 Delete locally build images, as we want to test how images will be pulled from gcr registry in the next step: docker rmi gcr.io/$PROJECT_ID/gowebapp:v1 docker rmi gcr.io/$PROJECT_ID/gowebapp-mysql:v1 2.3 Test application by running with Docker Engine. \u00b6 Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Wait for mysql container to start docker ps docker logs <id> You should see following output: [Server] /usr/sbin/mysqld: ready for connections Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. 2.4 Cleanup running applications and unused networks \u00b6 docker ps docker rm -f <container_id_gowebapp> <container_id_gowebapp-mysql> docker network docker network rm gowebapp 3 Docker Compose \u00b6 3.1 Test application locally with Docker Compose \u00b6 Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v3 documentations Implementation Ensure that Mysql start first and then webapp services Ensure that Mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Step 1 Create compose file cd ~/$student_name-notepad/Mod6_assignment Edit existing docker-compose file: edit docker-compose.yaml Create structure as following: #TODO Specify docker-compose file '3.4' services: gowebapp-mysql: #TODO Make a build in the `gowebapp-mysql` folder #TODO Add Environment variable for MYSQL_DATABASE #TODO Add healthcheck test that you can connect to Database and execute `SHOW DATABASES` #TODO Add healthcheck, timeout: 45s, interval: 10s, retries: 10, start_period 15 sec #TODO Reference doc for healthcheck https://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck #TODO Add Restart `always` #TODO Attach to created to User-defined network `gowebapp1` gowebapp: #TODO Make a build in the appropriate folder #TODO Allocate ports so that contaiener port mapped to local port #TODO Restart container in case failed #TODO Attach to created to User-defined network `gowebapp1` #TODO Add Environment variable for GOPATH #TODO Add Dependency for `gowebapp-mysql` to start first networks: #TODO Create User-defined network `gowebapp1` type `driver: bridge` Note In this case as we using Compose v3.4, you can also try start_period in the health-check Step 2 Run compose file export CLOUDSDK_PYTHON=python2 # https://github.com/google-github-actions/setup-gcloud/issues/128 docker-compose up -d Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!! 3.2 Commit docker-compose file to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.3 Submit link to your assignment to LMS \u00b6 Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$student_name-notepad e.g: https://source.cloud.google.com/ycit019_2022-project/ayratk-notepad","title":"Module 6 Assignment"},{"location":"ycit019_ass2/#1-containerize-applications","text":"Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose","title":"1 Containerize Applications"},{"location":"ycit019_ass2/#prepare-the-cloud-source-repository-environment-with-module-6-assignment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. Cloud Source Repositories: Qwik Start Step 1 Locate directory where docker-compose manifest going to be stored. cd ~/ycit019_2022/ git pull # Pull latest Mod6_assignment In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019_2022 cd ~/ycit019_2022/Mod6_assignment/ ls Step 2 Go into the local repository you've created: export student_name=<write_your_name_here_and_remove_brakets> Important Replace above with your project_id student_name cd ~/$student_name-notepad Step 3 Copy Mod6_assignment folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019_2022/Mod6_assignment/ . Step 4 Commit Mod6_assignment folder using the following Git commands: git status git add . git commit -m \"adding `Mod6_assignment` with docker-compose manifest\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Review Cloud Source Repositories Use the Google Cloud Source Repositories code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Mod6_assignment files you pushed to the repository by opening the Navigation menu and selecting Source Repositories: Click Menu -> Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit.","title":"Prepare the Cloud Source Repository Environment with Module 6 Assignment"},{"location":"ycit019_ass2/#2-build-and-push-docker-images-to-google-container-registry-gcr","text":"","title":"2 Build and push Docker images to Google Container Registry (GCR)"},{"location":"ycit019_ass2/#21-build-and-push-gowebapp-mysql-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$student_name-notepad/Mod6_assignment/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Set the Project ID in Environment Variable: export PROJECT_ID=<project_id> Here is how you can find you project_ID: Set the project ID as default gcloud config set project $PROJECT_ID Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"2.1 Build and push gowebapp-mysql Image to GCR"},{"location":"ycit019_ass2/#21-build-and-push-gowebapp-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$student_name-notepad/Mod6_assignment/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Note We've updated our application to support golang version 1.16. Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry Step 7 Delete locally build images, as we want to test how images will be pulled from gcr registry in the next step: docker rmi gcr.io/$PROJECT_ID/gowebapp:v1 docker rmi gcr.io/$PROJECT_ID/gowebapp-mysql:v1","title":"2.1 Build and push gowebapp Image to GCR"},{"location":"ycit019_ass2/#23-test-application-by-running-with-docker-engine","text":"Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Wait for mysql container to start docker ps docker logs <id> You should see following output: [Server] /usr/sbin/mysqld: ready for connections Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed.","title":"2.3 Test application by running with Docker Engine."},{"location":"ycit019_ass2/#24-cleanup-running-applications-and-unused-networks","text":"docker ps docker rm -f <container_id_gowebapp> <container_id_gowebapp-mysql> docker network docker network rm gowebapp","title":"2.4 Cleanup running applications and unused networks"},{"location":"ycit019_ass2/#3-docker-compose","text":"","title":"3 Docker Compose"},{"location":"ycit019_ass2/#31-test-application-locally-with-docker-compose","text":"Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v3 documentations Implementation Ensure that Mysql start first and then webapp services Ensure that Mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Step 1 Create compose file cd ~/$student_name-notepad/Mod6_assignment Edit existing docker-compose file: edit docker-compose.yaml Create structure as following: #TODO Specify docker-compose file '3.4' services: gowebapp-mysql: #TODO Make a build in the `gowebapp-mysql` folder #TODO Add Environment variable for MYSQL_DATABASE #TODO Add healthcheck test that you can connect to Database and execute `SHOW DATABASES` #TODO Add healthcheck, timeout: 45s, interval: 10s, retries: 10, start_period 15 sec #TODO Reference doc for healthcheck https://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck #TODO Add Restart `always` #TODO Attach to created to User-defined network `gowebapp1` gowebapp: #TODO Make a build in the appropriate folder #TODO Allocate ports so that contaiener port mapped to local port #TODO Restart container in case failed #TODO Attach to created to User-defined network `gowebapp1` #TODO Add Environment variable for GOPATH #TODO Add Dependency for `gowebapp-mysql` to start first networks: #TODO Create User-defined network `gowebapp1` type `driver: bridge` Note In this case as we using Compose v3.4, you can also try start_period in the health-check Step 2 Run compose file export CLOUDSDK_PYTHON=python2 # https://github.com/google-github-actions/setup-gcloud/issues/128 docker-compose up -d Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!!","title":"3.1 Test application locally with Docker Compose"},{"location":"ycit019_ass2/#32-commit-docker-compose-file-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"3.2 Commit docker-compose file to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass2/#33-submit-link-to-your-assignment-to-lms","text":"Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$student_name-notepad e.g: https://source.cloud.google.com/ycit019_2022-project/ayratk-notepad","title":"3.3 Submit link to your assignment to LMS"},{"location":"ycit019_ass2_solution/","text":"1 Containerize Applications \u00b6 Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose Prepare the Cloud Source Repository Environment with Module 6 Assignment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. Cloud Source Repositories: Qwik Start Step 1 Locate directory where docker-compose manifest going to be stored. cd ~/ycit019_2022/ git pull # Pull latest Mod6_assignment In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019_2022 cd ~/ycit019_2022/Mod6_assignment/ ls Step 2 Go into the local repository you've created: export student_name=<write_your_name_here_and_remove_brakets> Important Replace above with your project_id student_name cd ~/$student_name-notepad Step 3 Copy Mod6_assignment folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019_2022/Mod6_assignment/ . Step 4 Commit Mod6_assignment folder using the following Git commands: git status git add . git commit -m \"adding `Mod6_assignment` with docker-compose manifest\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Review Cloud Source Repositories Use the Google Cloud Source Repositories code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Mod6_assignment files you pushed to the repository by opening the Navigation menu and selecting Source Repositories: Click Menu -> Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. 2 Build and push Docker images to Google Container Registry (GCR) \u00b6 2.1 Build and push gowebapp-mysql Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$student_name-notepad/Mod6_assignment/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Set the Project ID in Environment Variable: export PROJECT_ID=<project_id> Here is how you can find you project_ID: Set the project ID as default gcloud config set project $PROJECT_ID Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 2.1 Build and push gowebapp Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$student_name-notepad/Mod6_assignment/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Note We've updated our application to support golang version 1.16. Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry Step 7 Delete locally build images, as we want to test how images will be pulled from gcr registry in the next step: docker rmi gcr.io/$PROJECT_ID/gowebapp:v1 docker rmi gcr.io/$PROJECT_ID/gowebapp-mysql:v1 2.3 Test application by running with Docker Engine. \u00b6 Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Wait for mysql container to start docker ps docker logs <id> You should see following output: [Server] /usr/sbin/mysqld: ready for connections Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. 2.4 Cleanup running applications and unused networks \u00b6 docker ps docker rm -f <container_id_gowebapp> <container_id_gowebapp-mysql> docker network docker network rm gowebapp 3 Docker Compose \u00b6 3.1 Test application locally with Docker Compose \u00b6 Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v3 documentations Implementation Ensure that Mysql start first and then webapp services Ensure that Mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Step 1 Create compose file cd ~/$student_name-notepad/Mod6_assignment Edit existing docker-compose file: edit docker-compose.yaml Create structure as following: #TODO Specify docker-compose file '3.4' services: gowebapp-mysql: #TODO Make a build in the `gowebapp-mysql` folder #TODO Add Environment variable for MYSQL_DATABASE #TODO Add healthcheck test that you can connect to Database and execute `SHOW DATABASES` #TODO Add healthcheck, timeout: 45s, interval: 10s, retries: 10, start_period 15 sec #TODO Reference doc for healthcheck https://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck #TODO Add Restart `always` #TODO Attach to created to User-defined network `gowebapp1` gowebapp: #TODO Make a build in the appropriate folder #TODO Allocate ports so that contaiener port mapped to local port #TODO Restart container in case failed #TODO Attach to created to User-defined network `gowebapp1` #TODO Add Environment variable for GOPATH #TODO Add Dependency for `gowebapp-mysql` to start first networks: #TODO Create User-defined network `gowebapp1` type `driver: bridge` Note In this case as we using Compose v3.4, you can also try start_period in the health-check Step 2 Run compose file export CLOUDSDK_PYTHON=python2 # https://github.com/google-github-actions/setup-gcloud/issues/128 docker-compose up -d Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!! 3.2 Commit docker-compose file to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.3 Grant viewing permissions for a repository to Instructors/Teachers \u00b6 Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$student_name-notepad e.g: https://source.cloud.google.com/ycit019_2022-project/ayratk-notepad","title":"1 Containerize Applications"},{"location":"ycit019_ass2_solution/#1-containerize-applications","text":"Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose","title":"1 Containerize Applications"},{"location":"ycit019_ass2_solution/#prepare-the-cloud-source-repository-environment-with-module-6-assignment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. Cloud Source Repositories: Qwik Start Step 1 Locate directory where docker-compose manifest going to be stored. cd ~/ycit019_2022/ git pull # Pull latest Mod6_assignment In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019_2022 cd ~/ycit019_2022/Mod6_assignment/ ls Step 2 Go into the local repository you've created: export student_name=<write_your_name_here_and_remove_brakets> Important Replace above with your project_id student_name cd ~/$student_name-notepad Step 3 Copy Mod6_assignment folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019_2022/Mod6_assignment/ . Step 4 Commit Mod6_assignment folder using the following Git commands: git status git add . git commit -m \"adding `Mod6_assignment` with docker-compose manifest\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Review Cloud Source Repositories Use the Google Cloud Source Repositories code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Mod6_assignment files you pushed to the repository by opening the Navigation menu and selecting Source Repositories: Click Menu -> Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit.","title":"Prepare the Cloud Source Repository Environment with Module 6 Assignment"},{"location":"ycit019_ass2_solution/#2-build-and-push-docker-images-to-google-container-registry-gcr","text":"","title":"2 Build and push Docker images to Google Container Registry (GCR)"},{"location":"ycit019_ass2_solution/#21-build-and-push-gowebapp-mysql-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$student_name-notepad/Mod6_assignment/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Set the Project ID in Environment Variable: export PROJECT_ID=<project_id> Here is how you can find you project_ID: Set the project ID as default gcloud config set project $PROJECT_ID Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"2.1 Build and push gowebapp-mysql Image to GCR"},{"location":"ycit019_ass2_solution/#21-build-and-push-gowebapp-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$student_name-notepad/Mod6_assignment/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Note We've updated our application to support golang version 1.16. Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry Step 7 Delete locally build images, as we want to test how images will be pulled from gcr registry in the next step: docker rmi gcr.io/$PROJECT_ID/gowebapp:v1 docker rmi gcr.io/$PROJECT_ID/gowebapp-mysql:v1","title":"2.1 Build and push gowebapp Image to GCR"},{"location":"ycit019_ass2_solution/#23-test-application-by-running-with-docker-engine","text":"Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Wait for mysql container to start docker ps docker logs <id> You should see following output: [Server] /usr/sbin/mysqld: ready for connections Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed.","title":"2.3 Test application by running with Docker Engine."},{"location":"ycit019_ass2_solution/#24-cleanup-running-applications-and-unused-networks","text":"docker ps docker rm -f <container_id_gowebapp> <container_id_gowebapp-mysql> docker network docker network rm gowebapp","title":"2.4 Cleanup running applications and unused networks"},{"location":"ycit019_ass2_solution/#3-docker-compose","text":"","title":"3 Docker Compose"},{"location":"ycit019_ass2_solution/#31-test-application-locally-with-docker-compose","text":"Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v3 documentations Implementation Ensure that Mysql start first and then webapp services Ensure that Mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Step 1 Create compose file cd ~/$student_name-notepad/Mod6_assignment Edit existing docker-compose file: edit docker-compose.yaml Create structure as following: #TODO Specify docker-compose file '3.4' services: gowebapp-mysql: #TODO Make a build in the `gowebapp-mysql` folder #TODO Add Environment variable for MYSQL_DATABASE #TODO Add healthcheck test that you can connect to Database and execute `SHOW DATABASES` #TODO Add healthcheck, timeout: 45s, interval: 10s, retries: 10, start_period 15 sec #TODO Reference doc for healthcheck https://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck #TODO Add Restart `always` #TODO Attach to created to User-defined network `gowebapp1` gowebapp: #TODO Make a build in the appropriate folder #TODO Allocate ports so that contaiener port mapped to local port #TODO Restart container in case failed #TODO Attach to created to User-defined network `gowebapp1` #TODO Add Environment variable for GOPATH #TODO Add Dependency for `gowebapp-mysql` to start first networks: #TODO Create User-defined network `gowebapp1` type `driver: bridge` Note In this case as we using Compose v3.4, you can also try start_period in the health-check Step 2 Run compose file export CLOUDSDK_PYTHON=python2 # https://github.com/google-github-actions/setup-gcloud/issues/128 docker-compose up -d Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!!","title":"3.1 Test application locally with Docker Compose"},{"location":"ycit019_ass2_solution/#32-commit-docker-compose-file-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"3.2 Commit docker-compose file to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass2_solution/#33-grant-viewing-permissions-for-a-repository-to-instructorsteachers","text":"Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$student_name-notepad e.g: https://source.cloud.google.com/ycit019_2022-project/ayratk-notepad","title":"3.3 Grant viewing permissions for a repository to Instructors/Teachers"},{"location":"ycit019_ass3/","text":"1 Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating NameSpaces Review process of changing Context Review process of creating K8s: Services Labels, Selectors Deployments Rolling Updates 1.1 Development Tools \u00b6 Note This steps is optional Step 1 Choose one of the following option to develop the YAML manifests: Option 1: You can develop in Google Cloud Shell Editor Option 2: You can also develop locally on your laptop using VCScode . We recommend to use it in conjunction with VSC YAML extension from Redhat Option 4: Use your preferred text editor on Linux VM (vim, nano). Option 3: Use your preferred text editor on Linux VM (vim, nano). 2.1 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c 2.2 Setup KUBECTL AUTOCOMPLETE \u00b6 Since we going to use a lot of kubectl cli let's setup autocomplete. source <(kubectl completion bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 2.3 Create 'dev' namespace and make it default. \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl #TO DO(create namespace `dev`) Step 2 Use dev context to create K8s resources inside this namespace. kubectl #TO DO (make `dev` context default) Step 3 Verify current context: kubectl config current-context Result dev 2.4 Create Service Object for MySQL \u00b6 Step 1 Locate directory where kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment3/ ls Step 2 Go into the local repository you've created: cd ~/$MY_REPO Step 3 Copy Assignment 3 deploy folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019/Assignment3/deploy . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests in deploy folder\" Step 5 Once you've committed code to the local repository , add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Define a Kubernetes Service object for the backend MySQL database. cd ~/$MY_REPO/deploy Follow instructions below to populate gowebapp-mysql-service.yaml For reference, please see Service docs : https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service Additionally, you can use kubectl built-in docs for any type of resources: kubectl explain service vim gowebapp-mysql-service.yaml Note You can also use VCS or Cloud Code to work with yaml manifest. #TODO: Specify Kubernetes API apiVersion #TODO: Identify the kind of Object metadata: #TODO: Give the service a name: \"gowebapp-mysql\" labels: #TODO: Add a label KV \"run: gowebapp-mysql\" spec: #TODO: leave the clusterIP to None. We allow k8s to assign clusterIP. ports: #TODO: Define a \"port\" as 3306 #TODO: Define a \"targetPort\" as 3306 #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp-mysql\" Step 3 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 4 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\" 2.5 Create Deployment object for the backend MySQL database \u00b6 Step 1 Follow instructions below to populate gowebapp-mysql-deployment.yaml For reference, please see Deployment doc: https://kubernetes.io/docs/concepts/workloads/controllers/deployment vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 #TODO: Identify the type of Object metadata: #TODO: Give the Deployment a name \"gowebapp-mysql\" labels: #TODO: Add a label KV \"run: gowebapp-mysql\" #TODO: give the Deployment a label: tier: backend spec: #TODO: Define number of replicas, set it to 1 #TODO: Starting from Deplloyment v1 selectors are mandatory #add selector KV \"run: gowebapp-mysql\" strategy: type: # Set strategy type as `Recreate` template: metadata: #TODO: Add a label called \"run\" with the name of the service: \"gowebapp-mysql\" spec: containers: - env: - name: # TODO add MYSQL_ROOT_PASSWORD env value image: #TODO define mysql image created in previous assignment, located in gcr registry name: gowebapp-mysql ports: #TODO: define containerPort: 3306 Step 2 Create a Deployment object for MySQL kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp-mysql\" Step 3 Check mysql pod logs: List mysql Pods and note the name the pod : kubectl get pods -l \"run=gowebapp-mysql\" Ensure Mysql is up by looking at pod logs: kubectl logs <Pod_name> Result We have created Service and Deployment for backend application. 2.5 Create a K8s Service for the frontend gowebapp. \u00b6 Step 1 Follow instructions below to populate gowebapp-service.yaml vim gowebapp-service.yaml apiVersion: v1 kind: Service metadata: name: gowebapp labels: #TODO: give the Service a label: run: gowebapp #TODO: give the Service a label: tier: frontend spec: #TODO: Define a \"ports\" array with the \"port\" attribute: 9000 and \"targetPort\" attributes: 80 #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp\" #TODO: Add a Service Type of LoadBalancer #If you need help, see reference: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types Step 2 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 3 Check to make sure it worked kubectl get service -l \"run=gowebapp\" 2.6 Create a K8s Deployment object for the frontend gowebapp \u00b6 Step 1 Follow instructions below to populate gowebapp-deployment.yaml vim gowebapp-deployment.yaml apiVersion: apps/v1 #TODO: define the kind of object as Deployment metadata: #TODO: Add a name attribute for the service as \"gowebapp\" labels: #TODO: give the Deployment a label: run: gowebapp #TODO: give the Deployment a label: tier: frontend spec: #TODO: Define number of replicas, set it to 2 #TODO: add selector KV \"run: gowebapp\" template: metadata: labels: run: gowebapp tier: frontend spec: containers: - env: - #TODO: define name as MYSQL_ROOT_PASSWORD #TODO: define value as cloudops #TODO: Replace <user-name> with value you Docker-Hub ID. image: #TODO define gowebapp image created in previous assignment, located in gcr registry name: gowebapp ports: - #TODO: define the container port as 80 Step 2 Create a Deployment object for gowebapp kubectl apply -f gowebapp-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp\" Step 4 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 5 Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes 2.7 Fix gowebapp code bugs and build a new image. \u00b6 Task: As you've noticed gowebapp frontend app has YCIT019 logo in it. Since you may want to use application for you personal needs, let's change YCIT019 logo to you Name . Step 1 Modify gowebapp frontend so that it has name of you company and link to company web page e.g. vim ~/$MY_REPO/gowebapp/code/template/partial/footer.tmpl Step 2 Build a new version of Image cd ~/$MY_REPO/gowebapp docker build -t gcr.io/${PROJECT_ID}/gowebapp:v2 . docker push gcr.io/${PROJECT_ID}/gowebapp:v2 . 2.7 Rolling Upgrade \u00b6 For gowebapp frontend deployment manifest we've not specified any upgrade strategy type. It means application will use default Upgrade strategy called RollingUpdate . RollingUpdate strategy - updates Pods in a rolling update fashion. maxUnavailable - is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. By default, it ensures that at least 25% less than the desired number of Pods are up (25% max unavailable). Max Surge - is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. By default, it ensures that at most 25% more than the desired number of Pods are up (25% max surge). Step 1 Locate directory with manifest cd ~/$MY_REPO/deploy Step 2 Trigger rolling upgrade using kubectl set command #TO DO Step 3 Verify rollout history #TO DO Step 4 Perform Rollback to v1 #TO DO 2.8 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 2.9 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"Module 8 Assignment"},{"location":"ycit019_ass3/#1-deploy-applications-on-kubernetes","text":"Objective: Review process of creating NameSpaces Review process of changing Context Review process of creating K8s: Services Labels, Selectors Deployments Rolling Updates","title":"1 Deploy Applications on Kubernetes"},{"location":"ycit019_ass3/#11-development-tools","text":"Note This steps is optional Step 1 Choose one of the following option to develop the YAML manifests: Option 1: You can develop in Google Cloud Shell Editor Option 2: You can also develop locally on your laptop using VCScode . We recommend to use it in conjunction with VSC YAML extension from Redhat Option 4: Use your preferred text editor on Linux VM (vim, nano). Option 3: Use your preferred text editor on Linux VM (vim, nano).","title":"1.1 Development Tools"},{"location":"ycit019_ass3/#21-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c","title":"2.1 Create GKE Cluster"},{"location":"ycit019_ass3/#22-setup-kubectl-autocomplete","text":"Since we going to use a lot of kubectl cli let's setup autocomplete. source <(kubectl completion bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"2.2 Setup KUBECTL AUTOCOMPLETE"},{"location":"ycit019_ass3/#23-create-dev-namespace-and-make-it-default","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl #TO DO(create namespace `dev`) Step 2 Use dev context to create K8s resources inside this namespace. kubectl #TO DO (make `dev` context default) Step 3 Verify current context: kubectl config current-context Result dev","title":"2.3 Create 'dev' namespace and make it default."},{"location":"ycit019_ass3/#24-create-service-object-for-mysql","text":"Step 1 Locate directory where kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment3/ ls Step 2 Go into the local repository you've created: cd ~/$MY_REPO Step 3 Copy Assignment 3 deploy folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019/Assignment3/deploy . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests in deploy folder\" Step 5 Once you've committed code to the local repository , add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Define a Kubernetes Service object for the backend MySQL database. cd ~/$MY_REPO/deploy Follow instructions below to populate gowebapp-mysql-service.yaml For reference, please see Service docs : https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service Additionally, you can use kubectl built-in docs for any type of resources: kubectl explain service vim gowebapp-mysql-service.yaml Note You can also use VCS or Cloud Code to work with yaml manifest. #TODO: Specify Kubernetes API apiVersion #TODO: Identify the kind of Object metadata: #TODO: Give the service a name: \"gowebapp-mysql\" labels: #TODO: Add a label KV \"run: gowebapp-mysql\" spec: #TODO: leave the clusterIP to None. We allow k8s to assign clusterIP. ports: #TODO: Define a \"port\" as 3306 #TODO: Define a \"targetPort\" as 3306 #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp-mysql\" Step 3 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 4 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\"","title":"2.4 Create Service Object for MySQL"},{"location":"ycit019_ass3/#25-create-deployment-object-for-the-backend-mysql-database","text":"Step 1 Follow instructions below to populate gowebapp-mysql-deployment.yaml For reference, please see Deployment doc: https://kubernetes.io/docs/concepts/workloads/controllers/deployment vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 #TODO: Identify the type of Object metadata: #TODO: Give the Deployment a name \"gowebapp-mysql\" labels: #TODO: Add a label KV \"run: gowebapp-mysql\" #TODO: give the Deployment a label: tier: backend spec: #TODO: Define number of replicas, set it to 1 #TODO: Starting from Deplloyment v1 selectors are mandatory #add selector KV \"run: gowebapp-mysql\" strategy: type: # Set strategy type as `Recreate` template: metadata: #TODO: Add a label called \"run\" with the name of the service: \"gowebapp-mysql\" spec: containers: - env: - name: # TODO add MYSQL_ROOT_PASSWORD env value image: #TODO define mysql image created in previous assignment, located in gcr registry name: gowebapp-mysql ports: #TODO: define containerPort: 3306 Step 2 Create a Deployment object for MySQL kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp-mysql\" Step 3 Check mysql pod logs: List mysql Pods and note the name the pod : kubectl get pods -l \"run=gowebapp-mysql\" Ensure Mysql is up by looking at pod logs: kubectl logs <Pod_name> Result We have created Service and Deployment for backend application.","title":"2.5 Create Deployment object for the backend MySQL database"},{"location":"ycit019_ass3/#25-create-a-k8s-service-for-the-frontend-gowebapp","text":"Step 1 Follow instructions below to populate gowebapp-service.yaml vim gowebapp-service.yaml apiVersion: v1 kind: Service metadata: name: gowebapp labels: #TODO: give the Service a label: run: gowebapp #TODO: give the Service a label: tier: frontend spec: #TODO: Define a \"ports\" array with the \"port\" attribute: 9000 and \"targetPort\" attributes: 80 #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp\" #TODO: Add a Service Type of LoadBalancer #If you need help, see reference: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types Step 2 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 3 Check to make sure it worked kubectl get service -l \"run=gowebapp\"","title":"2.5 Create a K8s Service for the frontend gowebapp."},{"location":"ycit019_ass3/#26-create-a-k8s-deployment-object-for-the-frontend-gowebapp","text":"Step 1 Follow instructions below to populate gowebapp-deployment.yaml vim gowebapp-deployment.yaml apiVersion: apps/v1 #TODO: define the kind of object as Deployment metadata: #TODO: Add a name attribute for the service as \"gowebapp\" labels: #TODO: give the Deployment a label: run: gowebapp #TODO: give the Deployment a label: tier: frontend spec: #TODO: Define number of replicas, set it to 2 #TODO: add selector KV \"run: gowebapp\" template: metadata: labels: run: gowebapp tier: frontend spec: containers: - env: - #TODO: define name as MYSQL_ROOT_PASSWORD #TODO: define value as cloudops #TODO: Replace <user-name> with value you Docker-Hub ID. image: #TODO define gowebapp image created in previous assignment, located in gcr registry name: gowebapp ports: - #TODO: define the container port as 80 Step 2 Create a Deployment object for gowebapp kubectl apply -f gowebapp-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp\" Step 4 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 5 Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes","title":"2.6 Create a K8s Deployment object for the frontend gowebapp"},{"location":"ycit019_ass3/#27-fix-gowebapp-code-bugs-and-build-a-new-image","text":"Task: As you've noticed gowebapp frontend app has YCIT019 logo in it. Since you may want to use application for you personal needs, let's change YCIT019 logo to you Name . Step 1 Modify gowebapp frontend so that it has name of you company and link to company web page e.g. vim ~/$MY_REPO/gowebapp/code/template/partial/footer.tmpl Step 2 Build a new version of Image cd ~/$MY_REPO/gowebapp docker build -t gcr.io/${PROJECT_ID}/gowebapp:v2 . docker push gcr.io/${PROJECT_ID}/gowebapp:v2 .","title":"2.7 Fix gowebapp code bugs and build a new image."},{"location":"ycit019_ass3/#27-rolling-upgrade","text":"For gowebapp frontend deployment manifest we've not specified any upgrade strategy type. It means application will use default Upgrade strategy called RollingUpdate . RollingUpdate strategy - updates Pods in a rolling update fashion. maxUnavailable - is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. By default, it ensures that at least 25% less than the desired number of Pods are up (25% max unavailable). Max Surge - is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. By default, it ensures that at most 25% more than the desired number of Pods are up (25% max surge). Step 1 Locate directory with manifest cd ~/$MY_REPO/deploy Step 2 Trigger rolling upgrade using kubectl set command #TO DO Step 3 Verify rollout history #TO DO Step 4 Perform Rollback to v1 #TO DO","title":"2.7 Rolling Upgrade"},{"location":"ycit019_ass3/#28-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"2.8 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ycit019_ass3/#29-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"2.9 Cleaning Up"}]}