{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cloud Architecture","text":""},{"location":"020_Lab_10_cloudrun/","title":"Lab 5 Cloudrun for Anthos","text":"<p>Objective:</p> <ul> <li>Enable CloudRun on a Kubernetes cluster</li> <li>Deploy a web application</li> <li>Autoscale applications from 0-to-1 and back to 0</li> </ul>"},{"location":"020_Lab_10_cloudrun/#0-create-regional-gke-cluster-on-gcp-and-enable-cloudrun","title":"0 Create Regional GKE Cluster on GCP and Enable CloudRun","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable \\\ncontainer.googleapis.com \\\ncontainerregistry.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node and enable CloudRun:</p> <pre><code>gcloud container clusters create cloudrun-demo \\\n  --zone=us-central1-b \\\n  --num-nodes=1 \\\n  --machine-type=n1-standard-4 \\\n  --enable-autoscaling --min-nodes=1 --max-nodes=5 \\\n  --enable-ip-alias \\\n  --addons=HttpLoadBalancing,CloudRun \\\n  --enable-stackdriver-kubernetes \\\n  --release-channel stable\n</code></pre> <p>Enabling CloudRun will install Knative components on the cluster. As we discussed in class, Knative makes it possible to:</p> <ul> <li>Deploy and serve applications using Knative API. </li> <li>Allow applications automatically scale from zero-to-N, and back to zero, based on requests.</li> <li>Build and package the application code inside the cluster.</li> <li>Deliver events to your application; However, this feature is not enabled by CloudRun.</li> </ul> <p>Step 3 Verify the Knative components are properly installed.</p> <pre><code>kubectl get pods --namespace=knative-serving\n</code></pre>"},{"location":"020_Lab_10_cloudrun/#1-deploy-a-knative-service","title":"1 Deploy a Knative Service","text":"<p>Step 1 deploy a Knatvie service to the cluster you just created. For this example, we will use <code>kubectl</code>; however, you can also use the CloudRun UI. To expose an application on Knative, we need to define a Service object, which is different than the Kubernetes Service type.</p> <pre><code>cat &lt;&lt;EOF&gt; helloworld.yaml\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/cloudrun/hello\n          env:\n            - name: TARGET\n              value: \"Go Sample v1\"\nEOF \n</code></pre> <pre><code>kubectl apply -f helloworld.yaml\n</code></pre> <p>Step 2 Verify the service is deployed by querying \"ksvc\" (Knative Service) objects</p> <pre><code>kubectl get ksvc\n</code></pre> <p>Step 3 Although the Knative service is deployed, there are no Pod created yet, given that there are no requests on this service.</p> <pre><code>kubectl get pod\n</code></pre>"},{"location":"020_Lab_10_cloudrun/#2-test-the-knative-service","title":"2 Test the Knative Service","text":"<p>Step 1 Let's test the service by making a request using <code>curl</code>.</p> <p>To avoid having to set up DNS, we'll test the deployed service by sending a request to the Istio ingress gateway with the target host that should handle the request as an HTTP header. That hostname should be the URL listed in the previous deployment step and of the form: http://service-name.namespace.example.com</p> <p>Step 2 Obtain the IP address of the ingress gateway</p> <pre><code>kubectl get svc -n gke-system istio-ingress\n</code></pre> <p>Step 3 From Cloud Shell, use curl to access the service. Use the URL we obtained in step 5, and replace [YOUR-IP] with the IP address you obtained in the previous step.</p> <pre><code>curl -v -H \"Host: helloworld-go.default.example.com\" http://[YOUR-IP]\n</code></pre> <p>Step 4 Verify that a pod was created and that this pod will get termintated if there are no more requests on this service.</p> <pre><code>kubectl get pod\n</code></pre>"},{"location":"020_Lab_10_cloudrun/#3-knative-serving-api","title":"3 Knative Serving API","text":"<p>When we deploy the helloworld Service to Knative, it creates three kinds of objects: Configuration, Route, and Revision.</p> <pre><code>kubectl get configuration,revision,route\n</code></pre> <ul> <li>Service Describes an application on Knative.</li> <li>Revision Read-only snapshot of an application's image and other settings</li> <li>Configuration Created automatically by the service. It creates a new Revision when the revisionTemplate field changes.</li> <li>Route Configures how the traffic coming to the Service should be split between Revisions.</li> </ul>"},{"location":"020_Lab_10_cloudrun/#4-cleanup","title":"4 Cleanup","text":"<p>Delete the GKE cluster:</p> <pre><code>gcloud container clusters delete cloudrun-demo --region us-central1\n</code></pre>"},{"location":"020_Lab_11_GitOps/","title":"GitOps with Flux","text":"<p>In this lab you'll build a GitOps pipeline using Flux.</p> <p>Objective:</p> <ul> <li>Install <code>Flux</code></li> <li>Bootstrap <code>Flux</code> with a new <code>flux-infra</code> repository</li> <li>Add a <code>GitRepository</code> source type to track the <code>microservices-demo</code>Public application repository which also contains the deployment code. Flux should regularly sync the code from it.</li> <li>Use the <code>Kustomization</code> Controller to set up automated deployment to the Kubernetes environment. Any changes in the source should be automatically reconciled with the cluster.</li> </ul>"},{"location":"020_Lab_11_GitOps/#0-create-regional-gke-cluster-on-gcp","title":"0 Create Regional GKE Cluster on GCP","text":"<p>This step is optional, feel free to use the notepad cluster instead.</p> <p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud container clusters create k8s-gitops-lab \\\n--region us-central1 \\\n--enable-ip-alias \\\n--enable-network-policy \\\n--num-nodes 1 \\\n--machine-type \"e2-standard-2\" \\\n--release-channel stable\n</code></pre> <pre><code>gcloud container clusters get-credentials k8s-gitops-lab --region us-central1\n</code></pre> <p>Step 3: (Optional) Setup kubectx</p> <pre><code>sudo apt install kubectx\n</code></pre> <p>Note</p> <p>we've installed <code>kubectx</code> + <code>kubens</code>: Power tools for <code>kubectl</code>: - <code>kubectx</code> helps you switch between clusters back and forth - <code>kubens</code> helps you switch between Kubernetes namespaces smoothly</p>"},{"location":"020_Lab_11_GitOps/#1-install-and-configure-flux-with-github-repository","title":"1 Install and Configure Flux with GitHub Repository","text":"<p>To Install Flux we going to perform following tasks:</p> <ul> <li>Create a Personal Access Token on GitHub.</li> <li>Set up environment variables.</li> <li>Install flux CLI.</li> <li>Run pre-flight checks.</li> <li>Bootstrap the Flux v2 infrastructure.</li> <li>Validate the Flux installation.</li> </ul>"},{"location":"020_Lab_11_GitOps/#11-install-the-flux-cli","title":"1.1 Install the Flux CLI","text":"<p>With <code>Bash</code> for macOS and Linux:</p> <pre><code>curl -s https://fluxcd.io/install.sh | sudo bash\nflux check --pre\n</code></pre> <p>Reference: Get Started with Flux</p>"},{"location":"020_Lab_11_GitOps/#12-configure-a-github-personal-access-token-with-repo-permissions","title":"1.2 Configure a GitHub personal access token with repo permissions.","text":"<p>Follow the GitHub documentation on creating a Personal Access Token (PAT). Ensure you give the token ability to create repositories as Flux will manage a repository with the contents.</p> <p>Result</p> <p>You've created <code>GITHUB_TOKEN</code> that will be used to boostrap the Flux</p> <p>Note</p> <p>Personal Access Token (PAT):</p> <p>Flux <code>bootstrap github</code> command creates a GitHub repository if one doesn\u2019t exist and commits the Flux components manifests to specified branch. Then it configures the target cluster to synchronize with that repository by setting up an SSH deploy key or by using token-based authentication. That is why Flux requires a PAT for initial boostrapping.</p> <p>Deploy key:</p> <p>The bootstrap command creates an SSH key which it stores as a secret in the Kubernetes cluster. The key is also used to create a deploy key in the GitHub repository. The new deploy key will be linked to the personal access token used to authenticate. Removing the personal access token will also remove the deploy key.</p>"},{"location":"020_Lab_11_GitOps/#13-export-your-credentials","title":"1.3 Export your credentials","text":"<p>Export your GitHub personal access token, generated in step 1.2 and <code>github</code> username:</p> <pre><code>export GITHUB_TOKEN=&lt;your-token&gt;\nexport GITHUB_USER=&lt;your-username&gt;\n</code></pre>"},{"location":"020_Lab_11_GitOps/#14-install-flux-into-gke-cluster","title":"1.4 Install Flux into GKE cluster","text":"<p>Run the bootstrap command:</p> <pre><code>flux bootstrap github \\\n  --owner $GITHUB_USER \\\n  --repository flux-infra \\\n  --branch main \\\n  --path \"./clusters/dev\" \\\n  --personal \\\n  --log-level debug\n</code></pre> <p>Output:</p> <pre><code>\u2714 repository \"\"https://github.com/fawix/flux-infra.git\" created\n\u2714 cloned repository\n\u2714 generated component manifests\n\u2714 committed sync manifests to \"main\" (\"6cd871ba49834e8106884f3c8daad2f75c6b341f\")\n\u2714 installed components\n\u2714 reconciled components\n\u2714 configured deploy key \"flux-system-main-flux-system-./clusters/dev\" for \"\"https://github.com/fawix/flux-infra.git\"\n\u2714 reconciled source secret\n\u2714 generated sync manifests\n\u2714 committed sync manifests to \"main\" (\"7d0122f2e4e57ada4f88d7a968528879b07b56e8\")\n\u2714 reconciled sync configuration\n\u2714 Kustomization reconciled successfully\n\u2714 helm-controller: deployment ready\n\u2714 kustomize-controller: deployment ready\n\u2714 source-controller: deployment ready\n\u2714 all components are healthy\n</code></pre> <p>Verify Install:</p> <pre><code>flux check\n</code></pre> <pre><code>kubectl get crd | grep flux\n</code></pre> <p>Output:</p> <pre><code>buckets.source.toolkit.fluxcd.io                 2021-09-08T18:07:34Z\ngitrepositories.source.toolkit.fluxcd.io         2021-09-08T18:07:34Z\nhelmcharts.source.toolkit.fluxcd.io              2021-09-08T18:07:34Z\nhelmreleases.helm.toolkit.fluxcd.io              2021-09-08T18:07:34Z\nhelmrepositories.source.toolkit.fluxcd.io        2021-09-08T18:07:35Z\nkustomizations.kustomize.toolkit.fluxcd.io       2021-09-08T18:07:35Z\n</code></pre> <pre><code>kubectl get gitrepositories -n flux-system\n</code></pre> <pre><code>flux get all\n</code></pre> <p>Summary</p> <p>We've <code>flux bootstrap</code> command we've achieved:</p> <ul> <li>flux agent components which includes (namespace, CRDs, Sources, Controllers, RBAC) on our cluster<ul> <li>3 Controllers:</li> <li>source</li> <li>kustomize</li> <li>helm</li> <li>6 CRDs: </li> <li>buckets.source</li> <li>gitrepositories.source</li> <li>helmcharts.source</li> <li>helmrepositories.source</li> <li>helmreleases.helm</li> <li>kustomizations.kustomize</li> </ul> </li> <li>Created <code>flux-infra</code> Private Github repo with Deploy key</li> <li><code>flux-infra</code> repo contains flux-infra/clusters/dev/flux-system/ folder</li> </ul>"},{"location":"020_Lab_11_GitOps/#15-deploy-microservices-applications-with-gitops-using-kustomize-controller","title":"1.5 Deploy <code>microservices</code> applications with GitOps using Kustomize Controller","text":"<p>Let's make a fork of the app, in case we want to deploy changes to test.</p> <p>Step 1 Fork microservice application</p> <p>Browse to:</p> <pre><code>https://github.com/GoogleCloudPlatform/microservices-demo\n</code></pre> <pre><code>Click: Fork button\nSelect: Fork to a different account\n</code></pre> <p>Step 2 Clone the flux-infra repo</p> <p>Now that Flux has created the git repo let's clone it so we can start managing our sources with it.</p> <pre><code>git clone git@github.com:$GITHUB_USER/flux-infra.git\ncd flux-infra\n</code></pre> <p>And let's apply gitops principles to manage our sources and kustomizations. So, let's tell flux where is our source:</p> <pre><code>flux create source git microservices-demo \\\n  --url=https://github.com/$GITHUB_USER/microservices-demo \\\n  --branch=main \\\n  --interval=30s \\\n  --export | tee clusters/dev/onlineboutique.yaml\n</code></pre> <p>Note</p> <p>If you want you can commit now to see how flux picks it up but that is not necessary</p> <p>And let's tell it how to deploy that source, we will use the kustomizaiton in the <code>./kustomize</code> folder of the repo</p> <pre><code>flux create kustomization  microservices-demo-dev \\\n  --source microservices-demo \\\n  --path \"./kustomize/\" \\\n  --prune true \\\n  --interval 30s \\\n  --target-namespace onlineboutique \\\n  --export | tee -a clusters/dev/onlineboutique.yaml\n</code></pre> <p>Finally we must create the namespace ourselves:</p> <pre><code>kubectl create ns onlineboutique\nkubens onlineboutique\n</code></pre> <p>Now commit the changes and watch flux deploy the app:</p> <pre><code>git add clusters/dev/onlineboutique.yaml\ngit commit -m \"adding onlineboutique definition\" \ngit push\n\nflux get source git \nflux get kustomization\nkubectl get pods -n onlineboutique\n</code></pre> <p>Output:   - You'll see <code>microservices-demo</code> under the git sources in Flux   - You'll see <code>microservices-demo-dev</code> under the kustomizations in Flux   - You'll see the app pods come online in the namespace <code>onlineboutique</code></p> <p>Result</p> <p>Onlineboutique pods has been deployed with FluxCD via GitOps</p> <pre><code>kubectl get svc\n</code></pre> <p>Access Onlineboutique <code>Frontend</code> via <code>LoadBalancer</code> IP</p> <p>Summary</p> <p>We've deployed microservices onlineboutique application using GitOps</p> <p>Flux keeps the definitions that were created in the repo it manages, so you can easily update it through the repo. If you run the command without the <code>--export</code> flag it will directly create the definitions in the cluster instead, however with gitops principle we want to manage it via the repo.</p> <p>In case you deployed without the <code>--export</code> flag you may recover those files with <code>flux export</code> command.</p> <p>Optional challenge:   - Modify something on your kustomize folder and see Flux deploy it.</p>"},{"location":"020_Lab_11_GitOps/#16-install-opa-gatekeeper","title":"1.6 Install OPA Gatekeeper","text":"<p>Let's follow the same principle and install it via commit on the repo.</p> <p>Let's create the source for the OPA helm chart more info.</p> <pre><code>flux create source helm gatekeeper \\\n  --url=https://open-policy-agent.github.io/gatekeeper/charts \\\n  --interval=1m \\\n  --export | tee clusters/dev/opa-gatekeeper.yaml\n</code></pre> <p>Note that is not pointing to a git repo, it's pointing to a chart registry. Now instead of a kustomization we use that chart release.</p> <pre><code>flux create helmrelease gatekeeper \\\n  --interval=1m \\\n  --source=HelmRepository/gatekeeper \\\n  --chart=gatekeeper \\\n  --target-namespace=gatekeeper \\\n  --chart-version=\"3.10.0\" \\\n  --export | tee -a clusters/dev/opa-gatekeeper.yaml\n</code></pre> <p>Note</p> <p>Inspect the created file to make sure it's following the specs</p> <p>Let's create our namesapce:</p> <pre><code>kubectl create ns gatekeeper\n</code></pre> <p>And commit the changes to the git repo:</p> <pre><code>git add clusters/dev/onlineboutique.yaml\ngit commit -m \"adding gatekeeper to the cluster\" \ngit push\n</code></pre> <p>With that now Flux will add the a helm chart source, and deploy a helm release to the cluster in the <code>gatekeeper</code> namespace</p> <pre><code>flux get source helm \nflux get helmrelease\nkubectl get pods -n gatekeeper\n</code></pre> <p>Output:   - You'll see <code>gatekeeper</code> under the helm sources in Flux   - You'll see <code>gatekeeper</code> under the helm releases in Flux   - You'll see the app pods come online in the namespace <code>onlineboutique</code></p> <p>Result</p> <p>We've deployed Gatekeeper to our GKE cluster. It's time to add some policies!</p>"},{"location":"020_Lab_11_GitOps/#17-deploy-and-test-allowedrepos-policy","title":"1.7 Deploy and Test allowedrepos Policy","text":"<p>Step 1 Let's create a <code>policy</code> folder where we going to store our cluster wide policies:</p> <pre><code>cd ~\nmkdir -p policy/templates\nmkdir -p policy/constraints\n</code></pre> <p>Step 2 Download OSS Gatekeeper libraries:</p> <pre><code>cd ~\ngit clone https://github.com/open-policy-agent/gatekeeper-library\n</code></pre> <p>Step 3 Review <code>allowedrepos</code> constraint template:</p> <pre><code>cp gatekeeper-library/library/general/allowedrepos/template.yaml policy/templates\ncat policy/templates/allowedrepos_template.yaml\n</code></pre> <pre><code>cd  ~/policy/templates\nkubens gatekeeper\nkubectl apply -f allowedrepos_template.yaml\n</code></pre> <p>Review a <code>ConstraintTemplate</code> CRD and observe created resource:</p> <pre><code>kubectl get  ConstraintTemplate\n</code></pre> <p>Note</p> <p>Even if k8sallowedrepos <code>ConstraintTemplate</code> has been deployed in <code>gatekeeper</code> namespace in our case. The Policy will be applied Cluster Wide.</p> <p>Step 4 Let's create a Gatekeeper constraint that will block all Pods, which images are non  <code>gcr.io</code> registries. We first going to test our constraint in a current system and so we going to run it in <code>dryrun</code> enforcementAction mode. We will </p> <pre><code>cd  ~/policy/constraints\ncat &lt;&lt; EOF&gt; allowed-repos.yaml\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sAllowedRepos\nmetadata:\n  name: allowed-repos\nspec:\n  enforcementAction: dryrun\n  match:\n    kinds:\n      - apiGroups: [\"\"]\n        kinds: [\"Pod\"]\n    excludedNamespaces: [\"onlineboutique\"]\n  parameters:\n    repos:\n      - \"gcr.io\"\n      - \"gke.gcr.io\"\n      - \"k8s.gcr.io\"\nEOF\n</code></pre> <p>Note</p> <p>We've added <code>excludedNamespaces</code> clause, as our microservices app has some redis image that is downloaded from dockerhub.</p> <pre><code>kubectl apply -f allowed-repos.yaml\n</code></pre> <pre><code>kubectl describe K8sAllowedRepos\n</code></pre> <p>Output:</p> <pre><code>Total Violations:  12\n  Violations:\n    Enforcement Action:  dryrun\n    Kind:                Pod\n    Message:             container &lt;manager&gt; has an invalid image repo &lt;ghcr.io/fluxcd/helm-controller:v0.11.2&gt;, allowed repos are [\"gcr.io\", \"gke.gcr.io\"]\n    Name:                helm-controller-5cbd57f468-cr7ht\n    Namespace:           flux-system\n</code></pre> <p>Note</p> <p>We can observe ~12 Violations in our cluster. However since our Enforcement Action is <code>dryrun</code>, non the policy will take effect. </p> <pre><code>kubectl describe K8sAllowedRepos | grep Namespace\n</code></pre> <p>Output:</p> <pre><code>    Namespace:           flux-system\n    Namespace:           flux-system\n    Namespace:           flux-system\n    Namespace:           gatekeeper\n    Namespace:           gatekeeper\n    Namespace:           gatekeeper\n    Namespace:           gatekeeper\n    Namespace:           kube-system\n    Namespace:           kube-system\n    Namespace:           kube-system\n    Namespace:           kube-system\n    Namespace:           kube-system\n</code></pre> <p>Note</p> <p>We can observe that all violations are for <code>kube-system</code>, <code>flux-system</code> and <code>gatekeeper</code>. This is due to they using images from following repos <code>k8s.gcr.io</code>, <code>ghcr.io/fluxcd</code>, <code>openpolicyagent/gatekeeper</code> respectively. To workaround this issue we can add above repo's in trusted listed of repos. Another alternative is to <code>copy</code> required images to your personal <code>gcr</code> repo using tools like crane.</p> <p>Step 5 Update K8sAllowedRepos constraint to support trusted registries:</p> <pre><code>cat &lt;&lt; EOF&gt; allowed-repos.yaml\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sAllowedRepos\nmetadata:\n  name: allowed-repos\nspec:\n  enforcementAction: dryrun\n  match:\n    kinds:\n      - apiGroups: [\"\"]\n        kinds: [\"Pod\"]\n    excludedNamespaces: [\"onlineboutique\"]\n  parameters:\n    repos:\n      - \"gcr.io\"\n      - \"gke.gcr.io\"\n      - \"k8s.gcr.io\"\n      - \"ghcr.io/fluxcd\"\n      - \"openpolicyagent/gatekeeper\"\nEOF\n</code></pre> <pre><code>kubectl apply -f allowed-repos.yaml\n</code></pre> <pre><code>kubectl describe K8sAllowedRepos\n</code></pre> <p>Note</p> <p>All violations has been resolved, however it may take some time until the messages will be cleared in the Events.</p> <p>Step 6 Update K8sAllowedRepos constraint and set <code>enforcementAction</code> to <code>deny</code> in order to block all Pods with registry violations:</p> <pre><code>cat &lt;&lt; EOF&gt; allowed-repos.yaml\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sAllowedRepos\nmetadata:\n  name: allowed-repos\nspec:\n  enforcementAction: deny\n  match:\n    kinds:\n      - apiGroups: [\"\"]\n        kinds: [\"Pod\"]\n    excludedNamespaces: [\"onlineboutique\"]\n  parameters:\n    repos:\n      - \"gcr.io\"\n      - \"gke.gcr.io\"\n      - \"k8s.gcr.io\"\n      - \"ghcr.io/fluxcd\"\n      - \"openpolicyagent/gatekeeper\"\nEOF\n</code></pre> <pre><code>kubectl apply -f allowed-repos.yaml\n</code></pre> <pre><code>kubectl describe K8sAllowedRepos\n</code></pre> <p>Result</p> <p>Total Violations is 0, as expected.</p> <p>Step 7 Deploy <code>nginx</code> Pod from dockerhub in <code>default</code> namespace and thus violate our cluster Policy</p> <pre><code>kubectl run nginx --image=nginx -n default\n</code></pre> <p>Output:</p> <pre><code>Error from server ([allowed-repos] container &lt;nginx&gt; has an invalid image repo &lt;nginx&gt;, allowed repos are [\"gcr.io\", \"gke.gcr.io\", \"k8s.gcr.io\", \"ghcr.io/fluxcd\", \"openpolicyagent/gatekeeper\"]): admissionwebhook \"validation.gatekeeper.sh\" denied the request: [allowed-repos] container &lt;nginx&gt; has an invalid image repo &lt;nginx&gt;, allowed repos are [\"gcr.io\", \"gke.gcr.io\", \"k8s.gcr.io\", \"ghcr.io/fluxcd\", \"openpolicyagent/gatekeeper\"]\n</code></pre> <p>Error</p> <p>Gatekeeper Admission Controller blocked nginx Pod deployment due to it's violating the <code>allowedrepos</code> constraint.</p> <p>Summary</p> <p>We've secured our cluster from deployments of dangerous and unauthorized images</p> <p>Optional Challenges:   - Deploy the OPA policies using GitOps   - Deploy the Notepad Applicaiton using GitOps</p>"},{"location":"020_Lab_11_GitOps/#2-cleanup","title":"2 Cleanup","text":"<p>Cluster clean up:</p> <pre><code>flux delete kustomization microservices-demo-dev\nflux delete source git microservices-demo\nflux delete helmrelease gatekeeper\nflux delete source helm gatekeeper\nkubectl delete ns gatekeeper\nkubectl delete ns onlineboutique\n</code></pre> <p>Uninstall flux:</p> <pre><code>flux uninstall --namespace=flux-system\n</code></pre> <p>The above command performs the following operations:</p> <ul> <li>deletes Flux components (deployments and services)</li> <li>deletes Flux network policies</li> <li>deletes Flux RBAC (service accounts, cluster roles and cluster role bindings)</li> <li>removes the Kubernetes finalizers from Flux custom resources</li> <li>deletes Flux custom resource definitions and custom resources</li> <li>deletes the namespace where Flux was installed</li> </ul> <p>Delete GKE cluster:</p> <pre><code>gcloud container clusters delete k8s-gitops-lab --region us-central1\n</code></pre>"},{"location":"020_Lab_4_Traffic_control/","title":"Istio Traffic Control - Canary Deployments","text":"<p>Objective:</p> <ul> <li>Install Istio</li> <li>Deploy an application and enable automatic sidecar injection</li> <li>Deploy a canary service and use Istio to route some traffic to the new service</li> </ul>"},{"location":"020_Lab_4_Traffic_control/#prerequisite-delete-private-cluster","title":"Prerequisite  Delete Private Cluster","text":"<p>Step 1: Locate Terraform Configuration directory:</p> <pre><code>cd ~/$MY_REPO/notepad-infrastructure\n</code></pre> <p>Step 2: Delete Private Cluster that we've deployed with terraform:</p> <pre><code>terraform destroy -var-file terraform.tfvars\n</code></pre> <p>Result</p> <p>Private GKE Cluster, VPC and Subnets has been deleted</p>"},{"location":"020_Lab_4_Traffic_control/#0-create-regional-gke-cluster-on-gcp","title":"0 Create Regional GKE Cluster on GCP","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud compute networks create default \\\n    --subnet-mode=auto \\\n    --bgp-routing-mode=regional \\\n    --mtu=1460\n</code></pre> <pre><code>gcloud container clusters create istio-traffic-lab \\\n--region us-central1 \\\n--enable-ip-alias \\\n--enable-network-policy \\\n--num-nodes 1 \\\n--machine-type \"e2-standard-2\" \\\n--release-channel stable\n</code></pre> <pre><code>gcloud container clusters get-credentials istio-traffic-lab --region us-central1\n</code></pre> <p>Step 3: Setup kubectx</p> <pre><code>sudo apt install kubectx\n</code></pre> <p>Note</p> <p>we've installed <code>kubectx</code> + <code>kubens</code>: Power tools for <code>kubectl</code>: - <code>kubectx</code> helps you switch between clusters back and forth - <code>kubens</code> helps you switch between Kubernetes namespaces smoothly</p>"},{"location":"020_Lab_4_Traffic_control/#1-deploy-istio-using-custom-resources","title":"1  Deploy Istio using Custom Resources","text":"<p>This installation guide uses the istioctl command line tool to provide rich customization of the Istio control plane and of the sidecars for the Istio data plane.</p> <p>Download and extract the latest release:</p> <pre><code>curl -L https://istio.io/downloadIstio | sh -\\\ncd istio-1.11.1\nexport PATH=$PWD/bin:$PATH\n</code></pre> <p>Success</p> <p>The above command will fetch Istio packages and untar them in the same folder.</p> <p>Note</p> <p>Istio installation directory contains:</p> <ul> <li><code>bin</code> directory contains <code>istioctl</code> client binary</li> <li><code>manifests</code> Installation Profiles, configurations and Helm Charts</li> <li><code>samples</code> directory contains sample applications deployment</li> <li><code>tools</code>  directory contains auto-completion tooling and etc.</li> </ul> <p>Step 2 Deploy Istio Custom Resource Definitions (CRDs)</p> <p>Istio extends Kubernetes using Custom Resource Definitions (CRDs).  CRDs allow registration of new/non-default Kubernetes resources. When Istio CRDs are deployed, Istio\u2019s objects are registered as Kubernetes objects, providing a highly integrated experience with  Kubernetes as a deployment platform and thus allowing Kubernetes to store configuration of Istio features such as routing, security and telemetry and etc.</p> <p>We going to install using using <code>demo</code> configuration profile. The <code>demo</code> configuration profile allows to experiment with most of Istio features with modest resource requirements.  Since it enables high levels of tracing and access logging, it is not suitable for production use cases.</p> <pre><code>export STATIC_IP=&lt;set your Static IP&gt;\n</code></pre> <pre><code>istioctl install --set profile=demo --set values.gateways.istio-ingressgateway.loadBalancerIP=$STATIC_IP\n</code></pre> <p>Output:</p> <pre><code>\u2714 Istio core installed\n\u2714 Istiod installed\n\u2714 Egress gateways installed\n\u2714 Ingress gateways installed\n\u2714 Installation complete\nThank you for installing Istio 1.11.  Please take a few minutes to tell us about your install/upgrade experience!  \n</code></pre> <p>Note</p> <p>Wait a few seconds for the CRDs to be committed in the Kubernetes API-server.</p> <p>Step 3 Verify Istio CRDs successfully applied to Kubernetes Cluster.</p> <pre><code>kubectl get crds | grep istio\n</code></pre> <p>Step 3 Verify that Istio Ingressgateway is setup using STATIC_IP Load Balancer:</p> <p>List <code>STATIC_IP</code> Load Balancer</p> <pre><code>gcloud compute addresses list\n</code></pre> <p>Output:</p> <pre><code>NAME              ADDRESS/RANGE  TYPE      PURPOSE  NETWORK  REGION       SUBNET  STATUS\nstatic-ingres-ip  35.192.101.76  EXTERNAL                    us-central1          IN_USE\n</code></pre> <p>Result</p> <p>Our static IP showed as in use</p> <p>Verify that Istio Ingressgateway using STATIC_IP:</p> <pre><code>kubectl get svc istio-ingressgateway -n istio-system\n</code></pre> <pre><code>NAME                   TYPE           CLUSTER-IP    EXTERNAL-IP     Ports\nistio-ingressgateway   LoadBalancer   10.32.1.153   35.192.101.76   15021:32714/TCP,80:30042/TCP,443:31889/TCP,...  62s\n</code></pre> <p>Result</p> <p>Istio Gateway using </p>"},{"location":"020_Lab_4_Traffic_control/#2-deploy-an-micorservices-application","title":"2 Deploy an Micorservices Application","text":"<p>Step 1 Get the source code for a sample application. For this lab, we will use the Online Boutique sample app.</p> <pre><code>cd ~\ngit clone https://github.com/GoogleCloudPlatform/microservices-demo.git\ncd microservices-demo\n</code></pre> <p>Step 2 Create a namespace <code>online-boutique</code> for example</p> <pre><code>kubectl create namespace online-boutique\n</code></pre> <p>Step 3 Enable automatic sidecar injection for the online-boutique namespace</p> <pre><code>kubectl label namespace online-boutique istio-injection=enabled --overwrite\n</code></pre> <p>Check namespace:</p> <pre><code>kubectl get ns online-boutique --show-labels\n</code></pre> <p>Output:</p> <pre><code>NAME              STATUS   AGE     LABELS\nonline-boutique   Active   4m30s   istio-injection=enabled\n</code></pre> <p>Result</p> <p>We've enabled Automatic Sidecar Injection on <code>online-boutique</code> namespace</p> <p>Step 5 Switch to <code>online-boutique</code> namespace:</p> <p>List namespaces:</p> <pre><code>kubens\n</code></pre> <p>Output:</p> <pre><code>**default**\nistio-system\nkube-node-lease\nkube-public\nkube-system\nonline-boutique\n</code></pre> <p>!!! result we listed all namespaces and we see that active namespace is <code>default</code></p> <p>Switch to <code>online-boutique</code> namespace and make it active:</p> <pre><code>kubens online-boutique\n</code></pre> <pre><code>kubens\n</code></pre> <p>Output:</p> <pre><code>**default**\nistio-system\nkube-node-lease\nkube-public\nkube-system\n**online-boutique**\n</code></pre> <p>Result</p> <p>We switched context to <code>online-boutique</code> and it's now became active </p> <p>Step 4 Deploy the application by creating the kubernetes manifests</p> <pre><code>kubectl apply -f ./release/kubernetes-manifests.yaml\n</code></pre> <p>Step 5 Watch the pods and verify that all pods were started successfully with the sidecar injected</p> <pre><code>watch kubectl get pods\n</code></pre> <p>We can see containers starting as <code>Init:0/1</code> in this phase pod firewall rules gets update to establish communication with future Envoy Sidecar. Than <code>PodInitializing</code> start <code>Envoy</code> and application container.</p> <p>Output:</p> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\nadservice-5844cffbd4-h7lkk               2/2     Running   0          84s\ncartservice-fdc659ddc-fzmn9              2/2     Running   0          86s\ncheckoutservice-64db75877d-hk25l         2/2     Running   0          88s\ncurrencyservice-9b7cdb45b-9xzhl          2/2     Running   0          85s\nemailservice-64d98b6f9d-v44rm            2/2     Running   0          89s\nfrontend-76ff9556-p62g9                  2/2     Running   0          87s\nloadgenerator-589648f87f-4dfkp           2/2     Running   0          85s\npaymentservice-65bdf6757d-zd4w4          2/2     Running   0          87s\nproductcatalogservice-5cd47f8cc8-c6hdn   2/2     Running   0          86s\nrecommendationservice-b75687c5b-clcbt    2/2     Running   0          88s\nredis-cart-74594bd569-9gpjt              2/2     Running   0          84s\nshippingservice-778554994-ttwf2          2/2     Running   0          85s\n</code></pre> <p>Result</p> <p>Pods Started with sidecar auto-injected</p>"},{"location":"020_Lab_4_Traffic_control/#3-allow-external-traffic-to-online-boutique-application","title":"3 Allow External traffic to Online Boutique Application","text":""},{"location":"020_Lab_4_Traffic_control/#31-create-gateway-resource","title":"3.1 Create Gateway resource","text":"<p>In order for traffic to flow to the frontend of the application, we need to create an ingress gateway and a virtual service attached to it.</p> <p>Step 1 Apply the istio manifests to configure the Istio <code>Gateway</code>:</p> <pre><code>cat &lt;&lt;EOF&gt; frontend-gateway.yaml\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: frontend-gateway\nspec:\n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\nEOF\n</code></pre> <pre><code>kubectl apply -f frontend-gateway.yaml\n</code></pre> <p>Info</p> <p>Gateway resource important fields:</p> <ul> <li><code>name</code> - name of gateway</li> <li><code>spec.selector.istio</code> - which gateway to use (can be several)</li> <li><code>spec.servers</code> - defines which hosts will use this gateway proxy</li> <li><code>spec.servers.port.number</code> - ports to expose</li> <li><code>spec.servers.port.host</code> - host(s) for this port</li> </ul> <p>Note</p> <p>At this point our gateway is listening for request that match any host  on port 80 and it's accessible on the public internet.</p> <p>Check the created gateway:</p> <pre><code>kubectl get gateway\nkubectl get gateway -o yaml\n</code></pre> <p>Check the Envoy listeners been created:</p> <pre><code>export INGRESS_POD=$(kubectl get pods -n istio-system | grep 'ingress' |awk '{ print $1}')\nistioctl proxy-config listener $INGRESS_POD  -n istio-system\n</code></pre> <p>Output:</p> <pre><code>ADDRESS PORT  MATCH DESTINATION\n0.0.0.0 8080  ALL   Route: http.8080\n0.0.0.0 15021 ALL   Inline Route: /healthz/ready*\n0.0.0.0 15090 ALL   Inline Route: /stats/prometheus*\n</code></pre> <p>Success</p> <p>HTTP port correctly exposed correctly</p> <p>Verify what Ingress Gateway spec.selector.istio selects for use:</p> <pre><code>kubectl get pod --selector=\"istio=ingressgateway\" --all-namespaces\n</code></pre> <p>Result</p> <p>istio-ingressgateway-xxx pod in istio-system (our edge envoy pod) will proxy traffic further to Kubernetes services.</p> <p>Try to connect to the Istio Ingress IP:</p> <pre><code>curl -i 35.192.101.76\n</code></pre> <p>Output:</p> <pre><code>HTTP/1.1 404 Not Found\n</code></pre> <p>Result</p> <p>It's returning 404 error.</p> <p>Summary</p> <p>We've create gateway resource that is listening for request that match any host on port 80 and it's accessible on the public internet, however it's returning 404 error because once the gateway receives the request it doesn't know where it needs to send it.</p> <p>In order to tell <code>Gateway</code> which Kubernets <code>service</code> it is supposed to receive the requests from  it is require to attach <code>Gateway</code> to <code>VirtualService</code>.</p>"},{"location":"020_Lab_4_Traffic_control/#32-create-virtualservice-resource","title":"3.2 Create VirtualService resource","text":"<p>Step 1 Apply the <code>istio</code> manifests to configure <code>VirtualService</code> to allow external traffic into the mesh and route it to the frontend service.</p> <p>When traffic comes into the <code>gateway</code>, it is required to get it to a specific service within the <code>service</code> mesh and to do that, we\u2019ll use the <code>VirtualService</code> resource.</p> <p>In Istio, a <code>VirtualService</code> defines the rules that control how requests for a service are routed within an Istio service mesh. For instance, a <code>VirtualService</code> can route requests to different versions of a <code>service</code>,  Requests can be routed based on the request source and destination, HTTP paths and header field and weights associated with individual service versions.  As well as use advanced routing properties such as retries, request timeouts, circuit braking, fault injection and etc.</p> <p>export STUDENT= #Used to configure DNS <pre><code>cat &lt;&lt;EOF&gt; frontend-vs.yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: frontend-ingress\nspec:\n  hosts:\n  - \"$STUDENT.cloud-montreal.ca\"\n  gateways:\n  - frontend-gateway\n  http:\n  - route:\n    - destination:\n        host: frontend\n        port:\n          number: 80\nEOF\n</code></pre> <pre><code>kubectl apply -f frontend-vs.yaml\n</code></pre> <p>Acquire <code>frontend</code> app URL:</p> <pre><code>kubectl get virtualservices\n</code></pre> <p>Access <code>frontend</code> app URL from your browser:</p> <pre><code>$STUDENT.cloud-montreal.ca\n</code></pre> <p>Success</p> <p>We can access <code>frontend</code> web app via Istio Ingress Gateway</p> <ul> <li>frontend.yaml: defines another virtual service to be used by our load generator to make sure traffic directed to the frontend from within the mesh stays internal, and doesn't go through the gateway.</li> <li>Whitelist-egress-googleapis.yaml: Used to whitelist various google APIs used by services within the mesh.</li> </ul> <p>Step 3 Defines another <code>Virtualservice</code> to be used by our load generator to make sure traffic directed to the frontend from within the mesh stays internal,  and doesn't go through the gateway.</p> <pre><code>cat &lt;&lt;EOF&gt; frontend-load-vs.yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: frontend\nspec:\n  hosts:\n  - \"frontend.default.svc.cluster.local\"\n  http:\n  - route:\n    - destination:\n        host: frontend\n        port:\n          number: 80\nEOF\n</code></pre> <pre><code>kubectl apply -f frontend-load-vs.yaml\n</code></pre> <p>Summary</p> <p>We've </p>"},{"location":"020_Lab_4_Traffic_control/#4-routing-traffic-to-services-outside-of-the-mesh","title":"4 Routing traffic to services outside of the Mesh","text":"<p>Step 1 Create <code>ServiceEntry</code> custom resource to allow access to external APIs <code>googleapis.com</code> and GCE metadata service <code>metadata.google.internal</code> that app is using:</p> <p>Because all outbound traffic from an Istio-enabled pod is redirected to its sidecar proxy by default, accessibility of URLs outside of the cluster depends on the configuration of the proxy.  By default, Istio configures the Envoy proxy to passthrough requests for unknown services.  Although this provides a convenient way to get started with Istio, configuring stricter control is usually preferable in production scenarios.</p> <p>Istio has an option, <code>global.outboundTrafficPolicy.mode</code>, that configures the sidecar handling of external services, that is, those services that are not defined in Istio\u2019s internal service registry.  If this option is set to <code>ALLOW_ANY</code>, the Istio proxy lets calls to unknown services pass through. If the option is set to <code>REGISTRY_ONLY</code>,  then the Istio proxy blocks any host without an HTTP service or service entry defined within the mesh.</p> <pre><code>cat &lt;&lt;EOF&gt; allow-egress-googleapis.yaml\napiVersion: networking.istio.io/v1alpha3\nkind: ServiceEntry\nmetadata:\n  name: allow-egress-googleapis\nspec:\n  hosts:\n  - \"accounts.google.com\" # Used to get token\n  - \"*.googleapis.com\"\n  ports:\n  - number: 80\n    protocol: HTTP\n    name: http\n  - number: 443\n    protocol: HTTPS\n    name: https\n---\napiVersion: networking.istio.io/v1alpha3\nkind: ServiceEntry\nmetadata:\n  name: allow-egress-google-metadata\nspec:\n  hosts:\n  - metadata.google.internal\n  addresses:\n  - 169.254.169.254 # GCE metadata server\n  ports:\n  - number: 80\n    name: http\n    protocol: HTTP\n  - number: 443\n    name: https\n    protocol: HTTPS\nEOF\n</code></pre> <pre><code>kubectl apply -f allow-egress-googleapis.yaml\n</code></pre> <pre><code>kubectl get serviceentries  \n</code></pre> <p>Result</p> <p><code>ServiceEntry</code> resource inserts an entry into Istio\u2019s service registry which makes explicit that clients within the mesh are allowed to call a <code>metadata.google.internal</code>, <code>accounts.google.com</code>, <code>*.googleapis.com</code> regardless the namespaces</p>"},{"location":"020_Lab_4_Traffic_control/#4-canary-deployments-with-istio","title":"4 Canary Deployments with Istio","text":"<p>We going to deploy a new version of <code>recommendationservice</code> with <code>canary</code> label.</p> <p>Once we\u2019ve deployed the new version, we can start releasing it gradually by routing 20% of all incoming requests to the latest version, while the rest of the requests (98%) still goes to the existing version.</p> <p>Step 1 Patch the Recommendation service deployment to give it a <code>production</code> label</p> <pre><code>kubectl patch deployment -n online-boutique recommendationservice --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/metadata/labels/version\", \"value\": \"production\"}]'\n</code></pre> <p>Step 2 Check the labels on the deployment</p> <pre><code>export RECOMMMENDATIONSERVICE=$(kubectl get pods | grep 'recommendationservice' |awk '{ print $1}')\nkubectl get pod -n online-boutique $RECOMMMENDATIONSERVICE --show-labels\n</code></pre> <p>Step 3 Create a DestinationRule that defines two subsets <code>production</code> and <code>canary</code></p> <pre><code>cat &lt;&lt;EOF&gt;  destinationrule-recommendation.yaml\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: recommendationservice\n  namespace: online-boutique\nspec:\n  host: recommendationservice\n  subsets:\n  - name: production\n    labels:\n      version: production\n  - name: canary\n    labels:\n      version: canary\nEOF\n</code></pre> <pre><code>kubectl apply -f  destinationrule-recommendation.yaml\n</code></pre> <pre><code>kubectl get destinationrules\n</code></pre> <p>Result</p> <p>We've defined DestinationRule with 2 subsets.</p> <p>Step 4 Make a change to the <code>Recommendation</code> service and by modifying it's <code>max_responses</code> parameter from <code>5</code> to <code>2</code>.</p> <pre><code>cd ~/microservices-demo\nls\ncat src/recommendationservice/Dockerfile\n</code></pre> <pre><code>sed -i 's/max_responses = 5/max_responses = 2/g' \\\n   src/recommendationservice/recommendation_server.py\n</code></pre> <p>Step 5 Build a new version of the image</p> <pre><code>PROJECT_ID=$(gcloud config list --format \"value(core.project)\")\n\ndocker build -t \\\n  gcr.io/${PROJECT_ID}/microservices_demo/recommendationservice:canary \\\n  src/recommendationservice\n</code></pre> <p>Step 6 Push the image to GCR</p> <pre><code>docker push gcr.io/${PROJECT_ID}/microservices_demo/recommendationservice:canary\n</code></pre> <p>Step 7 Deploy the <code>canary</code> version of the Recommendation service:</p> <pre><code>cat &lt;&lt;EOF&gt;  recommendation-deployment-vcanary.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: recommendationservice\n    version: canary\n  name: recommendationservice-canary\n  namespace: online-boutique\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: recommendationservice\n      version: canary\n  template:\n    metadata:\n      labels:\n        app: recommendationservice\n        version: canary\n    spec:\n      containers:\n      - env:\n        - name: PORT\n          value: \"8080\"\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: ENABLE_PROFILER\n          value: \"0\"\n        image: gcr.io/$PROJECT_ID/microservices_demo/recommendationservice:canary\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8080\n          failureThreshold: 3\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: server\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8080\n          failureThreshold: 3\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 200m\n            memory: 450Mi\n          requests:\n            cpu: 100m\n            memory: 220Mi\nEOF\n</code></pre> <pre><code>kubectl apply -f recommendation-deployment-vcanary.yaml\n</code></pre> <pre><code>kubectl get deploy | grep recommendationservice \n</code></pre> <p>Output:</p> <pre><code>recommendationservice          1/1     1            1           16m\nrecommendationservice-canary   1/1     1            1           31s\n</code></pre> <p>Result</p> <p>2 versions of <code>recommendationservice</code> has been deployed</p> <p>Step 8 Create a virtual service to direct 20% of the traffic to the <code>canary</code> version</p> <pre><code>cat &lt;&lt;EOF&gt;  vs-recommendation.yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: recommendationservice\n  namespace: online-boutique\nspec:\n  hosts:\n  - recommendationservice\n  http:\n  - route:\n    - destination:\n        host: recommendationservice\n        port:\n          number: 8080\n        subset: production\n      weight: 80\n    - destination:\n        host: recommendationservice\n        port:\n          number: 8080\n        subset: canary \n      weight: 20\nEOF\n</code></pre> <pre><code>kubectl apply -f vs-recommendation.yaml\n</code></pre> <pre><code>kubectl get VirtualService\n</code></pre> <p>Step 9 Refresh the page for one of the products where recommendations are shown multiple times, and notice how most of the time you get 5 recommendations while if 20% of the time you get only 2 recommendations.</p>"},{"location":"020_Lab_4_Traffic_control/#5-cleanup","title":"5 Cleanup","text":"<p>Delete GKE cluster:</p> <pre><code>gcloud container clusters delete istio-traffic-lab --region us-central1\n</code></pre>"},{"location":"020_Lab_5_EFK/","title":"Lab 5 Logging with EFK Stack","text":"<p>In a complicated distributed system such as Kubernetes, we will have different logs for different components, and extracting insights from logs can be a daunting task. The EFK stack (ElasticSearch, FluentD, Kibana) can help make this task easier. In this Lab, we are going to deploy EFK stack.</p> <p>Deploy <code>online-boutique</code> app that emits logs to stdout in JSON format.</p> <p>We will then deploy Cloud Native FluentD logging agent and configure:</p> <ul> <li> <p>source directive (input) that will use logs from <code>/var/log/containers</code> folder (location used by docker daemon on a Kubernetes node to store stdout from running containers).  This events will be tailed read from text file using  <code>in_tail</code> Input Plugin. And parsed using <code>multi_format</code> plugin.</p> </li> <li> <p>filter directive that will use kubernetes metadata plugin to add metadata to the log and parse all kubernetes logs in specific format.</p> </li> <li> <p>match directive (output) that will use <code>out_elasticsearch</code> Output Plugin and send all logs to ElasticSearch under <code>fluentd-*</code> prefix.</p> </li> </ul> <p>Objective:</p> <ul> <li>Install Elasticsearch and Kibanna</li> <li>Deploy online boutique Application</li> <li>Install and Configure FluentD</li> <li>Configure ElastackSearch with FluentD</li> </ul>"},{"location":"020_Lab_5_EFK/#0-create-regional-gke-cluster-on-gcp","title":"0 Create Regional GKE Cluster on GCP","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud container clusters create k8s-efk-lab \\\n--region us-central1 \\\n--enable-ip-alias \\\n--enable-network-policy \\\n--num-nodes 1 \\\n--machine-type \"e2-standard-4\" \\\n--release-channel stable\n</code></pre> <pre><code>gcloud container clusters get-credentials k8s-efk-lab --region us-central1\n</code></pre>"},{"location":"020_Lab_5_EFK/#1-install-elasticsearch-kibana-fluentd-helm-charts","title":"1 Install Elasticsearch, Kibana, Fluentd Helm Charts","text":""},{"location":"020_Lab_5_EFK/#11-elasticsearch-installation","title":"1.1 Elasticsearch Installation","text":"<p>Create a new namespace for EFK stack:</p> <pre><code>kubectl create ns efk\n</code></pre> <p>Install Elasticsearch using Helm:</p> <pre><code>helm repo add elastic https://helm.elastic.co\nhelm install elasticsearch elastic/elasticsearch -n efk\n</code></pre> <p>Check the status of deployment:</p> <pre><code>kubectl get pods --namespace=efk -l app=elasticsearch-master -w\n</code></pre>"},{"location":"020_Lab_5_EFK/#12-kibana-installation","title":"1.2 Kibana installation","text":"<p>Step 1: Create custom Kibana configuration, that will allow to expose Kibana Dashboard:</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; kibana_values.yaml\nservice:\n  type: LoadBalancer\nEOF\n</code></pre> <pre><code>cat kibana_values.yaml  \n</code></pre> <p>Make sure values are correct</p> <p>Step 3: Deploy Kibana Ingress Charts with custom parameters in <code>efk</code> namespace:</p> <pre><code>helm install kibana elastic/kibana -n efk --values kibana_values.yaml\n</code></pre> <pre><code>helm list -n efk\n</code></pre> <p>Access Kibana from your browser using <code>LoadBalancer</code> IP on port 5601:</p> <pre><code>kubectl get svc -n efk\n</code></pre>"},{"location":"020_Lab_5_EFK/#13-fluentd-installation","title":"1.3 Fluentd installation","text":"<p>Step 1: Create custom Fluentd configuration, below snippet is updating existing <code>configmap</code> with some parameters:</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; fluentd_values.yaml\nfileConfigs:\n# here we read the logs from Docker's containers and parse them\n  01_sources.conf: |-\n    ## logs from podman\n    &lt;source&gt;\n      @type tail\n      @id in_tail_container_logs\n      @label @KUBERNETES\n      path /var/log/containers/*.log\n      pos_file /var/log/fluentd-containers.log.pos\n      tag kubernetes.*\n      read_from_head true\n      &lt;parse&gt;\n        @type multi_format\n        &lt;pattern&gt;\n          format json\n          time_key time\n          time_type string\n          time_format \"%Y-%m-%dT%H:%M:%S.%NZ\"\n          keep_time_key true\n        &lt;/pattern&gt;\n        &lt;pattern&gt;\n          format regexp\n          expression /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr)( (.))? (?&lt;log&gt;.*)$/\n          time_format '%Y-%m-%dT%H:%M:%S.%NZ'\n          keep_time_key true\n        &lt;/pattern&gt;\n      &lt;/parse&gt;\n      emit_unmatched_lines true\n    &lt;/source&gt;\n\n# we use kubernetes metadata plugin to add metadata to the log\n  02_filters.conf: |-\n    &lt;label @KUBERNETES&gt;\n      &lt;match kubernetes.var.log.containers.fluentd**&gt;\n        @type relabel\n        @label @FLUENT_LOG\n      &lt;/match&gt;\n\n      # &lt;match kubernetes.var.log.containers.**_kube-system_**&gt;\n      #   @type null\n      #   @id ignore_kube_system_logs\n      # &lt;/match&gt;\n\n      &lt;filter kubernetes.**&gt;\n        @type kubernetes_metadata\n        @id filter_kube_metadata\n        skip_labels false\n        skip_container_metadata false\n        skip_namespace_metadata true\n        skip_master_url true\n      &lt;/filter&gt;\n\n      &lt;match **&gt;\n        @type relabel\n        @label @DISPATCH\n      &lt;/match&gt;\n    &lt;/label&gt;\n # We filtering what logs will be send\n  03_dispatch.conf: |-\n    &lt;label @DISPATCH&gt;\n      &lt;filter **&gt;\n        @type prometheus\n        &lt;metric&gt;\n          name fluentd_input_status_num_records_total\n          type counter\n          desc The total number of incoming records\n          &lt;labels&gt;\n            tag ${tag}\n            hostname ${hostname}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n      &lt;/filter&gt;\n\n      &lt;match **&gt;\n        @type relabel\n        @label @OUTPUT\n      &lt;/match&gt;\n    &lt;/label&gt;\n# we send the logs to Elasticsearch\n  04_outputs.conf: |-\n    &lt;label @OUTPUT&gt;\n      &lt;match **&gt;\n        @type elasticsearch\n        host \"elasticsearch-master\"\n        port 9200\n        path \"\"\n        user elastic\n        password changeme\n      &lt;/match&gt;\n    &lt;/label&gt;\nEOF\n</code></pre> <p>Step 2: Deploy <code>Fluentd</code> Chart with custom parameters in <code>efk</code> namespace:</p> <p>Deploy <code>Fluentd</code> with Helm:</p> <pre><code>helm repo add fluent https://fluent.github.io/helm-charts\nhelm repo update\nhelm install fluentd fluent/fluentd -n efk --values fluentd_values.yaml\n</code></pre>"},{"location":"020_Lab_5_EFK/#2-configure-kibana-and-create-index","title":"2 Configure Kibana and Create Index:","text":"<p>Kibana requires an index pattern to access the Elasticsearch data that you want to explore. An index pattern selects the data to use and allows you to define properties of the fields.</p> <p>In our case we configured <code>fluentd</code> index, that should be populated at <code>Elasticsearch</code></p> <p>Step 1: Locate Kibana URL:</p> <pre><code>kubectl get svc -n efk\n</code></pre> <p>Step 2: Launch the Kibana web interface:</p> <pre><code>loadbalancer_ip:5601\n</code></pre> <p>Result</p> <p>You should see your Kibana interface</p> <p></p> <p>Step 3: Update Kibana Configuration to support <code>time</code> Meta fields:</p> <p>In UI Go to: Management - Stack Management:</p> <p></p> <p>Step 4: Then Kibana - Advanced Setting:</p> <p></p> <p>Step 5: Find field: <code>Meta fields</code>, and add <code>time</code> field as following:</p> <p></p> <p>Step 7: Create an index pattern. </p> <p>In UI  Click: Discover (under analytics)</p> <p></p> <p></p> <p>Click <code>Create Index Pattern</code> button.</p> <p>Note</p> <p>alternative path: <code>Management - Stack Management -&gt; Kibana Index Patterns</code></p> <p>Step 8: In <code>Create index pattern</code> window. Type inside <code>Index pattern name</code>: <code>fluentd*</code> as the index pattern name. More documentation can be found here</p> <p></p> <p>Result</p> <p>Your index pattern matches 1 source</p> <pre><code>Click Next Step &gt;\n</code></pre> <p>Step 9: Configure which field <code>Kibana</code> will use to filter log data by time. In the dropdown, select the <code>@time</code> field, and hit <code>Create index pattern</code>.</p> <p>Summary</p> <p>Our ElasticSearch and Kibana fully configured. You should be able now see some cluster logs in Kibana.</p> <p>Step 10: View Kibana Logs from our GKE cluster:</p> <p>In UI  Click: Discover (under analytics)</p> <pre><code>In Query search field paste: `kubernetes.namespace_name: kube-system`\nIn Range field paste: Select last 15 minutes\nClick Refresh\n</code></pre> <p></p> <p>Result</p> <p>you can see logs from <code>kube-system</code> namespace</p> <p>Note</p> <p>You can save you Query for future use</p>"},{"location":"020_Lab_5_EFK/#3-deploy-onlineboutique-application","title":"3 Deploy onlineboutique application","text":"<p>Deploy microservices application <code>onlineboutique</code>:</p> <p>Create Namespace <code>onlineboutique</code></p> <pre><code>kubectl create ns onlineboutique\n</code></pre> <p>Deploy Microservice application</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/microservices-demo.git\ncd microservices-demo\n</code></pre> <pre><code>kubectl apply -f ./release/kubernetes-manifests.yaml -n onlineboutique\n</code></pre> <p>Verify Deployment:</p> <pre><code>kubectl get pods -n onlineboutique\n</code></pre>"},{"location":"020_Lab_5_EFK/#4-observe-onlineboutique-logs","title":"4 Observe <code>onlineboutique</code> logs","text":"<p>Step 1: First let's review the logs using <code>kubectl logs</code> command locally on the cluster. We going to check <code>paymentservice</code> pod logs</p> <pre><code>export PAYMENTSERVICE_POD=$(kubectl get pods -n onlineboutique | grep paymentservice |awk '{ print $1}')\nkubectl logs $PAYMENTSERVICE_POD -n onlineboutique\n</code></pre> <p>Step 2: Now let's review Kibana Logs from our GKE cluster on <code>onlineboutique</code> namespace:</p> <pre><code>In Query search field paste: `kubernetes.namespace_name: onlineboutique`\nIn Range field paste: Select last 15 minutes\nClick Refresh\n</code></pre> <p></p> <p>Result</p> <p>We can see all logs from <code>onlineboutique</code> namespace</p> <p>Step 2: Now let's review Kibana Logs from <code>paymentservice</code> pod only</p> <p>echo $PAYMENTSERVICE_POD</p> <pre><code>In Query search field paste: `kubernetes.namespace_name: onlineboutique`\nIn Range field paste: Select last 15 minutes\nClick Refresh\n</code></pre> <p>Summary</p> <p>Developers should not have access to Kubernetes CLI, however they can get access to Observability tools like Kibana to allow them troubleshooting!</p>"},{"location":"020_Lab_5_EFK/#5-review-fluentd-configuration","title":"5 Review <code>Fluentd</code> configuration","text":"<p>In some cases, you want to filter logs from only your applications to be seen by your team. To achieve this, Fluentd should be configured to only intake specific logs so that no resources are wasted. </p> <p>Review <code>sources</code> configuration:</p> <pre><code>kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" |  grep -A20 01_sources.conf\n</code></pre> <pre><code>  01_sources.conf: |-\n    &lt;source&gt;\n      @type tail\n      @id in_tail_container_logs\n      @label @KUBERNETES\n      path /var/log/containers/*.log\n      pos_file /var/log/fluentd-containers.log.pos\n      tag kubernetes.*\n      read_from_head true\n      &lt;parse&gt;\n        @type multi_format\n        &lt;pattern&gt;\n          format json\n          time_key time\n          time_type string\n          time_format \"%Y-%m-%dT%H:%M:%S.%NZ\"\n          keep_time_key false\n        &lt;/pattern&gt;\n        &lt;pattern&gt;\n          format regexp\n          expression /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr)( (.))? (?&lt;log&gt;.*)$/\n</code></pre> <p>Note</p> <p><code>id</code>: A unique identifier to reference this source. This can be used for further filtering and routing of structured log data</p> <p><code>type</code>: Inbuilt directive understood by fluentd. In this case, \u201ctail\u201d instructs fluentd to gather data by tailing logs from a given location. Another example is \u201chttp\u201d which instructs fluentd to collect data by using GET on http endpoint. </p> <p><code>path</code>: Specific to type \u201ctail\u201d. Instructs fluentd to collect all logs under /var/log/containers directory. This is the location used by docker daemon on a Kubernetes node to store stdout from running containers</p> <p><code>pos_file</code>: Used as a checkpoint. In case the fluentd process restarts, it uses the position from this file to resume log data collection</p> <p><code>tag</code>: A custom string for matching source to destination/filters. fluentd matches source/destination tags to route log data</p> <p>Step 2 Review <code>Filter</code> configuration</p> <pre><code>kubectl get configmap  -n efk fluentd-config -oyaml | grep -v \"#\" |  grep -A20 03_dispatch.conf\n</code></pre> <pre><code>  02_filters.conf: |-\n    &lt;label @KUBERNETES&gt;\n      &lt;match kubernetes.var.log.containers.fluentd**&gt;\n        @type relabel\n        @label @FLUENT_LOG\n      &lt;/match&gt;\n\n\n      &lt;filter kubernetes.**&gt;\n        @type kubernetes_metadata\n        @id filter_kube_metadata\n        skip_labels false\n        skip_container_metadata false\n        skip_namespace_metadata true\n        skip_master_url true\n      &lt;/filter&gt;\n\n      &lt;match **&gt;\n        @type relabel\n        @label @DISPATCH\n      &lt;/match&gt;\n</code></pre> <p>Step 3 Review <code>Dispatch</code> configuration</p> <pre><code>kubectl get configmap  -n efk fluentd-config -oyaml | grep -v \"#\" |  grep -A20 03_dispatch.conf\n</code></pre> <pre><code>  03_dispatch.conf: |-\n    &lt;label @DISPATCH&gt;\n      &lt;filter **&gt;\n        @type prometheus\n        &lt;metric&gt;\n          name fluentd_input_status_num_records_total\n          type counter\n          desc The total number of incoming records\n          &lt;labels&gt;\n            tag ${tag}\n            hostname ${hostname}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n      &lt;/filter&gt;\n\n      &lt;match **&gt;\n        @type relabel\n        @label @OUTPUT\n      &lt;/match&gt;\n    &lt;/label&gt;\n</code></pre> <p>Step 4 Review <code>Output Plugin</code> configuration</p> <pre><code>kubectl get configmap  -n efk fluentd-config -oyaml | grep -v \"#\" |  grep -A10 04_outputs.conf\n</code></pre> <pre><code>  04_outputs.conf: |-\n    &lt;label @OUTPUT&gt;\n      &lt;match **&gt;\n        @type elasticsearch\n        host \"elasticsearch-master\"\n        port 9200\n        path \"\"\n        user elastic\n        password changeme\n      &lt;/match&gt;\n    &lt;/label&gt;\n</code></pre> <p>Note</p> <p><code>match</code>: tag indicates a destination. It is followed by a regular expression for matching the source. In this case, we want to capture all logs and send them to Elasticsearch, so simply use **</p> <p><code>type</code>: Supported output plugin identifier. In this case, we are using ElasticSearch which is a built-in plugin of fluentd.</p> <p><code>host/port</code>: ElasticSearch host/port. Credentials can be configured as well, but not shown here.</p>"},{"location":"020_Lab_5_EFK/#6-configure-fluentd-to-specific-logs","title":"6 Configure <code>Fluentd</code> to specific logs","text":"<p>Let's configure Fluentd to only send <code>onlineboutique</code> namespace logs to elasticsearch:</p> <p>Step 1: Modify helm <code>fluentd_values.yaml</code> values file for <code>03_dispatch.conf</code> config so that only log files that matches <code>onlineboutique</code> namespace is labeled to be sent:</p> <pre><code>cd ../\nedit fluentd_values.yaml\n</code></pre> <p>Replace line under <code>03_dispatch.conf: |-</code> config:</p> <pre><code>&lt;match **&gt; # Send all logs\n</code></pre> <p>To: </p> <pre><code>&lt;match kubernetes.var.log.containers.**_onlineboutique_**&gt;\n</code></pre> <p>So it looks as following:</p> <pre><code>  03_dispatch.conf: |-\n    &lt;label @DISPATCH&gt;\n      &lt;filter **&gt;\n        @type prometheus\n        &lt;metric&gt;\n          name fluentd_input_status_num_records_total\n          type counter\n          desc The total number of incoming records\n          &lt;labels&gt;\n            tag ${tag}\n            hostname ${hostname}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n      &lt;/filter&gt;\n\n      &lt;match kubernetes.var.log.containers.**_onlineboutique_**&gt;\n        @type relabel\n        @label @OUTPUT\n      &lt;/match&gt;\n    &lt;/label&gt;\n</code></pre> <p><code>&lt;match kubernetes.var.log.containers.**_onlineboutique_**&gt;</code> will filter out logs with <code>_onlineboutique_</code>. This configuration will only relabel the logs that matches the configuration as <code>@OUTPUT</code>. As specified in <code>04_outputs.conf</code>, only logs labelled as <code>@OUTPUT</code> will be sent to elasticsearch. </p> <p>Note that this is not the only way to configure fluentd to send one namespace's logs. </p> <p>Step 2: Install helm diff plugin</p> <pre><code>helm plugin install https://github.com/databus23/helm-diff\n</code></pre> <p>Step 3: Verify diff</p> <pre><code>helm diff upgrade fluentd fluent/fluentd -n efk  --values fluentd_values.yaml\n</code></pre> <p>Output:</p> <pre><code>-       &lt;match **&gt;\n+       &lt;match kubernetes.var.log.containers.**_onlineboutique_**&gt;\n</code></pre> <p>Step 4: Update fluentd <code>configmap</code> via Helm Upgrade</p> <pre><code>helm upgrade fluentd fluent/fluentd -n efk  --values fluentd_values.yaml\n</code></pre> <p>Step 5: Observe that fluentd stop sending logs for other namespaces than <code>_onlineboutique_</code></p> <pre><code>In Query search field paste: `kubernetes.namespace_name: kube-system`\nIn Range field paste: Select last 15 minute\nClick Refresh\n</code></pre> <p></p> <p>Result</p> <p>No more <code>kube-system</code> logs send to the ElasticSearch and hence Kibana can't display them. However we can see all historical logs!!! Prior we disabled the logging for all namespaces.</p>"},{"location":"020_Lab_5_EFK/#7-cleanup","title":"7 Cleanup","text":"<p>Uninstall Helm Charts:</p> <pre><code>helm uninstall fluentd -n efk\nhelm uninstall kibana -n efk\nhelm uninstall elasticsearch -n efk\n</code></pre> <p>Delete GKE cluster:</p> <pre><code>gcloud container clusters delete k8s-efk-lab --region us-central1\n</code></pre>"},{"location":"020_Lab_6_Prometheus/","title":"Lab 6 Monitoring with Prometheus Operator","text":"<p>The <code>Prometheus</code> Operator is a tool developed and opnesourced by CoreOS that aims to automate manual operations of Prometheus and Alertmanager on Kubernetes using Kubernetes Custom Resource Definitions (CRDs).</p> <p>The <code>Prometheus</code> Operator provides easy monitoring definitions for Kubernetes services and deployment and management of Prometheus instances.</p> <p>Once installed, the Prometheus Operator provides the following features:</p> <ul> <li>Kubernetes Custom Resources: Use <code>Kubernetes</code> custom resources to deploy and manage <code>Prometheus</code>, <code>Alertmanager</code>, and related components.</li> <li>Simplified Deployment Configuration: Configure the fundamentals of Prometheus like versions, persistence, retention policies, and replicas from a native Kubernetes resource.</li> <li>Target Services via Labels: Automatically generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language.</li> </ul> <p>Objective:</p> <ul> <li>Explore the operators' deployment setup</li> <li>Configure <code>ServiceMonitor</code>, which declaratively specifies how groups of Kubernetes services should be monitored. The Operator automatically generates Prometheus scrape configuration based on the current state of the objects in the API server.</li> <li>Configure <code>PrometheusRule</code> and <code>AlertmanagerConfig</code> to be able to send Slack alerts</li> </ul>"},{"location":"020_Lab_6_Prometheus/#0-create-regional-gke-cluster-on-gcp","title":"0 Create Regional GKE Cluster on GCP","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud container clusters create k8s-prometheus-labs \\\n--region us-central1 \\\n--enable-ip-alias \\\n--enable-network-policy \\\n--num-nodes 1 \\\n--machine-type \"t2d-standard-8\" \\\n--release-channel regular\n</code></pre> <pre><code>gcloud container clusters get-credentials k8s-prometheus-labs --region us-central1\n</code></pre> <p>Step 3: (Optional) Setup kubectx</p> <pre><code>sudo apt install kubectx\n</code></pre> <p>Note</p> <p>we've installed <code>kubectx</code> + <code>kubens</code>: Power tools for <code>kubectl</code>: - <code>kubectx</code> helps you switch between clusters back and forth - <code>kubens</code> helps you switch between Kubernetes namespaces smoothly</p>"},{"location":"020_Lab_6_Prometheus/#1-install-prometheus-operator-and-grafana-helm-charts","title":"1 Install Prometheus Operator and Grafana Helm Charts","text":""},{"location":"020_Lab_6_Prometheus/#11-install-kube-prometheus-stack-helm-chart","title":"1.1 Install <code>kube-prometheus-stack</code> helm chart","text":"<p>kube-prometheus kube-prometheus provides example configurations for a complete cluster monitoring stack based on Prometheus and the Prometheus Operator.  This includes deployment of multiple Prometheus and Alertmanager instances, metrics exporters such as the node_exporter for gathering node metrics, scrape target configuration linking Prometheus to various metrics endpoints, and example alerting rules for notification of potential issues in the cluster.</p> <p>helm chart The prometheus-community/kube-prometheus-stack helm chart provides a similar feature set to kube-prometheus. For more information, please see the chart's readme</p> <p>Step 1 Configure <code>Helm</code> repository</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n</code></pre> <p>Step 2 Fetch  <code>Helm</code> repository to local filesystem</p> <pre><code>cd ~/$MY_REPO/notepad-infrastructure/helm\nhelm pull prometheus-community/kube-prometheus-stack\ntar -xvzf kube-prometheus-stack-18.0.5.tgz\ncd kube-prometheus-stack\ntree -L 2\n</code></pre> <p>Output:</p> <pre><code>\u2500\u2500 charts\n\u2502   \u251c\u2500\u2500 grafana\n\u2502   \u251c\u2500\u2500 kube-state-metrics\n\u2502   \u2514\u2500\u2500 prometheus-node-exporter\n\u251c\u2500\u2500 crds\n\u2502   \u251c\u2500\u2500 crd-alertmanagerconfigs.yaml\n\u2502   \u251c\u2500\u2500 crd-alertmanagers.yaml\n\u2502   \u251c\u2500\u2500 crd-podmonitors.yaml\n\u2502   \u251c\u2500\u2500 crd-probes.yaml\n\u2502   \u251c\u2500\u2500 crd-prometheuses.yaml\n\u2502   \u251c\u2500\u2500 crd-prometheusrules.yaml\n\u2502   \u251c\u2500\u2500 crd-servicemonitors.yaml\n\u2502   \u2514\u2500\u2500 crd-thanosrulers.yaml\n\u251c\u2500\u2500 templates\n\u2502   \u251c\u2500\u2500 NOTES.txt\n\u2502   \u251c\u2500\u2500 _helpers.tpl\n\u2502   \u251c\u2500\u2500 alertmanager\n\u2502   \u251c\u2500\u2500 exporters\n\u2502   \u2502   \u251c\u2500\u2500 core-dns\n\u2502   \u2502   \u251c\u2500\u2500 kube-api-server\n\u2502   \u2502   \u251c\u2500\u2500 kube-controller-manager\n\u2502   \u2502   \u251c\u2500\u2500 kube-dns\n\u2502   \u2502   \u251c\u2500\u2500 kube-etcd\n\u2502   \u2502   \u251c\u2500\u2500 kube-proxy\n\u2502   \u2502   \u251c\u2500\u2500 kube-scheduler\n\u2502   \u2502   \u251c\u2500\u2500 kube-state-metrics\n\u2502   \u2502   \u251c\u2500\u2500 kubelet\n\u2502   \u2502   \u2514\u2500\u2500 node-exporter\n\u2502   \u251c\u2500\u2500 grafana\n\u2502   \u251c\u2500\u2500 prometheus\n\u2502   \u2514\u2500\u2500 prometheus-operator\n\u2514\u2500\u2500 values.yaml\n</code></pre> <p>Summary</p> <p>This chart is maintained by the Prometheus community and contains everything you need to get started including:</p> <ul> <li><code>prometheus operator</code></li> <li><code>alertmanager</code></li> <li><code>grafana</code> with predefined dashboards</li> <li><code>prometheus-node-exporter</code> - Prometheus exporter for hardware and OS metrics exposed by *NIX kernels, written in Go with pluggable metric collectors</li> <li>Kubernetes Control and Data plane  <code>exporters</code> such as <code>kube-api-server</code>, <code>core-dns</code>, <code>kube-controller-manager</code>, <code>kube-etcd</code>, <code>kube-scheduler</code>, <code>kubelet</code></li> <li><code>kube-state-metrics</code> - is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects</li> </ul> <p>Step 3 Create custom Grafana configuration, that will allow to expose Grafana Dashboard: Option 1 Ingress</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; grafana_values.yaml\ngrafana:\n  adminPassword: admin\n  ingress:\n    enabled: true\n    path: /*\n    pathType: Prefix\n  service:\n    type: NodePort\nEOF\n</code></pre> <p>Option 1 LoadBalancer</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; grafana_values.yaml\ngrafana:\n  adminPassword: admin\n  service:\n    type: LoadBalancer\nEOF\n</code></pre> <p>Step 4 Install Helm Chart to <code>monitoring</code> namespace</p> <pre><code>kubectl create ns monitoring\nhelm install prometheus-stack prometheus-community/kube-prometheus-stack -n monitoring --values grafana_values.yaml\nkubens monitoring\n</code></pre>"},{"location":"020_Lab_6_Prometheus/#4-observe-grafana-dashboards","title":"4 Observe Grafana Dashboards","text":"<p>Step 1: Locate Grafana Dashboard URL:</p> <pre><code>kubectl get svc prometheus-stack-grafana\n</code></pre> <p>Step 2: Launch the Grafana Dashboard and see Predefined Dashboards:</p> <pre><code>loadbalancer_ip\n</code></pre> <p>We've setup <code>admin</code> user password as: <code>admin</code>. So we will use this values to login to Grafana dashboard:</p> <pre><code>admin/admin\n</code></pre> <p>Result</p> <p>You should see your Grafana interface</p> <p>Step 2 Access Grafana Dashboard and see Predefined Dashboards:</p> <p></p> <p>Step 3 Observe following Dashboards:</p> <pre><code>General /Kubernetes / Compute Resources / Cluster\n</code></pre> <p>Summary</p> <p>Observe Usage, Totals, Quotas, Requests for Memory and CPU per namespace This values could be good input for Pods <code>requests</code> and <code>limits</code> measurements</p> <pre><code>General /Kubernetes / Compute Resources / Nodes (Pods)\n</code></pre> <p>Summary</p> <p>Observe CPU and Memory per Node</p> <pre><code>General /Kubernetes / Compute Resources / Namespace (Pods)\n</code></pre> <p>Summary</p> <p>Observe CPU and Memory per Pods</p> <p>Observe Dashboards for Control Plane monitoring:</p> <pre><code>General / Kubernetes / API server\nGeneral / Kubernetes / Kubelet\nGeneral / Kubernetes / Scheduler\nGeneral / Kubernetes / Controller Manager\nGeneral / etcd\n</code></pre> <p>Note</p> <p>some of the Dashboards (etcd, Scheduler, Controller Manager) are empty, this is because GKE is Managed Kubernetes so some metrics are not available for scraping.</p>"},{"location":"020_Lab_6_Prometheus/#3-deploy-onlineboutique-application","title":"3 Deploy onlineboutique application","text":"<p>Deploy microservices application <code>onlineboutique</code>:</p> <p>Step 1 Create Namespace <code>onlineboutique</code></p> <pre><code>kubectl create ns onlineboutique\n</code></pre> <p>Step 2 Deploy Microservice application</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/microservices-demo.git\ncd microservices-demo\n</code></pre> <pre><code>kubens onlineboutique\nkubectl apply -f ./release/kubernetes-manifests.yaml\n</code></pre> <p>Step 5 Verify Deployment:</p> <pre><code>kubectl get pods\n</code></pre> <p>Step 3 Observe following Dashboards:</p> <pre><code>General /Kubernetes / Compute Resources / Namespace (Pods)\nGeneral /Kubernetes / Compute Resources / Namespace (Workloads)\n</code></pre> <p>Choose:   * Relative time ranges: <code>Last 15 minutes</code>   * Namespace: <code>onlineboutique</code></p> <p>Summary</p> <p>Observe CPU and Memory per Pods</p>"},{"location":"020_Lab_6_Prometheus/#4-import-dashboard","title":"4 Import dashboard","text":"<p>Step 1 Search Kubernetes related Dashboards:</p> <pre><code>https://grafana.com/grafana/dashboards\n</code></pre> <p>Filter by:  <code>Name / Description</code> - </p> <pre><code>Kubernetes Deployment Statefulset Daemonset metrics\n</code></pre> <p>This will give you several outputs, we will use following Dashboards:</p> <pre><code>https://grafana.com/grafana/dashboards/8588\n</code></pre> <pre><code>Get this dashboard:\n8588\nCopy ID to Clipboard\n</code></pre> <p>Step 2 To import a dashboard we will start by creating a new <code>Folder</code>, click the + icon in the side menu, and then click Folder</p> <p>Name folder:</p> <pre><code>Custom\n</code></pre> <p>Step 3 To import a dashboard click the + icon in the side menu, and then click Import.</p> <p>In Import via grafana.com prompt Paste Grafana URL</p> <pre><code>8588\n</code></pre> <p>Choose:</p> <ul> <li>Folder: <code>Custom</code></li> <li>Select Prometheus data source: <code>Prometheus (default)</code> </li> </ul> <p>Extra</p> <p>Experiments with different dashboards, e.g. ElasticSearch, Nginx, Cert-Manager, Istio</p>"},{"location":"020_Lab_6_Prometheus/#7-cleanup","title":"7 Cleanup","text":"<p>Uninstall Helm Charts:</p> <pre><code>helm uninstall prometheus-stack -n monitoring\n</code></pre> <pre><code>kubectl delete crd alertmanagerconfigs.monitoring.coreos.com\nkubectl delete crd alertmanagers.monitoring.coreos.com\nkubectl delete crd podmonitors.monitoring.coreos.com\nkubectl delete crd probes.monitoring.coreos.com\nkubectl delete crd prometheuses.monitoring.coreos.com\nkubectl delete crd prometheusrules.monitoring.coreos.com\nkubectl delete crd servicemonitors.monitoring.coreos.com\nkubectl delete crd thanosrulers.monitoring.coreos.com\n</code></pre> <p>Delete GKE cluster:</p> <pre><code>gcloud container clusters delete k8s-prometheus-labs --region us-central1\n</code></pre>"},{"location":"020_Module7_Lab_Istio_Install/","title":"Istio Traffic Control - Canary Deployments","text":"<p>Objective:</p> <ul> <li>Install Istio</li> <li>Deploy an application and enable automatic sidecar injection</li> <li>Deploy a canary service and use Istio to route some traffic to the new service</li> </ul>"},{"location":"020_Module7_Lab_Istio_Install/#0-create-regional-gke-cluster-on-gcp","title":"0 Create Regional GKE Cluster on GCP","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud compute networks create default \\\n    --subnet-mode=auto \\\n    --bgp-routing-mode=regional \\\n    --mtu=1460\n</code></pre> <pre><code>gcloud container clusters create istio-lab \\\n--region us-central1 \\\n--enable-ip-alias \\\n--enable-network-policy \\\n--num-nodes 1 \\\n--machine-type \"e2-standard-2\" \\\n--release-channel stable\n</code></pre> <pre><code>gcloud container clusters get-credentials istio-lab --region us-central1\n</code></pre>"},{"location":"020_Module7_Lab_Istio_Install/#1-deploy-istio-using-custom-resources","title":"1  Deploy Istio using Custom Resources","text":"<p>This installation guide uses the istioctl command line tool to provide rich customization of the Istio control plane and of the sidecars for the Istio data plane.</p> <p>Download and extract the latest release:</p> <pre><code>curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.13.9 TARGET_ARCH=x86_64 sh -\ncd istio-1.13.9\nexport PATH=$PWD/bin:$PATH\n</code></pre> <p>Success</p> <p>The above command will fetch Istio packages and untar them in the same folder.</p> <p>Note</p> <p>Istio installation directory contains:</p> <ul> <li><code>bin</code> directory contains <code>istioctl</code> client binary</li> <li><code>manifests</code> Installation Profiles, configurations and Helm Charts</li> <li><code>samples</code> directory contains sample applications deployment</li> <li><code>tools</code>  directory contains auto-completion tooling and etc.</li> </ul> <p>Step 2 Deploy Istio Custom Resource Definitions (CRDs)</p> <p>Istio extends Kubernetes using Custom Resource Definitions (CRDs).  CRDs allow registration of new/non-default Kubernetes resources. When Istio CRDs are deployed, Istio\u2019s objects are registered as Kubernetes objects, providing a highly integrated experience with  Kubernetes as a deployment platform and thus allowing Kubernetes to store configuration of Istio features such as routing, security and telemetry and etc.</p> <p>We going to install using using <code>demo</code> configuration profile. The <code>demo</code> configuration profile allows to experiment with most of Istio features with modest resource requirements.  Since it enables high levels of tracing and access logging, it is not suitable for production use cases.</p> <pre><code>istioctl install --set profile=demo -y\n</code></pre> <p>Output:</p> <pre><code>\u2714 Istio core installed\n\u2714 Istiod installed\n\u2714 Egress gateways installed\n\u2714 Ingress gateways installed\n\u2714 Installation complete\nThank you for installing Istio 1.13.  \n</code></pre> <p>Note</p> <p>Wait a few seconds for the CRDs to be committed in the Kubernetes API-server.</p> <p>Step 3 Verify Istio CRDs successfully applied to Kubernetes Cluster.</p> <pre><code>kubectl get crds | grep istio\n</code></pre> <p>Step 4 Verify Istio components installed on Kubernetes Cluster:</p> <pre><code>kubectl get pods -n istio-system\n</code></pre> <p>Output:</p> <pre><code>istio-egressgateway-6cd5c96795-7dhxf    1/1     Running   0          35m\nistio-ingressgateway-79996fbb89-w46xc   1/1     Running   0          35m\nistiod-67bdf94c4f-4j7h6                 1/1     Running   0          35m\n</code></pre> <p>Info</p> <p>Istio control-plane include following components:</p> <ul> <li><code>istiod</code> - contains components such as <code>Citadel</code> and <code>Pilot</code></li> <li><code>istio-ingressgateway</code> Istio Ingress Gateway</li> <li><code>istio-ingressgateway</code> Istio Egress Gateway</li> </ul> <p>Step 5 Verify installation with <code>istioctl</code> CLI:</p> <pre><code>istioctl verify-install\n</code></pre> <pre><code>istioctl version\n</code></pre> <pre><code>istioctl verify-install\n</code></pre> <p>Output:</p> <pre><code>Checked 13 custom resource definitions\nChecked 3 Istio Deployments\n\u2714 Istio is installed and verified successfully\n</code></pre> <p>Step 6 Verify that Istio Ingressgateway is setup using Load Balancer:</p> <p>Verify that Istio Ingressgateway using external Load Balancer:</p> <pre><code>kubectl get svc istio-ingressgateway -n istio-system\n</code></pre> <pre><code>NAME                   TYPE           CLUSTER-IP    EXTERNAL-IP     Ports\nistio-ingressgateway   LoadBalancer   10.32.1.153   35.192.101.76   15021:32714/TCP,80:30042/TCP,443:31889/TCP,...  62s\n</code></pre> <p>Result</p> <p>Istio Gateway Configured to expose application via Istio Gateway</p>"},{"location":"020_Module7_Lab_Istio_Install/#2-deploy-an-micorservices-application","title":"2 Deploy an Micorservices Application","text":"<p>Step 1 Get the source code for a sample application. For this lab, we will use the Online Boutique sample app.</p> <pre><code>cd ~\ngit clone https://github.com/GoogleCloudPlatform/microservices-demo.git\ncd microservices-demo\n</code></pre> <p>Step 2 Create a namespace <code>online-boutique</code> for example</p> <pre><code>kubectl create namespace online-boutique\n</code></pre> <p>Step 3 Enable automatic sidecar injection for the online-boutique namespace</p> <pre><code>kubectl label namespace online-boutique istio-injection=enabled --overwrite\n</code></pre> <p>Check namespace:</p> <pre><code>kubectl get ns online-boutique --show-labels\n</code></pre> <p>Output:</p> <pre><code>NAME              STATUS   AGE     LABELS\nonline-boutique   Active   4m30s   istio-injection=enabled\n</code></pre> <p>Result</p> <p>We've enabled Automatic Sidecar Injection on <code>online-boutique</code> namespace</p> <p>Step 5 Switch to <code>online-boutique</code> namespace:</p> <p>List namespaces:</p> <pre><code>kubens\n</code></pre> <p>Output:</p> <pre><code>**default**\nistio-system\nkube-node-lease\nkube-public\nkube-system\nonline-boutique\n</code></pre> <p>!!! result we listed all namespaces and we see that active namespace is <code>default</code></p> <p>Switch to <code>online-boutique</code> namespace and make it active:</p> <pre><code>kubens online-boutique\n</code></pre> <pre><code>kubens\n</code></pre> <p>Output:</p> <pre><code>**default**\nistio-system\nkube-node-lease\nkube-public\nkube-system\n**online-boutique**\n</code></pre> <p>Result</p> <p>We switched context to <code>online-boutique</code> and it's now became active </p> <p>Step 4 Deploy the application by creating the kubernetes manifests</p> <pre><code>kubectl apply -f ./release/kubernetes-manifests.yaml\n</code></pre> <p>Step 5 Watch the pods and verify that all pods were started successfully with the sidecar injected</p> <pre><code>watch kubectl get pods\n</code></pre> <p>We can see containers starting as <code>Init:0/1</code> in this phase pod firewall rules gets update to establish communication with future Envoy Sidecar. Than <code>PodInitializing</code> start <code>Envoy</code> and application container.</p> <p>Output:</p> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\nadservice-5844cffbd4-h7lkk               2/2     Running   0          84s\ncartservice-fdc659ddc-fzmn9              2/2     Running   0          86s\ncheckoutservice-64db75877d-hk25l         2/2     Running   0          88s\ncurrencyservice-9b7cdb45b-9xzhl          2/2     Running   0          85s\nemailservice-64d98b6f9d-v44rm            2/2     Running   0          89s\nfrontend-76ff9556-p62g9                  2/2     Running   0          87s\nloadgenerator-589648f87f-4dfkp           2/2     Running   0          85s\npaymentservice-65bdf6757d-zd4w4          2/2     Running   0          87s\nproductcatalogservice-5cd47f8cc8-c6hdn   2/2     Running   0          86s\nrecommendationservice-b75687c5b-clcbt    2/2     Running   0          88s\nredis-cart-74594bd569-9gpjt              2/2     Running   0          84s\nshippingservice-778554994-ttwf2          2/2     Running   0          85s\n</code></pre> <p>Result</p> <p>Pods Started with sidecar auto-injected</p>"},{"location":"020_Module7_Lab_Istio_Install/#3-allow-external-traffic-to-online-boutique-application","title":"3 Allow External traffic to Online Boutique Application","text":""},{"location":"020_Module7_Lab_Istio_Install/#31-create-gateway-resource","title":"3.1 Create Gateway resource","text":"<p>In order for traffic to flow to the frontend of the application, we need to create an ingress gateway and a virtual service attached to it.</p> <p>Step 1 Apply the istio manifests to configure the Istio <code>Gateway</code>:</p> <pre><code>cat &lt;&lt;EOF&gt; frontend-gateway.yaml\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: frontend-gateway\nspec:\n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\nEOF\n</code></pre> <pre><code>kubectl apply -f frontend-gateway.yaml\n</code></pre> <p>Info</p> <p>Gateway resource important fields:</p> <ul> <li><code>name</code> - name of gateway</li> <li><code>spec.selector.istio</code> - which gateway to use (can be several)</li> <li><code>spec.servers</code> - defines which hosts will use this gateway proxy</li> <li><code>spec.servers.port.number</code> - ports to expose</li> <li><code>spec.servers.port.host</code> - host(s) for this port</li> </ul> <p>Note</p> <p>At this point our gateway is listening for request that match any host  on port 80 and it's accessible on the public internet.</p> <p>Check the created gateway:</p> <pre><code>kubectl get gateway\nkubectl get gateway -o yaml\n</code></pre> <p>Check the Envoy listeners been created:</p> <pre><code>export INGRESS_POD=$(kubectl get pods -n istio-system | grep 'ingress' |awk '{ print $1}')\nistioctl proxy-config listener $INGRESS_POD  -n istio-system\n</code></pre> <p>Output:</p> <pre><code>ADDRESS PORT  MATCH DESTINATION\n0.0.0.0 8080  ALL   Route: http.8080\n0.0.0.0 15021 ALL   Inline Route: /healthz/ready*\n0.0.0.0 15090 ALL   Inline Route: /stats/prometheus*\n</code></pre> <p>Success</p> <p>HTTP port correctly exposed correctly</p> <p>Verify what Ingress Gateway spec.selector.istio selects for use:</p> <pre><code>kubectl get pod --selector=\"istio=ingressgateway\" --all-namespaces\n</code></pre> <p>Result</p> <p>istio-ingressgateway-xxx pod in istio-system (our edge envoy pod) will proxy traffic further to Kubernetes services.</p> <p>List Istio GW Ingress  EXTERNAL-IP</p> <pre><code>kubectl get svc --selector=\"istio=ingressgateway\" --all-namespaces\n</code></pre> <p>Try to connect to the Istio Ingress IP:</p> <pre><code>curl -i 34.136.248.211\n</code></pre> <p>Output:</p> <pre><code>HTTP/1.1 404 Not Found\n</code></pre> <p>Result</p> <p>It's returning 404 error.</p> <p>Summary</p> <p>We've create gateway resource that is listening for request that match any host on port 80 and it's accessible on the public internet, however it's returning 404 error because once the gateway receives the request it doesn't know where it needs to send it.</p> <p>In order to tell <code>Gateway</code> which Kubernets <code>service</code> it is supposed to receive the requests from  it is require to attach <code>Gateway</code> to <code>VirtualService</code>.</p>"},{"location":"020_Module7_Lab_Istio_Install/#32-create-virtualservice-resource","title":"3.2 Create VirtualService resource","text":"<p>Step 1 Apply the <code>istio</code> manifests to configure <code>VirtualService</code> to allow external traffic into the mesh and route it to the frontend service.</p> <p>When traffic comes into the <code>gateway</code>, it is required to get it to a specific service within the <code>service</code> mesh and to do that, we\u2019ll use the <code>VirtualService</code> resource.</p> <p>In Istio, a <code>VirtualService</code> defines the rules that control how requests for a service are routed within an Istio service mesh. For instance, a <code>VirtualService</code> can route requests to different versions of a <code>service</code>,  Requests can be routed based on the request source and destination, HTTP paths and header field and weights associated with individual service versions.  As well as use advanced routing properties such as retries, request timeouts, circuit braking, fault injection and etc.</p> <pre><code>cat &lt;&lt;EOF&gt; frontend-vs.yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: frontend-ingress\nspec:\n  hosts:\n  - \"*\"\n  gateways:\n  - frontend-gateway\n  http:\n  - route:\n    - destination:\n        host: frontend\n        port:\n          number: 80\nEOF\n</code></pre> <pre><code>kubectl apply -f frontend-vs.yaml\n</code></pre> <p>Acquire <code>frontend</code> app URL:</p> <pre><code>kubectl get virtualservices\n</code></pre> <p>Shows us that <code>*</code> meaning that we just need to access GW API Address.</p> <pre><code>kubectl get svc --selector=\"istio=ingressgateway\" --all-namespaces\n</code></pre> <p>Access <code>frontend</code> app URL from your browser:</p> <pre><code>EXTERNAL-IP\n</code></pre> <p>Success</p> <p>We can access <code>frontend</code> web app via Istio Ingress Gateway</p> <ul> <li>frontend.yaml: defines another virtual service to be used by our load generator to make sure traffic directed to the frontend from within the mesh stays internal, and doesn't go through the gateway.</li> <li>Whitelist-egress-googleapis.yaml: Used to whitelist various google APIs used by services within the mesh.</li> </ul> <p>Step 3 Defines another <code>Virtualservice</code> to be used by  load generator to make sure traffic directed to the frontend from within the mesh stays internal,  and doesn't go through the gateway.</p> <pre><code>cat &lt;&lt;EOF&gt; frontend-load-vs.yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: frontend\nspec:\n  hosts:\n  - \"frontend.default.svc.cluster.local\"\n  http:\n  - route:\n    - destination:\n        host: frontend\n        port:\n          number: 80\nEOF\n</code></pre> <pre><code>kubectl apply -f frontend-load-vs.yaml\n</code></pre> <p>Summary</p> <p>We've deployed microservices application on GKE with Istio.</p>"},{"location":"020_Module7_Lab_Istio_Install/#5-cleanup","title":"5 Cleanup","text":"<p>Delete GKE cluster:</p> <pre><code>gcloud container clusters delete istio-lab --region us-central1\n</code></pre>"},{"location":"020_Module_10_Assignment_EFK/","title":"Assignment 10 Logging with EFK Stack","text":"<p>In a complicated distributed system such as Kubernetes, we will have different logs for different components, and extracting insights from logs can be a daunting task. The EFK stack (ElasticSearch, FluentD, Kibana) can help make this task easier. We are going to deploy EFK stack and see it in action.</p> <p>Deploy <code>online-boutique</code> app that emits logs to stdout in JSON format.</p> <p>We will then deploy Cloud Native FluentD logging agent and configure:</p> <ul> <li> <p>source directive (input) that will use logs from <code>/var/log/containers</code> folder (location used by docker daemon on a Kubernetes node to store stdout from running containers).  This events will be tailed read from text file using  <code>in_tail</code> Input Plugin. And parsed using <code>multi_format</code> plugin.</p> </li> <li> <p>filter directive that will use kubernetes metadata plugin to add metadata to the log and parse all kubernetes logs in specific format.</p> </li> <li> <p>match directive (output) that will use <code>out_elasticsearch</code> Output Plugin and send all logs to ElasticSearch under <code>fluentd-*</code> prefix.</p> </li> </ul> <p>Objective:</p> <ul> <li>Install Elasticsearch and Kibanna</li> <li>Deploy online boutique Application</li> <li>Install and Configure FluentD</li> <li>Configure ElastackSearch with FluentD</li> </ul>"},{"location":"020_Module_10_Assignment_EFK/#optional-0-create-regional-gke-cluster-on-gcp","title":"(Optional) 0 Create Regional GKE Cluster on GCP","text":"<p>Note</p> <p>Skip this step if you still have the notepad cluster from previous assignment, you can reuse it <code>gke-auto-&lt;STUDENT_NAME&gt;-notepad-dev</code></p> <p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud container clusters create gke-auto-&lt;STUDENT_NAME&gt;-notepad-dev \\\n--region us-central1 \\\n--enable-ip-alias \\\n--enable-network-policy \\\n--num-nodes 1 \\\n--machine-type \"e2-standard-4\" \\\n--release-channel stable\n</code></pre> <pre><code>gcloud container clusters get-credentials gke-auto-&lt;STUDENT_NAME&gt;-notepad-dev --region us-central1\n</code></pre>"},{"location":"020_Module_10_Assignment_EFK/#1-install-elasticsearch-kibana-fluentd-helm-charts","title":"1 Install Elasticsearch, Kibana, Fluentd Helm Charts","text":""},{"location":"020_Module_10_Assignment_EFK/#11-elasticsearch-installation","title":"1.1 Elasticsearch Installation","text":"<p>Step 1: Create a new namespace for EFK stack:</p> <pre><code>kubectl create ns efk\n</code></pre> <p>(Optional) use kubens it to make default namespace <code>efk</code></p> <pre><code>kubens efk\n</code></pre> <p>If you execute this step you can safely omit <code>-n efk</code> from all subsequent commands, since this is an optional step we will stell show that prefix on the commands below.</p> <p>Step 2: Install Elasticsearch using Helm:</p> <pre><code>helm repo add elastic https://helm.elastic.co\nhelm repo update\nhelm install elasticsearch elastic/elasticsearch -n efk --version 7.17.3\n</code></pre> <p>Note</p> <p>The latest version of Kibana no longer allows for Fluentd, insted you need to use log-stash. Since our goal is to see Fluend in action we are installing a slightly older version instead.</p> <p>Check the status of deployment:</p> <pre><code>kubectl get pods --namespace=efk -l app=elasticsearch-master -w\n</code></pre>"},{"location":"020_Module_10_Assignment_EFK/#12-kibana-installation","title":"1.2 Kibana installation","text":"<p>Step 1: Create custom Kibana configuration, that will allow to expose Kibana Dashboard:</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; kibana_values.yaml\nservice:\n  type: LoadBalancer\nEOF\n</code></pre> <pre><code>cat kibana_values.yaml  \n</code></pre> <p>Make sure values are correct</p> <p>Step 2: Deploy Kibana Ingress Charts with custom parameters in <code>efk</code> namespace:</p> <pre><code>helm install kibana elastic/kibana -n efk --values kibana_values.yaml --version 7.17.3\n</code></pre> <pre><code>helm list -n efk\n</code></pre> <p>Access Kibana from your browser using <code>LoadBalancer</code> IP on port 5601:</p> <pre><code>kubectl get svc -n efk\n</code></pre> <p>You should see a service named <code>kibana-kibana</code> of type LoadBalancer. That is your public Kibana IP. Now you can access your Kibana dashboard using:</p> <pre><code>http://${KIBANA_IP}:5601/app/home#/\n</code></pre> <p>Note</p> <p>Unlike Kibana 8, the version 7 does not implement authentication.</p>"},{"location":"020_Module_10_Assignment_EFK/#13-fluentd-installation","title":"1.3 Fluentd installation","text":"<p>Step 1: Create custom Fluentd configuration, below snippet is updating existing <code>configmap</code> with some parameters:</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; fluentd_values.yaml\nfileConfigs:\n# here we read the logs from Docker's containers and parse them\n  01_sources.conf: |-\n    ## logs from podman\n    &lt;source&gt;\n      @type tail\n      @id in_tail_container_logs\n      @label @KUBERNETES\n      path /var/log/containers/*.log\n      pos_file /var/log/fluentd-containers.log.pos\n      tag kubernetes.*\n      read_from_head true\n      &lt;parse&gt;\n        @type multi_format\n        ## pattern matching rules for incoming messages from the defined sources\n        &lt;pattern&gt;\n          format json\n          time_key time\n          time_type string\n          time_format \"%Y-%m-%dT%H:%M:%S.%NZ\"\n          keep_time_key true\n        &lt;/pattern&gt;\n        &lt;pattern&gt;\n          format regexp\n          expression /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr)( (.))? (?&lt;log&gt;.*)$/\n          time_format '%Y-%m-%dT%H:%M:%S.%NZ'\n          keep_time_key true\n        &lt;/pattern&gt;\n      &lt;/parse&gt;\n      emit_unmatched_lines true\n    &lt;/source&gt;\n\n# we use kubernetes metadata plugin to add metadata to the log\n  02_filters.conf: |-\n    &lt;label @KUBERNETES&gt;\n      &lt;match kubernetes.var.log.containers.fluentd**&gt;\n        @type relabel\n        @label @FLUENT_LOG\n      &lt;/match&gt;\n\n      # &lt;match kubernetes.var.log.containers.**_kube-system_**&gt;\n      #   @type null\n      #   @id ignore_kube_system_logs\n      # &lt;/match&gt;\n\n      &lt;filter kubernetes.**&gt;\n        @type kubernetes_metadata\n        @id filter_kube_metadata\n        skip_labels false\n        skip_container_metadata false\n        skip_namespace_metadata true\n        skip_master_url true\n      &lt;/filter&gt;\n\n      &lt;match **&gt;\n        @type relabel\n        @label @DISPATCH\n      &lt;/match&gt;\n    &lt;/label&gt;\n # We filtering what logs will be send\n  03_dispatch.conf: |-\n    &lt;label @DISPATCH&gt;\n      &lt;filter **&gt;\n        @type prometheus\n        &lt;metric&gt;\n          name fluentd_input_status_num_records_total\n          type counter\n          desc The total number of incoming records\n          &lt;labels&gt;\n            tag ${tag}\n            hostname ${hostname}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n      &lt;/filter&gt;\n\n      &lt;match **&gt;\n        @type relabel\n        @label @OUTPUT\n      &lt;/match&gt;\n    &lt;/label&gt;\n# we send the logs to Elasticsearch\n  04_outputs.conf: |-\n    &lt;label @OUTPUT&gt;\n      &lt;match **&gt;\n        @type elasticsearch\n        host \"elasticsearch-master\"\n        port 9200\n        path \"\"\n        user elastic\n        password changeme\n      &lt;/match&gt;\n    &lt;/label&gt;\nEOF\n</code></pre> <p>Note</p> <p>In a real world deployment you would use service account authentication or dedicated credentials, this is ok for trial and test but not for production.</p> <p>Ensure you spend some time reading the comments in the configuration above.</p> <p>Task 2: Deploy <code>Fluentd</code> Chart with custom parameters in <code>efk</code> namespace:</p> <p>Deploy <code>Fluentd</code> with Helm:</p> <ul> <li>Use the repository name: fluent </li> <li>Use the repository url: https://fluent.github.io/helm-charts</li> <li>install fluent/fluend in the <code>efk</code> namespace with the <code>fluentd_values.yaml</code> as values file.</li> </ul>"},{"location":"020_Module_10_Assignment_EFK/#2-configure-kibana-and-create-index","title":"2 Configure Kibana and Create Index:","text":"<p>Kibana requires an index pattern to access the Elasticsearch data that you want to explore. An index pattern selects the data to use and allows you to define properties of the fields.</p> <p>In our case we configured <code>fluentd</code> index, that should be populated at <code>Elasticsearch</code></p> <p>Step 1: Locate Kibana URL:</p> <pre><code>kubectl get svc -n efk\n</code></pre> <p>Step 2: Launch the Kibana web interface:</p> <pre><code>loadbalancer_ip:5601\n</code></pre> <p>Result</p> <p>You should see your Kibana interface</p> <p></p> <p>Step 3: Update Kibana Configuration to support <code>time</code> Meta fields:</p> <p>In UI Go to: Management - Stack Management:</p> <p></p> <p>Step 4: Then Kibana - Advanced Setting:</p> <p></p> <p>Step 5: Find field: <code>Meta fields</code>, and add <code>time</code> field as following:</p> <p></p> <p>Step 7: Create an index pattern. </p> <p>In UI  Click: Discover (under analytics)</p> <p></p> <p></p> <p>Click <code>Create Index Pattern</code> button.</p> <p>Note</p> <p>alternative path: <code>Management - Stack Management -&gt; Kibana Index Patterns</code></p> <p>Step 8: In <code>Create index pattern</code> window. Type inside <code>Index pattern name</code>: <code>fluentd*</code> as the index pattern name, for timestamp select <code>time</code>. More documentation can be found here</p> <p></p> <p>Result</p> <p>Your index pattern matches 1 source</p> <pre><code>Click Next Step &gt;\n</code></pre> <p>Step 9: Configure which field <code>Kibana</code> will use to filter log data by time. In the dropdown, select the <code>@time</code> field, and hit <code>Create index pattern</code>.</p> <p>Summary</p> <p>Our ElasticSearch and Kibana fully configured. You should be able now see some cluster logs in Kibana.</p> <p>Step 10: View Kibana Logs from our GKE cluster:</p> <p>In UI  Click: Discover (under analytics)</p> <pre><code>In Query search field paste: `kubernetes.namespace_name: kube-system`\nIn Range field paste: Select last 15 minutes\nClick Refresh\n</code></pre> <p></p> <p>Result</p> <p>you can see logs from <code>kube-system</code> namespace</p> <p>Note</p> <p>You can save you Query for future use</p>"},{"location":"020_Module_10_Assignment_EFK/#3-deploy-onlineboutique-application","title":"3 Deploy onlineboutique application","text":"<p>Deploy microservices application <code>onlineboutique</code>:</p> <p>Create Namespace <code>onlineboutique</code></p> <pre><code>kubectl create ns onlineboutique\n</code></pre> <p>Deploy Microservice application</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/microservices-demo.git\ncd microservices-demo\n</code></pre> <pre><code>kubectl apply -f ./release/kubernetes-manifests.yaml -n onlineboutique\n</code></pre> <p>Verify Deployment:</p> <pre><code>kubectl get pods -n onlineboutique\n</code></pre>"},{"location":"020_Module_10_Assignment_EFK/#4-observe-onlineboutique-logs","title":"4 Observe <code>onlineboutique</code> logs","text":"<p>Step 1: First let's review the logs using <code>kubectl logs</code> command locally on the cluster. We going to check <code>paymentservice</code> pod logs</p> <pre><code>export PAYMENTSERVICE_POD=$(kubectl get pods -n onlineboutique | grep paymentservice |awk '{ print $1}')\nkubectl logs $PAYMENTSERVICE_POD -n onlineboutique\n</code></pre> <p>Step 2: Now let's review Kibana Logs from our GKE cluster on <code>onlineboutique</code> namespace:</p> <pre><code>In Query search field paste: `kubernetes.namespace_name: onlineboutique`\nIn Range field paste: Select last 15 minutes\nClick Refresh\n</code></pre> <p></p> <p>Result</p> <p>We can see all logs from <code>onlineboutique</code> namespace</p> <p>Step 2: Now let's review Kibana Logs from <code>paymentservice</code> pod only</p> <p>echo $PAYMENTSERVICE_POD</p> <pre><code>In Query search field paste: `kubernetes.namespace_name: onlineboutique`\nIn Range field paste: Select last 15 minutes\nClick Refresh\n</code></pre> <p>Summary</p> <p>Developers should not have access to Kubernetes CLI, however they can get access to Observability tools like Kibana to allow them troubleshooting!</p>"},{"location":"020_Module_10_Assignment_EFK/#5-review-fluentd-configuration","title":"5 Review <code>Fluentd</code> configuration","text":"<p>In some cases, you want to filter logs from only your applications to be seen by your team. To achieve this, Fluentd should be configured to only intake specific logs so that no resources are wasted. </p> <p>Review <code>sources</code> configuration:</p> <pre><code>kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" |  grep -A20 01_sources.conf\n</code></pre> <pre><code>  01_sources.conf: |-\n    &lt;source&gt;\n      @type tail\n      @id in_tail_container_logs\n      @label @KUBERNETES\n      path /var/log/containers/*.log\n      pos_file /var/log/fluentd-containers.log.pos\n      tag kubernetes.*\n      read_from_head true\n      &lt;parse&gt;\n        @type multi_format\n        &lt;pattern&gt;\n          format json\n          time_key time\n          time_type string\n          time_format \"%Y-%m-%dT%H:%M:%S.%NZ\"\n          keep_time_key false\n        &lt;/pattern&gt;\n        &lt;pattern&gt;\n          format regexp\n          expression /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr)( (.))? (?&lt;log&gt;.*)$/\n</code></pre> <p>Note</p> <p><code>id</code>: A unique identifier to reference this source. This can be used for further filtering and routing of structured log data</p> <p><code>type</code>: Inbuilt directive understood by fluentd. In this case, \u201ctail\u201d instructs fluentd to gather data by tailing logs from a given location. Another example is \u201chttp\u201d which instructs fluentd to collect data by using GET on http endpoint. </p> <p><code>path</code>: Specific to type \u201ctail\u201d. Instructs fluentd to collect all logs under /var/log/containers directory. This is the location used by docker daemon on a Kubernetes node to store stdout from running containers</p> <p><code>pos_file</code>: Used as a checkpoint. In case the fluentd process restarts, it uses the position from this file to resume log data collection</p> <p><code>tag</code>: A custom string for matching source to destination/filters. fluentd matches source/destination tags to route log data</p> <p>Step 2 Review <code>Filter</code> configuration</p> <pre><code>kubectl get configmap  -n efk fluentd-config -oyaml | grep -v \"#\" |  grep -A20 03_dispatch.conf\n</code></pre> <pre><code>  02_filters.conf: |-\n    &lt;label @KUBERNETES&gt;\n      &lt;match kubernetes.var.log.containers.fluentd**&gt;\n        @type relabel\n        @label @FLUENT_LOG\n      &lt;/match&gt;\n\n\n      &lt;filter kubernetes.**&gt;\n        @type kubernetes_metadata\n        @id filter_kube_metadata\n        skip_labels false\n        skip_container_metadata false\n        skip_namespace_metadata true\n        skip_master_url true\n      &lt;/filter&gt;\n\n      &lt;match **&gt;\n        @type relabel\n        @label @DISPATCH\n      &lt;/match&gt;\n</code></pre> <p>Step 3 Review <code>Dispatch</code> configuration</p> <pre><code>kubectl get configmap  -n efk fluentd-config -oyaml | grep -v \"#\" |  grep -A20 03_dispatch.conf\n</code></pre> <pre><code>  03_dispatch.conf: |-\n    &lt;label @DISPATCH&gt;\n      &lt;filter **&gt;\n        @type prometheus\n        &lt;metric&gt;\n          name fluentd_input_status_num_records_total\n          type counter\n          desc The total number of incoming records\n          &lt;labels&gt;\n            tag ${tag}\n            hostname ${hostname}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n      &lt;/filter&gt;\n\n      &lt;match **&gt;\n        @type relabel\n        @label @OUTPUT\n      &lt;/match&gt;\n    &lt;/label&gt;\n</code></pre> <p>Step 4 Review <code>Output Plugin</code> configuration</p> <pre><code>kubectl get configmap  -n efk fluentd-config -oyaml | grep -v \"#\" |  grep -A10 04_outputs.conf\n</code></pre> <pre><code>  04_outputs.conf: |-\n    &lt;label @OUTPUT&gt;\n      &lt;match **&gt;\n        @type elasticsearch\n        host \"elasticsearch-master\"\n        port 9200\n        path \"\"\n        user elastic\n        password changeme\n      &lt;/match&gt;\n    &lt;/label&gt;\n</code></pre> <p>Note</p> <p><code>match</code>: tag indicates a destination. It is followed by a regular expression for matching the source. In this case, we want to capture all logs and send them to Elasticsearch, so simply use **</p> <p><code>type</code>: Supported output plugin identifier. In this case, we are using ElasticSearch which is a built-in plugin of fluentd.</p> <p><code>host/port</code>: ElasticSearch host/port. Credentials can be configured as well, but not shown here.</p>"},{"location":"020_Module_10_Assignment_EFK/#6-configure-fluentd-to-specific-logs","title":"6 Configure <code>Fluentd</code> to specific logs","text":"<p>Let's configure Fluentd to only send <code>onlineboutique</code> namespace logs to elasticsearch:</p> <p>Step 1: Modify helm <code>fluentd_values.yaml</code> values file for <code>03_dispatch.conf</code> config so that only log files that matches <code>onlineboutique</code> namespace is labeled to be sent:</p> <pre><code>cd ../\nedit fluentd_values.yaml\n</code></pre> <p>Replace line under <code>03_dispatch.conf: |-</code> config:</p> <pre><code>&lt;match **&gt; # Send all logs\n</code></pre> <p>To: </p> <pre><code>&lt;match kubernetes.var.log.containers.**_onlineboutique_**&gt;\n</code></pre> <p>So it looks as following:</p> <pre><code>  03_dispatch.conf: |-\n    &lt;label @DISPATCH&gt;\n      &lt;filter **&gt;\n        @type prometheus\n        &lt;metric&gt;\n          name fluentd_input_status_num_records_total\n          type counter\n          desc The total number of incoming records\n          &lt;labels&gt;\n            tag ${tag}\n            hostname ${hostname}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n      &lt;/filter&gt;\n\n      &lt;match kubernetes.var.log.containers.**_onlineboutique_**&gt;\n        @type relabel\n        @label @OUTPUT\n      &lt;/match&gt;\n    &lt;/label&gt;\n</code></pre> <p><code>&lt;match kubernetes.var.log.containers.**_onlineboutique_**&gt;</code> will filter out logs with <code>_onlineboutique_</code>. This configuration will only relabel the logs that matches the configuration as <code>@OUTPUT</code>. As specified in <code>04_outputs.conf</code>, only logs labelled as <code>@OUTPUT</code> will be sent to elasticsearch. </p> <p>Note that this is not the only way to configure fluentd to send one namespace's logs. </p> <p>Step 2: Install helm diff plugin</p> <pre><code>helm plugin install https://github.com/databus23/helm-diff\n</code></pre> <p>Step 3: Verify diff</p> <pre><code>helm diff upgrade fluentd fluent/fluentd -n efk  --values fluentd_values.yaml\n</code></pre> <p>Output:</p> <pre><code>-       &lt;match **&gt;\n+       &lt;match kubernetes.var.log.containers.**_onlineboutique_**&gt;\n</code></pre> <p>Step 4: Update fluentd <code>configmap</code> via Helm Upgrade</p> <pre><code>helm upgrade fluentd fluent/fluentd -n efk  --values fluentd_values.yaml\n</code></pre> <p>Step 5: Observe that fluentd stop sending logs for other namespaces than <code>_onlineboutique_</code></p> <pre><code>In Query search field paste: `kubernetes.namespace_name: kube-system`\nIn Range field paste: Select last 15 minute\nClick Refresh\n</code></pre> <p></p> <p>Result</p> <p>No more <code>kube-system</code> logs send to the ElasticSearch and hence Kibana can't display them. However we can see all historical logs!!! Prior we disabled the logging for all namespaces.</p> <p>Feel free to play around with EFK, there are a lot mroe things it does that this assignmet didn't cover.</p> <p>Optional challenges:</p> <ul> <li>Execute aggregate log searches to see:<ul> <li>Which service fails the most?</li> <li>What is the frequency of failures?</li> </ul> </li> <li>Setup a monitoring dashboard for the online boutique</li> <li>The online boutique also exports metrics and traces, can you get them into EFK?</li> </ul> <p>Optional reading material:</p> <p>Feel free to save these for future reference.</p> <ul> <li>How to choose the right open-source log collector</li> <li>Introduction to Fluentd: Collect logs and send almost anywhere</li> <li>Introduction to Fluentd &amp; Fluent Bit<ul> <li>03:00 - Introduction to Fluentd and Fluent Bit (Slides)</li> <li>18:30 - When should I use Fluentd vs Fluent Bit</li> <li>22:00 - Deploying Fluentd with kubectl</li> <li>30:00 - Setting up Kibana</li> <li>34:00 - Deploying Fluent Bit with Helm</li> <li>40:00 - Config Visualizer</li> <li>48:00 - Filtering logs</li> <li>01:00:00 - Stream processing with Fluent Bit</li> </ul> </li> <li>Introduction to fluentbit</li> <li>Grafana Loki: like Prometheus, but for Logs</li> <li>Correlate Your Metrics, Logs &amp; Traces with the curated OSS observability stack from Grafana Labs</li> </ul>"},{"location":"020_Module_10_Assignment_EFK/#7-submit-your-assignment","title":"7 Submit your assignment","text":"<p>Save the output of the following commands and submit to the assignment folder:</p> <pre><code>kubectl describe pods -n efk &gt; k8s_pods_$STUDENT_NAME_efk.txt\nkubectl describe pods -n onlineboutique &gt; k8s_pods_$STUDENT_NAME_onlineboutique.txt\nkubectl get all -A &gt; k8s_resources_$STUDENT_NAME.txt\n</code></pre>"},{"location":"020_Module_10_Assignment_EFK/#8-cleanup","title":"8 Cleanup","text":"<p>Uninstall Helm Charts:</p> <pre><code>helm uninstall fluentd -n efk\nhelm uninstall kibana -n efk\nhelm uninstall elasticsearch -n efk\n</code></pre> <p>Delete GKE cluster:</p> <pre><code>gcloud container clusters delete gke-auto-&lt;STUDENT_NAME&gt;-notepad-dev --region us-central1\n</code></pre>"},{"location":"020_Module_10_Assignment_EFK_solution/","title":"Assignment 10 Logging with EFK Stack","text":"<p>In a complicated distributed system such as Kubernetes, we will have different logs for different components, and extracting insights from logs can be a daunting task. The EFK stack (ElasticSearch, FluentD, Kibana) can help make this task easier. We are going to deploy EFK stack and see it in action.</p> <p>Deploy <code>online-boutique</code> app that emits logs to stdout in JSON format.</p> <p>We will then deploy Cloud Native FluentD logging agent and configure:</p> <ul> <li> <p>source directive (input) that will use logs from <code>/var/log/containers</code> folder (location used by docker daemon on a Kubernetes node to store stdout from running containers).  This events will be tailed read from text file using  <code>in_tail</code> Input Plugin. And parsed using <code>multi_format</code> plugin.</p> </li> <li> <p>filter directive that will use kubernetes metadata plugin to add metadata to the log and parse all kubernetes logs in specific format.</p> </li> <li> <p>match directive (output) that will use <code>out_elasticsearch</code> Output Plugin and send all logs to ElasticSearch under <code>fluentd-*</code> prefix.</p> </li> </ul> <p>Objective:</p> <ul> <li>Install Elasticsearch and Kibanna</li> <li>Deploy online boutique Application</li> <li>Install and Configure FluentD</li> <li>Configure ElastackSearch with FluentD</li> </ul>"},{"location":"020_Module_10_Assignment_EFK_solution/#optional-0-create-regional-gke-cluster-on-gcp","title":"(Optional) 0 Create Regional GKE Cluster on GCP","text":"<p>Note</p> <p>Skip this step if you still have the notepad cluster from previous assignment, you can reuse it <code>gke-auto-&lt;STUDENT_NAME&gt;-notepad-dev</code></p> <p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud container clusters create gke-auto-&lt;STUDENT_NAME&gt;-notepad-dev \\\n--region us-central1 \\\n--enable-ip-alias \\\n--enable-network-policy \\\n--num-nodes 1 \\\n--machine-type \"e2-standard-4\" \\\n--release-channel stable\n</code></pre> <pre><code>gcloud container clusters get-credentials gke-auto-&lt;STUDENT_NAME&gt;-notepad-dev --region us-central1\n</code></pre>"},{"location":"020_Module_10_Assignment_EFK_solution/#1-install-elasticsearch-kibana-fluentd-helm-charts","title":"1 Install Elasticsearch, Kibana, Fluentd Helm Charts","text":""},{"location":"020_Module_10_Assignment_EFK_solution/#11-elasticsearch-installation","title":"1.1 Elasticsearch Installation","text":"<p>Step 1: Create a new namespace for EFK stack:</p> <pre><code>kubectl create ns efk\n</code></pre> <p>(Optional) use kubens it to make default namespace <code>efk</code></p> <pre><code>kubens efk\n</code></pre> <p>If you execute this step you can safely omit <code>-n efk</code> from all subsequent commands, since this is an optional step we will stell show that prefix on the commands below.</p> <p>Step 2: Install Elasticsearch using Helm:</p> <pre><code>helm repo add elastic https://helm.elastic.co\nhelm install elasticsearch elastic/elasticsearch -n efk\n</code></pre> <p>Check the status of deployment:</p> <pre><code>kubectl get pods --namespace=efk -l app=elasticsearch-master -w\n</code></pre>"},{"location":"020_Module_10_Assignment_EFK_solution/#12-kibana-installation","title":"1.2 Kibana installation","text":"<p>Step 1: Create custom Kibana configuration, that will allow to expose Kibana Dashboard:</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; kibana_values.yaml\nservice:\n  type: LoadBalancer\nEOF\n</code></pre> <pre><code>cat kibana_values.yaml  \n</code></pre> <p>Make sure values are correct</p> <p>Step 2: Deploy Kibana Ingress Charts with custom parameters in <code>efk</code> namespace:</p> <pre><code>helm install kibana elastic/kibana -n efk --values kibana_values.yaml\n</code></pre> <pre><code>helm list -n efk\n</code></pre> <p>Access Kibana from your browser using <code>LoadBalancer</code> IP on port 5601:</p> <pre><code>kubectl get svc -n efk\n</code></pre>"},{"location":"020_Module_10_Assignment_EFK_solution/#13-fluentd-installation","title":"1.3 Fluentd installation","text":"<p>Step 1: Create custom Fluentd configuration, below snippet is updating existing <code>configmap</code> with some parameters:</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; fluentd_values.yaml\nfileConfigs:\n# here we read the logs from Docker's containers and parse them\n  01_sources.conf: |-\n    ## logs from podman\n    &lt;source&gt;\n      @type tail\n      @id in_tail_container_logs\n      @label @KUBERNETES\n      path /var/log/containers/*.log\n      pos_file /var/log/fluentd-containers.log.pos\n      tag kubernetes.*\n      read_from_head true\n      &lt;parse&gt;\n        @type multi_format\n        ## pattern matching rules for incoming messages from the defined sources\n        &lt;pattern&gt;\n          format json\n          time_key time\n          time_type string\n          time_format \"%Y-%m-%dT%H:%M:%S.%NZ\"\n          keep_time_key true\n        &lt;/pattern&gt;\n        &lt;pattern&gt;\n          format regexp\n          expression /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr)( (.))? (?&lt;log&gt;.*)$/\n          time_format '%Y-%m-%dT%H:%M:%S.%NZ'\n          keep_time_key true\n        &lt;/pattern&gt;\n      &lt;/parse&gt;\n      emit_unmatched_lines true\n    &lt;/source&gt;\n\n# we use kubernetes metadata plugin to add metadata to the log\n  02_filters.conf: |-\n    &lt;label @KUBERNETES&gt;\n      &lt;match kubernetes.var.log.containers.fluentd**&gt;\n        @type relabel\n        @label @FLUENT_LOG\n      &lt;/match&gt;\n\n      # &lt;match kubernetes.var.log.containers.**_kube-system_**&gt;\n      #   @type null\n      #   @id ignore_kube_system_logs\n      # &lt;/match&gt;\n\n      &lt;filter kubernetes.**&gt;\n        @type kubernetes_metadata\n        @id filter_kube_metadata\n        skip_labels false\n        skip_container_metadata false\n        skip_namespace_metadata true\n        skip_master_url true\n      &lt;/filter&gt;\n\n      &lt;match **&gt;\n        @type relabel\n        @label @DISPATCH\n      &lt;/match&gt;\n    &lt;/label&gt;\n # We filtering what logs will be send\n  03_dispatch.conf: |-\n    &lt;label @DISPATCH&gt;\n      &lt;filter **&gt;\n        @type prometheus\n        &lt;metric&gt;\n          name fluentd_input_status_num_records_total\n          type counter\n          desc The total number of incoming records\n          &lt;labels&gt;\n            tag ${tag}\n            hostname ${hostname}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n      &lt;/filter&gt;\n\n      &lt;match **&gt;\n        @type relabel\n        @label @OUTPUT\n      &lt;/match&gt;\n    &lt;/label&gt;\n# we send the logs to Elasticsearch\n  04_outputs.conf: |-\n    &lt;label @OUTPUT&gt;\n      &lt;match **&gt;\n        @type elasticsearch\n        host \"elasticsearch-master\"\n        port 9200\n        path \"\"\n        user elastic\n        password changeme\n      &lt;/match&gt;\n    &lt;/label&gt;\nEOF\n</code></pre> <p>Ensure you spend some time reading the comments in the configuration above.</p> <p>Task 2: Deploy <code>Fluentd</code> Chart with custom parameters in <code>efk</code> namespace:</p> <p>Deploy <code>Fluentd</code> with Helm:</p> <ul> <li>Use the repository name: fluent </li> <li>Use the repository url: https://fluent.github.io/helm-charts</li> <li>install fluent/fluend in the <code>efk</code> namespace with the <code>fluentd_values.yaml</code> as values file.</li> </ul> <p>Solution</p> <p>Run the following commands:</p> <p>helm repo add fluent https://fluent.github.io/helm-charts   helm repo update   helm install fluentd fluent/fluentd -n efk --values fluentd_values.yaml</p>"},{"location":"020_Module_10_Assignment_EFK_solution/#2-configure-kibana-and-create-index","title":"2 Configure Kibana and Create Index:","text":"<p>Kibana requires an index pattern to access the Elasticsearch data that you want to explore. An index pattern selects the data to use and allows you to define properties of the fields.</p> <p>In our case we configured <code>fluentd</code> index, that should be populated at <code>Elasticsearch</code></p> <p>Step 1: Locate Kibana URL:</p> <pre><code>kubectl get svc -n efk\n</code></pre> <p>Step 2: Launch the Kibana web interface:</p> <pre><code>loadbalancer_ip:5601\n</code></pre> <p>Result</p> <p>You should see your Kibana interface</p> <p></p> <p>Step 3: Update Kibana Configuration to support <code>time</code> Meta fields:</p> <p>In UI Go to: Management - Stack Management:</p> <p></p> <p>Step 4: Then Kibana - Advanced Setting:</p> <p></p> <p>Step 5: Find field: <code>Meta fields</code>, and add <code>time</code> field as following:</p> <p></p> <p>Step 7: Create an index pattern. </p> <p>In UI  Click: Discover (under analytics)</p> <p></p> <p></p> <p>Click <code>Create Index Pattern</code> button.</p> <p>Note</p> <p>alternative path: <code>Management - Stack Management -&gt; Kibana Index Patterns</code></p> <p>Step 8: In <code>Create index pattern</code> window. Type inside <code>Index pattern name</code>: <code>fluentd*</code> as the index pattern name. More documentation can be found here</p> <p></p> <p>Result</p> <p>Your index pattern matches 1 source</p> <pre><code>Click Next Step &gt;\n</code></pre> <p>Step 9: Configure which field <code>Kibana</code> will use to filter log data by time. In the dropdown, select the <code>@time</code> field, and hit <code>Create index pattern</code>.</p> <p>Summary</p> <p>Our ElasticSearch and Kibana fully configured. You should be able now see some cluster logs in Kibana.</p> <p>Step 10: View Kibana Logs from our GKE cluster:</p> <p>In UI  Click: Discover (under analytics)</p> <pre><code>In Query search field paste: `kubernetes.namespace_name: kube-system`\nIn Range field paste: Select last 15 minutes\nClick Refresh\n</code></pre> <p></p> <p>Result</p> <p>you can see logs from <code>kube-system</code> namespace</p> <p>Note</p> <p>You can save you Query for future use</p>"},{"location":"020_Module_10_Assignment_EFK_solution/#3-deploy-onlineboutique-application","title":"3 Deploy onlineboutique application","text":"<p>Deploy microservices application <code>onlineboutique</code>:</p> <p>Create Namespace <code>onlineboutique</code></p> <pre><code>kubectl create ns onlineboutique\n</code></pre> <p>Deploy Microservice application</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/microservices-demo.git\ncd microservices-demo\n</code></pre> <pre><code>kubectl apply -f ./release/kubernetes-manifests.yaml -n onlineboutique\n</code></pre> <p>Verify Deployment:</p> <pre><code>kubectl get pods -n onlineboutique\n</code></pre>"},{"location":"020_Module_10_Assignment_EFK_solution/#4-observe-onlineboutique-logs","title":"4 Observe <code>onlineboutique</code> logs","text":"<p>Step 1: First let's review the logs using <code>kubectl logs</code> command locally on the cluster. We going to check <code>paymentservice</code> pod logs</p> <pre><code>export PAYMENTSERVICE_POD=$(kubectl get pods -n onlineboutique | grep paymentservice |awk '{ print $1}')\nkubectl logs $PAYMENTSERVICE_POD -n onlineboutique\n</code></pre> <p>Step 2: Now let's review Kibana Logs from our GKE cluster on <code>onlineboutique</code> namespace:</p> <pre><code>In Query search field paste: `kubernetes.namespace_name: onlineboutique`\nIn Range field paste: Select last 15 minutes\nClick Refresh\n</code></pre> <p></p> <p>Result</p> <p>We can see all logs from <code>onlineboutique</code> namespace</p> <p>Step 2: Now let's review Kibana Logs from <code>paymentservice</code> pod only</p> <p>echo $PAYMENTSERVICE_POD</p> <pre><code>In Query search field paste: `kubernetes.namespace_name: onlineboutique`\nIn Range field paste: Select last 15 minutes\nClick Refresh\n</code></pre> <p>Summary</p> <p>Developers should not have access to Kubernetes CLI, however they can get access to Observability tools like Kibana to allow them troubleshooting!</p>"},{"location":"020_Module_10_Assignment_EFK_solution/#5-review-fluentd-configuration","title":"5 Review <code>Fluentd</code> configuration","text":"<p>In some cases, you want to filter logs from only your applications to be seen by your team. To achieve this, Fluentd should be configured to only intake specific logs so that no resources are wasted. </p> <p>Review <code>sources</code> configuration:</p> <pre><code>kubectl get configmap -n efk fluentd-config -oyaml | grep -v \"#\" |  grep -A20 01_sources.conf\n</code></pre> <pre><code>  01_sources.conf: |-\n    &lt;source&gt;\n      @type tail\n      @id in_tail_container_logs\n      @label @KUBERNETES\n      path /var/log/containers/*.log\n      pos_file /var/log/fluentd-containers.log.pos\n      tag kubernetes.*\n      read_from_head true\n      &lt;parse&gt;\n        @type multi_format\n        &lt;pattern&gt;\n          format json\n          time_key time\n          time_type string\n          time_format \"%Y-%m-%dT%H:%M:%S.%NZ\"\n          keep_time_key false\n        &lt;/pattern&gt;\n        &lt;pattern&gt;\n          format regexp\n          expression /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr)( (.))? (?&lt;log&gt;.*)$/\n</code></pre> <p>Note</p> <p><code>id</code>: A unique identifier to reference this source. This can be used for further filtering and routing of structured log data</p> <p><code>type</code>: Inbuilt directive understood by fluentd. In this case, \u201ctail\u201d instructs fluentd to gather data by tailing logs from a given location. Another example is \u201chttp\u201d which instructs fluentd to collect data by using GET on http endpoint. </p> <p><code>path</code>: Specific to type \u201ctail\u201d. Instructs fluentd to collect all logs under /var/log/containers directory. This is the location used by docker daemon on a Kubernetes node to store stdout from running containers</p> <p><code>pos_file</code>: Used as a checkpoint. In case the fluentd process restarts, it uses the position from this file to resume log data collection</p> <p><code>tag</code>: A custom string for matching source to destination/filters. fluentd matches source/destination tags to route log data</p> <p>Step 2 Review <code>Filter</code> configuration</p> <pre><code>kubectl get configmap  -n efk fluentd-config -oyaml | grep -v \"#\" |  grep -A20 03_dispatch.conf\n</code></pre> <pre><code>  02_filters.conf: |-\n    &lt;label @KUBERNETES&gt;\n      &lt;match kubernetes.var.log.containers.fluentd**&gt;\n        @type relabel\n        @label @FLUENT_LOG\n      &lt;/match&gt;\n\n\n      &lt;filter kubernetes.**&gt;\n        @type kubernetes_metadata\n        @id filter_kube_metadata\n        skip_labels false\n        skip_container_metadata false\n        skip_namespace_metadata true\n        skip_master_url true\n      &lt;/filter&gt;\n\n      &lt;match **&gt;\n        @type relabel\n        @label @DISPATCH\n      &lt;/match&gt;\n</code></pre> <p>Step 3 Review <code>Dispatch</code> configuration</p> <pre><code>kubectl get configmap  -n efk fluentd-config -oyaml | grep -v \"#\" |  grep -A20 03_dispatch.conf\n</code></pre> <pre><code>  03_dispatch.conf: |-\n    &lt;label @DISPATCH&gt;\n      &lt;filter **&gt;\n        @type prometheus\n        &lt;metric&gt;\n          name fluentd_input_status_num_records_total\n          type counter\n          desc The total number of incoming records\n          &lt;labels&gt;\n            tag ${tag}\n            hostname ${hostname}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n      &lt;/filter&gt;\n\n      &lt;match **&gt;\n        @type relabel\n        @label @OUTPUT\n      &lt;/match&gt;\n    &lt;/label&gt;\n</code></pre> <p>Step 4 Review <code>Output Plugin</code> configuration</p> <pre><code>kubectl get configmap  -n efk fluentd-config -oyaml | grep -v \"#\" |  grep -A10 04_outputs.conf\n</code></pre> <pre><code>  04_outputs.conf: |-\n    &lt;label @OUTPUT&gt;\n      &lt;match **&gt;\n        @type elasticsearch\n        host \"elasticsearch-master\"\n        port 9200\n        path \"\"\n        user elastic\n        password changeme\n      &lt;/match&gt;\n    &lt;/label&gt;\n</code></pre> <p>Note</p> <p><code>match</code>: tag indicates a destination. It is followed by a regular expression for matching the source. In this case, we want to capture all logs and send them to Elasticsearch, so simply use **</p> <p><code>type</code>: Supported output plugin identifier. In this case, we are using ElasticSearch which is a built-in plugin of fluentd.</p> <p><code>host/port</code>: ElasticSearch host/port. Credentials can be configured as well, but not shown here.</p>"},{"location":"020_Module_10_Assignment_EFK_solution/#6-configure-fluentd-to-specific-logs","title":"6 Configure <code>Fluentd</code> to specific logs","text":"<p>Let's configure Fluentd to only send <code>onlineboutique</code> namespace logs to elasticsearch:</p> <p>Step 1: Modify helm <code>fluentd_values.yaml</code> values file for <code>03_dispatch.conf</code> config so that only log files that matches <code>onlineboutique</code> namespace is labeled to be sent:</p> <pre><code>cd ../\nedit fluentd_values.yaml\n</code></pre> <p>Replace line under <code>03_dispatch.conf: |-</code> config:</p> <pre><code>&lt;match **&gt; # Send all logs\n</code></pre> <p>To: </p> <pre><code>&lt;match kubernetes.var.log.containers.**_onlineboutique_**&gt;\n</code></pre> <p>So it looks as following:</p> <pre><code>  03_dispatch.conf: |-\n    &lt;label @DISPATCH&gt;\n      &lt;filter **&gt;\n        @type prometheus\n        &lt;metric&gt;\n          name fluentd_input_status_num_records_total\n          type counter\n          desc The total number of incoming records\n          &lt;labels&gt;\n            tag ${tag}\n            hostname ${hostname}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n      &lt;/filter&gt;\n\n      &lt;match kubernetes.var.log.containers.**_onlineboutique_**&gt;\n        @type relabel\n        @label @OUTPUT\n      &lt;/match&gt;\n    &lt;/label&gt;\n</code></pre> <p><code>&lt;match kubernetes.var.log.containers.**_onlineboutique_**&gt;</code> will filter out logs with <code>_onlineboutique_</code>. This configuration will only relabel the logs that matches the configuration as <code>@OUTPUT</code>. As specified in <code>04_outputs.conf</code>, only logs labelled as <code>@OUTPUT</code> will be sent to elasticsearch. </p> <p>Note that this is not the only way to configure fluentd to send one namespace's logs. </p> <p>Step 2: Install helm diff plugin</p> <pre><code>helm plugin install https://github.com/databus23/helm-diff\n</code></pre> <p>Step 3: Verify diff</p> <pre><code>helm diff upgrade fluentd fluent/fluentd -n efk  --values fluentd_values.yaml\n</code></pre> <p>Output:</p> <pre><code>-       &lt;match **&gt;\n+       &lt;match kubernetes.var.log.containers.**_onlineboutique_**&gt;\n</code></pre> <p>Step 4: Update fluentd <code>configmap</code> via Helm Upgrade</p> <pre><code>helm upgrade fluentd fluent/fluentd -n efk  --values fluentd_values.yaml\n</code></pre> <p>Step 5: Observe that fluentd stop sending logs for other namespaces than <code>_onlineboutique_</code></p> <pre><code>In Query search field paste: `kubernetes.namespace_name: kube-system`\nIn Range field paste: Select last 15 minute\nClick Refresh\n</code></pre> <p></p> <p>Result</p> <p>No more <code>kube-system</code> logs send to the ElasticSearch and hence Kibana can't display them. However we can see all historical logs!!! Prior we disabled the logging for all namespaces.</p> <p>Feel free to play around with EFK, there are a lot mroe things it does that this assignmet didn't cover.</p> <p>Optional challenges:</p> <ul> <li>Execute aggregate log searches to see:<ul> <li>Which service fails the most?</li> <li>What is the frequency of failures?</li> </ul> </li> <li>Setup a monitoring dashboard for the online boutique</li> <li>The online boutique also exports metrics and traces, can you get them into EFK?</li> </ul> <p>Optional reading material: Feel free to save these for future reference.</p> <ul> <li>How to choose the right open-source log collector</li> <li>Introduction to Fluentd: Collect logs and send almost anywhere</li> <li>Introduction to Fluentd &amp; Fluent Bit<ul> <li>03:00 - Introduction to Fluentd and Fluent Bit (Slides)</li> <li>18:30 - When should I use Fluentd vs Fluent Bit</li> <li>22:00 - Deploying Fluentd with kubectl</li> <li>30:00 - Setting up Kibana</li> <li>34:00 - Deploying Fluent Bit with Helm</li> <li>40:00 - Config Visualizer</li> <li>48:00 - Filtering logs</li> <li>01:00:00 - Stream processing with Fluent Bit</li> </ul> </li> <li>Introduction to fluentbit</li> <li>Grafana Loki: like Prometheus, but for Logs</li> <li>Correlate Your Metrics, Logs &amp; Traces with the curated OSS observability stack from Grafana Labs</li> </ul>"},{"location":"020_Module_10_Assignment_EFK_solution/#7-submit-your-assignment","title":"7 Submit your assignment","text":"<p>Save the output of the following commands and submit to the assignment folder:</p> <pre><code>kubectl describe pods -n efk &gt; k8s_pods_$STUDENT_NAME_efk.txt\nkubectl describe pods -n onlineboutique &gt; k8s_pods_$STUDENT_NAME_onlineboutique.txt\nkubectl get all -A &gt; k8s_resources_$STUDENT_NAME.txt\n</code></pre>"},{"location":"020_Module_10_Assignment_EFK_solution/#8-cleanup","title":"8 Cleanup","text":"<p>Uninstall Helm Charts:</p> <pre><code>helm uninstall fluentd -n efk\nhelm uninstall kibana -n efk\nhelm uninstall elasticsearch -n efk\n</code></pre> <p>Delete GKE cluster:</p> <pre><code>gcloud container clusters delete gke-auto-&lt;STUDENT_NAME&gt;-notepad-dev --region us-central1\n</code></pre>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/","title":"020 Module 2 Assignment Terraform Foundation","text":"<p>Objective:</p> <ul> <li>Learn Terraform Commands</li> <li>Learn GCP Terraform Provider</li> <li>Learn Terraform variables</li> <li>Store TF State in GCS buckets</li> <li>Learn how to create GCP resources with Terraform </li> </ul>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#prepare-lab-environment","title":"Prepare Lab Environment","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#working-with-git-and-iac","title":"Working with Git and IaC","text":"<p>Git-based workflows, merge requests and peer reviews create a level of documented transparency that is great for security teams and audits. They ensure every change is documented as well as the metadata surrounding the change, answering the why, who, when and what questions.</p> <p>In this set of exercises you will continue using <code>notepad</code> Google Source Repository that already contains you application code, <code>dockerfiles</code> and kubernetes manifests, that we've used in <code>ycit019</code></p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#add-terrform-iac-code-to-source-code-repository","title":"Add Terrform IaC code to Source Code repository","text":"<p>Step 1 Go into your personal Google Cloud Source Repository:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 2 Create 2 folders: <code>foundation-infrastructure</code> and <code>notepad-infrastructure</code>  that will contain terraform infrastructure for you GCP foundation and infrastructure that will be used Notepad application such as GKE clusters and CloudSQL database.</p> <pre><code>mkdir ycit020_module2\ncd ycit020_module2\nmkdir foundation-infrastructure\nmkdir notepad-infrastructure\n</code></pre> <p>Step 3 Create a <code>README.md</code> file describing <code>foundation-infrastructure</code> folder purpose:  </p> <pre><code>cat &lt;&lt;EOF&gt; foundation-infrastructure/README.md\n\n# Creation of GCP Foundation Layer\n\nThis terraform automation, is focused on creating GCP Foundation: GCP folders (if any) and GCP projects, GCS bucket to store terraform state and required IAM Configuration.\n\nEOF\n</code></pre> <p>Step 4 Create a <code>README.md</code> file describing <code>notepad-infrastructure</code> folder purpose:  </p> <pre><code>cat &lt;&lt;EOF&gt; notepad-infrastructure/README.md\n# Creation of GCP Services Layer\n\nThis terraform automation, is focused on creating gcp services such as GKE, VPC, CloudSQL and etc. inside of projects created by `foundation-infrastructure` terraform automation.\n\nEOF\n</code></pre> <p>Step 5 Create a .gitignore file in your working directory:</p> <pre><code>cat&lt;&lt;EOF&gt;&gt; .gitignore\n.terraform.*\n.terraform\nterraform.tfstate*\nEOF\n</code></pre> <p>Note</p> <p>Ignore files are used to tell <code>git</code> not to track files. You should also always include any files with credentials in a <code>gitignore</code> file.</p> <p>List created folder structure along with <code>gitignore</code>:</p> <pre><code>ls -ltra\n</code></pre> <p>Step 6 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"Terraform Folder structure for module 2\"\n</code></pre> <p>Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#1-build-gcp-foundation-layer","title":"1 Build GCP Foundation Layer","text":"<p>In this assignment we going to build GCP Foundation layer. As we learned in the class this includes Org Structure creation with Folders and Projects, creation IAM Roles and assigning them to users and groups in organization. This Layer usually build be DevOps or Infra team that.</p> <p>Since we don't have organization registered for each student, we going to skip creation of folders and IAM groups. We going to start from creation of a Project, deleting <code>default</code> VPC and configuring inside that project a <code>gcs</code> bucket that will be used in the next terraform layer to store a Terraform state.</p> <p>Part 1: Create Terraform Configurations file to create's  GCP Foundation Layer:</p> <p>Layer 1 From existing  (we going to call it seeding) project that you currently use to store code in Google Cloud Source Repository:</p> <ul> <li>Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf</li> <li>Create a new <code>notepad-dev</code> Project<ul> <li>Delete Default VPC</li> </ul> </li> <li>Create a bucket in this project to store terraform state</li> </ul> <p></p> <p>Part 2: Create Terraform Configurations file that create's  GCP Services Layer:</p> <p>Layer 2 From <code>notepad-dev</code> GCP Project:</p> <ul> <li>Enable Google Project Service APIs</li> <li>Create VPC (google_compute_network) and Subnet (google_compute_subnetwork)</li> </ul> <p></p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#11-installing-terraform","title":"1.1 Installing Terraform","text":"<p>GCP Cloud Shell comes with many common tools pre-installed including <code>terraform</code></p> <p>Verify and validate the version of Terraform that is installed:</p> <pre><code>terraform --version\n</code></pre> <p>If you want to use specific version of terraform or want to install terraform in you local machine use following link to download binary.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#12-configure-gcp-credentials-for-terraform","title":"1.2 Configure GCP credentials for Terraform","text":"<p>If you would like to use terraform on GCP you have 2 options:</p> <p>1). Using you <code>user credentials</code>, great option for testing terraform from you laptop and for learning purposes.</p> <p>2). Using service account, great option if you going to use terraform with CI/CD system and fully automate Terraform deployment.</p> <p>In this assignment we going to use Option 1 - using user credentials.</p> <p>Step 1: In order to make requests against the GCP API, you need to authenticate to prove that it's you making the request.  The preferred method of provisioning resources with Terraform on your workstation is to use the Google Cloud SDK (Option 1) </p> <pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#13-initializing-terraform","title":"1.3 Initializing Terraform","text":"<p>Terraform relies on <code>providers</code> to interact with remote systems. Every resource type is implemented by a provider; without <code>providers</code>, Terraform can't manage any kind of infrastructure; in order for terraform to install and use a provider it must be declared.</p> <p>In this exercise you will declare and configure the Terraform provider(s) that will be used for the rest of the assignment.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#131-declare-terraform-providers","title":"1.3.1 Declare Terraform Providers","text":"<p>The <code>required_providers</code> block defines the providers terraform will use to provision resources and their source.</p> <ul> <li> <p><code>version</code>: The version argument is optional and is used to tell terraform to pick a particular version from the availale versions</p> </li> <li> <p><code>source</code>: The source is the provider registry e.g. hashicorp/gcp is the short for registry.terraform.io/hashicorp/gcp</p> </li> </ul> <pre><code>cd ~/$student_name-notepad/ycit020_module2/foundation-infrastructure/\n</code></pre> <pre><code>cat &lt;&lt; EOF&gt;&gt; provider.tf\nterraform {\n  required_providers {\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~&gt; 4.37.0\"\n    }\n  }\n}\nEOF\n</code></pre>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#132-configure-the-terraform-provider","title":"1.3.2 Configure the Terraform Provider","text":"<p>Providers often require configuration (like endpoint URLs or cloud regions) before they can be used. These configurations are defined by a <code>provider</code> block. Multiple provider configuration blocks can be declared for the same by adding the <code>alias</code> argument</p> <p>Set you current <code>PROJECT_ID</code> value here:</p> <pre><code>export PROJECT_ID=&lt;YOUR_PROJECT_ID&gt;\n</code></pre> <pre><code>cat &lt;&lt; EOF&gt;&gt; main.tf\nprovider \"google\" {\n  alias   = \"gcp-provider\"\n  project = \"$PROJECT_ID\"\n  region  = \"us-central1\"\n}\nEOF\n</code></pre> <p>Result</p> <p>We configured <code>provider</code>, so that it can provision resource in specified gcp project in <code>us-central1</code> region.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#133-initialize-terraform","title":"1.3.3 Initialize Terraform","text":"<p>Now that you have declared and configured the GCP provider for terraform, initialize terraform:</p> <pre><code>cd ~/$student_name-notepad/ycit020_module2/foundation-infrastructure/\nterraform init\n</code></pre> <p>Explore your directory. What has changed?</p> <pre><code>ls -ltra\n</code></pre> <p>Result</p> <p>We can see that new directory <code>.terraform</code> and <code>.terraform.lock.hcl</code> file.</p> <p>Extra Research! Not Mandatory</p> <p>Investigate available providers in the Terraform Provider Registry</p> <p>Select another provider from the list, add it to your required providers, and to your main.tf</p> <p>Run terraform init to load your new provider!</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#14-terraform-variables","title":"1.4 Terraform Variables","text":"<p>Input variables are used to increase your Terraform configuration's flexibility by defining values that can be assigned to customize the configuration. They provide a consistent interface to change how a given configuration behaves. Input variables blocks have a defined format:</p> <p>Input variables blocks have a defined format:</p> <pre><code>variable \"variable_name\" {\n  type = \"The variable type eg. string , list , number\"\n  description = \"A description to understand what the variable will be used for\"\n  default = \"A default value can be provided, terraform will use this if no other value is provided at terraform apply\"\n}\n</code></pre> <p>Terraform CLI defines the following optional arguments for variable declarations:</p> <ul> <li>default - A default value which then makes the variable optional.</li> <li>type - This argument specifies what value types are accepted for the variable.</li> <li>description - This specifies the input variable's documentation.</li> <li>validation - A block to define validation rules, usually in addition to type constraints.</li> <li>sensitive - Limits Terraform UI output when the variable is used in configuration.</li> </ul> <p>In our above example for <code>main.tf</code> we can actually declare <code>project</code> and <code>region</code> as variables, so let's do it.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#141-declare-input-variables","title":"1.4.1 Declare Input Variables","text":"<p>Variables are declared in a <code>variables.tf</code> file inside your terraform working directory.</p> <p>The label after the variable keyword is a name for the variable, which must be unique among all variables in the same module.  You can choose variable name based on you preference, or in some cases based on agreed in company naming convention. In our case we declaring variables names: <code>gcp_region</code> and <code>gcp_project_id</code></p> <p>We are going to declare a type as a  <code>string</code> variable for <code>gcp_project_id</code> and <code>gcp_region</code>.</p> <p>It is always good idea to specify a clear <code>description</code> for the variable for documentation purpose as well as code reusability and readability.</p> <p>Finally, let's configured <code>default</code> argument. In our case we going to use \"us-central1\" as a <code>default</code> for <code>gcp_region</code> and we going to keep <code>default</code> value empty for <code>gcp_project_id</code>.</p> <pre><code>cat &lt;&lt;EOF&gt; variables.tf\nvariable \"gcp_region\" {\n  type        = string\n  description = \"The GCP Region\"\n  default     = \"us-central1\"\n}\n\nvariable \"gcp_project_id\" {\n  type        = string\n  description = \"The GCP Seeding project ID\"\n  default     = \"\"\n}\nEOF\n</code></pre> <p>Note</p> <p>Explore other type of variables such as <code>number</code>, <code>bool</code> and type constructors such as: list(), set(), map()"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#142-using-variables","title":"1.4.2 Using Variables","text":"<p>Now that you have created your input variables, let's re-create <code>main.tf</code> that currently has the terraform configuration for GCP Provider.</p> <pre><code>rm main.tf\ncat &lt;&lt; EOF&gt;&gt; main.tf\nprovider \"google\" {\n  alias   = \"gcp-provider\"\n  project = var.gcp_project_id\n  region  = var.gcp_region\n}\nEOF\n</code></pre> <p>Validate your terraform configuration:</p> <pre><code>terraform validate\n</code></pre>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#143-working-with-variables-files","title":"1.4.3 Working with Variables files.","text":"<p>Create a <code>terraform.tfvars</code> file to hold the values for your variables:</p> <pre><code>export gcp_project_id=&lt;YOUR_PROJECT_ID&gt;\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\ngcp_project_id = \"$PROJECT_ID\"\ngcp_region = \"us-central1\"\nEOF\n</code></pre> <p>Hint</p> <p>using different <code>env-name.tfvars</code> files you can create different set of terraform configuration for the same code. (e.g. code for dev, staging, prod)</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#144-validate-configuration-and-code-syntax","title":"1.4.4 Validate configuration and code syntax","text":"<p>Let's Validate configuration and code syntax that we've added so far.</p> <p>There are several tools that can analyze your Terraform code without running it, including:</p> <p>Terraform has build in command that you can use to check your Terraform syntax and types (a bit like a compiler):</p> <pre><code>terraform validate\n</code></pre> <p>Result</p> <p>Seems the code is legit so far</p> <p>Does your terraform files easy to read and follow? Terraform has a built-in function to <code>lint</code> your configuration manifests for readability and best practice spacing:</p> <p>Compare before lint:</p> <pre><code>cat terraform.tfvars\n</code></pre> <p>Perform lint:</p> <pre><code>terraform fmt --recursive\n</code></pre> <p>Check file after linting:</p> <pre><code>cat terraform.tfvars\n</code></pre> <p>The <code>--recursive</code> flag asks the <code>fmt</code> command to traverse all of your terraform directories and format the .tf files it finds. It will report the files it changed as part of the return information of the command</p> <p>Hint</p> <p>Use the git diff command to see what was changed.</p> <p>Result</p> <p>We can see that <code>terraform.tfvars</code> file had some spacing that been fixed for better code readability.</p> <p>Extra</p> <p>Another cool tool that you can use along you terraform development is tflint - framework and each feature is provided by plugins, the key features are as follows:</p> <ul> <li>Find possible errors (like illegal instance types) for Major Cloud providers (AWS/Azure/GCP).</li> <li>Warn about deprecated syntax, unused declarations.</li> <li>Enforce best practices, naming conventions.</li> </ul> <p>Finally let's run <code>terraform plan</code>:</p> <pre><code>terraform plan\n</code></pre> <p>Output:</p> <pre><code>No changes. Your infrastructure matches the configuration.\n</code></pre> <p>Result</p> <p>We don't have any errors, however we don't have any resources created so far. Let's create a GCP project resource to start with!</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#15-create-gcp-project-using-terraform","title":"1.5 Create GCP project using Terraform","text":""},{"location":"020_Module_2_Assignment_Terraform_Foundation/#151-configure-google_project-resource","title":"1.5.1 Configure <code>google_project</code> resource","text":"<p><code>Resources</code> describe the infrastructure objects you want terraform to create. A resource block can be used to create any object such as virtual private cloud, security groups, DNS records etc.</p> <p>Let's create our first Terraform resource: GCP project.</p> <p>In order to accomplish this we going to use <code>google_project</code> resource, documented here</p> <p>In order to create a new Project we need to define following arguments:   * name - (Required) The display name of the project.   * project_id - (Required) The project ID. Changing this forces a new project to be created.   * billing_account - The alphanumeric ID of the billing account this project belongs to.</p> <p>Step 1  Let's define <code>google_project</code> resource in <code>project.tf</code> file, we will replace actual values for <code>name</code> and <code>billing_account</code> with variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; project.tf\nresource \"google_project\" \"project\" {\n  name                = var.project_name\n  billing_account     = var.billing_account\n  project_id          = var.project_id\n}\nEOF\n</code></pre> <p>Note</p> <p>If you are creating as part of the GCP organization,  you have to add the variable <code>org_id</code>in the project.tf file.</p> <p>Step 2 Let's declare actual values for <code>name</code>, <code>project_id</code> and <code>billing_account</code> as variables in <code>variables.tf</code>:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\nvariable \"billing_account\" {\n  description = \"The billing account ID for this project\"\n}\n\nvariable \"project_name\" {\n  description = \"The human readable project name (min 4 letters)\"\n}\n\nvariable \"project_id\" {\n  description = \"The GCP project ID\"\n}\nEOF\n</code></pre> <p>Step 3  Update <code>terraform.tfvars</code> variables files, with actual values for  <code>name</code>, <code>project_id</code> and <code>billing_account</code>:</p> <p>As a best practices we going to follow specific naming convention when we going to create <code>PROJECT_ID</code> and <code>PROJECT_NAME</code>, in order to define your new GCP project:</p> <pre><code>export ORG=$student_name\nexport PRODUCT=notepad\nexport ENV=dev\nexport PROJECT_PREFIX=1\n</code></pre> <pre><code>export PROJECT_NAME=$ORG-$PRODUCT-$ENV\nexport PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX  # Project ID has to be unique\n</code></pre> <p>Verify that project name looks good:</p> <pre><code>echo $PROJECT_NAME\necho $PROJECT_ID\n</code></pre> <p>In order to get Education <code>Billing account number</code> run following command:</p> <pre><code>ACCOUNT_ID=$(gcloud beta billing accounts list | grep -B2 True  | head -1 | grep ACCOUNT_ID  |awk '{ print $2}') \n</code></pre> <p>Verify that number is good:</p> <pre><code>echo $ACCOUNT_ID\n</code></pre> <p>This might force installation of <code>gcloud beta</code> commands and may take up to 10 mins. It may also ask for some APIs to be enabled.</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\nbilling_account = \"$ACCOUNT_ID\"\nproject_name    = \"$PROJECT_NAME\"\nproject_id      = \"$PROJECT_ID\"\nEOF\n</code></pre> <p>Verify if <code>billing_account</code>, <code>project_name</code>, <code>project_id</code> are correct values:</p> <pre><code>cat terraform.tfvars\n</code></pre> <p>Important</p> <p>Verify if <code>billing_account</code>, <code>project_name</code>, <code>project_id</code> are correct values, if not set the values manually.</p> <p>Step 4 Run <code>terraform plan</code> command:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Result</p> <p><code>Plan: 1 to add, 0 to change, 0 to destroy.</code></p> <p>Note</p> <p>Take notice of plan command and see how some values that you declared in <code>*.tvfars</code> are visible and some values will <code>known after apply</code></p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify <code>GCP Project</code> has been created:</p> <pre><code>gcloud projects list | grep notepad\n</code></pre> <p>Output:</p> <pre><code>PROJECT_ID: $student-name-notepad-dev-1\nNAME: $student-name-notepad-dev\n</code></pre> <p>Success</p> <p>We've created our first resource <code>gcp_project</code> with terraform</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#152-making-project-resource-immutable","title":"1.5.2 Making Project resource Immutable","text":"<p>Read more about Immutable Infrastructure in this article</p> <p>Very often when you developing IaC, you need to destroy and recreate your resorces, e.g. for troubleshooting or creating a new resources using same config. Let's <code>destroy</code> our project and try to recreate it again.</p> <pre><code>terraform destroy -var-file terraform.tfvars\n</code></pre> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>\u2502 Error: error creating project archy-notepad-dev-1 (archy-notepad-dev): googleapi: Error 409: Requested entity already exists, alreadyExists. If you received a 403 error, make sure you have the `roles/resourcemanager.projectCreator` permission\n\u2502\n\u2502   with google_project.project,\n\u2502   on project.tf line 1, in resource \"google_project\" \"project\":\n\u2502    1: resource \"google_project\" \"project\" {\n</code></pre> <p>Failed</p> <p>What happened? Did you project been able to create? If not why ?</p> <p>If we want to make our infrastructure to be Immutable and fully automated, we need to make sure that we can destroy our service and recreate it any time the same way. In our case we can't do that because  <code>Project ID</code> always has to be unique. To tackle this problem we need to randomize our <code>Project ID</code>  creation within the terraform.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#153-create-a-project-using-terraform-random-provider","title":"1.5.3 Create a Project using Terraform Random Provider","text":"<p>In order to create a GCP Project with Random name, we can use Terraform's random_integer resource with following arguments:</p> <ul> <li>max (Number) The maximum inclusive value of the range - 100</li> <li>min (Number) The minimum inclusive value of the range - 999</li> </ul> <pre><code>cat &lt;&lt;EOF &gt;&gt; random.tf\nresource \"random_integer\" \"id\" {\n  min = 100\n  max = 999\n}\nEOF\n</code></pre> <p><code>random_integer</code> resource doesn't belongs to Google Provider, it requires Hashicorp Random Provider to be initialized. Since it's native hashicorp provider we can skip the step of defining and configuring that provider, as it will be automatically initialized. </p> <pre><code>terraform init\n</code></pre> <p>Result</p> <ul> <li>Installing hashicorp/random v3.1.0...</li> <li>Installed hashicorp/random v3.1.0 (signed by HashiCorp)</li> </ul>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#153-create-a-project_id-name-using-local-values","title":"1.5.3 Create a Project_ID name using Local Values","text":"<p><code>Local</code> Values allow you to assign a name to an expression, so the expression can be used multiple times, without repeating it.</p> <p>We going to define value of <code>project_id</code> as local. And it will be a combination of <code>project_name</code> <code>-</code> <code>random_number</code>.</p> <p>Let's add local value for <code>project_id</code> and replace value <code>var.project_id</code> with <code>local.project_id</code>.</p> <pre><code>edit project.tf\n</code></pre> <p>Remove previous code and paste new code snippet of <code>project.tf</code> as following and save:</p> <pre><code>locals {\n  project_id     = \"${var.project_name}-${random_integer.id.result}\"\n}\n\nresource \"google_project\" \"project\" {\n  name                = var.project_name\n  billing_account     = var.billing_account\n  project_id          = local.project_id\n}\n</code></pre> <p>We can now remove <code>project_id</code> value from <code>terraform.tfvars</code>, as we going to randomly generate it using <code>locals</code> expression with <code>random_integer</code></p> <pre><code>edit terraform.tfvars\n</code></pre> <p>Remove <code>project_id</code> line and save.</p> <p>We can now also remove <code>project_id</code> variable from <code>variables.tf</code>:</p> <pre><code>rm variables.tf\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\nvariable \"gcp_region\" {\n  type        = string\n  description = \"The GCP Region\"\n  default     = \"us-central1\"\n}\n\nvariable \"gcp_project_id\" {\n  type        = string\n  description = \"The GCP Seeding project ID\"\n  default     = \"\"\n}\nvariable \"billing_account\" {\n  description = \"The billing account ID for this project\"\n}\n\nvariable \"project_name\" {\n  description = \"The human readable project name (min 4 letters)\"\n}\nEOF\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>  # random_integer.id will be created\n  + resource \"random_integer\" \"id\" {\n      + id     = (known after apply)\n      + max    = 999\n      + min    = 100\n      + result = (known after apply)\n    }\n</code></pre> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify <code>GCP Project</code> has been created:</p> <pre><code>gcloud projects list | grep notepad\n</code></pre> <p>Output:</p> <pre><code>PROJECT_ID: $student-name-notepad-dev-XXX\nNAME: $student-name-notepad-dev\n</code></pre> <p>Result</p> <p>Project has been created with <code>Random</code> project_id. This will help to make our infrastructure creation immutable, as every time project will be created with new project_id.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#154-configure-terraform-output-for-gcp-project","title":"1.5.4 Configure Terraform Output for GCP Project","text":"<p><code>Outputs</code> provide return values of a Terraform state at any given time. So you can get for example values of what GCP resources like project, VMs, GKE cluster been created and their parametres.</p> <p><code>Outputs</code> can be used used to export structured data about resources. This data can be used to configure other parts of your infrastructure, or as a data source for another Terraform workspace. Outputs are also necessary to share data from a child module to your root module.</p> <p>Outputs follow a similar structure to variables:</p> <pre><code>output \"output_name\" {\n  description = \"A description to understand what information is provided by the output\"\n  value = \"An expression and/or resource_name.attribute\"\n  sensitive = \"Optional argument, marking an output sensitive will supress the value from plan/apply phases\"\n}\n</code></pre> <p>Let's declare GCP Project <code>output</code> values for <code>project_id</code> and <code>project_number</code>.</p> <p>You can find available <code>output</code> in each respective <code>resource</code> documents, under `Attributes Reference.</p> <p>For example for GCP project available outputs are:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"id\" {\n  value = google_project.project.project_id\n  description = \"GCP project ID\"\n}\n\noutput \"number\" {\n  value = google_project.project.number\n  description = \"GCP project number\"\n  sensitive = true\n}\n\nEOF\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>We can see that we have new changes to output:</p> <p>Output:</p> <pre><code>Changes to Outputs:\n  + id     = \"ayrat-notepad-dev-631\"\n  + number = (sensitive value)\n</code></pre> <p>Let's apply this changes:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>Apply complete! Resources: 0 added, 0 changed, 0 destroyed.\n</code></pre> <p>Let's now run <code>terraform output</code> command:</p> <pre><code>terraform output\n</code></pre> <p>Result</p> <p>We can see value of project <code>id</code>, however we don't see project number as it's been marked as sensitive.</p> <p>Note</p> <p><code>Outputs</code> can be useful when you want to provide results of terraform resource creation to CI/CD or next automation tool like <code>helm</code> to deploy application.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#155-recreate-gcp-project-without-default-vpc","title":"1.5.5 Recreate GCP Project without Default VPC","text":"<p>One of the requirements for our solution is to create GCP project with <code>custom</code> VPC. However, when terraform creates GCP Project it creates <code>DEFAULT</code> vpc by default.</p> <pre><code>NEW_PROJECT_ID=$(gcloud projects list --sort-by=projectId | grep notepad | grep PROJECT_ID |awk '{ print $2}')\n</code></pre> <pre><code>echo $NEW_PROJECT_ID\n</code></pre> <pre><code>gcloud compute networks list --project `$NEW_PROJECT_ID`\n</code></pre> <p>Output:</p> <pre><code>NAME     SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4\ndefault  AUTO         REGIONAL\n</code></pre> <p>In order to remove automatically created <code>default</code> VPC, specify special attribute during <code>google_project</code> resource creation. Check documentation here and find this argument.</p> <p>Vpc get's deleted during project creation only, so let's delete again our project and recreate it without VPC following Task N1 instructions.</p> <pre><code>terraform destroy -var-file terraform.tfvars\n</code></pre> <p>Task N1: Find Attribute to remove <code>default</code> VPC during  <code>google_project</code> resource creation. Define in it <code>project.tf</code> and set it's value in <code>variables.tf</code> so that VPC will not be created by default.</p> <pre><code>edit project.tf\nTODO\n</code></pre> <pre><code>edit variables.tf\nTODO\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Note</p> <p>That way your VPC will not be created during project creation based on the plan.</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Note</p> <p>How long project is now getting created. This is due to after project creation, <code>default</code> vpc is being removed as an extra step.</p> <p>Verify that <code>default</code> VPC has been deleted:</p> <pre><code>NEW_PROJECT_ID=$(gcloud projects list --sort-by=projectId | grep notepad | grep PROJECT_ID |awk '{ print $2}')\n</code></pre> <pre><code>gcloud compute networks list --project $NEW_PROJECT_ID\n</code></pre> <p>Output:</p> <pre><code>Listed 0 items.\n</code></pre> <p>Note</p> <p>You should <code>0</code> VPC networks. That means GCP project has been created without any VPC.</p> <p>Success</p> <p>We now able to create a GCP project without <code>Default</code> VPC.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#16-create-gcp-storage-bucket","title":"1.6 Create GCP Storage bucket","text":""},{"location":"020_Module_2_Assignment_Terraform_Foundation/#161-create-gcp-storage-bucket-in-new-gcp-project","title":"1.6.1 Create GCP Storage bucket in New GCP Project","text":"<p>Using reference doc for google_storage_bucket , let's create <code>google_storage_bucket</code> resource that will create MULTI_REGIONAL GCS bucket in newly created project. We going to give bucket <code>name</code>: <code>$ORG-notepad-dev-tfstate</code>, and we going to use this bucket to store Terraform state for GCP Service Layer.</p> <pre><code>cat &lt;&lt;EOF &gt;&gt;  bucket.tf\nresource \"google_storage_bucket\" \"state\" {\n  name          = var.bucket_name\n  project       = local.project_id\n  storage_class = var.storage_class\n  location      = var.gcp_region\n  force_destroy = \"true\"\n}\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\nvariable \"bucket_name\" {\n  description = \"The name of the bucket.\"\n}\n\nvariable \"storage_class\" {\n  default = \"REGIONAL\"\n}\nEOF\n</code></pre> <p>Set variable that will be used to build name for a bucket:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\nbucket_name   = \"$ORG-notepad-dev-tfstate\"\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"bucket_name\" {\n  value = google_storage_bucket.state.name\n}\nEOF\n</code></pre> <p>Let's review the plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>And create google_storage_bucket resource:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify created bucket in GCP UI:</p> <pre><code>Storage -&gt; Cloud Storage\n</code></pre> <pre><code>gsutil ls -L -b gs://$ORG-notepad-dev-tfstate\n</code></pre> <p>Result</p> <p>GCS bucket for terraform state has been created</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#162-configure-versioning-on-gcp-storage-bucket","title":"1.6.2 Configure <code>versioning</code> on GCP Storage bucket.","text":"<p>Check if versioning enabled on the created bucket:</p> <pre><code>gsutil versioning get gs://$ORG-notepad-dev-tfstate\n</code></pre> <p>Result</p> <p>Versioning: Suspended</p> <p>It is highly recommended that if you going to use GCS bucket as Terraform storage backend you should enable Object Versioning on the GCS bucket to allow for state recovery in the case of accidental deletions and human error.</p> <p>Task N2: Using reference doc for google_storage_bucket  find and configure argument that enables gcs bucket versioning feature.</p> <p>Edit <code>bucket.tf</code> with correct argument.</p> <pre><code>edit bucket.tf\nTODO\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>Plan: 0 to add, 1 to change, 0 to destroy.\n</code></pre> <p>Result</p> <p>Configuration for Versioning looks correct</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify versioning is ON:</p> <pre><code>gsutil versioning get gs://$ORG-notepad-dev-tfstate\n</code></pre> <p>Output:</p> <pre><code>Versioning Enabled\n</code></pre> <p>Result</p> <p>We've finished building the Foundation Layer. So far we able to accomplish following:</p> <ul> <li>Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf</li> <li>Create a new <code>notepad-dev</code> Project</li> <li>Delete Default VPC</li> <li>Create a bucket in this project to store terraform state</li> </ul>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#2-build-gcp-services-layer","title":"2 Build GCP Services Layer","text":"<p>Once we finished building a GCP Foundation layer which is essentially our project, we can start building  GCP Services Layer inside that project.</p> <p>This second layer will configure following items:</p> <ul> <li>Enable Google Project Service APIs</li> <li>Create VPC (google_compute_network) and Subnet (google_compute_subnetwork)</li> </ul> <p></p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#21-initialize-terraform-for-gcp-services-layer","title":"2.1  Initialize Terraform for GCP Services Layer","text":""},{"location":"020_Module_2_Assignment_Terraform_Foundation/#211-define-and-configure-terraform-provider","title":"2.1.1 Define and configure terraform provider","text":"<p>Step 1: Take note of newly created GCP Project_ID and BUCKET_ID:</p> <pre><code>cd ~/$student_name-notepad/ycit020_module2/foundation-infrastructure\n</code></pre> <pre><code>terraform output | grep 'id' |awk '{ print $3}'\nterraform output | grep 'bucket_name' |awk '{ print $3}'\n</code></pre> <p>Set it as variable:</p> <pre><code>export PROJECT_ID=$(terraform output | grep 'id' |awk '{ print $3}')\nexport BUCKET_ID=$(terraform output | grep 'bucket_name' |awk '{ print $3}')\necho $PROJECT_ID\necho $BUCKET_ID\n</code></pre> <p>Step 2: Declare the Terraform Provider for GCP Services Layer:</p> <p>We now going to switch to <code>notepad-infrastructure</code> where we going to create a new GCP service Layer terraform configuration:</p> <pre><code>cd ~/$student_name-notepad/ycit020_module2/notepad-infrastructure\n</code></pre> <pre><code>cat &lt;&lt; EOF&gt;&gt; provider.tf\nterraform {\n  required_providers { \n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~&gt; 4.37.0\"\n    }\n  }\n}\nEOF\n</code></pre> <p>Step 4: Configure the Terraform Provider</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; main.tf\nprovider \"google\" {\n  project = var.gcp_project_id\n  region  = var.gcp_region\n}\nEOF\n</code></pre> <p>Step 5: Define variables:</p> <pre><code>cat &lt;&lt;EOF&gt; variables.tf\nvariable \"gcp_region\" {\n  type        = string\n  description = \"The GCP Region\"\n  default     = \"us-central1\"\n}\n\nvariable \"gcp_project_id\" {\n  type        = string\n  description = \"The newly created GCP project ID\"\n}\nEOF\n</code></pre> <p>Step 5: Set variables in <code>terraform.tfvars</code></p> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\ngcp_project_id = $PROJECT_ID\nEOF\n</code></pre> <p>Step 4: Now that you have declared and configured the GCP provider for terraform, initialize terraform:</p> <pre><code>terraform init\n</code></pre>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#212-configure-terraform-state-backend","title":"2.1.2 Configure Terraform State backend","text":"<p>Terraform records information about what infrastructure it created in a Terraform state file. This information, or state, is stored by default in a <code>local</code> file named terraform.tfstate. This allows Terraform to compare what's in your configurations with what's in the state file, and determine what changes need to be applied.</p> <p>Local vs Remote backend: <code>Local</code> state files are the default for terraform. <code>Remote</code> state allows for collaboration between members of your team, for example multiple users or systems can deploy terraform configuration to the same environment as state is stored remotely and can be pulled <code>localy</code> during the execution. This however may create issues if 2 users running terraform <code>plan</code> and <code>apply</code> at the same time, however some remote backends including gcs provide <code>locking</code> feature that we covered and enabled in previous section <code>Configure versioning on GCP Storage bucket.</code></p> <p>In addition to that, <code>Remote</code> state is more secure storage of sensitive values that might be contained in variables or outputs.</p> <p>See documents for remote backend for reference:</p> <p>Step 1 Configure remote backend using gcs bucket:</p> <p>Let's configure a backend for your state, using the <code>gcs</code> bucket you previously created in Foundation Layer.</p> <p>Backends are configured with a nested backend block within the top-level terraform block While a backend can be declared anywhere, it is recommended to use a <code>backend.tf</code>.</p> <p>Since we running on GCP we going to use GCS remote backend. It stores the state as an object in a configurable prefix in a pre-existing bucket on Google Cloud Storage (GCS). This backend also supports state locking. The bucket must exist prior to configuring the backend.</p> <p>We going to use following arguments:</p> <ul> <li>bucket - (Required) The name of the GCS bucket. This name must be globally unique. For more information, see Bucket Naming Guidelines.</li> <li>prefix - (Optional) GCS prefix inside the bucket. Named states for workspaces are stored in an object called /.tfstate. <pre><code>cat &lt;&lt;EOF &gt;&gt; backend.tf\nterraform {\n  backend \"gcs\" {\n    bucket = $BUCKET_ID\n    prefix = \"state\"\n  }\n}\nEOF\n</code></pre> <p>Step 2  When you change, configure or unconfigure a backend, terraform must be re-initialized:</p> <pre><code>terraform init\n</code></pre> <p>Verify if <code>folder</code> state has been created in our bucket:</p> <pre><code>gsutil ls gs://$ORG-notepad-dev-tfstate/state\n</code></pre> <p>Summary</p> <p><code>state</code> Folder has been created in gcs bucket and terrafrom has been initialized with  <code>remote</code> backend</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#22-enable-required-gcp-services-api","title":"2.2 Enable required GCP Services API","text":"<p>Before we continue with creating GCP services like VPC, routers, Cloud Nat and GKE it is required to enable underlining GCP API services. When we creating a new project most of the services API's are disabled, and requires explicitly to be enabled. <code>google_project_service</code> resource allows management of a single API service for an existing Google Cloud Platform project.</p> <p>Let's enable <code>compute engine API</code> service with terraform:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; services.tf\nresource \"google_project_service\" \"compute\" {\n  service = \"compute.googleapis.com\"\n  disable_on_destroy = false\n}\nEOF\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\nterraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify Compute Engine API service has been enabled:</p> <pre><code> gcloud services list | grep Compute\n</code></pre> <p>Result</p> <p>Compute Engine API is enabled as it is listed.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#22-set-variables-to-define-standard-for-naming-convention","title":"2.2 Set variables to define standard for Naming Convention","text":"<p>As per terraform best practices, when you creating terraform resources, you need to follow naming convention, that is clear for you organization.</p> <ul> <li>Configuration objects should be named using underscores to delimit multiple words.</li> <li>Object's name should be named using dashes</li> </ul> <p>This practice ensures consistency with the naming convention for resource types, data source types, and other predefined values and helps prevent accidental deletion or outages:</p> <p>Example:</p> <pre><code># Good\nresource \"google_compute_instance\" \"web_server\" {\n  name = \u201cweb-server-$org-$app-$env\u201d\n  # ...\n}\n\n# Bad\nresource \u201cgoogle_compute_instance\u201d \u201cweb-server\u201d {\n  name =  \u201cweb-server\u201d\n  # \u2026\n</code></pre> <p>Create variables to define standard for Naming Convention:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\nvariable \"org\" {\n  type = string\n}\nvariable \"product\" {\n  type = string\n}\nvariable \"environment\" {\n  type = string\n}\nEOF\n</code></pre> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=dev\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\norg            = \"$ORG\"\nproduct        = \"$PRODUCT\"\nenvironment    = \"$ENV\"\nEOF\n</code></pre> <p>Review if created files are correct.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#23-create-a-custom-mode-network-vpc-with-terraform","title":"2.3 Create a <code>custom mode</code> network (VPC) with terraform","text":"<p>Using google_compute_network resource create a VPC network with terraform.</p> <p>Task N3: Create <code>vpc.tf</code> file and define <code>custom mode</code> network (VPC) with following requirements:   * Description: <code>VPC that will be used by the GKE private cluster on the related project</code>   * <code>name</code> - vpc name with following pattern: \"vpc-$ORG-$PRODUCT-$ENV\"   * Subnet mode: <code>custom</code>   * Bgp routing mode: <code>regional</code>   * MTUs: <code>default</code></p> <p>Hint</p> <p>To create \"vpc-$ORG-$PRODUCT-$ENV\" name, use format function, and use <code>%s</code> to convert variables to string values.</p> <p>Define variables in <code>variables.tf</code> and <code>terraform.tfvars</code> if required.</p> <p>Define <code>Output</code> for:</p> <ul> <li>Generated VPC name: <code>google_compute_network.vpc_network.name</code></li> <li>self_link - The URI of the created resource.</li> </ul> <pre><code>cat &lt;&lt;EOF &gt;&gt; vpc.tf\nTODO\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\nTODO\nEOF\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>  + create\n\nTerraform will perform the following actions:\n  # google_compute_network.vpc_network will be created\n  + resource \"google_compute_network\" \"vpc_network\" {\n      + auto_create_subnetworks         = false\n      + delete_default_routes_on_create = false\n      + description                     = \"VPC that will be used by the GKE private cluster on the related project\"\n      + gateway_ipv4                    = (known after apply)\n      + id                              = (known after apply)\n      + mtu                             = (known after apply)\n      + name                            = \"vpc-ayrat-notepad-dev\"\n      + project                         = (known after apply)\n      + routing_mode                    = \"REGIONAL\"\n      + self_link                       = (known after apply)\n</code></pre> <p>Create VPC:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify that <code>custom-mode</code> VPC has been created:</p> <pre><code>gcloud compute networks list \ngcloud compute networks describe $(gcloud compute networks list | grep NAME |awk '{ print $2 }')\n</code></pre> <p>Output:</p> <pre><code>autoCreateSubnetworks: false\ndescription: VPC that will be used by the GKE private cluster on the related project\nkind: compute#network\nname: vpc-student_name-notepad-dev\nroutingConfig:\n  routingMode: REGIONAL\nselfLink: https://www.googleapis.com/compute/v1/projects/XXX\nx_gcloud_bgp_routing_mode: REGIONAL\nx_gcloud_subnet_mode: CUSTOM\n</code></pre> <p>Result</p> <p>VPC Network has been created, without auto subnets.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation/#5-commit-terraform-configurations-to-repository-and-share-it-with-instructorteacher","title":"5 Commit Terraform configurations to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>ycit020_module2</code> folder using the following Git commands:</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git add .\ngit commit -m \"TF manifests for Module 2 Assignment\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre> <p>Step 3 This step will grant view access for Instructor to check you assignments</p> <p>In your Cloud Terminal:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID --member='user:ayrat.khayretdinov@gmail.com' --role=roles/viewer\n</code></pre> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID --member='user:valavan@gmail.com' --role=roles/viewer\n</code></pre> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID --member='user:mfsilv@gmail.com' --role=roles/viewer\n</code></pre> <p>Result</p> <p>Your instructor will be able to review you code and grade it.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/","title":"020 Module 2 Assignment Terraform Foundation solution","text":"<p>Objective:</p> <ul> <li>Learn Terraform Commands</li> <li>Learn GCP Terraform Provider</li> <li>Learn Terraform variables</li> <li>Store TF State in GCS buckets</li> <li>Learn how to create GCP resources with Terraform </li> </ul>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#prepare-lab-environment","title":"Prepare Lab Environment","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#working-with-git-and-iac","title":"Working with Git and IaC","text":"<p>Git-based workflows, merge requests and peer reviews create a level of documented transparency that is great for security teams and audits. They ensure every change is documented as well as the metadata surrounding the change, answering the why, who, when and what questions.</p> <p>In this set of exercises you will continue using <code>notepad</code> Google Source Repository that already contains you application code, <code>dockerfiles</code> and kubernetes manifests, that we've used in <code>ycit019</code></p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#add-terrform-iac-code-to-source-code-repository","title":"Add Terrform IaC code to Source Code repository","text":"<p>Step 1 Go into your personal Google Cloud Source Repository:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 2 Create 2 folders: <code>foundation-infrastructure</code> and <code>notepad-infrastructure</code>  that will contain terraform infrastructure for you GCP foundation and infrastructure that will be used Notepad application such as GKE clusters and CloudSQL database.</p> <pre><code>mkdir ycit020_module2\ncd ycit020_module2\nmkdir foundation-infrastructure\nmkdir notepad-infrastructure\n</code></pre> <p>Step 3 Create a <code>README.md</code> file describing <code>foundation-infrastructure</code> folder purpose:  </p> <pre><code>cat &lt;&lt;EOF&gt; foundation-infrastructure/README.md\n\n# Creation of GCP Foundation Layer\n\nThis terraform automation, is focused on creating GCP Foundation: GCP folders (if any) and GCP projects, GCS bucket to store terraform state and required IAM Configuration.\n\nEOF\n</code></pre> <p>Step 4 Create a <code>README.md</code> file describing <code>notepad-infrastructure</code> folder purpose:  </p> <pre><code>cat &lt;&lt;EOF&gt; notepad-infrastructure/README.md\n# Creation of GCP Services Layer\n\nThis terraform automation, is focused on creating gcp services such as GKE, VPC, CloudSQL and etc. inside of projects created by `foundation-infrastructure` terraform automation.\n\nEOF\n</code></pre> <p>Step 5 Create a .gitignore file in your working directory:</p> <pre><code>cat&lt;&lt;EOF&gt;&gt; .gitignore\n.terraform.*\n.terraform\nterraform.tfstate*\nEOF\n</code></pre> <p>Note</p> <p>Ignore files are used to tell <code>git</code> not to track files. You should also always include any files with credentials in a <code>gitignore</code> file.</p> <p>List created folder structure along with <code>gitignore</code>:</p> <pre><code>ls -ltra\n</code></pre> <p>Step 6 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"Terraform Folder structure for module 2\"\n</code></pre> <p>Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#1-build-gcp-foundation-layer","title":"1 Build GCP Foundation Layer","text":"<p>In this assignment we going to build GCP Foundation layer. As we learned in the class this includes Org Structure creation with Folders and Projects, creation IAM Roles and assigning them to users and groups in organization. This Layer usually build be DevOps or Infra team that.</p> <p>Since we don't have organization registered for each student, we going to skip creation of folders and IAM groups. We going to start from creation of a Project, deleting <code>default</code> VPC and configuring inside that project a <code>gcs</code> bucket that will be used in the next terraform layer to store a Terraform state.</p> <p>Part 1: Create Terraform Configurations file to create's  GCP Foundation Layer:</p> <p>Layer 1 From existing  (we going to call it seeding) project that you currently use to store code in Google Cloud Source Repository:</p> <ul> <li>Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf</li> <li>Create a new <code>notepad-dev</code> Project<ul> <li>Delete Default VPC</li> </ul> </li> <li>Create a bucket in this project to store terraform state</li> </ul> <p></p> <p>Part 2: Create Terraform Configurations file that create's  GCP Services Layer:</p> <p>Layer 2 From <code>notepad-dev</code> GCP Project:</p> <ul> <li>Enable Google Project Service APIs</li> <li>Create VPC (google_compute_network) and Subnet (google_compute_subnetwork)</li> </ul> <p></p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#11-installing-terraform","title":"1.1 Installing Terraform","text":"<p>GCP Cloud Shell comes with many common tools pre-installed including <code>terraform</code></p> <p>Verify and validate the version of Terraform that is installed:</p> <pre><code>terraform --version\n</code></pre> <p>If you want to use specific version of terraform or want to install terraform in you local machine use following link to download binary.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#12-configure-gcp-credentials-for-terraform","title":"1.2 Configure GCP credentials for Terraform","text":"<p>If you would like to use terraform on GCP you have 2 options:</p> <p>1). Using you <code>user credentials</code>, great option for testing terraform from you laptop and for learning purposes.</p> <p>2). Using service account, great option if you going to use terraform with CI/CD system and fully automate Terraform deployment.</p> <p>In this assignment we going to use Option 1 - using user credentials.</p> <p>Step 1: In order to make requests against the GCP API, you need to authenticate to prove that it's you making the request.  The preferred method of provisioning resources with Terraform on your workstation is to use the Google Cloud SDK (Option 1) </p> <pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#13-initializing-terraform","title":"1.3 Initializing Terraform","text":"<p>Terraform relies on <code>providers</code> to interact with remote systems. Every resource type is implemented by a provider; without <code>providers</code>, Terraform can't manage any kind of infrastructure; in order for terraform to install and use a provider it must be declared.</p> <p>In this exercise you will declare and configure the Terraform provider(s) that will be used for the rest of the assignment.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#131-declare-terraform-providers","title":"1.3.1 Declare Terraform Providers","text":"<p>The <code>required_providers</code> block defines the providers terraform will use to provision resources and their source.</p> <ul> <li> <p><code>version</code>: The version argument is optional and is used to tell terraform to pick a particular version from the availale versions</p> </li> <li> <p><code>source</code>: The source is the provider registry e.g. hashicorp/gcp is the short for registry.terraform.io/hashicorp/gcp</p> </li> </ul> <pre><code>cd ~/$student_name-notepad/ycit020_module2/foundation-infrastructure/\n</code></pre> <pre><code>cat &lt;&lt; EOF&gt;&gt; provider.tf\nterraform {\n  required_providers {\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~&gt; 4.37.0\"\n    }\n  }\n}\nEOF\n</code></pre>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#132-configure-the-terraform-provider","title":"1.3.2 Configure the Terraform Provider","text":"<p>Providers often require configuration (like endpoint URLs or cloud regions) before they can be used. These configurations are defined by a <code>provider</code> block. Multiple provider configuration blocks can be declared for the same by adding the <code>alias</code> argument</p> <p>Set you current <code>PROJECT_ID</code> value here:</p> <pre><code>export PROJECT_ID=&lt;YOUR_PROJECT_ID&gt;\n</code></pre> <pre><code>cat &lt;&lt; EOF&gt;&gt; main.tf\nprovider \"google\" {\n  alias   = \"gcp-provider\"\n  project = \"$PROJECT_ID\"\n  region  = \"us-central1\"\n}\nEOF\n</code></pre> <p>Result</p> <p>We configured <code>provider</code>, so that it can provision resource in specified gcp project in <code>us-central1</code> region.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#133-initialize-terraform","title":"1.3.3 Initialize Terraform","text":"<p>Now that you have declared and configured the GCP provider for terraform, initialize terraform:</p> <pre><code>cd ~/$student_name-notepad/ycit020_module2/foundation-infrastructure/\nterraform init\n</code></pre> <p>Explore your directory. What has changed?</p> <pre><code>ls -ltra\n</code></pre> <p>Result</p> <p>We can see that new directory <code>.terraform</code> and <code>.terraform.lock.hcl</code> file.</p> <p>Extra Research! Not Mandatory</p> <p>Investigate available providers in the Terraform Provider Registry</p> <p>Select another provider from the list, add it to your required providers, and to your main.tf</p> <p>Run terraform init to load your new provider!</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#14-terraform-variables","title":"1.4 Terraform Variables","text":"<p>Input variables are used to increase your Terraform configuration's flexibility by defining values that can be assigned to customize the configuration. They provide a consistent interface to change how a given configuration behaves. Input variables blocks have a defined format:</p> <p>Input variables blocks have a defined format:</p> <pre><code>variable \"variable_name\" {\n  type = \"The variable type eg. string , list , number\"\n  description = \"A description to understand what the variable will be used for\"\n  default = \"A default value can be provided, terraform will use this if no other value is provided at terraform apply\"\n}\n</code></pre> <p>Terraform CLI defines the following optional arguments for variable declarations:</p> <ul> <li>default - A default value which then makes the variable optional.</li> <li>type - This argument specifies what value types are accepted for the variable.</li> <li>description - This specifies the input variable's documentation.</li> <li>validation - A block to define validation rules, usually in addition to type constraints.</li> <li>sensitive - Limits Terraform UI output when the variable is used in configuration.</li> </ul> <p>In our above example for <code>main.tf</code> we can actually declare <code>project</code> and <code>region</code> as variables, so let's do it.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#141-declare-input-variables","title":"1.4.1 Declare Input Variables","text":"<p>Variables are declared in a <code>variables.tf</code> file inside your terraform working directory.</p> <p>The label after the variable keyword is a name for the variable, which must be unique among all variables in the same module.  You can choose variable name based on you preference, or in some cases based on agreed in company naming convention. In our case we declaring variables names: <code>gcp_region</code> and <code>gcp_project_id</code></p> <p>We are going to declare a type as a  <code>string</code> variable for <code>gcp_project_id</code> and <code>gcp_region</code>.</p> <p>It is always good idea to specify a clear <code>description</code> for the variable for documentation purpose as well as code reusability and readability.</p> <p>Finally, let's configured <code>default</code> argument. In our case we going to use \"us-central1\" as a <code>default</code> for <code>gcp_region</code> and we going to keep <code>default</code> value empty for <code>gcp_project_id</code>.</p> <pre><code>cat &lt;&lt;EOF&gt; variables.tf\nvariable \"gcp_region\" {\n  type        = string\n  description = \"The GCP Region\"\n  default     = \"us-central1\"\n}\n\nvariable \"gcp_project_id\" {\n  type        = string\n  description = \"The GCP Seeding project ID\"\n  default     = \"\"\n}\nEOF\n</code></pre> <p>Note</p> <p>Explore other type of variables such as <code>number</code>, <code>bool</code> and type constructors such as: list(), set(), map()"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#142-using-variables","title":"1.4.2 Using Variables","text":"<p>Now that you have created your input variables, let's re-create <code>main.tf</code> that currently has the terraform configuration for GCP Provider.</p> <pre><code>rm main.tf\ncat &lt;&lt; EOF&gt;&gt; main.tf\nprovider \"google\" {\n  alias   = \"gcp-provider\"\n  project = var.gcp_project_id\n  region  = var.gcp_region\n}\nEOF\n</code></pre> <p>Validate your terraform configuration:</p> <pre><code>terraform validate\n</code></pre>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#143-working-with-variables-files","title":"1.4.3 Working with Variables files.","text":"<p>Create a <code>terraform.tfvars</code> file to hold the values for your variables:</p> <pre><code>export gcp_project_id=&lt;YOUR_PROJECT_ID&gt;\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\ngcp_project_id = \"$PROJECT_ID\"\ngcp_region = \"us-central1\"\nEOF\n</code></pre> <p>Hint</p> <p>using different <code>env-name.tfvars</code> files you can create different set of terraform configuration for the same code. (e.g. code for dev, staging, prod)</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#144-validate-configuration-and-code-syntax","title":"1.4.4 Validate configuration and code syntax","text":"<p>Let's Validate configuration and code syntax that we've added so far.</p> <p>There are several tools that can analyze your Terraform code without running it, including:</p> <p>Terraform has build in command that you can use to check your Terraform syntax and types (a bit like a compiler):</p> <pre><code>terraform validate\n</code></pre> <p>Result</p> <p>Seems the code is legit so far</p> <p>Does your terraform files easy to read and follow? Terraform has a built-in function to <code>lint</code> your configuration manifests for readability and best practice spacing:</p> <p>Compare before lint:</p> <pre><code>cat terraform.tfvars\n</code></pre> <p>Perform lint:</p> <pre><code>terraform fmt --recursive\n</code></pre> <p>Check file after linting:</p> <pre><code>cat terraform.tfvars\n</code></pre> <p>The <code>--recursive</code> flag asks the <code>fmt</code> command to traverse all of your terraform directories and format the .tf files it finds. It will report the files it changed as part of the return information of the command</p> <p>Hint</p> <p>Use the git diff command to see what was changed.</p> <p>Result</p> <p>We can see that <code>terraform.tfvars</code> file had some spacing that been fixed for better code readability.</p> <p>Extra</p> <p>Another cool tool that you can use along you terraform development is tflint - framework and each feature is provided by plugins, the key features are as follows:</p> <ul> <li>Find possible errors (like illegal instance types) for Major Cloud providers (AWS/Azure/GCP).</li> <li>Warn about deprecated syntax, unused declarations.</li> <li>Enforce best practices, naming conventions.</li> </ul> <p>Finally let's run <code>terraform plan</code>:</p> <pre><code>terraform plan\n</code></pre> <p>Output:</p> <pre><code>No changes. Your infrastructure matches the configuration.\n</code></pre> <p>Result</p> <p>We don't have any errors, however we don't have any resources created so far. Let's create a GCP project resource to start with!</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#15-create-gcp-project-using-terraform","title":"1.5 Create GCP project using Terraform","text":""},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#151-configure-google_project-resource","title":"1.5.1 Configure <code>google_project</code> resource","text":"<p><code>Resources</code> describe the infrastructure objects you want terraform to create. A resource block can be used to create any object such as virtual private cloud, security groups, DNS records etc.</p> <p>Let's create our first Terraform resource: GCP project.</p> <p>In order to accomplish this we going to use <code>google_project</code> resource, documented here</p> <p>In order to create a new Project we need to define following arguments:   * name - (Required) The display name of the project.   * project_id - (Required) The project ID. Changing this forces a new project to be created.   * billing_account - The alphanumeric ID of the billing account this project belongs to.</p> <p>Step 1  Let's define <code>google_project</code> resource in <code>project.tf</code> file, we will replace actual values for <code>name</code> and <code>billing_account</code> with variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; project.tf\nresource \"google_project\" \"project\" {\n  name                = var.project_name\n  billing_account     = var.billing_account\n  project_id          = var.project_id\n}\nEOF\n</code></pre> <p>Note</p> <p>If you are creating as part of the GCP organization,  you have to add the variable <code>org_id</code>in the project.tf file.</p> <p>Step 2 Let's declare actual values for <code>name</code>, <code>project_id</code> and <code>billing_account</code> as variables in <code>variables.tf</code>:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\nvariable \"billing_account\" {\n  description = \"The billing account ID for this project\"\n}\n\nvariable \"project_name\" {\n  description = \"The human readable project name (min 4 letters)\"\n}\n\nvariable \"project_id\" {\n  description = \"The GCP project ID\"\n}\nEOF\n</code></pre> <p>Step 3  Update <code>terraform.tfvars</code> variables files, with actual values for  <code>name</code>, <code>project_id</code> and <code>billing_account</code>:</p> <p>As a best practices we going to follow specific naming convention when we going to create <code>PROJECT_ID</code> and <code>PROJECT_NAME</code>, in order to define your new GCP project:</p> <pre><code>export ORG=$student_name\nexport PRODUCT=notepad\nexport ENV=dev\nexport PROJECT_PREFIX=1\n</code></pre> <pre><code>export PROJECT_NAME=$ORG-$PRODUCT-$ENV\nexport PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX  # Project ID has to be unique\n</code></pre> <p>Verify that project name looks good:</p> <pre><code>echo $PROJECT_NAME\necho $PROJECT_ID\n</code></pre> <p>In order to get Education <code>Billing account number</code> run following command:</p> <pre><code>ACCOUNT_ID=$(gcloud beta billing accounts list | grep -B2 True  | head -1 | grep ACCOUNT_ID  |awk '{ print $2}') \n</code></pre> <p>Verify that number is good:</p> <pre><code>echo $ACCOUNT_ID\n</code></pre> <p>This might force installation of <code>gcloud beta</code> commands and may take up to 10 mins. It may also ask for some APIs to be enabled.</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\nbilling_account = \"$ACCOUNT_ID\"\nproject_name    = \"$PROJECT_NAME\"\nproject_id      = \"$PROJECT_ID\"\nEOF\n</code></pre> <p>Verify if <code>billing_account</code>, <code>project_name</code>, <code>project_id</code> are correct values:</p> <pre><code>cat terraform.tfvars\n</code></pre> <p>Important</p> <p>Verify if <code>billing_account</code>, <code>project_name</code>, <code>project_id</code> are correct values, if not set the values manually.</p> <p>Step 4 Run <code>terraform plan</code> command:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Result</p> <p><code>Plan: 1 to add, 0 to change, 0 to destroy.</code></p> <p>Note</p> <p>Take notice of plan command and see how some values that you declared in <code>*.tvfars</code> are visible and some values will <code>known after apply</code></p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify <code>GCP Project</code> has been created:</p> <pre><code>gcloud projects list | grep notepad\n</code></pre> <p>Output:</p> <pre><code>PROJECT_ID: $student-name-notepad-dev-1\nNAME: $student-name-notepad-dev\n</code></pre> <p>Success</p> <p>We've created our first resource <code>gcp_project</code> with terraform</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#152-making-project-resource-immutable","title":"1.5.2 Making Project resource Immutable","text":"<p>Read more about Immutable Infrastructure in this article</p> <p>Very often when you developing IaC, you need to destroy and recreate your resorces, e.g. for troubleshooting or creating a new resources using same config. Let's <code>destroy</code> our project and try to recreate it again.</p> <pre><code>terraform destroy -var-file terraform.tfvars\n</code></pre> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>\u2502 Error: error creating project archy-notepad-dev-1 (archy-notepad-dev): googleapi: Error 409: Requested entity already exists, alreadyExists. If you received a 403 error, make sure you have the `roles/resourcemanager.projectCreator` permission\n\u2502\n\u2502   with google_project.project,\n\u2502   on project.tf line 1, in resource \"google_project\" \"project\":\n\u2502    1: resource \"google_project\" \"project\" {\n</code></pre> <p>Failed</p> <p>What happened? Did you project been able to create? If not why ?</p> <p>If we want to make our infrastructure to be Immutable and fully automated, we need to make sure that we can destroy our service and recreate it any time the same way. In our case we can't do that because  <code>Project ID</code> always has to be unique. To tackle this problem we need to randomize our <code>Project ID</code>  creation within the terraform.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#153-create-a-project-using-terraform-random-provider","title":"1.5.3 Create a Project using Terraform Random Provider","text":"<p>In order to create a GCP Project with Random name, we can use Terraform's random_integer resource with following arguments:</p> <ul> <li>max (Number) The maximum inclusive value of the range - 100</li> <li>min (Number) The minimum inclusive value of the range - 999</li> </ul> <pre><code>cat &lt;&lt;EOF &gt;&gt; random.tf\nresource \"random_integer\" \"id\" {\n  min = 100\n  max = 999\n}\nEOF\n</code></pre> <p><code>random_integer</code> resource doesn't belongs to Google Provider, it requires Hashicorp Random Provider to be initialized. Since it's native hashicorp provider we can skip the step of defining and configuring that provider, as it will be automatically initialized. </p> <pre><code>terraform init\n</code></pre> <p>Result</p> <ul> <li>Installing hashicorp/random v3.1.0...</li> <li>Installed hashicorp/random v3.1.0 (signed by HashiCorp)</li> </ul>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#153-create-a-project_id-name-using-local-values","title":"1.5.3 Create a Project_ID name using Local Values","text":"<p><code>Local</code> Values allow you to assign a name to an expression, so the expression can be used multiple times, without repeating it.</p> <p>We going to define value of <code>project_id</code> as local. And it will be a combination of <code>project_name</code> <code>-</code> <code>random_number</code>.</p> <p>Let's add local value for <code>project_id</code> and replace value <code>var.project_id</code> with <code>local.project_id</code>.</p> <pre><code>edit project.tf\n</code></pre> <p>Remove previous code and paste new code snippet of <code>project.tf</code> as following and save:</p> <pre><code>locals {\n  project_id     = \"${var.project_name}-${random_integer.id.result}\"\n}\n\nresource \"google_project\" \"project\" {\n  name                = var.project_name\n  billing_account     = var.billing_account\n  project_id          = local.project_id\n}\n</code></pre> <p>We can now remove <code>project_id</code> value from <code>terraform.tfvars</code>, as we going to randomly generate it using <code>locals</code> expression with <code>random_integer</code></p> <pre><code>edit terraform.tfvars\n</code></pre> <p>Remove <code>project_id</code> line and save.</p> <p>We can now also remove <code>project_id</code> variable from <code>variables.tf</code>:</p> <pre><code>rm variables.tf\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\nvariable \"gcp_region\" {\n  type        = string\n  description = \"The GCP Region\"\n  default     = \"us-central1\"\n}\n\nvariable \"gcp_project_id\" {\n  type        = string\n  description = \"The GCP Seeding project ID\"\n  default     = \"\"\n}\nvariable \"billing_account\" {\n  description = \"The billing account ID for this project\"\n}\n\nvariable \"project_name\" {\n  description = \"The human readable project name (min 4 letters)\"\n}\nEOF\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>  # random_integer.id will be created\n  + resource \"random_integer\" \"id\" {\n      + id     = (known after apply)\n      + max    = 999\n      + min    = 100\n      + result = (known after apply)\n    }\n</code></pre> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify <code>GCP Project</code> has been created:</p> <pre><code>gcloud projects list | grep notepad\n</code></pre> <p>Output:</p> <pre><code>PROJECT_ID: $student-name-notepad-dev-XXX\nNAME: $student-name-notepad-dev\n</code></pre> <p>Result</p> <p>Project has been created with <code>Random</code> project_id. This will help to make our infrastructure creation immutable, as every time project will be created with new project_id.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#154-configure-terraform-output-for-gcp-project","title":"1.5.4 Configure Terraform Output for GCP Project","text":"<p><code>Outputs</code> provide return values of a Terraform state at any given time. So you can get for example values of what GCP resources like project, VMs, GKE cluster been created and their parametres.</p> <p><code>Outputs</code> can be used used to export structured data about resources. This data can be used to configure other parts of your infrastructure, or as a data source for another Terraform workspace. Outputs are also necessary to share data from a child module to your root module.</p> <p>Outputs follow a similar structure to variables:</p> <pre><code>output \"output_name\" {\n  description = \"A description to understand what information is provided by the output\"\n  value = \"An expression and/or resource_name.attribute\"\n  sensitive = \"Optional argument, marking an output sensitive will supress the value from plan/apply phases\"\n}\n</code></pre> <p>Let's declare GCP Project <code>output</code> values for <code>project_id</code> and <code>project_number</code>.</p> <p>You can find available <code>output</code> in each respective <code>resource</code> documents, under `Attributes Reference.</p> <p>For example for GCP project available outputs are:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"id\" {\n  value = google_project.project.project_id\n  description = \"GCP project ID\"\n}\n\noutput \"number\" {\n  value = google_project.project.number\n  description = \"GCP project number\"\n  sensitive = true\n}\n\nEOF\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>We can see that we have new changes to output:</p> <p>Output:</p> <pre><code>Changes to Outputs:\n  + id     = \"ayrat-notepad-dev-631\"\n  + number = (sensitive value)\n</code></pre> <p>Let's apply this changes:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>Apply complete! Resources: 0 added, 0 changed, 0 destroyed.\n</code></pre> <p>Let's now run <code>terraform output</code> command:</p> <pre><code>terraform output\n</code></pre> <p>Result</p> <p>We can see value of project <code>id</code>, however we don't see project number as it's been marked as sensitive.</p> <p>Note</p> <p><code>Outputs</code> can be useful when you want to provide results of terraform resource creation to CI/CD or next automation tool like <code>helm</code> to deploy application.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#155-recreate-gcp-project-without-default-vpc","title":"1.5.5 Recreate GCP Project without Default VPC","text":"<p>One of the requirements for our solution is to create GCP project with <code>custom</code> VPC. However, when terraform creates GCP Project it creates <code>DEFAULT</code> vpc by default.</p> <pre><code>NEW_PROJECT_ID=$(gcloud projects list --sort-by=projectId | grep notepad | grep PROJECT_ID |awk '{ print $2}')\n</code></pre> <pre><code>echo $NEW_PROJECT_ID\n</code></pre> <pre><code>gcloud compute networks list --project `$NEW_PROJECT_ID`\n</code></pre> <p>Output:</p> <pre><code>NAME     SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4\ndefault  AUTO         REGIONAL\n</code></pre> <p>In order to remove automatically created <code>default</code> VPC, specify special attribute during <code>google_project</code> resource creation. Check documentation here and find this argument.</p> <p>Vpc get's deleted during project creation only, so let's delete again our project and recreate it without VPC following Task N1 instructions.</p> <pre><code>terraform destroy -var-file terraform.tfvars\n</code></pre> <p>Task N1: Find Attribute to remove <code>default</code> VPC during  <code>google_project</code> resource creation. Define in it <code>project.tf</code> and set it's value in <code>variables.tf</code> so that VPC will not be created by default.</p> <pre><code>edit project.tf\n</code></pre> <p>Update to:</p> <pre><code>locals {\n  project_id     = \"${var.project_name}-${random_integer.id.result}\"\n}\n\nresource \"google_project\" \"project\" {\n  name                = var.project_name\n  billing_account     = var.billing_account\n  project_id          = local.project_id\n  auto_create_network = var.auto_vpc\n}\n</code></pre> <pre><code>edit variables.tf\n</code></pre> <p>Update to:</p> <pre><code>variable \"auto_vpc\" {\n  description = \"Choose if it is required to create the 'default' network automatically\"\n  default     = \"false\"\n}\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Note</p> <p>That way your VPC will not be created during project creation based on the plan.</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Note</p> <p>How long project is now getting created. This is due to after project creation, <code>default</code> vpc is being removed as an extra step.</p> <p>Verify that <code>default</code> VPC has been deleted:</p> <pre><code>NEW_PROJECT_ID=$(gcloud projects list --sort-by=projectId | grep notepad | grep PROJECT_ID |awk '{ print $2}')\n</code></pre> <pre><code>gcloud compute networks list --project $NEW_PROJECT_ID\n</code></pre> <p>Output:</p> <pre><code>Listed 0 items.\n</code></pre> <p>Note</p> <p>You should <code>0</code> VPC networks. That means GCP project has been created without any VPC.</p> <p>Success</p> <p>We now able to create a GCP project without <code>Default</code> VPC.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#16-create-gcp-storage-bucket","title":"1.6 Create GCP Storage bucket","text":""},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#161-create-gcp-storage-bucket-in-new-gcp-project","title":"1.6.1 Create GCP Storage bucket in New GCP Project","text":"<p>Using reference doc for google_storage_bucket , let's create <code>google_storage_bucket</code> resource that will create MULTI_REGIONAL GCS bucket in newly created project. We going to give bucket <code>name</code>: <code>$ORG-notepad-dev-tfstate</code>, and we going to use this bucket to store Terraform state for GCP Service Layer.</p> <pre><code>cat &lt;&lt;EOF &gt;&gt;  bucket.tf\nresource \"google_storage_bucket\" \"state\" {\n  name          = var.bucket_name\n  project       = local.project_id\n  storage_class = var.storage_class\n  location      = var.gcp_region\n  force_destroy = \"true\"\n}\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\nvariable \"bucket_name\" {\n  description = \"The name of the bucket.\"\n}\n\nvariable \"storage_class\" {\n  default = \"REGIONAL\"\n}\nEOF\n</code></pre> <p>Set variable that will be used to build name for a bucket:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\nbucket_name   = \"$ORG-notepad-dev-tfstate\"\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"bucket_name\" {\n  value = google_storage_bucket.state.name\n}\nEOF\n</code></pre> <p>Let's review the plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>And create google_storage_bucket resource:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify created bucket in GCP UI:</p> <pre><code>Storage -&gt; Cloud Storage\n</code></pre> <pre><code>gsutil ls -L -b gs://$ORG-notepad-dev-tfstate\n</code></pre> <p>Result</p> <p>GCS bucket for terraform state has been created</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#162-configure-versioning-on-gcp-storage-bucket","title":"1.6.2 Configure <code>versioning</code> on GCP Storage bucket.","text":"<p>Check if versioning enabled on the created bucket:</p> <pre><code>gsutil versioning get gs://$ORG-notepad-dev-tfstate\n</code></pre> <p>Result</p> <p>Versioning: Suspended</p> <p>It is highly recommended that if you going to use GCS bucket as Terraform storage backend you should enable Object Versioning on the GCS bucket to allow for state recovery in the case of accidental deletions and human error.</p> <p>Task N2: Using reference doc for google_storage_bucket  find and configure argument that enables gcs bucket versioning feature.</p> <p>Edit <code>bucket.tf</code> with correct argument.</p> <pre><code>edit bucket.tf\nresource \"google_storage_bucket\" \"state\" {\n  name          = var.bucket_name\n  project       = local.project_id\n  storage_class = var.storage_class\n  location      = var.gcp_region\n  force_destroy = \"true\"\n  versioning {\n      enabled = \"true\"\n  }\n}\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>Plan: 0 to add, 1 to change, 0 to destroy.\n</code></pre> <p>Result</p> <p>Configuration for Versioning looks correct</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify versioning is ON:</p> <pre><code>gsutil versioning get gs://$ORG-notepad-dev-tfstate\n</code></pre> <p>Output:</p> <pre><code>gs://$student-notepad-dev-tfstate: Enabled\n</code></pre> <p>Result</p> <p>We've finished building the Foundation Layer. So far we able to accomplish following:</p> <ul> <li>Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf</li> <li>Create a new <code>notepad-dev</code> Project</li> <li>Delete Default VPC</li> <li>Create a bucket in this project to store terraform state</li> </ul>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#2-build-gcp-services-layer","title":"2 Build GCP Services Layer","text":"<p>Once we finished building a GCP Foundation layer which is essentially our project, we can start building  GCP Services Layer inside that project.</p> <p>This second layer will configure following items:</p> <ul> <li>Enable Google Project Service APIs</li> <li>Create VPC (google_compute_network) and Subnet (google_compute_subnetwork)</li> </ul> <p></p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#21-initialize-terraform-for-gcp-services-layer","title":"2.1  Initialize Terraform for GCP Services Layer","text":""},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#211-define-and-configure-terraform-provider","title":"2.1.1 Define and configure terraform provider","text":"<p>Step 1: Take note of newly created GCP Project_ID and BUCKET_ID:</p> <pre><code>cd ~/$student_name-notepad/ycit020_module2/foundation-infrastructure\n</code></pre> <pre><code>terraform output | grep 'id' |awk '{ print $3}'\nterraform output | grep 'bucket_name' |awk '{ print $3}'\n</code></pre> <p>Set it as variable:</p> <pre><code>export PROJECT_ID=$(terraform output | grep 'id' |awk '{ print $3}')\nexport BUCKET_ID=$(terraform output | grep 'bucket_name' |awk '{ print $3}')\necho $PROJECT_ID\necho $BUCKET_ID\n</code></pre> <p>Step 2: Declare the Terraform Provider for GCP Services Layer:</p> <p>We now going to switch to <code>notepad-infrastructure</code> where we going to create a new GCP service Layer terraform configuration:</p> <pre><code>cd ~/$student_name-notepad/ycit020_module2/notepad-infrastructure\n</code></pre> <pre><code>cat &lt;&lt; EOF&gt;&gt; provider.tf\nterraform {\n  required_providers { \n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~&gt; 4.37.0\"\n    }\n  }\n}\nEOF\n</code></pre> <p>Step 4: Configure the Terraform Provider</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; main.tf\nprovider \"google\" {\n  project = var.gcp_project_id\n  region  = var.gcp_region\n}\nEOF\n</code></pre> <p>Step 5: Define variables:</p> <pre><code>cat &lt;&lt;EOF&gt; variables.tf\nvariable \"gcp_region\" {\n  type        = string\n  description = \"The GCP Region\"\n  default     = \"us-central1\"\n}\n\nvariable \"gcp_project_id\" {\n  type        = string\n  description = \"The newly created GCP project ID\"\n}\nEOF\n</code></pre> <p>Step 5: Set variables in <code>terraform.tfvars</code></p> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\ngcp_project_id = $PROJECT_ID\nEOF\n</code></pre> <p>Step 4: Now that you have declared and configured the GCP provider for terraform, initialize terraform:</p> <pre><code>terraform init\n</code></pre>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#212-configure-terraform-state-backend","title":"2.1.2 Configure Terraform State backend","text":"<p>Terraform records information about what infrastructure it created in a Terraform state file. This information, or state, is stored by default in a <code>local</code> file named terraform.tfstate. This allows Terraform to compare what's in your configurations with what's in the state file, and determine what changes need to be applied.</p> <p>Local vs Remote backend: <code>Local</code> state files are the default for terraform. <code>Remote</code> state allows for collaboration between members of your team, for example multiple users or systems can deploy terraform configuration to the same environment as state is stored remotely and can be pulled <code>localy</code> during the execution. This however may create issues if 2 users running terraform <code>plan</code> and <code>apply</code> at the same time, however some remote backends including gcs provide <code>locking</code> feature that we covered and enabled in previous section <code>Configure versioning on GCP Storage bucket.</code></p> <p>In addition to that, <code>Remote</code> state is more secure storage of sensitive values that might be contained in variables or outputs.</p> <p>See documents for remote backend for reference:</p> <p>Step 1 Configure remote backend using gcs bucket:</p> <p>Let's configure a backend for your state, using the <code>gcs</code> bucket you previously created in Foundation Layer.</p> <p>Backends are configured with a nested backend block within the top-level terraform block While a backend can be declared anywhere, it is recommended to use a <code>backend.tf</code>.</p> <p>Since we running on GCP we going to use GCS remote backend. It stores the state as an object in a configurable prefix in a pre-existing bucket on Google Cloud Storage (GCS). This backend also supports state locking. The bucket must exist prior to configuring the backend.</p> <p>We going to use following arguments:</p> <ul> <li>bucket - (Required) The name of the GCS bucket. This name must be globally unique. For more information, see Bucket Naming Guidelines.</li> <li>prefix - (Optional) GCS prefix inside the bucket. Named states for workspaces are stored in an object called /.tfstate. <pre><code>cat &lt;&lt;EOF &gt;&gt; backend.tf\nterraform {\n  backend \"gcs\" {\n    bucket = $BUCKET_ID\n    prefix = \"state\"\n  }\n}\nEOF\n</code></pre> <p>Step 2  When you change, configure or unconfigure a backend, terraform must be re-initialized:</p> <pre><code>terraform init\n</code></pre> <p>Verify if <code>folder</code> state has been created in our bucket:</p> <pre><code>gsutil ls gs://$ORG-notepad-dev-tfstate/state\n</code></pre> <p>Summary</p> <p><code>state</code> Folder has been created in gcs bucket and terrafrom has been initialized with  <code>remote</code> backend</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#22-enable-required-gcp-services-api","title":"2.2 Enable required GCP Services API","text":"<p>Before we continue with creating GCP services like VPC, routers, Cloud Nat and GKE it is required to enable underlining GCP API services. When we creating a new project most of the services API's are disabled, and requires explicitly to be enabled. <code>google_project_service</code> resource allows management of a single API service for an existing Google Cloud Platform project.</p> <p>Let's enable <code>compute engine API</code> service with terraform:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; services.tf\nresource \"google_project_service\" \"compute\" {\n  service = \"compute.googleapis.com\"\n  disable_on_destroy = false\n}\nEOF\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\nterraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify Compute Engine API service has been enabled:</p> <pre><code> gcloud services list | grep Compute\n</code></pre> <p>Result</p> <p>Compute Engine API is enabled as it is listed.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#22-set-variables-to-define-standard-for-naming-convention","title":"2.2 Set variables to define standard for Naming Convention","text":"<p>As per terraform best practices, when you creating terraform resources, you need to follow naming convention, that is clear for you organization.</p> <ul> <li>Configuration objects should be named using underscores to delimit multiple words.</li> <li>Object's name should be named using dashes</li> </ul> <p>This practice ensures consistency with the naming convention for resource types, data source types, and other predefined values and helps prevent accidental deletion or outages:</p> <p>Example:</p> <pre><code># Good\nresource \"google_compute_instance\" \"web_server\" {\n  name = \u201cweb-server-$org-$app-$env\u201d\n  # ...\n}\n\n# Bad\nresource \u201cgoogle_compute_instance\u201d \u201cweb-server\u201d {\n  name =  \u201cweb-server\u201d\n  # \u2026\n</code></pre> <p>Create variables to define standard for Naming Convention:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\nvariable \"org\" {\n  type = string\n}\nvariable \"product\" {\n  type = string\n}\nvariable \"environment\" {\n  type = string\n}\nEOF\n</code></pre> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=dev\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\norg            = \"$ORG\"\nproduct        = \"$PRODUCT\"\nenvironment    = \"$ENV\"\nEOF\n</code></pre> <p>Review if created files are correct.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#23-create-a-custom-mode-network-vpc-with-terraform","title":"2.3 Create a <code>custom mode</code> network (VPC) with terraform","text":"<p>Using google_compute_network resource create a VPC network with terraform.</p> <p>Task N3: Create <code>vpc.tf</code> file and define <code>custom mode</code> network (VPC) with following requirements:   * Description: <code>VPC that will be used by the GKE private cluster on the related project</code>   * <code>name</code> - vpc name with following pattern: \"vpc-$ORG-$PRODUCT-$ENV\"   * Subnet mode: <code>custom</code>   * Bgp routing mode: <code>regional</code>   * MTUs: <code>default</code></p> <p>Hint</p> <p>To create \"vpc-$ORG-$PRODUCT-$ENV\" name, use format function, and use <code>%s</code> to convert variables to string values.</p> <p>Define variables in <code>variables.tf</code> and <code>terraform.tfvars</code> if required.</p> <p>Define <code>Output</code> for:</p> <ul> <li>Generated VPC name: <code>google_compute_network.vpc_network.name</code></li> <li>self_link - The URI of the created resource.</li> </ul> <pre><code>cat &lt;&lt;EOF &gt;&gt; vpc.tf\nresource \"google_compute_network\" \"vpc_network\" {\n  name                    = \"vpc-$ORG-$PRODUCT-$ENV\"\n  auto_create_subnetworks = false\n  mtu                     = 1460\n  routing_mode            = \"REGIONAL\"\n}\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"vpc_network_name\" {\n  value = google_compute_network.vpc_network.id\n  description = \"full path to vpc\"\n}\noutput \"vpc_network_self_link\" {\n  value = google_compute_network.vpc_network.self_link\n  description = \"The URI of the created VPC\"\n}\nEOF\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>  + create\n\nTerraform will perform the following actions:\n  # google_compute_network.vpc_network will be created\n  + resource \"google_compute_network\" \"vpc_network\" {\n      + auto_create_subnetworks         = false\n      + delete_default_routes_on_create = false\n      + description                     = \"VPC that will be used by the GKE private cluster on the related project\"\n      + gateway_ipv4                    = (known after apply)\n      + id                              = (known after apply)\n      + mtu                             = (known after apply)\n      + name                            = \"vpc-ayrat-notepad-dev\"\n      + project                         = (known after apply)\n      + routing_mode                    = \"REGIONAL\"\n      + self_link                       = (known after apply)\n</code></pre> <p>Create VPC:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify that <code>custom-mode</code> VPC has been created:</p> <pre><code>gcloud compute networks list \ngcloud compute networks describe $(gcloud compute networks list | grep NAME |awk '{ print $2 }')\n</code></pre> <p>Output:</p> <pre><code>autoCreateSubnetworks: false\ndescription: VPC that will be used by the GKE private cluster on the related project\nkind: compute#network\nname: vpc-student_name-notepad-dev\nroutingConfig:\n  routingMode: REGIONAL\nselfLink: https://www.googleapis.com/compute/v1/projects/XXX\nx_gcloud_bgp_routing_mode: REGIONAL\nx_gcloud_subnet_mode: CUSTOM\n</code></pre> <p>Result</p> <p>VPC Network has been created, without auto subnets.</p>"},{"location":"020_Module_2_Assignment_Terraform_Foundation_solution/#5-commit-terraform-configurations-to-repository-and-share-it-with-instructorteacher","title":"5 Commit Terraform configurations to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git add .\ngit commit -m \"TF manifests for Module 2 Assignment\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre> <p>Step 3 This step will grant view access for Instructor to check you assignments</p> <p>In your Cloud Terminal:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID --member='user:ayrat.khayretdinov@gmail.com' --role=roles/viewer\n</code></pre> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID --member='user:valavan@gmail.com' --role=roles/viewer\n</code></pre> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID --member='user:mfsilv@gmail.com' --role=roles/viewer\n</code></pre> <p>Result</p> <p>Your instructor will be able to review you code and grade it.</p>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/","title":"020 Module 2 Lab Terraform Fundamentals","text":"<p>Lab 2 Terraform Fundamentals</p> <p>Objective:</p> <ul> <li>Automating through code the configuration and provisioning of resources</li> </ul>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#1-what-is-terraform","title":"1 What is Terraform?","text":"<p>Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing, popular service providers and custom in-house solutions.</p> <p>Configuration files describe to Terraform the components needed to run a single application or your entire data center. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform can determine what changed and create incremental execution plans that can be applied.</p> <p>The infrastructure Terraform can manage includes both low-level components such as compute instances, storage, and networking, and high-level components such as DNS entries and SaaS features.</p>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#12-key-features","title":"1.2 Key features","text":"<p>Infrastructure as code Infrastructure is described using a high-level configuration syntax. This allows a blueprint of your data center to be versioned and treated as you would any other code. Additionally, infrastructure can be shared and re-used.</p> <p>Execution plans Terraform has a planning step in which it generates an execution plan. The execution plan shows what Terraform will do when you execute the apply command. This lets you avoid any surprises when Terraform manipulates infrastructure.</p> <p>Resource graph Terraform builds a graph of all your resources and parallelizes the creation and modification of any non-dependent resources. Because of this, Terraform builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure.</p> <p>Change automation Complex changesets can be applied to your infrastructure with minimal human interaction. With the previously mentioned execution plan and resource graph, you know exactly what Terraform will change and in what order, which helps you avoid many possible human errors.</p>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-demos","title":"Lab Demos","text":""},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-1-simple-terraform","title":"Lab 1: Simple Terraform","text":"<p>You will be performing the following steps in Cloud Shell. Launch Cloud Shell from Google Cloud Console</p> <p>Step 1 Ensure that you have a GCP Project and Billing enabled on the project. </p> <pre><code>gcloud config set project &lt;PROJECT_ID&gt;\n</code></pre> <p>Step 2  Clone the following gitlab repository to your Cloud Shell, which going to use for our work:</p> <pre><code>git clone https://github.com/Cloud-Architects-Program/ycit020_2022.git\ncd ycit020_2022/tf-samples/terraform-basics-1\n</code></pre> <p>Step 3 First we will explore a simple Terraform configuration. It has a single main.tf file. In this file you will have a section called provider. By stating the provider as \"google\", Terraform knows to invoke Google API using the provider to provision resources  </p> <pre><code>provider \"google\" {\n    project = PROJECT_ID\n    region = \"us-central1\"\n}\n</code></pre> <p>In the next section,  you will see the declaration and configuration of compute_instance resource, which will be used to provision a virtual machine. Now, lets execute the terraform using the commands</p> <pre><code>terraform init \n</code></pre> <p>You can notice how the provider plugin has been initialized</p> <pre><code>Initializing provider plugins...\n- Finding latest version of hashicorp/google...\n- Installing hashicorp/google v4.36.0...\n- Installed hashicorp/google v4.36.0 (signed by HashiCorp)\n</code></pre> <p>Step 4 You can now run the validate command to check if the Terraform file and syntax are valid </p> <pre><code>terraform validate \n</code></pre> <p>You will find the following error</p> <pre><code>\u2502 Error: Invalid reference\n\u2502 \n\u2502   on main.tf line 3, in provider \"google\":\n\u2502    3:     project = PROJECT_ID\n</code></pre> <p>Open the main.tf file and modify the PROJECT_ID with you actual project-id and run <code>terraform validate</code> again</p> <p>Step 5 Next step is to preview the resources that are going to be built as part of the Terraform Configuration </p> <pre><code>terraform plan\n</code></pre> <p>Scroll through the output to see the resource type and values for each (identify few key ones you would like to know the output for eg: network_ip)</p> <pre><code>Plan: 1 to add, 0 to change, 0 to destroy.\n</code></pre> <p>Step 6 Perform a terraform deployment </p> <pre><code>terraform apply\n</code></pre> <p>When terraform tries to deploy your resource, you might run into a few issues</p> <pre><code>If its a Permission issue --&gt; make sure you are authenticated to google cloud. You might have already been prompted with login prompt before \nIf its an API based error --&gt; then you might not have enabled Compute Engine API in your GCP Project \nIf its a invalid configuration error ---&gt; check if you have provided the correct project id \n</code></pre> <p>You can delete the resource created using the destroy command</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-2-terraform-with-multiple-config-files-and-variables","title":"Lab 2: Terraform with multiple config files and variables","text":"<p>In the next terraform we will look at how Terraform configurations can be separated as multiple files, but can be built together. We will also look at how to use <code>variables.tf</code> file and <code>.tfvars</code> files for ease of configuration</p> <p>Step 1 Navigate to the second lab folder </p> <pre><code>cd..\ncd terraform-basic-2\n</code></pre> <p>Analyze the files and the contents of the files.</p> <pre><code>firewall.tf\nmain.tf\nprovider.tf\nterraform.tfvars\nvariables.tf\n</code></pre> <p>Look at the configs on both <code>firewall.tf</code> and <code>provider.tf</code> to under resource definition segregation.All of the variable definition and default values are listed in <code>variables.tf</code> file. The run time values for these variables are provided in the <code>.tfvars</code> file</p> <p>Step 2 Open the main.tf file and modify the PROJECT_ID with you actual project-id and run </p> <pre><code>terraform init \nterraform validate\nterraform plan\n</code></pre> <p>Even though there are terraform files, there is only one planfile. Terraform automatically will create resources in the right order, if output of one is declared as input for other. You can deploy the terraform using </p> <pre><code>terraform apply\n</code></pre> <p>You can delete the resource created using the destroy command</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-3-terraform-deployment-to-multiple-environments-and-the-use-of-terraform-workspaces","title":"Lab 3: Terraform deployment to multiple environments and the use of terraform workspaces","text":"<p>In this lab, we will look at storing the terraform state file in a central storage (GCS backend). We will also look at how to deploy the same terraform code to mulitple environments, similar to how we use environment files in high level programming languages.</p> <p>Step 1 Navigate to the right folder and take a note of the 3 different <code>.tfvars</code> file, one for each environment</p> <pre><code>cd..\ncd terraform-basic-3\nls\n</code></pre> <p>Step 2  You can also notice that we have a new file called <code>backend.tf</code> to store our state files in a central bucket instead of on the local filesystem. This ensure consistency when multiple people are working on the terraform file that the state file is synchronized with changes </p> <p>Create a storage bucket in the project </p> <pre><code>gsutil mb gs://&lt;PROJECT-ID&gt;-tfstate\n</code></pre> <p>Update the <code>backend.tf</code> file with the appropiriate storage bucket name </p> <pre><code>terraform {\n  backend \"gcs\" {\n    bucket = \"PROJECT_ID-tfstate\"\n  }\n}\n</code></pre> <p>Step 3 Next, observe closely the different <code>.tfvars</code> file created for the corresponding environment. Notice the VM name change.  </p> <pre><code>region=\"us-central1\"\nproject=PROJECT_ID\nname=\"flask-server-dev\"\nmachine_type=\"f1-micro\"\nzone=\"us-central1-c\"\n</code></pre> <p>Update the <code>project</code> field to your project_id on the <code>dev.tfvars</code> file. You can deploy terraform config to different environments by passing the respective .tfvars file as below</p> <pre><code>terraform init\nterraform plan -var-file=\"dev.tfvars\"\nterraform apply -var-file=\"dev.tfvars\"\n</code></pre> <p>Modify the <code>project</code> field in <code>uat.tfvars</code> file and proceed with next step</p> <pre><code>terraform plan -var-file=\"uat.tfvars\"\n</code></pre> <p>If you try to deploy similarly to another environment, you will notice that terraform will force a replacement of the current deployment. </p> <p>Because there is only one state file and the state file has a single resource, its trying to replace the resource with new config. We will now use Terraform workspace to get it working without replace You can use </p> <pre><code>terraform workspace list\n</code></pre> <p>to identify the current workspace. By default it uses the default workspace. Lets create worksapce corresponding to each environment. </p> <pre><code>terraform workspace new uat\nterraform init\nterraform plan -var-file=\"uat.tfvars\"\nterraform apply -var-file=\"uat.tfvars\"\n</code></pre> <p>In order to ensure that the state file doesnt get overwritten due to env changes, you need to use separate workspace for each environment. Also naviagte to the GCS bucket and you will be able to see different state file for each environment </p> <pre><code>gsutil ls gs://&lt;PROJECT-ID&gt;-tfstate\n  gs://course-valavan-tfstate/default.tfstate\n  gs://course-valavan-tfstate/dev.tfstate\n  gs://course-valavan-tfstate/prod.tfstate\n  gs://course-valavan-tfstate/uat.tfstate\n</code></pre> <p>You can now perform the terraform destroy on these deployments</p> <pre><code>terraform destroy\nterraform workspace select default\nterraform destroy\n</code></pre> <p>You can also delete the newly created GCS state files. </p>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-4-building-terraform-configuration-using-devops-pipeline-cloudbuild","title":"Lab 4: Building Terraform configuration using DevOps Pipeline (Cloudbuild)","text":"<p>In this lab, we will look to how DevOps team normally deploys the terraform configuration in a pipeline. We will take a closer look at how to build for different environments using the same pipeline and the base terraform code </p> <p>Step 1 Navigate to the right folder and take a note of the 3 different <code>.tfvars</code> file, one for each environment</p> <pre><code>cd..\ncd terraform-basic-4\nls\n</code></pre> <p>Step 2  Lets look at the cloudbuild.yaml pipeline file </p> <pre><code>steps:\n- id: 'tf init'\n  name: 'hashicorp/terraform:1.0.3'\n  entrypoint: 'sh'\n  args: \n  - '-c'\n  - |\n      echo \"Branch '$BRANCH_NAME'\"\n      terraform init\n      terraform workspace select $BRANCH_NAME || terraform workspace new $BRANCH_NAME\n\n# [START tf-plan]\n- id: 'tf plan'\n  name: 'hashicorp/terraform:1.0.3'\n  entrypoint: 'sh'\n  args: \n  - '-c'\n  - | \n      terraform workspace select $BRANCH_NAME || terraform workspace new $BRANCH_NAME\n      terraform plan -var-file=\"$BRANCH_NAME.tfvars\" -lock=false\n# [END tf-plan]\n\n# [START tf-apply]\n- id: 'tf apply'\n  name: 'hashicorp/terraform:1.0.3'\n  entrypoint: 'sh'\n  args: \n  - '-c'\n  - | \n      terraform workspace select $BRANCH_NAME || terraform workspace new $BRANCH_NAME\n      terraform apply -var-file=\"$BRANCH_NAME.tfvars\" -auto-approve -lock=false\n</code></pre> <p>Step 3 Before deploying the pipeline, you need to make sure that in the GCP Project IAM console that the <code>cloudbuild</code> service account needs to have Compute Admin Permissions as it uses the service account to deploy the VM</p> <p>Step 4 Modify the <code>project</code> field in the following files </p> <pre><code>backend.tf\ndev.tfvars\nuat.tfvars\nprod.tfvars\n</code></pre> <p>Step 5 On the last line, you should also take note of the <code>auto-approve</code> flag, which will not wait for the user input before terraform deplyoment. You can trigger the pipeline by specifying the tfvars file to use for the build as Paramter as part of the cloud build job initiation</p> <pre><code> gcloud builds submit --substitutions=BRANCH_NAME=\"dev\"\n gcloud builds submit --substitutions=BRANCH_NAME=\"uat\"\n gcloud builds submit --substitutions=BRANCH_NAME=\"prod\"\n</code></pre> <p>If you need to destroy a deployment you can invoke cloudbuild, by specifying the destroy cloudbuild.yaml</p> <pre><code>gcloud builds submit --substitutions=BRANCH_NAME=\"dev\" --config=cloudbuild-destroy.yaml\n</code></pre>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-5-instructor-demo-only-building-terraform-configuration-using-devops-pipeline-gitlab","title":"Lab 5: [Instructor Demo Only] Building Terraform configuration using DevOps Pipeline (GITLAB)","text":"<p>In the previous pipeline, we werent able to verify the plan output before applying. You will need to build seperate triggers for each and manully deploy the respective pipeline. Gitlab pipelines reduce this complexity to pause between steps in a pipeline and also provides a nice UI to trigger and view the status of deployments. </p> <p>Before deploying the gitlab pipeline, you need to make sure that Gitlab can deploy to your project by using a service account. You will need to create a service account first, download the key and upload it in the Gitlab pipeline variables. Before the gitlab pipeline executes the first step, it will authenticate to the project using the Service account key</p> <p></p> <p>Sample Terraform Gitlab Pipeline Code </p> <pre><code>image:\n  name: hashicorp/terraform:light\n  entrypoint:\n    - '/usr/bin/env'\n    - 'PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\n\nbefore_script:\n  - rm -rf .terraform\n  - terraform --version\n  #- mkdir -p ./creds\n  #- echo $SERVICEACCOUNT | base64 -d &gt; ./creds/serviceaccount.json\n  - export GOOGLE_APPLICATION_CREDENTIALS=${SERVICEACCOUNT}\n  - terraform init\n\nstages:\n  - validate\n  - plan\n  - apply\n  - destroy\n\nvalidate:\n  stage: validate\n  script:\n    - terraform validate\n\nplan:\n  stage: plan\n  script:\n    - terraform plan -out \"planfile\"\n  dependencies:\n    - validate\n  artifacts:\n    paths:\n      - planfile\n\napply:\n  stage: apply\n  script:\n    - terraform apply -input=false \"planfile\"\n  dependencies:\n    - plan\n  when: manual\n\n\ndestroy: \n  stage: destroy\n  script:\n    - terraform destroy -auto-approve\n  when: manual\n</code></pre> <p>Output and manual trigger options, to be shown by the instructor during the class</p>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-6-terrform-deployment-using-modules","title":"Lab 6: Terrform deployment using modules","text":"<p>In this section, we will look at the advantages provided by Terraform Modules. Modules help with abstraction of complexity, encapsulaion of serveral resources into a simple format and also provides re-usability and ease of use. Similar in concepts to OOO Programming and Classes in higher level languages. </p> <p>Step 1Navigate to the right folder and take a note of the 3 different <code>.tfvars</code> file, one for each environment</p> <pre><code>cd..\ncd terraform-sql\ncat main.tf\n</code></pre> <p>Here we are building a private MYSQL instance. There are about 4 resouces that get created - including a private network, reserved IP, Service Peering, SQL instance. We need to build all of the configuration ourselves and know that to build a Private Cloud SQL, you will need all 4 resources. It doesnt include best practice configuration like having a Cloud SQL Proxy, configure default username for logging to the database and so on. </p> <p>Lets use a pre-built module that has all of the configurations ready and we will use this module to pass input paramaters alone and module can take care of the resource deployment in the backend.</p> <pre><code>cd ..\ngit clone https://github.com/terraform-google-modules/terraform-google-sql-db.git\ncd terraform-google-sql-db/examples\nls\n</code></pre> <p>Prebuilt configuration on how to use the specific module for certain configurations can be found.</p> <pre><code>mssql-public\nmysql-backup-create-service-account\nmysql-ha\nmysql-private\nmysql-public\npostgresql-backup-provided-service-account\npostgresql-ha\npostgresql-public-iam\npostgresql-public\n</code></pre> <p>You will need to how to use the module and what the various parameters in the module mean. But you dont have to reasearch terraform website on how to build each of the resource configuration yourself. </p> <p>Apply the configuration using the commands below. Since these are database creations, they generally take about 15-20 mins to complete deployment</p> <pre><code>cd mysql-private\nterraform init\nterraform plan   ## Enter the project id when prompted\nterraform apply  ## Enter the project id when prompted\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE/","title":"020 Module 3 Assignment Production GKE","text":"<p>Objective:</p> <ul> <li>GKE Regional, Private Standard Cluster for Production Deployment</li> <li>GKE Autopilot Cluster </li> </ul>"},{"location":"020_Module_3_Assignment_Production_GKE/#1-creating-production-gke-cluster","title":"1 Creating Production GKE Cluster","text":"<p>In Part 1 of the Assignment we going to deploy <code>GKE</code> Standard cluster for production usage. This cluster will have <code>Regional</code> control plain for high availability. Also following production security Best Practices we going to setup <code>GKE</code> with <code>private</code> nodes (No Public IP) and learn how to setup <code>Cloud Nat</code> which allows Private cluster's Pods access securely Internet.</p>"},{"location":"020_Module_3_Assignment_Production_GKE/#11-locate-assignment-2","title":"1.1 Locate Assignment 2","text":"<p>Step 1  Clone <code>ycit020</code> repo with Kubernetes manifests, which going to use for our work:</p> <pre><code>cd ~/ycit020_2022/\ngit pull       # Pull latest Mod3_assignment\n</code></pre> <p>In case you don't have this folder clone it as following:</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit020_2022\ncd ~/ycit020_2022/module3_assignment/\nls\n</code></pre> <p>Result</p> <p>You can see Kubernetes manifests </p> <p>Step 2 Go into your personal Google Cloud Source Repository:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 3 Copy <code>module3_assignment</code> folder to your repo:</p> <pre><code>cp -r ~/ycit020_2022/module3_assignment ycit020_module3\n</code></pre> <p>Step 4 Create <code>production_gke.txt</code> file that will have instructions how to deploy Production GKE Clusters.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\ncat &gt; production_gke.txt &lt;&lt; EOF\n# Creating Production GKE Cluster\n\n**Step 1** Create a vpc network:\n\n**Step 2** Create a subnet VPC design:\n\n**Step 3** Create a subnet with primary and secondary ranges:\n\n**Step 4** Create a Private GKE Cluster:\n\n**Step 5** Create a Cloud Router:\n\n**Step 6** Create a Cloud Nat Gateway:\n\n**Step 7** Create an Autopilot GKE Cluster:\n\nEOF\n</code></pre> <p>Important</p> <p>This document will be graded.</p> <p>Step 5 Commit <code>ycit020_module3</code> folder using the following Git commands:</p> <pre><code>cd ~/$student_name-notepad\ngit status \ngit add .\ngit commit -m \"adding documentation for ycit020 module 3 assignment\"\n</code></pre> <p>Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE/#12-creating-a-gcp-project","title":"1.2 Creating a GCP project","text":"<p>Note</p> <p>We are going to create a temporary project for this assignment. While you going to store code in existing project that we've used so far in class</p> <p>Set variables:</p> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\nexport PROJECT_PREFIX=1   # Project has to be unique\nexport PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX\n</code></pre> <p>Create a project:</p> <pre><code>gcloud projects create $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX\n</code></pre> <p>List billings and take note of <code>ACCOUNT_ID</code>:</p> <pre><code>ACCOUNT_ID=$(gcloud beta billing accounts list | grep -B2 True  | head -1 | grep ACCOUNT_ID  |awk '{ print $2}') \n</code></pre> <p>Check account ID was set properly as variable:</p> <pre><code>echo $ACCOUNT_ID\n</code></pre> <p>Attach your project to the correct billing account:</p> <pre><code>gcloud beta billing projects link $PROJECT_ID --billing-account=$ACCOUNT_ID\n</code></pre> <p>Set the newly created project as default for gcloud:</p> <pre><code>gcloud config set project $PROJECT_ID\n</code></pre> <p>Enable <code>compute</code>, <code>container</code>, <code>cloudresourcemanager</code> APIs:</p> <pre><code>gcloud services enable container.googleapis.com\ngcloud services enable compute.googleapis.com \ngcloud services enable cloudresourcemanager.googleapis.com\n</code></pre> <p>Note</p> <p>Enabling GCP Service APIs very important step in automation (e.g. terraform)</p> <p>Get a list of services that enabled in your project:</p> <pre><code>gcloud services list\n</code></pre> <p>Note</p> <p>Some services APIs enabled by default during project creation</p> <pre><code>gcloud config set compute/region us-central1\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE/#12-deleting-default-vpc-and-associated-firewall-rules-to-it","title":"1.2 Deleting Default VPC and associated Firewall Rules to it","text":"<p>Observe Asset Inventory in GCP UI:</p> <pre><code>Products -&gt; IAM &amp; Admin -&gt; Asset Inventory -&gt; Overview\n</code></pre> <p>Select resource type <code>compute.Subnetwork</code></p> <p>Result</p> <p>You can see that <code>vpc</code> default network spans across all GCP Regions, which for many companies will not be acceptable practice (e.g. GDPR)</p> <p>List all networks in a project:</p> <pre><code>gcloud compute networks list\n</code></pre> <p>Output:</p> <pre><code>NAME     SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4\ndefault  AUTO         REGIONAL\n</code></pre> <p>Review existing firewall rules for <code>default</code> vpc:</p> <pre><code>gcloud compute firewall-rules list\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>VPC Network-&gt;Firewall\n</code></pre> <p>Delete firewall rules associated with <code>default</code> vpc network:</p> <pre><code>gcloud compute firewall-rules delete default-allow-internal\ngcloud compute firewall-rules delete default-allow-ssh\ngcloud compute firewall-rules delete default-allow-rdp\ngcloud compute firewall-rules delete default-allow-icmp\n</code></pre> <p>Delete the <code>Default</code> network, following best practices:</p> <pre><code>gcloud compute networks delete default\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE/#13-creating-a-custom-mode-network-vpc","title":"1.3 Creating a custom mode network (VPC)","text":"<p>Task N1: Using reference doc: Creating a custom mode network. Create a new custom mode VPC network using gcloud command with following parameters:</p> <ul> <li>Network name: <code>$ORG-$PRODUCT-$ENV-vpc</code></li> <li>Subnet mode: <code>custom</code></li> <li>Bgp routing mode: <code>regional</code></li> <li>MTUs: <code>default</code></li> </ul> <p>Step 1: Create a new custom mode VPC network:</p> <p>with name <code>$ORG-$PRODUCT-$ENV-vpc</code>, with subnet mode <code>custom</code>, and <code>regional</code>.</p> <p>Step 1: Create a new custom mode VPC network:</p> <p>Set variables:</p> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\n</code></pre> <pre><code>TODO: gcloud create network\n</code></pre> <p>Review created network:</p> <pre><code>gcloud compute networks list\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>Networking-&gt;VPC Networks\n</code></pre> <p>Document a command to create <code>vpc</code> network in <code>production_gke.txt</code> doc under <code>step 1</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Important</p> <p>If <code>production_gke.txt</code> is not updated with commands this will reduce score for you assignment</p> <p>Step 2: Create firewall rules  <code>default-allow-ssh</code>:</p> <pre><code>gcloud compute firewall-rules create $ORG-$PRODUCT-$ENV-allow-tcp-ssh-icmp --network $ORG-$PRODUCT-$ENV-vpc --allow tcp:22,tcp:3389,icmp\n</code></pre> <p>Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/firewall-rules</p> <p>Review created firewall rules:</p> <pre><code>gcloud compute firewall-rules list\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>Networking-&gt;Firewalls\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE/#14-design-and-create-a-user-managed-subnet","title":"1.4 Design and create a <code>user-managed</code> subnet","text":"<p>After you've created VPC network, it is require to add subnet to it.</p> <p>Task N2: In order to ensure that GKE clusters in you organization doesn't overlap each other, design VPC subnet ranges for <code>dev</code>, <code>stg</code>, <code>prd</code> VPC-native GKE clusters, where</p> <ul> <li> <p>2 GKE Clusters per Project</p> </li> <li> <p>Max 200 <code>Nodes</code> per cluster and primary range belongs to Class A and starting from (10.130.0.0/x) </p> </li> <li> <p>Max 110 <code>Pods</code> per node and pod secondary ranges  belongs to Class A and starting from (10.0.0.0/x) </p> </li> <li> <p>Max 500 <code>Services</code> per cluster and service secondary ranges  belongs to Class A and starting from (10.100.0.0/x)</p> </li> <li> <p>CIDR range for master-ipv4-cidr (k8s api range) belongs to Class B (172.16.0.0/x)</p> </li> </ul> <p>Use following reference docs:</p> <ol> <li>VPC-native clusters</li> <li>GKE address management </li> </ol> <p>Using tables from VPC-native clusters document and online subnet calculator, create a table for <code>dev</code>, <code>stg</code>, <code>prd</code> in the following format and store result under <code>production_gke.txt</code>:</p> <pre><code>Project   | Subnet Name |     subnet          |    pod range     |    srv range   | kubectl api range\napp 1 Dev | dev-1       |     (10.130.0.0/x)  |     IP range     |     IP range   |     IP range    \n          | dev-2       |     (10.130.0.0/x)  |     IP range     |     IP range   |     IP range      \n</code></pre> <p>Document a subnet VPC design in <code>production_gke.txt</code> doc under <code>step 2</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Task N3: Create a <code>user-managed</code> subnet.</p> <p>Create a subnet for <code>dev</code> cluster, taking in consideration VPC subnet ranges created in above table, where:</p> <p>Subnet N1 for <code>GKE Standard Cluster</code>:</p> <ul> <li> <p>Subnet name: <code>gke-standard-$ORG-$PRODUCT-$ENV-subnet</code></p> </li> <li> <p>Region: <code>us-central1</code></p> </li> <li> <p>Node Range: See column <code>subnet</code> in above table for <code>dev-1</code> cluster</p> </li> <li> <p>Secondary Ranges:</p> <ul> <li> <p>Service range name: <code>gke-standard-services</code></p> </li> <li> <p>Service range CIDR: See column <code>srv range</code> in above table for <code>dev-1</code> cluster</p> </li> <li> <p>Pods range name: <code>gke-standard-pods</code></p> </li> <li> <p>Pods range CIDR: See column <code>pod range</code> in above table for <code>dev-1</code> cluster</p> </li> </ul> </li> <li> <p>Features:</p> <ul> <li>Flow Logs</li> <li>Private IP Google Access</li> </ul> </li> </ul> <p>Subnet N2 for <code>GKE Autopilot</code>:</p> <ul> <li>Subnet name: <code>gke-auto-$ORG-$PRODUCT-$ENV-subnet</code></li> <li>Region: <code>us-central1</code></li> <li>Node Range: See column <code>subnet</code> in above table for <code>dev-2</code> cluster</li> <li>Secondary Ranges:<ul> <li>Service range name: <code>gke-auto-services</code></li> <li>Service range CIDR: See column <code>srv range</code> in above table for <code>dev-2</code> cluster</li> <li>Pods range name: <code>gke-auto-pods</code></li> <li>Pods range CIDR: See column <code>pod range</code> in above table for <code>dev-2</code> cluster</li> </ul> </li> <li>Features:<ul> <li>Flow Logs</li> <li>Private IP Google Access</li> </ul> </li> </ul> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\n</code></pre> <pre><code>TODO: gcloud compute networks subnets create gke-standard-$ORG-$PRODUCT-$ENV-subnet\n\nTODO: gcloud compute networks subnets create gke-auto-$ORG-$PRODUCT-$ENV-subnet\n</code></pre> <p>Reference docs:</p> <ol> <li>Creating a private cluster with custom subnet</li> <li>VPC-native clusters</li> <li>Use <code>gcloud subnets create</code> command reference for all available options.</li> </ol> <p>Review created subnet:</p> <pre><code>gcloud compute networks subnets list\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>Networking-&gt;VPC Networks -&gt; Click VPC network and check `Subnet` tab\n</code></pre> <p>Document a subnet VPC creation command in <code>production_gke.txt</code> doc under <code>step 3</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p>"},{"location":"020_Module_3_Assignment_Production_GKE/#15-creating-a-private-regional-and-vpc-native-gke-cluster","title":"1.5 Creating a Private, Regional and VPC Native GKE Cluster","text":"<p>Task N4: Create GKE Standard Cluster with  Private Nodes and following values:</p> <ul> <li>Cluster name: <code>$ORG-$PRODUCT-$ENV-standard-cluster</code></li> <li>VPC <code>network</code> name: <code>$ORG-$PRODUCT-$ENV-vpc</code></li> <li>VPC <code>subnet</code> name: <code>gke-standard-$ORG-$PRODUCT-$ENV-subnet</code></li> <li>Secondary <code>pod</code> range with name: <code>gke-standard-pods</code></li> <li>Secondary <code>service</code> range with name: <code>gke-standard-services</code></li> <li>VM size: <code>e2-micro</code></li> <li>Node count 1 per zone: <code>num-nodes</code></li> <li>GKE Control plane is replicated across three zones of a region: <code>us-central1</code></li> <li>GKE Release channel: <code>regular</code></li> <li>Enable Cilium based Networking DataplaneV2: <code>enable-dataplane-v2</code></li> <li>GKE version: \"1.22.12-gke.300\"</li> <li>Cluster Node Communication <code>VPC Native</code>: <code>enable-ip-alias</code></li> <li>Cluster Nodes access: Private Node GKE Cluster with Public API endpoint</li> <li>For <code>master-ipv4-cidr</code> ranges: See column <code>kubectl api range</code> in above table for <code>dev-1</code> cluster</li> </ul> <p>Use following reference docs:</p> <ol> <li>Creating a private cluster</li> <li>GKE Release Channels</li> <li>Use <code>gcloud container clusters create</code> command reference for all available options.</li> </ol> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\n</code></pre> <pre><code>#TODO gcloud clusters create $ORG-$PRODUCT-$ENV-cluster\n</code></pre> <p>Document a GKE cluster creation command in <code>production_gke.txt</code> doc under <code>step 4</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Result</p> <p>The private cluster is now created. <code>gcloud</code> has set up <code>kubectl</code> to authenticate with the private cluster but the cluster's K8s API server will only accept connections from the primary range of your subnetwork, and the secondary range of your subnetwork that is used for pods. That means nobody can access K8s API at this point of time, until we specify allowed ranges.</p> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials $ORG-$PRODUCT-$ENV-standard-cluster --region us-central1 \n</code></pre> <p>Step 4: Connecting to a Private Cluster</p> <p>Let's try to connect to the cluster:</p> <pre><code>kubectl get pods\n</code></pre> <pre><code>Unable to connect to the server i/o timeout\n</code></pre> <p>Fail</p> <p>This fails because private clusters firewall traffic to the master by default. In order to connect to the cluster you need to make use of the master authorized networks feature. </p> <p>Step 4: Enable Master Authorized networks on you cluster</p> <p>Suppose you have a group of machines, outside of your VPC network, belongs to your organization. You could authorize ONLY those machines to access the public endpoint (Kubernetes API).</p> <p>Here we will enable master authorized networks and whitelist the IP address for our Cloud Shell instance, to allow access to the master:</p> <pre><code>gcloud container clusters update $ORG-$PRODUCT-$ENV-standard-cluster --enable-master-authorized-networks --master-authorized-networks $(curl ipinfo.io/ip)/32 --region us-central1\n</code></pre> <p>Now we can access the API server using <code>kubectl</code> from GCP console:</p> <p>Note</p> <p>In real life it could be CIDR Range for you company, so only engineers or CI/CD systems from you company can connect to <code>kubectl</code> apis in secure manner.</p> <p>Now we can access the API server using kubectl:</p> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>No resources found in default namespace.\n</code></pre> <p>Let's deploy basic application on the GKE Standard Cluster:</p> <pre><code>kubectl run hello-web --image=gcr.io/google-samples/hello-app:1.0 --port=8080\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME        READY   STATUS    RESTARTS   AGE\nhello-web   1/1     Running   0          7s\n</code></pre> <p>Result</p> <p>We can deploy Pods to our Private Cluster.</p> <pre><code>kubectl delete pod hello-web\n</code></pre> <p>Step 2: Testing Outbound Traffic</p> <p>Outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads.</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: wget\nspec:\n  containers:\n  - name: wget\n    image: alpine\n    command: ['wget', '-T', '5', 'http://www.example.com/']\n  restartPolicy: Never\nEOF\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME   READY   STATUS   RESTARTS   AGE\nwget   0/1     Error    0          4m41s\n</code></pre> <pre><code>kubectl logs wget\n</code></pre> <p>Output:</p> <pre><code>Connecting to www.example.com (93.184.216.34:80)\nwget: download timed out\nwget: bad address 'www.example.com'\n</code></pre> <p>Results</p> <p>Pods can't access internet, because it's a private cluster</p> <p>Note</p> <p>Normally, if cluster is Private and doesn't have Cloud Nat configured, users can't even deploy images from Docker Hub. See troubleshooting details here</p> <p>Delete test pod:</p> <pre><code>kubectl delete pods wget\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE/#16-create-a-google-cloud-nat","title":"1.6 Create a Google Cloud Nat","text":"<p>Private clusters give you the ability to isolate nodes from having inbound and outbound connectivity to the public internet. This isolation is achieved as the nodes have internal IP addresses only.</p> <p>If you want to provide outbound internet access for certain private nodes, you can use Cloud NAT</p> <p>Cloud NAT is a distributed, software-defined managed service. It's not based on proxy VMs or appliances.</p> <p>You configure a NAT gateway on a Cloud Router, which provides the control plane for NAT, holding configuration parameters that you specify. Google Cloud runs and maintains processes on the physical machines that run your Google Cloud VMs.</p> <p>Cloud NAT can be configured to automatically scale the number of NAT IP addresses that it uses, and it supports VMs that belong to managed instance groups, including those with autoscaling enabled.</p> <p>Step 1: Create a Cloud NAT configuration using Cloud Router</p> <p>Create Cloud Router in the same region as the instances that use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway.</p> <p>Task N5: Create <code>Cloud Router</code></p> <p>Step 1a: Create a <code>Cloud Router</code>:</p> <pre><code>gcloud compute routers create gke-nat-router \\\n    --network $ORG-$PRODUCT-$ENV-vpc \\\n    --region us-central1\n</code></pre> <p>Verify created Cloud Router:</p> <pre><code>Networking -&gt; Hybrid Connectivity -&gt; Cloud Routers\n</code></pre> <p>Document <code>Cloud Router</code> creation  command in <code>production_gke.txt</code> doc under <code>step 5</code></p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Task N6: Create <code>Cloud Nat</code> Gateway</p> <p>Step 1b: Create a <code>Cloud Nat</code> Gateway:</p> <pre><code>gcloud compute routers nats create nat-config \\\n    --router-region us-central1 \\\n    --router gke-nat-router \\\n    --nat-all-subnet-ip-ranges \\\n    --auto-allocate-nat-external-ips\n</code></pre> <p>Verify created <code>Cloud Nat</code>:</p> <pre><code>Networking -&gt; Network Services -&gt; Cloud NAT\n</code></pre> <p>Note</p> <p>Cloud NAT uses Cloud Router only to group NAT configuration information (control plane). Cloud NAT does not direct a Cloud Router to use BGP or to add routes. NAT traffic does not pass through a Cloud Router (data plane).</p> <p>Document Cloud Nat creation  command in <code>production_gke.txt</code> doc under <code>step 6</code>:</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Step 2: Testing Outbound Traffic</p> <p>Most outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads.</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: wget\nspec:\n  containers:\n  - name: wget\n    image: alpine\n    command: ['wget', '-T', '5', 'http://www.example.com/']\n  restartPolicy: Never\nEOF\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME                              READY   STATUS      RESTARTS   AGE\nwget                              0/1     Completed   0          2m53s\n</code></pre> <pre><code>kubectl logs wget\n</code></pre> <p>Output:</p> <pre><code>Connecting to www.example.com (93.184.216.34:80)\nsaving to 'index.html'\nindex.html           100% |********************************|  1256  0:00:00 ETA\n'index.html' saved\n</code></pre> <p>Results</p> <p>Pods can access internet, thanks to our configured Cloud Nat.</p>"},{"location":"020_Module_3_Assignment_Production_GKE/#17-delete-default-node-pool-and-create-custom-node-pool","title":"1.7 Delete Default Node Pool and Create Custom Node Pool","text":"<p>A Kubernetes Engine cluster consists of a master and nodes. Kubernetes doesn't handle provisioning of nodes, so Google Kubernetes Engine handles this for you with a concept called node pools.</p> <p>A node pool is a subset of node instances within a cluster that all have the same configuration. They map to instance templates in Google Compute Engine, which provides the VMs used by the cluster. By default a Kubernetes Engine cluster has a single node pool, but you can add or remove them as you wish to change the shape of your cluster.</p> <p>In the previous example, you've created a Kubernetes Engine cluster. This gave us three nodes (three e2-micro (2 vCPUs, 1 GB memory), 100 GB of disk each) in a single node pool (called default-pool). Let's inspect the node pool:</p> <pre><code>gcloud config set compute/region us-central1\ngcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Output:</p> <pre><code>NAME: default-pool\nMACHINE_TYPE: e2-micro\nDISK_SIZE_GB: 100\nNODE_VERSION: 1.22.12-gke.300\n</code></pre> <p>If you want to add more nodes of this type, you can grow this node pool. If you want to add more nodes of a different type, you can add other node pools.</p> <p>A common method of moving a cluster to larger nodes is to add a new node pool, move the work from the old nodes to the new, and delete the old node pool.</p> <p>Let's add a second node pool. This time we will use the larger <code>e2-medium</code>  (2 vCPUs, 4 GB memory), 100 GB of disk each machine type.</p> <p>Task N7: Create Create Custom Node Pool and Delete Default Node Pool </p> <pre><code>gcloud container node-pools create custom-pool --cluster $ORG-$PRODUCT-$ENV-standard-cluster \\\n    --machine-type e2-medium --num-nodes 1\n</code></pre> <p>Output:</p> <pre><code>NAME: custom-pool\nMACHINE_TYPE: e2-medium\nDISK_SIZE_GB: 100\nNODE_VERSION: 1.22.12-gke.300\n</code></pre> <pre><code>gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Output:</p> <pre><code>NAME: default-pool\nMACHINE_TYPE: e2-micro\nDISK_SIZE_GB: 100\nNODE_VERSION: 1.22.12-gke.300\n\nNAME: custom-pool\nMACHINE_TYPE: e2-medium\nDISK_SIZE_GB: 100\nNODE_VERSION: 1.22.12-gke.300\n</code></pre> <p>You can now delete the original <code>default-pool</code> node pool:</p> <pre><code>gcloud container node-pools delete default-pool --cluster $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Check that node-pool has been deleted:</p> <pre><code>gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Result</p> <p>GKE cluster running only on <code>custom-pool</code> Node pool.</p>"},{"location":"020_Module_3_Assignment_Production_GKE/#2-creating-gke-autopilot-clusters","title":"2 Creating GKE Autopilot Clusters","text":"<p>In Part 2 of the Assignment we going to deploy GKE Autopilot Clusters for Production Usage. Autopilot Clusters provides easy way to configure and manage GKE Clusters. They configured with best practices and  high availability out of the box.  And can additionally configured with Private Clusters. </p> <p>Task N8: Create GKE Autopilot Cluster with following subnet ranges:</p> <ul> <li>Cluster name: <code>$ORG-$PRODUCT-$ENV-auto-cluster</code></li> <li>In <code>region</code>: <code>us-central1</code></li> <li>VPC <code>network</code> name: <code>$ORG-$PRODUCT-$ENV-vpc</code></li> <li>VPC <code>subnet</code> name: <code>gke-auto-$ORG-$PRODUCT-$ENV-subnet</code></li> <li>Secondary <code>pod</code> range with name: <code>gke-auto-pods</code></li> <li>Secondary <code>service</code> range with name: <code>gke-auto-services</code></li> </ul> <p>Use following reference docs:</p> <ol> <li>Creating a GKE Autopilot cluster</li> </ol> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\n</code></pre> <pre><code>#TODO gcloud container clusters create-auto $ORG-$PRODUCT-$ENV-auto-cluster\n</code></pre> <p>Result</p> <p>Production ready GKE Autopilot Cluster has been created</p> <p>Document a GKE Autopilot cluster creation command in <code>production_gke.txt</code> doc under <code>step 8</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Authenticate to GKE Autopilot cluster:</p> <pre><code>gcloud container clusters get-credentials $ORG-$PRODUCT-$ENV-auto-cluster --region us-central1 \n</code></pre> <p>GKE Autopilot starts with 2 small Nodes, that is not charged to Customers:</p> <pre><code>kubectl get nodes\n</code></pre> <pre><code>gk3-archy-notepad-gcloud-default-pool-5c17f5f3-lp4p   Ready    &lt;none&gt;   11m   v1.22.12-gke.300\ngk3-archy-notepad-gcloud-default-pool-fae5625e-cwxx   Ready    &lt;none&gt;   11m   v1.22.12-gke.300\n</code></pre> <p>This Nodes are used to run GKE system containers such as various agents and networking pods:</p> <pre><code>kubectl get pods -o wide --all-namespaces\n</code></pre> <p>Note</p> <p>With GKE Autopilot, you don't need to worry about nodes or nodepools at all. Autopilot will spin nodes up and down based on the resources needed by your deployments, but you will only be charged for the resources requested by your actual deployments.</p> <p>Let's deploy basic application on the GKE Standard Cluster:</p> <pre><code>kubectl run hello-web --image=gcr.io/google-samples/hello-app:1.0 --port=8080\n</code></pre> <p>Let's verify that Pod deployed:</p> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME        READY   STATUS    RESTARTS   AGE\nhello-web   0/1     Pending   0          9s\n</code></pre> <p>After about 1 munute you should see:</p> <pre><code>NAME        READY   STATUS              RESTARTS   AGE\nhello-web   0/1     ContainerCreating   0          101s\n</code></pre> <p>Note</p> <p>That the application takes longer than usual to start. This is because Autopilot NAP system starting off new Nodes for application.</p> <p>Let's verify that Pod deployment after few minutes:</p> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME        READY   STATUS    RESTARTS   AGE\nhello-web   1/1     Running   0          7s\n</code></pre> <p>Let's verify that Nodes deployed:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Output:</p> <pre><code>NAME                                                  STATUS   ROLES    AGE     VERSION\ngk3-archy-notepad-gcloud-default-pool-5c17f5f3-lp4p   Ready    &lt;none&gt;   24m     v1.22.12-gke.300\ngk3-archy-notepad-gcloud-default-pool-5c17f5f3-t89j   Ready    &lt;none&gt;   5m35s   v1.22.12-gke.300\ngk3-archy-notepad-gcloud-default-pool-fae5625e-cwxx   Ready    &lt;none&gt;   24m     v1.22.12-gke.300\n</code></pre> <p>Result</p> <p>You will see 3d node has been created. GKE Autopilot created a Node based on workload requirement</p> <p>Note</p> <p>If you don't specify resource requests in your deployment spec, then Autopilot will set <code>CPU</code> to 2000m and <code>Memory</code> to 4gb. If your app requires less resources than that, make sure you set the resource request for your deployment. The minimum CPU is 250m and the minimum memory is 512mb.</p>"},{"location":"020_Module_3_Assignment_Production_GKE/#3-commit-terraform-configurations-to-repository-and-share-it-with-instructorteacher","title":"3 Commit Terraform configurations to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>ycit020_module3</code> folder using the following Git commands:</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git add .\ngit commit -m \"Gcloud Documentation for Module 3 Assignment\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre> <p>Result</p> <p>Your instructor will be able to review your code and grade it.</p>"},{"location":"020_Module_3_Assignment_Production_GKE/#4-cleanup","title":"4 Cleanup","text":"<pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\nexport PROJECT_PREFIX=1   # Project has to be unique\nexport PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX\n</code></pre> <p>Delete GKE standard Cluster:</p> <pre><code>gcloud container clusters delete $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Delete GKE Autopilot Cluster:</p> <pre><code>gcloud container clusters delete $ORG-$PRODUCT-$ENV-auto-cluster\n</code></pre> <p>Important</p> <p>We are going to delete the <code>$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX</code> only, please do not delete your project with Source Code Repo.</p> <pre><code>gcloud projects delete $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE_solution/","title":"020 Module 3 Assignment Production GKE solution","text":"<p>Objective:</p> <ul> <li>GKE Regional, Private Standard Cluster for Production Deployment</li> <li>GKE Autopilot Cluster </li> </ul>"},{"location":"020_Module_3_Assignment_Production_GKE_solution/#1-creating-production-gke-cluster","title":"1 Creating Production GKE Cluster","text":"<p>In Part 1 of the Assignment we going to deploy <code>GKE</code> Standard cluster for production usage. This cluster will have <code>Regional</code> control plain for high availability. Also following production security Best Practices we going to setup <code>GKE</code> with <code>private</code> nodes (No Public IP) and learn how to setup <code>Cloud Nat</code> which allows Private cluster's Pods access securely Internet.</p>"},{"location":"020_Module_3_Assignment_Production_GKE_solution/#11-locate-assignment-2","title":"1.1 Locate Assignment 2","text":"<p>Step 1  Clone <code>ycit020</code> repo with Kubernetes manifests, which going to use for our work:</p> <pre><code>cd ~/ycit020_2022/\ngit pull       # Pull latest Mod3_assignment\n</code></pre> <p>In case you don't have this folder clone it as following:</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit020_2022\ncd ~/ycit020_2022/module3_assignment/\nls\n</code></pre> <p>Result</p> <p>You can see Kubernetes manifests </p> <p>Step 2 Go into your personal Google Cloud Source Repository:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 3 Copy <code>module3_assignment</code> folder to your repo:</p> <pre><code>cp -r ~/ycit020_2022/module3_assignment ycit020_module3\n</code></pre> <p>Step 4 Create <code>production_gke.txt</code> file that will have instructions how to deploy Production GKE Clusters.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\ncat &gt; production_gke.txt &lt;&lt; EOF\n# Creating Production GKE Cluster\n\n**Step 1** Create a vpc network:\n\n**Step 2** Create a subnet VPC design:\n\n**Step 3** Create a subnet with primary and secondary ranges:\n\n**Step 4** Create a Private GKE Cluster:\n\n**Step 5** Create a Cloud Router:\n\n**Step 6** Create a Cloud Nat Gateway:\n\n**Step 7** Create an Autopilot GKE Cluster:\n\nEOF\n</code></pre> <p>Important</p> <p>This document will be graded.</p> <p>Step 5 Commit <code>ycit020_module3</code> folder using the following Git commands:</p> <pre><code>cd ~/$student_name-notepad\ngit status \ngit add .\ngit commit -m \"adding documentation for ycit020 module 3 assignment\"\n</code></pre> <p>Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE_solution/#12-creating-a-gcp-project","title":"1.2 Creating a GCP project","text":"<p>Note</p> <p>We are going to create a temporary project for this assignment. While you going to store code in existing project that we've used so far in class</p> <p>Set variables:</p> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\nexport PROJECT_PREFIX=1   # Project has to be unique\nexport PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX\n</code></pre> <p>Create a project:</p> <pre><code>gcloud projects create $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX\n</code></pre> <p>List billings and take note of <code>ACCOUNT_ID</code>:</p> <pre><code>ACCOUNT_ID=$(gcloud beta billing accounts list | grep -B2 True  | head -1 | grep ACCOUNT_ID  |awk '{ print $2}') \n</code></pre> <p>Check account ID was set properly as variable:</p> <pre><code>echo $ACCOUNT_ID\n</code></pre> <p>Attach your project to the correct billing account:</p> <pre><code>gcloud beta billing projects link $PROJECT_ID --billing-account=$ACCOUNT_ID\n</code></pre> <p>Set the newly created project as default for gcloud:</p> <pre><code>gcloud config set project $PROJECT_ID\n</code></pre> <p>Enable <code>compute</code>, <code>container</code>, <code>cloudresourcemanager</code> APIs:</p> <pre><code>gcloud services enable container.googleapis.com\ngcloud services enable compute.googleapis.com \ngcloud services enable cloudresourcemanager.googleapis.com\n</code></pre> <p>Note</p> <p>Enabling GCP Service APIs very important step in automation (e.g. terraform)</p> <p>Get a list of services that enabled in your project:</p> <pre><code>gcloud services list\n</code></pre> <p>Note</p> <p>Some services APIs enabled by default during project creation</p> <pre><code>gcloud config set compute/region us-central1\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE_solution/#12-deleting-default-vpc-and-associated-firewall-rules-to-it","title":"1.2 Deleting Default VPC and associated Firewall Rules to it","text":"<p>Observe Asset Inventory in GCP UI:</p> <pre><code>Products -&gt; IAM &amp; Admin -&gt; Asset Inventory -&gt; Overview\n</code></pre> <p>Select resource type <code>compute.Subnetwork</code></p> <p>Result</p> <p>You can see that <code>vpc</code> default network spans across all GCP Regions, which for many companies will not be acceptable practice (e.g. GDPR)</p> <p>List all networks in a project:</p> <pre><code>gcloud compute networks list\n</code></pre> <p>Output:</p> <pre><code>NAME     SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4\ndefault  AUTO         REGIONAL\n</code></pre> <p>Review existing firewall rules for <code>default</code> vpc:</p> <pre><code>gcloud compute firewall-rules list\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>VPC Network-&gt;Firewall\n</code></pre> <p>Delete firewall rules associated with <code>default</code> vpc network:</p> <pre><code>gcloud compute firewall-rules delete default-allow-internal\ngcloud compute firewall-rules delete default-allow-ssh\ngcloud compute firewall-rules delete default-allow-rdp\ngcloud compute firewall-rules delete default-allow-icmp\n</code></pre> <p>Delete the <code>Default</code> network, following best practices:</p> <pre><code>gcloud compute networks delete default\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE_solution/#13-creating-a-custom-mode-network-vpc","title":"1.3 Creating a custom mode network (VPC)","text":"<p>Task N1: Using reference doc: Creating a custom mode network. Create a new custom mode VPC network using gcloud command with following parameters:</p> <ul> <li>Network name: <code>$ORG-$PRODUCT-$ENV-vpc</code></li> <li>Subnet mode: <code>custom</code></li> <li>Bgp routing mode: <code>regional</code></li> <li>MTUs: <code>default</code></li> </ul> <p>Step 1: Create a new custom mode VPC network:</p> <p>with name <code>ORG-$PRODUCT-$ENV-vpc</code>, with subnet mode <code>custom</code>, and <code>regional</code>.</p> <p>Step 1: Create a new custom mode VPC network:</p> <p>Set variables:</p> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\n</code></pre> <pre><code>gcloud compute networks create $ORG-$PRODUCT-$ENV-vpc \\\n    --subnet-mode=custom \\\n    --bgp-routing-mode=regional \\\n    --mtu=1460\n</code></pre> <p>Review created network:</p> <pre><code>gcloud compute networks list\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>Networking-&gt;VPC Networks\n</code></pre> <p>Document a command to create <code>vpc</code> network in <code>production_gke.txt</code> doc under <code>step 1</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Important</p> <p>If <code>production_gke.txt</code> is not updated with commands this will reduce score for you assignment</p> <p>Step 2: Create firewall rules <code>default-allow-internal</code> and <code>default-allow-ssh</code>:</p> <pre><code>gcloud compute firewall-rules create $ORG-$PRODUCT-$ENV-allow-tcp-ssh-icmp --network $ORG-$PRODUCT-$ENV-vpc --allow tcp:22,tcp:3389,icmp\n</code></pre> <p>Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/firewall-rules</p> <p>Review created firewall rules:</p> <pre><code>gcloud compute firewall-rules list\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>Networking-&gt;Firewalls\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE_solution/#14-design-and-create-a-user-managed-subnet","title":"1.4 Design and create a <code>user-managed</code> subnet","text":"<p>After you've created VPC network, it is require to add subnet to it.</p> <p>Task N2: In order to ensure that GKE clusters in you organization doesn't overlap each other, design VPC subnet ranges for <code>dev</code>, <code>stg</code>, <code>prd</code> VPC-native GKE clusters, where</p> <ul> <li> <p>Max 200 <code>Nodes</code> per cluster and primary range belongs to Class A and starting from (10.130.0.0/x) </p> </li> <li> <p>Max 110 <code>Pods</code> per node and pod secondary ranges  belongs to Class A and starting from (10.0.0.0/x) </p> </li> <li> <p>Max 500 <code>Services</code> per cluster and service secondary ranges  belongs to Class A and starting from (10.100.0.0/x)</p> </li> </ul> <p>In your design assume that each cluster will have maximum of 59 Nodes with max 110 Pods per node and 1700 Service per cluster.</p> <p>Use following reference docs:</p> <ol> <li>VPC-native clusters</li> <li>GKE address management </li> </ol> <p>Using tables from VPC-native clusters document and online subnet calculator, create a table for <code>dev</code>, <code>stg</code>, <code>prd</code> in the following format and store result under <code>production_gke.txt</code>:</p> <pre><code>Project   | Subnet Name |     subnet       |    pod range       |    srv range        | kubectl api range\napp 1 Dev | dev-1       |   10.130.0.0/24  |     10.0.0.0/16    |     10.100.0.0/23   |     172.16.0.0/28\n          | dev-2       |   10.131.0.0/24  |     10.1.0.0/16    |     10.100.2.0/23   |     172.16.0.16/28\n</code></pre> <p>Document a subnet VPC design in <code>production_gke.txt</code> doc under <code>step 2</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Task N3: Create a <code>user-managed</code> subnet.</p> <p>Create a subnet for <code>dev</code> cluster, taking in consideration VPC subnet ranges created in above table, where:</p> <p>Subnet N1 for <code>GKE Standard Cluster</code>:   * Subnet name: <code>gke-standard-$ORG-$PRODUCT-$ENV-subnet</code>   * Region: <code>us-central1</code>   * Node Range: See column <code>subnet</code> in above table for <code>dev-1</code> cluster   * Secondary Ranges:     * Service range name: <code>gke-standard-services</code>     * Service range CIDR: See column <code>srv range</code> in above table for <code>dev-1</code> cluster     * Pods range name: <code>gke-standard-pods</code>     * Pods range CIDR: See column <code>pod range</code> in above table for <code>dev-1</code> cluster   * Features:     * Flow Logs     * Private IP Google Access</p> <p>Subnet N2 for <code>GKE Standard Autopilot</code>:   * Subnet name: <code>gke-auto-$ORG-$PRODUCT-$ENV-subnet</code>   * Region: <code>us-central1</code>   * Node Range: See column <code>subnet</code> in above table for <code>dev-2</code> cluster   * Secondary Ranges:     * Service range name: <code>gke-auto-services</code>     * Service range CIDR: See column <code>srv range</code> in above table for <code>dev-2</code> cluster     * Pods range name: <code>gke-auto-pods</code>     * Pods range CIDR: See column <code>pod range</code> in above table for <code>dev-2</code> cluster   * Features:     * Flow Logs     * Private IP Google Access</p> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\n</code></pre> <pre><code>gcloud compute networks subnets create gke-standard-$ORG-$PRODUCT-$ENV-subnet \\\n--network $ORG-$PRODUCT-$ENV-vpc \\\n--range 10.130.0.0/24 \\\n--region us-central1 --enable-flow-logs \\\n--enable-private-ip-google-access \\\n--secondary-range gke-standard-services=10.100.0.0/23,gke-standard-pods=10.0.0.0/16\n</code></pre> <pre><code>gcloud compute networks subnets create gke-auto-$ORG-$PRODUCT-$ENV-subnet \\\n--network $ORG-$PRODUCT-$ENV-vpc \\\n--range 10.131.0.0/24 \\\n--region us-central1 --enable-flow-logs \\\n--enable-private-ip-google-access \\\n--secondary-range gke-auto-services=10.100.2.0/23,gke-auto-pods=10.1.0.0/16\n</code></pre> <p>Reference docs:</p> <ol> <li>Creating a private cluster with custom subnet</li> <li>VPC-native clusters</li> <li>Use <code>gcloud subnets create</code> command reference for all available options.</li> </ol> <p>Review created subnet:</p> <pre><code>gcloud compute networks subnets list\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>Networking-&gt;VPC Networks -&gt; Click VPC network and check `Subnet` tab\n</code></pre> <p>Document a subnet VPC creation command in <code>production_gke.txt</code> doc under <code>step 3</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p>"},{"location":"020_Module_3_Assignment_Production_GKE_solution/#15-creating-a-private-regional-and-vpc-native-gke-cluster","title":"1.5 Creating a Private, Regional and VPC Native GKE Cluster","text":"<p>Task N4: Create GKE Standard Cluster with  Private Nodes and following values:</p> <ul> <li>Cluster name: <code>$ORG-$PRODUCT-$ENV-standard-cluster</code></li> <li>VPC <code>network</code> name: <code>$ORG-$PRODUCT-$ENV-vpc</code></li> <li>VPC <code>subnet</code> name: <code>gke-standard-$ORG-$PRODUCT-$ENV-subnet</code></li> <li>Secondary <code>pod</code> range with name: <code>gke-standard-pods</code></li> <li>Secondary <code>service</code> range with name: <code>gke-standard-services</code></li> <li>VM size: <code>e2-micro</code></li> <li>Node count 1 per zone: <code>num-nodes</code></li> <li>GKE Control plane is replicated across three zones of a region: <code>us-central1</code></li> <li>GKE Release channel: <code>regular</code></li> <li>Enable Cilium based Networking DataplaneV2: <code>enable-dataplane-v2</code></li> <li>GKE version: \"1.22.12-gke.300\"</li> <li>Cluster Node Communication <code>VPC Native</code>: <code>enable-ip-alias</code></li> <li>Cluster Nodes access: Private Node GKE Cluster with Public API endpoint</li> <li>For <code>master-ipv4-cidr</code> ranges: See column <code>kubectl api range</code> in above table for <code>dev-1</code> cluster</li> </ul> <p>Use following reference docs:</p> <ol> <li>Creating a private cluster</li> <li>GKE Release Channels</li> <li>Use <code>gcloud container clusters create</code> command reference for all available options.</li> </ol> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\n</code></pre> <pre><code>gcloud container clusters create  $ORG-$PRODUCT-$ENV-standard-cluster \\\n    --region us-central1 \\\n    --num-nodes 1 \\\n    --machine-type \"e2-micro\" \\\n    --cluster-version \"1.22.12-gke.300\" \\\n    --release-channel \"regular\" \\\n    --enable-dataplane-v2 \\\n    --network $ORG-$PRODUCT-$ENV-vpc \\\n    --subnetwork gke-standard-$ORG-$PRODUCT-$ENV-subnet \\\n    --cluster-secondary-range-name gke-standard-pods \\\n    --services-secondary-range-name gke-standard-services \\\n    --enable-ip-alias \\\n    --enable-private-nodes \\\n    --master-ipv4-cidr 172.16.0.0/28\n</code></pre> <p>Document a GKE cluster creation command in <code>production_gke.txt</code> doc under <code>step 4</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Result</p> <p>The private cluster is now created. <code>gcloud</code> has set up <code>kubectl</code> to authenticate with the private cluster but the cluster's K8s API server will only accept connections from the primary range of your subnetwork, and the secondary range of your subnetwork that is used for pods. That means nobody can access K8s API at this point of time, until we specify allowed ranges.</p> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials $ORG-$PRODUCT-$ENV-standard-cluster --region us-central1 \n</code></pre> <p>Step 4: Connecting to a Private Cluster</p> <p>Let's try to connect to the cluster:</p> <pre><code>kubectl get pods\n</code></pre> <pre><code>Unable to connect to the server i/o timeout\n</code></pre> <p>Fail</p> <p>This fails because private clusters firewall traffic to the master by default. In order to connect to the cluster you need to make use of the master authorized networks feature. </p> <p>Step 4: Enable Master Authorized networks on you cluster</p> <p>Suppose you have a group of machines, outside of your VPC network, belongs to your organization. You could authorize ONLY those machines to access the public endpoint (Kubernetes API).</p> <p>Here we will enable master authorized networks and whitelist the IP address for our Cloud Shell instance, to allow access to the master:</p> <pre><code>gcloud container clusters update $ORG-$PRODUCT-$ENV-standard-cluster --enable-master-authorized-networks --master-authorized-networks $(curl ipinfo.io/ip)/32 --region us-central1\n</code></pre> <p>Now we can access the API server using <code>kubectl</code> from GCP console:</p> <p>Note</p> <p>In real life it could be CIDR Range for you company, so only engineers or CI/CD systems from you company can connect to <code>kubectl</code> apis in secure manner.</p> <p>Now we can access the API server using kubectl:</p> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>No resources found in default namespace.\n</code></pre> <p>Let's deploy basic application on the GKE Standard Cluster:</p> <pre><code>kubectl run hello-web --image=gcr.io/google-samples/hello-app:1.0 --port=8080\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME        READY   STATUS    RESTARTS   AGE\nhello-web   1/1     Running   0          7s\n</code></pre> <p>Result</p> <p>We can deploy Pods to our Private Cluster.</p> <pre><code>kubectl delete pod hello-web\n</code></pre> <p>Step 2: Testing Outbound Traffic</p> <p>Outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads.</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: wget\nspec:\n  containers:\n  - name: wget\n    image: alpine\n    command: ['wget', '-T', '5', 'http://www.example.com/']\n  restartPolicy: Never\nEOF\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME   READY   STATUS   RESTARTS   AGE\nwget   0/1     Error    0          4m41s\n</code></pre> <pre><code>kubectl logs wget\n</code></pre> <p>Output:</p> <pre><code>Connecting to www.example.com (93.184.216.34:80)\nwget: download timed out\nwget: bad address 'www.example.com'\n</code></pre> <p>Results</p> <p>Pods can't access internet, because it's a private cluster</p> <p>Note</p> <p>Normally, if cluster is Private and doesn't have Cloud Nat configured, users can't even deploy images from Docker Hub. See troubleshooting details here</p> <p>Delete test pod:</p> <pre><code>kubectl delete pods wget\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE_solution/#16-create-a-google-cloud-nat","title":"1.6 Create a Google Cloud Nat","text":"<p>Private clusters give you the ability to isolate nodes from having inbound and outbound connectivity to the public internet. This isolation is achieved as the nodes have internal IP addresses only.</p> <p>If you want to provide outbound internet access for certain private nodes, you can use Cloud NAT</p> <p>Cloud NAT is a distributed, software-defined managed service. It's not based on proxy VMs or appliances.</p> <p>You configure a NAT gateway on a Cloud Router, which provides the control plane for NAT, holding configuration parameters that you specify. Google Cloud runs and maintains processes on the physical machines that run your Google Cloud VMs.</p> <p>Cloud NAT can be configured to automatically scale the number of NAT IP addresses that it uses, and it supports VMs that belong to managed instance groups, including those with autoscaling enabled.</p> <p>Step 1: Create a Cloud NAT configuration using Cloud Router</p> <p>Create Cloud Router in the same region as the instances that use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway.</p> <p>Task N5: Create <code>Cloud Router</code></p> <p>Step 1a: Create a <code>Cloud Router</code>:</p> <pre><code>gcloud compute routers create gke-nat-router \\\n    --network $ORG-$PRODUCT-$ENV-vpc \\\n    --region us-central1\n</code></pre> <p>Verify created Cloud Router:</p> <pre><code>Networking -&gt; Hybrid Connectivity -&gt; Cloud Routers\n</code></pre> <p>Document <code>Cloud Router</code> creation  command in <code>production_gke.txt</code> doc under <code>step 5</code></p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Task N6: Create <code>Cloud Nat</code> Gateway</p> <p>Step 1b: Create a <code>Cloud Nat</code> Gateway:</p> <pre><code>gcloud compute routers nats create nat-config \\\n    --router-region us-central1 \\\n    --router gke-nat-router \\\n    --nat-all-subnet-ip-ranges \\\n    --auto-allocate-nat-external-ips\n</code></pre> <p>Verify created <code>Cloud Nat</code>:</p> <pre><code>Networking -&gt; Network Services -&gt; Cloud NAT\n</code></pre> <p>Note</p> <p>Cloud NAT uses Cloud Router only to group NAT configuration information (control plane). Cloud NAT does not direct a Cloud Router to use BGP or to add routes. NAT traffic does not pass through a Cloud Router (data plane).</p> <p>Document Cloud Nat creation  command in <code>production_gke.txt</code> doc under <code>step 6</code>:</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Step 2: Testing Outbound Traffic</p> <p>Most outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads.</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: wget\nspec:\n  containers:\n  - name: wget\n    image: alpine\n    command: ['wget', '-T', '5', 'http://www.example.com/']\n  restartPolicy: Never\nEOF\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME                              READY   STATUS      RESTARTS   AGE\nwget                              0/1     Completed   0          2m53s\n</code></pre> <pre><code>kubectl logs wget\n</code></pre> <p>Output:</p> <pre><code>Connecting to www.example.com (93.184.216.34:80)\nsaving to 'index.html'\nindex.html           100% |********************************|  1256  0:00:00 ETA\n'index.html' saved\n</code></pre> <p>Results</p> <p>Pods can access internet, thanks to our configured Cloud Nat.</p>"},{"location":"020_Module_3_Assignment_Production_GKE_solution/#17-delete-default-node-pool-and-create-custom-node-pool","title":"1.7 Delete Default Node Pool and Create Custom Node Pool","text":"<p>A Kubernetes Engine cluster consists of a master and nodes. Kubernetes doesn't handle provisioning of nodes, so Google Kubernetes Engine handles this for you with a concept called node pools.</p> <p>A node pool is a subset of node instances within a cluster that all have the same configuration. They map to instance templates in Google Compute Engine, which provides the VMs used by the cluster. By default a Kubernetes Engine cluster has a single node pool, but you can add or remove them as you wish to change the shape of your cluster.</p> <p>In the previous example, you've created a Kubernetes Engine cluster. This gave us three nodes (three e2-micro (2 vCPUs, 1 GB memory), 100 GB of disk each) in a single node pool (called default-pool). Let's inspect the node pool:</p> <pre><code>gcloud config set compute/region us-central1\ngcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Output:</p> <pre><code>NAME: default-pool\nMACHINE_TYPE: e2-micro\nDISK_SIZE_GB: 100\nNODE_VERSION: 1.22.12-gke.300\n</code></pre> <p>If you want to add more nodes of this type, you can grow this node pool. If you want to add more nodes of a different type, you can add other node pools.</p> <p>A common method of moving a cluster to larger nodes is to add a new node pool, move the work from the old nodes to the new, and delete the old node pool.</p> <p>Let's add a second node pool. This time we will use the larger <code>e2-medium</code>  (2 vCPUs, 4 GB memory), 100 GB of disk each machine type.</p> <p>Task N7: Create Create Custom Node Pool and Delete Default Node Pool </p> <pre><code>gcloud container node-pools create custom-pool --cluster $ORG-$PRODUCT-$ENV-standard-cluster \\\n    --machine-type e2-medium --num-nodes 1\n</code></pre> <p>Output:</p> <pre><code>NAME: custom-pool\nMACHINE_TYPE: e2-medium\nDISK_SIZE_GB: 100\nNODE_VERSION: 1.22.12-gke.300\n</code></pre> <pre><code>gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Output:</p> <pre><code>NAME: default-pool\nMACHINE_TYPE: e2-micro\nDISK_SIZE_GB: 100\nNODE_VERSION: 1.22.12-gke.300\n\nNAME: custom-pool\nMACHINE_TYPE: e2-medium\nDISK_SIZE_GB: 100\nNODE_VERSION: 1.22.12-gke.300\n</code></pre> <p>You can now delete the original <code>default-pool</code> node pool:</p> <pre><code>gcloud container node-pools delete default-pool --cluster $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Check that node-pool has been deleted:</p> <pre><code>gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Result</p> <p>GKE cluster running only on <code>custom-pool</code> Node pool.</p>"},{"location":"020_Module_3_Assignment_Production_GKE_solution/#2-creating-gke-autopilot-clusters","title":"2 Creating GKE Autopilot Clusters","text":"<p>In Part 2 of the Assignment we going to deploy GKE Autopilot Clusters for Production Usage. Autopilot Clusters provides easy way to configure and manage GKE Clusters. They configured with best practices and  high availability out of the box.  And can additionally configured with Private Clusters. </p> <p>Task N8: Create GKE Autopilot Cluster with following subnet ranges:</p> <ul> <li>Cluster name: <code>$ORG-$PRODUCT-$ENV-auto-cluster</code></li> <li>In <code>region</code>: <code>us-central1</code></li> <li>VPC <code>network</code> name: <code>$ORG-$PRODUCT-$ENV-vpc</code></li> <li>VPC <code>subnet</code> name: <code>gke-auto-$ORG-$PRODUCT-$ENV-subnet</code></li> <li>Secondary <code>pod</code> range with name: <code>gke-auto-pods</code></li> <li>Secondary <code>service</code> range with name: <code>gke-auto-services</code></li> </ul> <p>Use following reference docs:</p> <ol> <li>Creating a GKE Autopilot cluster</li> </ol> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\n</code></pre> <pre><code>gcloud container clusters create-auto $ORG-$PRODUCT-$ENV-auto-cluster \\\n    --region us-central1 \\\n    --network $ORG-$PRODUCT-$ENV-vpc \\\n    --subnetwork gke-auto-$ORG-$PRODUCT-$ENV-subnet \\\n    --cluster-secondary-range-name gke-auto-pods \\\n    --services-secondary-range-name gke-auto-services \\\n    --async\n</code></pre> <p>Result</p> <p>Production ready GKE Autopilot Cluster has been created</p> <p>Document a GKE Autopilot cluster creation command in <code>production_gke.txt</code> doc under <code>step 8</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Authenticate to GKE Autopilot cluster:</p> <pre><code>gcloud container clusters get-credentials $ORG-$PRODUCT-$ENV-auto-cluster --region us-central1 \n</code></pre> <p>GKE Autopilot starts with 2 small Nodes, that is not charged to Customers:</p> <pre><code>kubectl get nodes\n</code></pre> <pre><code>gk3-archy-notepad-gcloud-default-pool-5c17f5f3-lp4p   Ready    &lt;none&gt;   11m   v1.22.12-gke.300\ngk3-archy-notepad-gcloud-default-pool-fae5625e-cwxx   Ready    &lt;none&gt;   11m   v1.22.12-gke.300\n</code></pre> <p>This Nodes are used to run GKE system containers such as various agents and networking pods:</p> <pre><code>kubectl get pods -o wide --all-namespaces\n</code></pre> <p>Note</p> <p>With GKE Autopilot, you don't need to worry about nodes or nodepools at all. Autopilot will spin nodes up and down based on the resources needed by your deployments, but you will only be charged for the resources requested by your actual deployments.</p> <p>Let's deploy basic application on the GKE Standard Cluster:</p> <pre><code>kubectl run hello-web --image=gcr.io/google-samples/hello-app:1.0 --port=8080\n</code></pre> <p>Let's verify that Pod deployed:</p> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME        READY   STATUS    RESTARTS   AGE\nhello-web   0/1     Pending   0          9s\n</code></pre> <p>After about 1 munute you should see:</p> <pre><code>NAME        READY   STATUS              RESTARTS   AGE\nhello-web   0/1     ContainerCreating   0          101s\n</code></pre> <p>Note</p> <p>That the application takes longer than usual to start. This is because Autopilot NAP system starting off new Nodes for application.</p> <p>Let's verify that Pod deployment after few minutes:</p> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME        READY   STATUS    RESTARTS   AGE\nhello-web   1/1     Running   0          7s\n</code></pre> <p>Let's verify that Nodes deployed:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Output:</p> <pre><code>NAME                                                  STATUS   ROLES    AGE     VERSION\ngk3-archy-notepad-gcloud-default-pool-5c17f5f3-lp4p   Ready    &lt;none&gt;   24m     v1.22.12-gke.300\ngk3-archy-notepad-gcloud-default-pool-5c17f5f3-t89j   Ready    &lt;none&gt;   5m35s   v1.22.12-gke.300\ngk3-archy-notepad-gcloud-default-pool-fae5625e-cwxx   Ready    &lt;none&gt;   24m     v1.22.12-gke.300\n</code></pre> <p>Result</p> <p>You will see 3d node has been created. GKE Autopilot created a Node based on workload requirement</p> <p>Note</p> <p>If you don't specify resource requests in your deployment spec, then Autopilot will set <code>CPU</code> to 2000m and <code>Memory</code> to 4gb. If your app requires less resources than that, make sure you set the resource request for your deployment. The minimum CPU is 250m and the minimum memory is 512mb.</p>"},{"location":"020_Module_3_Assignment_Production_GKE_solution/#3-commit-terraform-configurations-to-repository-and-share-it-with-instructorteacher","title":"3 Commit Terraform configurations to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>ycit020_module3</code> folder using the following Git commands:</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git add .\ngit commit -m \"Gcloud Documentation for Module 3 Assignment\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre> <p>Result</p> <p>Your instructor will be able to review your code and grade it.</p>"},{"location":"020_Module_3_Assignment_Production_GKE_solution/#4-cleanup","title":"4 Cleanup","text":"<pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\nexport PROJECT_PREFIX=1   # Project has to be unique\nexport PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX\n</code></pre> <p>Delete GKE standard Cluster:</p> <pre><code>gcloud container clusters delete $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Delete GKE Autopilot Cluster:</p> <pre><code>gcloud container clusters delete $ORG-$PRODUCT-$ENV-auto-cluster\n</code></pre> <p>Important</p> <p>We are going to delete the <code>$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX</code> only, please do not delete your project with Source Code Repo.</p> <pre><code>gcloud projects delete $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/","title":"Creating Production GKE Cluster with Terraform","text":"<p>In Module 2 Assignment with created project <code>$ORG-$PRODUCT-$ENV</code> -&gt; <code>$student_name-notepad-dev</code> with custom VPC using terraform. We've also used best practices for naming conventions and stored state of the terraform in GCS backend with versioning. </p> <p>In Module 3 Assignment we've create GKE Production Cluster with <code>gcloud</code>, this is a first good step to start automating your Infrastructure environment. Next logical step is to take those <code>gcloud</code> as a basis building Terraform resources.</p> <p>In Module 4 we going to continue use infrastructure that we've created in Module 2 and going to create Production GKE Clusters in it. </p> <p>Objective:</p> <ul> <li>Create Subnet, Cloud Nat with Terraform</li> <li>Create GKE Regional, Private Standard Cluster with Terraform</li> <li>Delete custom Node Pool and Create Custom Node Pool with Terraform</li> <li>Create GKE Autopilot Cluster with Terraform</li> </ul>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#1-creating-production-gke-cluster","title":"1 Creating Production GKE Cluster","text":""},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#prepare-lab-environment","title":"Prepare Lab Environment","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#11-locate-module-4-assignment","title":"1.1 Locate Module 4 Assignment","text":"<p>Step 1 Locate your personal Google Cloud Source Repository:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 2 Create <code>ycit020_module4</code> folder from you <code>ycit020_module2</code></p> <p>Important</p> <p>Consider to finish all tasks of Module2 before doing this step.</p> <pre><code>cp -r ycit020_module2 ycit020_module4\n</code></pre> <p>Step 3 Commit <code>ycit020_module4</code> folder using the following Git commands:</p> <pre><code>cd ~/$student_name-notepad\ngit status \ngit add .\ngit commit -m \"adding documentation for ycit020 module 4 assignment\"\n</code></pre> <p>Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#12-create-a-user-managed-subnet-with-terraform","title":"1.2 Create a <code>user-managed</code> subnet with terraform","text":"<p>Using google_compute_subnetwork resource create a <code>user-managed</code> subnet with terraform.</p> <p>Locate <code>notepad-infrastructure</code> folder where we going to continue creating GCP service Layer using terraform configuration:</p> <pre><code>cd ~/$student_name-notepad/ycit020_module4/notepad-infrastructure\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; subnets.tf\nresource \"google_compute_subnetwork\" \"gke_standard_subnet\" {\n  name          = format(\"gke-standard-%s-%s-%s-subnet\", var.org, var.product, var.environment)\n  network       = google_compute_network.vpc_network.self_link\n  region        = var.gcp_region\n  project       = var.gcp_project_id\n  ip_cidr_range = var.network_cidr\n  secondary_ip_range {\n    range_name    = var.pods_cidr_name\n    ip_cidr_range = var.pods_cidr\n  }\n  secondary_ip_range {\n    range_name    = var.services_cidr_name\n    ip_cidr_range = var.services_cidr\n  }\n}\nresource \"google_compute_subnetwork\" \"gke_auto_subnet\" {\n  name          = format(\"gke-auto-%s-%s-%s-subnet\", var.org, var.product, var.environment)\n  network       = google_compute_network.vpc_network.self_link\n  region        = var.gcp_region\n  project       = var.gcp_project_id\n  ip_cidr_range = var.network_auto_cidr\n  secondary_ip_range {\n    range_name    = var.pods_auto_cidr_name\n    ip_cidr_range = var.pods_auto_cidr\n  }\n  secondary_ip_range {\n    range_name    = var.services_auto_cidr_name\n    ip_cidr_range = var.services_auto_cidr\n  }\n}\nEOF\n</code></pre> <p>Note</p> <p>Notice power of terraform outputs. Here we link <code>subnet</code> with our VPC <code>network</code> using <code>google_compute_network.vpc_network.self_link</code> <code>output</code> value of created <code>network</code> in previous step.</p> <p>Define variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\n\n# variables used to create VPC subnets\n\nvariable \"network_cidr\" {\n  type = string\n}\nvariable \"pods_cidr\" {\n  type = string\n}\nvariable \"pods_cidr_name\" {\n  type    = string\n  default = \"gke-standard-pods\"\n}\nvariable \"services_cidr\" {\n  type = string\n}\nvariable \"services_cidr_name\" {\n  type    = string\n  default = \"gke-standard-services\"\n}\nvariable \"network_auto_cidr\" {\n  type = string\n}\nvariable \"pods_auto_cidr\" {\n  type = string\n}\nvariable \"pods_auto_cidr_name\" {\n  type    = string\n  default = \"gke-auto-pods\"\n}\nvariable \"services_auto_cidr\" {\n  type = string\n}\nvariable \"services_auto_cidr_name\" {\n  type    = string\n  default = \"gke-auto-services\"\n}\nEOF\n</code></pre> <p>Define outputs:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"subnet_selflink\" {\n  value = \"\\${google_compute_subnetwork.gke_standard_subnet.self_link}\"\n}\noutput \"subnet_auto_selflink\" {\n  value = \"\\${google_compute_subnetwork.gke_auto_subnet.self_link}\"\n}\nEOF\n</code></pre> <p>Task N1: Update <code>terraform.tfvars</code> file values with following information:</p> <ul> <li>Node Range: See column <code>subnet</code> in above table for <code>dev</code> cluster</li> <li>GKE Standard Secondary Ranges:<ul> <li>Service range CIDR: See column <code>srv range</code> in above table for <code>dev</code> cluster</li> <li>Pods range CIDR: See column <code>pod range</code> in above table for <code>dev</code> cluster</li> </ul> </li> <li>GKE Auto Secondary Ranges:<ul> <li>Service range CIDR: See column <code>srv range</code> in above table for <code>dev</code> cluster</li> <li>Pods range CIDR: See column <code>pod range</code> in above table for <code>dev</code> cluster</li> </ul> </li> </ul> <pre><code>Table 1 \nProject   | Subnet Name       |     subnet       |    pod range       |    srv range        | kubectl api range\napp 1 Dev | gke-standard      |   10.130.0.0/24  |     10.0.0.0/16    |     10.100.0.0/23   |     172.16.0.0/28\n          | gke-auto          |   10.131.0.0/24  |     10.1.0.0/16    |     10.100.2.0/23   |     172.16.0.16/28\n</code></pre> <pre><code>edit terraform.tfvars\n</code></pre> <p>Update the file with values according to VPC subnet design in </p> <pre><code>#gke-standard subnet vars\nnetwork_cidr       = \"TODO\"\npods_cidr          = \"TODO\"\nservices_cidr      = \"TODO\"\n#gke-auto subnet vars\nnetwork_auto_cidr  = \"TODO\"\npods_auto_cidr     = \"TODO\"\nservices_auto_cidr = \"TODO\"\nEOF\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Create VPC:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Review created subnet:</p> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=dev\n</code></pre> <pre><code>gcloud compute networks subnets list\n</code></pre> <pre><code>gcloud compute networks subnets describe gke-standard-$ORG-$PRODUCT-$ENV-subnet --region us-central1\ngcloud compute networks subnets describe gke-auto-$ORG-$PRODUCT-$ENV-subnet --region us-central1\n</code></pre> <p>Example of the Output:</p> <pre><code>enableFlowLogs: false\nfingerprint: sWtjHJyqrM8=\ngatewayAddress: 10.130.0.1\nid: '251139681714314790'\nipCidrRange: 10.130.0.0/24\nkind: compute#subnetwork\nlogConfig:\n  enable: false\nname: gke-standard-archy-notepad-dev-subnet\nnetwork: https://www.googleapis.com/compute/v1/projects/archy-notepad-dev-898/global/networks/vpc-archy-notepad-dev\nprivateIpGoogleAccess: false\nprivateIpv6GoogleAccess: DISABLE_GOOGLE_ACCESS\npurpose: PRIVATE\nregion: https://www.googleapis.com/compute/v1/projects/archy-notepad-dev-898/regions/us-central1\nsecondaryIpRanges:\n- ipCidrRange: 10.0.0.0/16\n  rangeName: gke-standard-pods\n- ipCidrRange: 10.100.0.0/23\n  rangeName: gke-standard-services\nselfLink: https://www.googleapis.com/compute/v1/projects/archy-notepad-dev-898/regions/us-central1/subnetworks/gke-standard-archy-notepad-dev-subnet\nstackType: IPV4_ONLY\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>Networking-&gt;VPC Networks -&gt; Click VPC network and check `Subnet` tab\n</code></pre> <p>Task N2: Update  <code>subnets.tf</code> so that  <code>google_compute_subnetwork</code> resource supports following features:</p> <pre><code>* Flow Logs Configuration\n  * Aggregation interval: 15 min  # reduce cost for VPC Flow logging, as by default interval is 5 second\n  * Flow logs Metadata Config: EXCLUDE_ALL_METADATA \n* Private IP Google Access\n</code></pre> <p>Hint</p> <p>Use google_compute_subnetwork resource to update subnet configurations with terraform.</p> <pre><code>edit subnets.tf\nTODO\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Create VPC:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Review created subnet:</p> <pre><code>gcloud compute networks subnets describe gke-standard-$ORG-$PRODUCT-$ENV-subnet --region us-central1\ngcloud compute networks subnets describe gke-auto-$ORG-$PRODUCT-$ENV-subnet --region us-central1\n</code></pre> <p>Example of the Output:</p> <pre><code>creationTimestamp: '2022-10-03T19:56:09.579-07:00'\nenableFlowLogs: true\nfingerprint: Y8-BjoK1gK8=\ngatewayAddress: 10.130.0.1\nid: '251139681714314790'\nipCidrRange: 10.130.0.0/24\nkind: compute#subnetwork\nlogConfig:\n  aggregationInterval: INTERVAL_15_MIN\n  enable: true\n  filterExpr: 'true'\n  flowSampling: 0.5\n  metadata: EXCLUDE_ALL_METADATA\nname: gke-standard-archy-notepad-dev-subnet\nnetwork: https://www.googleapis.com/compute/v1/projects/archy-notepad-dev-898/global/networks/vpc-archy-notepad-dev\nprivateIpGoogleAccess: true\nprivateIpv6GoogleAccess: DISABLE_GOOGLE_ACCESS\npurpose: PRIVATE\nregion: https://www.googleapis.com/compute/v1/projects/archy-notepad-dev-898/regions/us-central1\nsecondaryIpRanges:\n- ipCidrRange: 10.0.0.0/16\n  rangeName: gke-standard-pods\n- ipCidrRange: 10.100.0.0/23\n  rangeName: gke-standard-services\nselfLink: https://www.googleapis.com/compute/v1/projects/archy-notepad-dev-898/regions/us-central1/subnetworks/gke-standard-archy-notepad-dev-subnet\nstackType: IPV4_ONLY\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#13-create-a-cloud-router","title":"1.3 Create a Cloud router","text":"<p>Create Cloud Router for <code>custom mode</code> network (VPC), in the same region as the instances that will use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway.</p> <p>Task N3: Define a <code>google_compute_router</code> inside <code>router.tf</code> that will be able to create a NAT router so the nodes can reach DockerHub and external APIs from private cluster, using following parameters:</p> <ul> <li>Create router for custom <code>vpc_network</code> created above with terraform</li> <li>Same project as VPC</li> <li>Same region as VPC</li> <li>Router name: <code>gke-net-router</code></li> <li>Local BGP Autonomous System Number (ASN): 64514</li> </ul> <p>Use following reference documentation to create Cloud Router Resource with terraform.</p> <p>Hint</p> <p>You can automatically recover <code>vpc</code> name from terraform output like this: <code>google_compute_network.vpc_network.self_link</code>.</p> <pre><code>edit router.tf\nTODO\nEOF\n</code></pre> <p>Save the populated file.</p> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Create Cloud Router:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify created Cloud Router:</p> <p>CLI:</p> <pre><code>gcloud compute routers list\ngcloud compute routers describe gke-net-router --region us-central1\n</code></pre> <p>Output:</p> <pre><code>bgp:\n  advertiseMode: DEFAULT\n  asn: 64514\n  keepaliveInterval: 20\nkind: compute#router\nname: gke-net-router\n</code></pre> <p>UI:</p> <pre><code>Networking -&gt; Hybrid Connectivity -&gt; Cloud Routers\n</code></pre> <p>Result</p> <p>Router resource has been created for VPC Network</p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#14-create-a-cloud-nat","title":"1.4 Create a Cloud Nat","text":"<p>Set up a simple Cloud Nat configuration using google_compute_router_nat resource, which will <code>automatically</code> allocates the necessary external IP addresses to provide NAT services to a region. </p> <p>When you use auto-allocation, Google Cloud reserves IP addresses in your project automatically. </p> <pre><code>cat &lt;&lt;EOF &gt;&gt; cloudnat.tf\nresource \"google_compute_router_nat\" \"gke_cloud_nat\" {\n  project                = var.gcp_project_id\n  name                   = \"gke-cloud-nat\"\n  router                 = google_compute_router.gke_net_router.name\n  region                 = var.gcp_region\n  nat_ip_allocate_option = \"AUTO_ONLY\"\n  source_subnetwork_ip_ranges_to_nat = \"ALL_SUBNETWORKS_ALL_IP_RANGES\"\n}\nEOF\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Create Cloud Router:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify created <code>Cloud Nat</code>:</p> <p>CLI:</p> <pre><code># List available Cloud Nat Routers\ngcloud compute routers nats list --router gke-net-router --router-region us-central1\n# Describe Cloud Nat Routers `gke-cloud-nat`:\ngcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1\n</code></pre> <p>Output:</p> <pre><code>enableEndpointIndependentMapping: true\nendpointTypes:\n- ENDPOINT_TYPE_VM\nicmpIdleTimeoutSec: 30\nname: gke-cloud-nat\nnatIpAllocateOption: AUTO_ONLY\nsourceSubnetworkIpRangesToNat: ALL_SUBNETWORKS_ALL_IP_RANGES\ntcpEstablishedIdleTimeoutSec: 1200\ntcpTransitoryIdleTimeoutSec: 30\nudpIdleTimeoutSec: 30\n</code></pre> <p>UI:</p> <pre><code>Networking -&gt; Network Services -&gt; Cloud NAT\n</code></pre> <p>Result</p> <p>A NAT service created in a router</p> <p>Task N4: Additionally turn <code>ON</code> logging feature for <code>ALL</code> log types of communication for Cloud Nat</p> <pre><code>edit cloudnat.tf\nTODO\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Update Cloud Nat Configuration:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>Apply complete! Resources: 0 added, 1 changed, 0 destroyed.\n</code></pre> <pre><code>gcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1\n</code></pre> <p>Result</p> <p>Cloud Nat now supports Logging. Cloud NAT logging allows you to log NAT connections and errors. When Cloud NAT logging is enabled, one log entry can be generated for each of the following scenarios:</p> <ul> <li>When a network connection using NAT is created.</li> <li>When a packet is dropped because no port was available for NAT.</li> </ul>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#15-create-a-private-gke-cluster-and-delete-default-node-pool","title":"1.5 Create a Private GKE Cluster and delete default node pool","text":""},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#151-enable-gcp-beta-provider","title":"1.5.1 Enable GCP Beta Provider","text":"<p>In order to create a GKE cluster with terraform we will be leveraging google_container_cluster resource.</p> <p>Some of  <code>google_container_cluster</code> arguments, such VPC-Native networking mode, VPA, Istio, CSI Driver add-ons, requires  <code>google-beta</code> Provider.</p> <p>The <code>google-beta</code> provider is distinct from the <code>google</code> provider in that it supports GCP products and features that are in beta, while google does not. Fields and resources that are only present in google-beta will be marked as such in the shared provider documentation.</p> <p>Configure and Initialize GCP Beta Provider, similar to how we did it for GCP Provider in 1.3.3 Initialize Terraform update <code>provider.tf</code> and <code>main.tf</code> configuration files.</p> <pre><code>rm provider.tf\ncat &lt;&lt;EOF &gt;&gt; provider.tf\nterraform {\n  required_providers { \n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~&gt; 4.37.0\"\n    }\n    google-beta = {\n      source  = \"hashicorp/google-beta\"\n      version = \"~&gt; 4.37.0\"\n    }\n  }\n}\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt;  main.tf\nprovider \"google-beta\" {\n  project = var.gcp_project_id\n  region  = var.gcp_region\n}\nEOF\n</code></pre> <p>Initialize <code>google-beta</code> provider plugin:</p> <pre><code>terraform init\n</code></pre> <p>Success</p> <p>Terraform has been successfully initialized!</p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#152-enable-kubernetes-engine-api","title":"1.5.2 Enable Kubernetes Engine API","text":"<p>Kubernetes Engine API used to build and manages container-based applications, powered by the open source Kubernetes technology. Before starting  GKE cluster creation it is required to enable it.</p> <p>Task N5: Enable <code>container.googleapis.com</code> in <code>services.tf</code> file similar to what we already did in 2.2 Enable required GCP Services API. Make sure that service is set to <code>disable_on_destroy=false</code>, which  helps to prevent errors during redeployments of the system (aka immutable infra)</p> <pre><code>edit services.tf\nTODO\n</code></pre> <p>Note</p> <p>Adding <code>disable_on_destroy=false</code> helps to prevent errors during redeployments of the system.</p> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Update Cloud Nat Configuration:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#153-create-a-private-gke-cluster-and-delete-default-node-pool","title":"1.5.3 Create a Private GKE Cluster and delete default node pool","text":"<p>Using Terraform resource google_container_cluster resource create a Regional, Private GKE cluster, with following characteristics:</p> <p>Cluster Configuration:</p> <ul> <li>Cluster name: <code>gke-$ORG-$PRODUCT-$ENV</code></li> <li>GKE Control plane is replicated across three zones of a region: <code>us-central1</code></li> <li>Private cluster with unrestricted access to the public endpoint:<ul> <li>Cluster Nodes access: Private Node GKE Cluster with Public API endpoint</li> <li>Cluster K8s API access: with unrestricted access to the public endpoint</li> </ul> </li> <li>Cluster Node Communication: <code>VPC Native</code></li> <li>Secondary pod range with name: <code>gke-standard-pods</code></li> <li>Secondary service range with name: <code>gke-standard-services</code></li> <li>GKE Release channel: <code>regular</code></li> <li>GKE master and node version: \"1.22.12-gke.300\"</li> <li>Terraform Provider: <code>google-beta</code></li> <li>Timeouts to finish creation of cluster and deletion of default node pool: 30M</li> <li>Features:<ul> <li>Enable Cilium based Networking DataplaneV2: <code>enable-dataplane-v2</code></li> <li>Configure Workload Identity Pool: <code>PROJECT_ID.svc.id.goog</code></li> <li>Enable HTTP Load-balancing Addon: <code>http_load_balancing</code></li> </ul> </li> </ul> <p>Terraform resource google_container_node_pool creates a <code>custom</code> GKE Node Pool.</p> <p>Custom Node Pool Configuration:</p> <ul> <li>The name of a GCE machine (VM) type: <code>e2-micro</code></li> <li>Node count: 1 per zone</li> <li>Node images: <code>Container-Optimized OS</code></li> <li>GKE Node Pool Boot Disk Size: \"100 Gb\"</li> </ul> <p>Note</p> <p>Why delete default node pool? The <code>default</code> node pools cause trouble with managing the cluster, when created with terraform as it is not part of the terraform lifecycle. GKE Architecture Best Practice recommends to delete <code>default</code> node pool and create a <code>custom</code> one instead and manage the node pools explicitly.</p> <p>Note</p> <p>Why define <code>Timeouts</code> for gke resource? Normally GKE creation takes few minutes. However, in our case we creating GKE Cluster, and then system cordon, drain and then destroy default node pool. This process may take 10-20 minutes and we want to make sure terraform will not time out during this time.</p> <p>Step 1: Let's define GKE resource first:</p> <pre><code>edit gke.tf\n</code></pre> <pre><code>resource \"google_container_cluster\" \"primary_cluster\" {\n  provider = google-beta\n\n  project = var.gcp_project_id\n\n  name               = format(\"gke-standard-%s-%s-%s\", var.org, var.product, var.environment)\n  min_master_version = var.kubernetes_version\n  network            = google_compute_network.vpc_network.self_link\n  subnetwork         = google_compute_subnetwork.gke_standard_subnet.self_link\n\n  location                    = var.gcp_region\n  logging_service             = var.logging_service\n  monitoring_service          = var.monitoring_service\n\n  remove_default_node_pool = true\n  initial_node_count       = 1\n\n  private_cluster_config {\n    enable_private_nodes   = var.enable_private_nodes\n    enable_private_endpoint = var.enable_private_endpoint\n    master_ipv4_cidr_block = var.master_ipv4_cidr_block\n  }\n\n  # Enable Dataplane V2\n  datapath_provider = \"ADVANCED_DATAPATH\"\n\n  release_channel {\n    channel = \"REGULAR\"\n  }\n\n\n  addons_config {\n    http_load_balancing {\n      disabled = var.disable_http_load_balancing\n    }\n\n  ip_allocation_policy {\n    cluster_secondary_range_name  = var.pods_cidr_name\n    services_secondary_range_name = var.services_cidr_name\n  }\n\n  timeouts {\n    create = \"30m\"\n    update = \"30m\"\n    delete = \"30m\"\n  }\n\n  workload_identity_config {\n    workload_pool  = \"${var.gcp_project_id}.svc.id.goog\"\n  }\n}\n</code></pre> <p>Step 2: Next define GKE cluster specific variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke_variables.tf\n\n# variables used to create GKE Cluster Control Plane\n\nvariable \"kubernetes_version\" {\n  default     = \"\"\n  type        = string\n  description = \"The GKE version of Kubernetes\"\n}\n\nvariable \"logging_service\" {\n  description = \"The logging service that the cluster should write logs to.\"\n  default     = \"logging.googleapis.com/kubernetes\"\n}\n\nvariable \"monitoring_service\" {\n  default     = \"monitoring.googleapis.com/kubernetes\"\n  description = \"The GCP monitoring service scope\"\n}\n\nvariable \"disable_http_load_balancing\" {\n  default     = false\n  description = \"Enable HTTP Load balancing GCP integration\"\n}\n\nvariable \"pods_range_name\" {\n  description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to pods\"\n  default     = \"\"\n}\n\nvariable \"services_range_name\" {\n  description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to services\"\n  default     = \"\"\n}\n\nvariable \"enable_private_nodes\" {\n  default     = false\n  description = \"Enable Private-IP Only GKE Nodes\"\n}\n\nvariable \"enable_private_endpoint\" {\n  default     = false\n  description = \"When true, the cluster's private endpoint is used as the cluster endpoint and access through the public endpoint is disabled.\"\n}\n\nvariable \"master_ipv4_cidr_block\" {\n  description = \"The ipv4 cidr block that the GKE masters use\"\n}\n\nvariable \"release_channel\" {\n  type        = string\n  default     = \"\"\n  description = \"The release channel of this cluster\"\n}\nEOF\n</code></pre> <p>Step 3: Define GKE cluster specific outputs:</p> <pre><code>edit outputs.tf\n</code></pre> <p>Add following outputs and save file:</p> <pre><code>output \"id\" {\n  value = \"${google_container_cluster.primary_cluster.id}\"\n}\noutput \"endpoint\" {\n  value = \"${google_container_cluster.primary_cluster.endpoint}\"\n}\noutput \"master_version\" {\n  value = \"${google_container_cluster.primary_cluster.master_version}\"\n}\n</code></pre> <p>Task N6: Complete <code>terraform.tfvars</code> with required values to GKE Cluster specified above:</p> <pre><code>edit terraform.tfvars\n</code></pre> <pre><code>//gke specific\nenable_private_nodes   = \"TODO\"  \nmaster_ipv4_cidr_block = \"TODO\"    # Using Table 1 `kubectl api range` for GKE Standard\nkubernetes_version     = \"TODO\"    # From GKE Cluster requirements\nrelease_channel        = \"TODO\"    # From GKE Cluster requirements\nEOF\n</code></pre> <p>In the next step, we going to create a <code>custom</code> GKE Node Pool.</p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#154-create-a-gke-custom-node-pool","title":"1.5.4 Create a GKE <code>custom</code> Node pool","text":"<p>Using google_container_node_pool resource create a <code>custom</code> GKE Node Pool with following characteristics:</p> <p>Node Pool Configuration:</p> <p>Custom Node Pool Configuration:</p> <ul> <li>The name of a GCE machine (VM) type: <code>e2-micro</code></li> <li>Node count: 1 per zone</li> <li>Node images: <code>Container-Optimized OS</code></li> <li>GKE Node Pool Boot Disk Size: \"100 Gb\"</li> </ul> <p>Step 1: Let's define GKE resource first:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke.tf\n\n#Node Pool Resource\nresource \"google_container_node_pool\" \"custom-node_pool\" {\n\n  name       = \"main-pool\"\n  location = var.gcp_region\n  project  = var.gcp_project_id\n  cluster    = google_container_cluster.primary_cluster.name\n  node_count = var.gke_pool_node_count\n  version    = var.kubernetes_version\n\n  node_config {\n    image_type   = var.gke_pool_image_type\n    disk_size_gb = var.gke_pool_disk_size_gb\n    disk_type    = var.gke_pool_disk_type\n    machine_type = var.gke_pool_machine_type\n  }\n\n  timeouts {\n    create = \"10m\"\n    delete = \"10m\"\n  }\n\n  lifecycle {\n    ignore_changes = [\n      node_count\n    ]\n  }\n}\nEOF\n</code></pre> <p>Step 2: Next define GKE cluster specific variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke_variables.tf\n\n#Node Pool specific variables\nvariable \"gke_pool_machine_type\" {\n  type = string\n}\nvariable \"gke_pool_node_count\" {\n  type = number\n}\nvariable \"gke_pool_disk_type\" {\n  type = string\n  default = \"pd-standard\"\n}\nvariable \"gke_pool_disk_size_gb\" {\n  type = string\n}\nvariable \"gke_pool_image_type\" {\n  type = string\n}\nEOF\n</code></pre> <p>Task 7 (Continued): Complete <code>terraform.tfvars</code> with required values to GKE Node Pool values specified above:</p> <pre><code>edit terraform.tfvars\n</code></pre> <pre><code>//pool specific\ngke_pool_node_count   = \"TODO\"\ngke_pool_image_type   = \"TODO\"\ngke_pool_disk_size_gb = \"TODO\"\ngke_pool_machine_type = \"TODO\"\n</code></pre> <p>Step 3: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Step 4: Create GKE Cluster and Node Pool:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>google_container_cluster.primary_cluster: Creating...\n...\ngoogle_container_cluster.primary_cluster: Creation complete after 20m9s \ngoogle_container_node_pool.custom-node_pool: Creating...\ngoogle_container_node_pool.custom-node_pool: Creation complete after 2m10s\n</code></pre> <p>Note</p> <p>GKE Cluster Control plain and deletion of <code>default</code> Node Pool may take about 12 minutes, creation custom Node Pool another 6 minutes.</p> <p>Verify Cluster has been created:</p> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=dev\n</code></pre> <pre><code>gcloud container clusters list\ngcloud compute networks subnets describe gke-standard-$ORG-$PRODUCT-$ENV --region us-central1\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#155-update-gke-node-pool-to-support-auto-upgrade-and-auto-recovery-features","title":"1.5.5 Update GKE Node Pool to support Auto Upgrade and Auto Recovery features","text":"<p>Note</p> <p>GKE Master Nodes are managed by Google and get's upgraded automatically. Users can only specify Maintenance Window if they have preference for that process to occur (e.g. after busy hours). Users can however control Node Pool upgrade lifecycle. They can choose to do it themselves or with Auto Upgrade.</p> <p>Task N8: Using google_container_node_pool resource update node pool to turn-off <code>Auto Upgrade</code> and <code>Auto Repair</code> features that enabled by default on Release Channels.</p> <pre><code>edit gke.tf\nTODO\n</code></pre> <p>Solution: According to GCP documentations it is only possible to disable auto-upgrade by Unsubscribing from a release channel.</p> <p>Replace channel release from <code>REGULAR</code> to <code>UNSPECIFIED</code> in edit <code>gke.tf</code> and turn off <code>auto_repair</code> and <code>auto_upgrade</code></p> <pre><code>edit gke.tf\n</code></pre> <pre><code>  release_channel {\n    channel = \"UNSPECIFIED\"\n  }\n</code></pre> <pre><code>  management {\n    auto_repair  = false\n    auto_upgrade = false\n  }\n</code></pre> <p>Step 3: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>No errors.</p> <p>Step 4: Update GKE Cluster Node Pool configuration:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Summary</p> <p>Congrats! You've now learned how to deploy production grade GKE clusters.</p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#16-create-auto-mode-gke-cluster-with-terraform","title":"1.6 Create Auto Mode GKE Cluster with Terraform","text":"<p>Step 1: Define GKE Autopilot resource first:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke_auto.tf\nresource \"google_container_cluster\" \"auto_cluster\" {\n  provider = google-beta\n\n  project = var.gcp_project_id\n\n  name               = format(\"gke-auto-%s-%s-%s\", var.org, var.product, var.environment)\n  min_master_version = var.kubernetes_version\n  network            = google_compute_network.vpc_network.self_link\n  subnetwork         = google_compute_subnetwork.gke_auto_subnet.self_link\n\n  location                    = var.gcp_region\n  logging_service             = var.logging_service\n  monitoring_service          = var.monitoring_service\n\n# Enable Autopilot for this cluster\n  enable_autopilot = true\n\n# Private Autopilot GKE cluster\n  private_cluster_config {\n    enable_private_nodes   = var.enable_private_nodes\n    enable_private_endpoint = var.enable_private_endpoint\n    master_ipv4_cidr_block = var.auto_master_ipv4_cidr_block\n  }\n\n# Configuration options for the Release channel feature, which provide more control over automatic upgrades of your GKE clusters.\n  release_channel {\n    channel = \"REGULAR\"\n  }\n\n# Configuration of cluster IP allocation for VPC-native clusters\n  ip_allocation_policy {\n    cluster_secondary_range_name  = var.pods_auto_cidr_name\n    services_secondary_range_name = var.services_auto_cidr_name\n  }\n\n  timeouts {\n    create = \"20m\"\n    update = \"20m\"\n    delete = \"20m\"\n  }\n}\n</code></pre> <p>Step 2: Next define GKE cluster specific variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke_variables.tf\n\n# variables used to create GKE AutoPilot Cluster Control Plane\n\nvariable \"auto_master_ipv4_cidr_block\" {\n  description = \"The ipv4 cidr block that the GKE masters use\"\n}\nEOF\n</code></pre> <p>Step 3: Define GKE cluster specific outputs:</p> <pre><code>edit outputs.tf\n</code></pre> <p>Add following outputs and save file:</p> <pre><code>output \"autopilot_id\" {\n  value = \"${google_container_cluster.auto_cluster.id}\"\n}\noutput \"autopilot_endpoint\" {\n  value = \"${google_container_cluster.auto_cluster.endpoint}\"\n}\noutput \"autopilot_master_version\" {\n  value = \"${google_container_cluster.auto_cluster.master_version}\"\n}\n</code></pre> <p>Update tfvars</p> <pre><code>edit terraform.tfvars\n</code></pre> <pre><code>//gke autopilot specific\nauto_master_ipv4_cidr_block = \"172.16.0.16/28\"\n</code></pre> <p>Step 4: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Step 5: Create GKE Autopilot Cluster:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>google_container_cluster.auto_cluster: Still creating..\n...\ngoogle_container_cluster.auto_cluster: Creation complete after 7m58s \n</code></pre> <p>Verify Cluster has been created:</p> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=dev\n</code></pre> <pre><code>gcloud container clusters list\ngcloud compute networks subnets describe gke-standard-$ORG-$PRODUCT-$ENV --region us-central1\n</code></pre> <p>Task 9: Using google_container_cluster make sure that <code>Maintenance Windows</code> are set to occur Daily on weekdays from 9:00-17:00 UTC-4, but skip weekends, starting from October 2nd:</p> <pre><code>--maintenance-window-start 2022-10-02T09:00:00-04:00 \\\n--maintenance-window-end 2022-10-02T17:00:00-04:00 \\\n--maintenance-window-recurrence 'FREQ=WEEKLY;BYDAY=MO,TU,WE,TH,FR'\n</code></pre> <pre><code>edit gke_auto.tf\nTODO\n</code></pre> <p>Step 6: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Step 7: Create GKE Autopilot Cluster:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <pre><code>google_container_cluster.auto_cluster: Modifying... [id=projects/archy-notepad-dev-898/locations/us-central1/clusters/gke-auto-archy-notepad-dev]\ngoogle_container_cluster.auto_cluster: Modifications complete after 2s [id=projects/archy-notepad-dev-898/locations/us-central1/clusters/gke-auto-archy-notepad-dev]\n</code></pre> <p>Browse GKE UI and verify that GKE Autopilot is configured with Maintenance window.</p> <p>Success</p> <p>We can now create GKE Autopilot cluster </p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#17-optional-repeatable-infrastructure","title":"1.7 (Optional) Repeatable Infrastructure","text":"<p>When you doing IaC it is important to insure that you can both create and destroy resources consistently. This is especially important when doing CI/CD testing.</p> <p>Step 3: Destroy all resources:</p> <pre><code>terraform destroy -var-file terraform.tfvars\n</code></pre> <p>No errors.</p> <p>Step 4: Recreate all resources:</p> <pre><code>terraform plan -var-file terraform.tfvars\nterraform apply -var-file terraform.tfvars\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#18-create-documentation-for-terraform-code","title":"1.8  Create Documentation for terraform code","text":"<p>Documentation for your terraform code is an important part of IaC. Make sure all your variables have a good description!</p> <p>There are community tools that have been developed to make the documentation process smoother, in terms of documenting Terraform resources and requirements.Its good practice to also include a usage example snippet.</p> <p>Terraform-Docs is a good example of one tool that can generate some documentation based on the description argument of your Input Variables, Output Values, and from your required_providers configurations.</p> <p>Task N10: Create Terraform Documentation for your infrastructure.</p> <pre><code>TODO\n</code></pre> <p>Step 1 Install the terraform-docs cli to your Google CloudShell environment: </p> <pre><code>curl -sSLo ./terraform-docs.tar.gz https://terraform-docs.io/dl/v0.16.0/terraform-docs-v0.16.0-$(uname)-amd64.tar.gz\ntar -xzf terraform-docs.tar.gz\nchmod +x terraform-docs\nsudo mv terraform-docs /usr/local/bin/\nterraform-docs\n</code></pre> <p>Generating terraform documentation with Terraform Docs:</p> <pre><code>cd ~/$student_name-notepad/ycit020_module4/foundation-infrastructure\nterraform-docs markdown . &gt; README.md\ncd ~/$student_name-notepad/ycit020_module4/notepad-infrastructure\nterraform-docs markdown . &gt; README.md\n</code></pre> <p>Verify created documentation:</p> <pre><code>edit README.md\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#19-commit-readme-doc-to-repository-and-share-it-with-instructorteacher","title":"1.9 Commit Readme doc to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>ycit020_module4</code> folder using the following Git commands:</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git add .\ngit commit -m \"TF manifests for Module 4 Assignment\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre> <p>Result</p> <p>Your instructor will be able to review you code and grade it.</p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#110-cleanup","title":"1.10 Cleanup","text":"<p>We only going to cleanup GCP Service foundation layer, as we going to use GCP project in future.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module4/notepad-infrastructure\nterraform destroy -var-file terraform.tfvars\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform/#2-workaround-for-project-quota-issue","title":"2. Workaround for Project Quota issue","text":"<p>If you see following error during project creation in foundation layer:</p> <pre><code>Error: Error setting billing account \"010BE6-CA1129-195D77\" for project \"projects/ayrat-notepad-dev-244\": googleapi: Error 400: Precondition check failed., failedPrecondition\n</code></pre> <p>This is due to our Billing account has quota of 5 projects per account.</p> <p>To solve this issue find all unused accounts:</p> <pre><code>gcloud beta billing projects list --billing-account $ACCOUNT_ID\n</code></pre> <p>And unlink them, so you have less then 5 projects per account:</p> <pre><code>gcloud beta billing projects unlink $PROJECT_ID\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/","title":"Creating Production GKE Cluster with Terraform","text":"<p>In Module 2 Assignment with created project <code>$ORG-$PRODUCT-$ENV</code> -&gt; <code>$student_name-notepad-dev</code> with custom VPC using terraform. We've also used best practices for naming conventions and stored state of the terraform in GCS backend with versioning. </p> <p>In Module 3 Assignment we've create GKE Production Cluster with <code>gcloud</code>, this is a first good step to start automating your Infrastructure environment. Next logical step is to take those <code>gcloud</code> as a basis building Terraform resources.</p> <p>In Module 4 we going to continue use infrastructure that we've created in Module 2 and going to create Production GKE Clusters in it. </p> <p>Objective:</p> <ul> <li>Create Subnet, Cloud Nat with Terraform</li> <li>Create GKE Regional, Private Standard Cluster with Terraform</li> <li>Delete custom Node Pool and Create Custom Node Pool with Terraform</li> <li>Create GKE Autopilot Cluster with Terraform</li> </ul>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#1-creating-production-gke-cluster","title":"1 Creating Production GKE Cluster","text":""},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#prepare-lab-environment","title":"Prepare Lab Environment","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#11-locate-module-4-assignment","title":"1.1 Locate Module 4 Assignment","text":"<p>Step 1 Locate your personal Google Cloud Source Repository:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 2 Create <code>ycit020_module4</code> folder from you <code>ycit020_module2</code></p> <p>Important</p> <p>Consider to finish all tasks of Module2 before doing this step.</p> <pre><code>cp -r ycit020_module2 ycit020_module4\n</code></pre> <p>Step 3 Commit <code>ycit020_module4</code> folder using the following Git commands:</p> <pre><code>cd ~/$student_name-notepad\ngit status \ngit add .\ngit commit -m \"adding documentation for ycit020 module 4 assignment\"\n</code></pre> <p>Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#12-create-a-user-managed-subnet-with-terraform","title":"1.2 Create a <code>user-managed</code> subnet with terraform","text":"<p>Using google_compute_subnetwork resource create a <code>user-managed</code> subnet with terraform.</p> <p>Locate <code>notepad-infrastructure</code> folder where we going to continue creating GCP service Layer using terraform configuration:</p> <pre><code>cd ~/$student_name-notepad/ycit020_module4/notepad-infrastructure\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; subnets.tf\nresource \"google_compute_subnetwork\" \"gke_standard_subnet\" {\n  name          = format(\"gke-standard-%s-%s-%s-subnet\", var.org, var.product, var.environment)\n  network       = google_compute_network.vpc_network.self_link\n  region        = var.gcp_region\n  project       = var.gcp_project_id\n  ip_cidr_range = var.network_cidr\n  secondary_ip_range {\n    range_name    = var.pods_cidr_name\n    ip_cidr_range = var.pods_cidr\n  }\n  secondary_ip_range {\n    range_name    = var.services_cidr_name\n    ip_cidr_range = var.services_cidr\n  }\n}\nresource \"google_compute_subnetwork\" \"gke_auto_subnet\" {\n  name          = format(\"gke-auto-%s-%s-%s-subnet\", var.org, var.product, var.environment)\n  network       = google_compute_network.vpc_network.self_link\n  region        = var.gcp_region\n  project       = var.gcp_project_id\n  ip_cidr_range = var.network_auto_cidr\n  secondary_ip_range {\n    range_name    = var.pods_auto_cidr_name\n    ip_cidr_range = var.pods_auto_cidr\n  }\n  secondary_ip_range {\n    range_name    = var.services_auto_cidr_name\n    ip_cidr_range = var.services_auto_cidr\n  }\n}\nEOF\n</code></pre> <p>Note</p> <p>Notice power of terraform outputs. Here we link <code>subnet</code> with our VPC <code>network</code> using <code>google_compute_network.vpc_network.self_link</code> <code>output</code> value of created <code>network</code> in previous step.</p> <p>Define variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\n\n# variables used to create VPC subnets\n\nvariable \"network_cidr\" {\n  type = string\n}\nvariable \"pods_cidr\" {\n  type = string\n}\nvariable \"pods_cidr_name\" {\n  type    = string\n  default = \"gke-standard-pods\"\n}\nvariable \"services_cidr\" {\n  type = string\n}\nvariable \"services_cidr_name\" {\n  type    = string\n  default = \"gke-standard-services\"\n}\nvariable \"network_auto_cidr\" {\n  type = string\n}\nvariable \"pods_auto_cidr\" {\n  type = string\n}\nvariable \"pods_auto_cidr_name\" {\n  type    = string\n  default = \"gke-auto-pods\"\n}\nvariable \"services_auto_cidr\" {\n  type = string\n}\nvariable \"services_auto_cidr_name\" {\n  type    = string\n  default = \"gke-auto-services\"\n}\nEOF\n</code></pre> <p>Define outputs:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"subnet_selflink\" {\n  value = \"\\${google_compute_subnetwork.gke_standard_subnet.self_link}\"\n}\noutput \"subnet_auto_selflink\" {\n  value = \"\\${google_compute_subnetwork.gke_auto_subnet.self_link}\"\n}\nEOF\n</code></pre> <p>Task N1: Update <code>terraform.tfvars</code> file values with following information:</p> <ul> <li>Node Range: See column <code>subnet</code> in above table for <code>dev</code> cluster</li> <li>GKE Standard Secondary Ranges:<ul> <li>Service range CIDR: See column <code>srv range</code> in above table for <code>dev</code> cluster</li> <li>Pods range CIDR: See column <code>pod range</code> in above table for <code>dev</code> cluster</li> </ul> </li> <li>GKE Auto Secondary Ranges:<ul> <li>Service range CIDR: See column <code>srv range</code> in above table for <code>dev</code> cluster</li> <li>Pods range CIDR: See column <code>pod range</code> in above table for <code>dev</code> cluster</li> </ul> </li> </ul> <pre><code>Table 1 \nProject   | Subnet Name       |     subnet       |    pod range       |    srv range        | kubectl api range\napp 1 Dev | gke-standard      |   10.130.0.0/24  |     10.0.0.0/16    |     10.100.0.0/23   |     172.16.0.0/28\n          | gke-auto          |   10.131.0.0/24  |     10.1.0.0/16    |     10.100.2.0/23   |     172.16.0.16/28\n</code></pre> <pre><code>edit terraform.tfvars\n</code></pre> <p>Update the file with values according to VPC subnet design in </p> <pre><code>//gke-standard subnet vars\nnetwork_cidr       = \"10.130.0.0/24\"\npods_cidr          = \"10.0.0.0/16\"\nservices_cidr      = \"10.100.0.0/23\"\n//gke-auto subnet vars\nnetwork_auto_cidr  = \"10.131.0.0/24\"\npods_auto_cidr     = \"10.1.0.0/16\"\nservices_auto_cidr = \"10.100.2.0/23\"\n</code></pre> <pre><code>#gke-standard subnet vars\nnetwork_cidr       = \"TODO\"\npods_cidr          = \"TODO\"\nservices_cidr      = \"TODO\"\n#gke-auto subnet vars\nnetwork_auto_cidr  = \"TODO\"\npods_auto_cidr     = \"TODO\"\nservices_auto_cidr = \"TODO\"\nEOF\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Create VPC:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Review created subnet:</p> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=dev\n</code></pre> <pre><code>gcloud compute networks subnets list\n</code></pre> <pre><code>gcloud compute networks subnets describe gke-standard-$ORG-$PRODUCT-$ENV-subnet --region us-central1\ngcloud compute networks subnets describe gke-auto-$ORG-$PRODUCT-$ENV-subnet --region us-central1\n</code></pre> <p>Example of the Output:</p> <pre><code>enableFlowLogs: false\nfingerprint: sWtjHJyqrM8=\ngatewayAddress: 10.130.0.1\nid: '251139681714314790'\nipCidrRange: 10.130.0.0/24\nkind: compute#subnetwork\nlogConfig:\n  enable: false\nname: gke-standard-archy-notepad-dev-subnet\nnetwork: https://www.googleapis.com/compute/v1/projects/archy-notepad-dev-898/global/networks/vpc-archy-notepad-dev\nprivateIpGoogleAccess: false\nprivateIpv6GoogleAccess: DISABLE_GOOGLE_ACCESS\npurpose: PRIVATE\nregion: https://www.googleapis.com/compute/v1/projects/archy-notepad-dev-898/regions/us-central1\nsecondaryIpRanges:\n- ipCidrRange: 10.0.0.0/16\n  rangeName: gke-standard-pods\n- ipCidrRange: 10.100.0.0/23\n  rangeName: gke-standard-services\nselfLink: https://www.googleapis.com/compute/v1/projects/archy-notepad-dev-898/regions/us-central1/subnetworks/gke-standard-archy-notepad-dev-subnet\nstackType: IPV4_ONLY\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>Networking-&gt;VPC Networks -&gt; Click VPC network and check `Subnet` tab\n</code></pre> <p>Task N2: Update  <code>subnets.tf</code> so that  <code>google_compute_subnetwork</code> resource supports following features:</p> <pre><code>* Flow Logs Configuration\n  * Aggregation interval: 15 min  # reduce cost for VPC Flow logging, as by default interval is 5 second\n  * Flow logs Metadata Config: EXCLUDE_ALL_METADATA \n* Private IP Google Access\n</code></pre> <p>Hint</p> <p>Use google_compute_subnetwork resource to update subnet configurations with terraform.</p> <pre><code>edit subnets.tf\nTODO\n</code></pre> <p>Solution:</p> <pre><code>edit subnets.tf\n</code></pre> <pre><code>resource \"google_compute_subnetwork\" \"gke_standard_subnet\" {\n...\n  private_ip_google_access = true\n  log_config {\n    aggregation_interval = \"INTERVAL_15_MIN\"\n    metadata             = \"EXCLUDE_ALL_METADATA\"\n  }\n\nresource \"google_compute_subnetwork\" \"gke_auto_subnet\" {\n...\n  private_ip_google_access = true\n  log_config {\n    aggregation_interval = \"INTERVAL_15_MIN\"\n    metadata             = \"EXCLUDE_ALL_METADATA\"\n  }\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Create VPC:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Review created subnet:</p> <pre><code>gcloud compute networks subnets describe gke-standard-$ORG-$PRODUCT-$ENV-subnet --region us-central1\ngcloud compute networks subnets describe gke-auto-$ORG-$PRODUCT-$ENV-subnet --region us-central1\n</code></pre> <p>Example of the Output:</p> <pre><code>creationTimestamp: '2022-10-03T19:56:09.579-07:00'\nenableFlowLogs: true\nfingerprint: Y8-BjoK1gK8=\ngatewayAddress: 10.130.0.1\nid: '251139681714314790'\nipCidrRange: 10.130.0.0/24\nkind: compute#subnetwork\nlogConfig:\n  aggregationInterval: INTERVAL_15_MIN\n  enable: true\n  filterExpr: 'true'\n  flowSampling: 0.5\n  metadata: EXCLUDE_ALL_METADATA\nname: gke-standard-archy-notepad-dev-subnet\nnetwork: https://www.googleapis.com/compute/v1/projects/archy-notepad-dev-898/global/networks/vpc-archy-notepad-dev\nprivateIpGoogleAccess: true\nprivateIpv6GoogleAccess: DISABLE_GOOGLE_ACCESS\npurpose: PRIVATE\nregion: https://www.googleapis.com/compute/v1/projects/archy-notepad-dev-898/regions/us-central1\nsecondaryIpRanges:\n- ipCidrRange: 10.0.0.0/16\n  rangeName: gke-standard-pods\n- ipCidrRange: 10.100.0.0/23\n  rangeName: gke-standard-services\nselfLink: https://www.googleapis.com/compute/v1/projects/archy-notepad-dev-898/regions/us-central1/subnetworks/gke-standard-archy-notepad-dev-subnet\nstackType: IPV4_ONLY\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#13-create-a-cloud-router","title":"1.3 Create a Cloud router","text":"<p>Create Cloud Router for <code>custom mode</code> network (VPC), in the same region as the instances that will use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway.</p> <p>Task N3: Define a <code>google_compute_router</code> inside <code>router.tf</code> that will be able to create a NAT router so the nodes can reach DockerHub and external APIs from private cluster, using following parameters:</p> <ul> <li>Create router for custom <code>vpc_network</code> created above with terraform</li> <li>Same project as VPC</li> <li>Same region as VPC</li> <li>Router name: <code>gke-net-router</code></li> <li>Local BGP Autonomous System Number (ASN): 64514</li> </ul> <p>Use following reference documentation to create Cloud Router Resource with terraform.</p> <p>Hint</p> <p>You can automatically recover <code>vpc</code> name from terraform output like this: <code>google_compute_network.vpc_network.self_link</code>.</p> <pre><code>edit router.tf\nTODO\nEOF\n</code></pre> <p>Save the populated file.</p> <p>Solution:</p> <pre><code>edit router.tf\n</code></pre> <pre><code>resource \"google_compute_router\" \"gke_net_router\" {\n  project = var.gcp_project_id\n  name    = \"gke-net-router\"\n  region  = var.gcp_region\n  network = google_compute_network.vpc_network.self_link\n  bgp {\n    asn = 64514\n  }\n}\nEOF\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Create Cloud Router:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify created Cloud Router:</p> <p>CLI:</p> <pre><code>gcloud compute routers list\ngcloud compute routers describe gke-net-router --region us-central1\n</code></pre> <p>Output:</p> <pre><code>bgp:\n  advertiseMode: DEFAULT\n  asn: 64514\n  keepaliveInterval: 20\nkind: compute#router\nname: gke-net-router\n</code></pre> <p>UI:</p> <pre><code>Networking -&gt; Hybrid Connectivity -&gt; Cloud Routers\n</code></pre> <p>Result</p> <p>Router resource has been created for VPC Network</p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#14-create-a-cloud-nat","title":"1.4 Create a Cloud Nat","text":"<p>Set up a simple Cloud Nat configuration using google_compute_router_nat resource, which will <code>automatically</code> allocates the necessary external IP addresses to provide NAT services to a region. </p> <p>When you use auto-allocation, Google Cloud reserves IP addresses in your project automatically. </p> <pre><code>cat &lt;&lt;EOF &gt;&gt; cloudnat.tf\nresource \"google_compute_router_nat\" \"gke_cloud_nat\" {\n  project                = var.gcp_project_id\n  name                   = \"gke-cloud-nat\"\n  router                 = google_compute_router.gke_net_router.name\n  region                 = var.gcp_region\n  nat_ip_allocate_option = \"AUTO_ONLY\"\n  source_subnetwork_ip_ranges_to_nat = \"ALL_SUBNETWORKS_ALL_IP_RANGES\"\n}\nEOF\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Create Cloud Router:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify created <code>Cloud Nat</code>:</p> <p>CLI:</p> <pre><code># List available Cloud Nat Routers\ngcloud compute routers nats list --router gke-net-router --router-region us-central1\n# Describe Cloud Nat Routers `gke-cloud-nat`:\ngcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1\n</code></pre> <p>Output:</p> <pre><code>enableEndpointIndependentMapping: true\nendpointTypes:\n- ENDPOINT_TYPE_VM\nicmpIdleTimeoutSec: 30\nname: gke-cloud-nat\nnatIpAllocateOption: AUTO_ONLY\nsourceSubnetworkIpRangesToNat: ALL_SUBNETWORKS_ALL_IP_RANGES\ntcpEstablishedIdleTimeoutSec: 1200\ntcpTransitoryIdleTimeoutSec: 30\nudpIdleTimeoutSec: 30\n</code></pre> <p>UI:</p> <pre><code>Networking -&gt; Network Services -&gt; Cloud NAT\n</code></pre> <p>Result</p> <p>A NAT service created in a router</p> <p>Task N4: Additionally turn <code>ON</code> logging feature for <code>ALL</code> log types of communication for Cloud Nat</p> <pre><code>edit cloudnat.tf\nTODO\n</code></pre> <pre><code>edit cloudnat.tf\n</code></pre> <p>Solution:</p> <p>Correctly add following configuration:</p> <pre><code>  log_config {\n    filter = \"ALL\"\n    enable = true\n  }\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Update Cloud Nat Configuration:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>Apply complete! Resources: 0 added, 1 changed, 0 destroyed.\n</code></pre> <pre><code>gcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1\n</code></pre> <p>Result</p> <p>Cloud Nat now supports Logging. Cloud NAT logging allows you to log NAT connections and errors. When Cloud NAT logging is enabled, one log entry can be generated for each of the following scenarios:</p> <ul> <li>When a network connection using NAT is created.</li> <li>When a packet is dropped because no port was available for NAT.</li> </ul>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#15-create-a-private-gke-cluster-and-delete-default-node-pool","title":"1.5 Create a Private GKE Cluster and delete default node pool","text":""},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#151-enable-gcp-beta-provider","title":"1.5.1 Enable GCP Beta Provider","text":"<p>In order to create a GKE cluster with terraform we will be leveraging google_container_cluster resource.</p> <p>Some of  <code>google_container_cluster</code> arguments, such VPC-Native networking mode, VPA, Istio, CSI Driver add-ons, requires  <code>google-beta</code> Provider.</p> <p>The <code>google-beta</code> provider is distinct from the <code>google</code> provider in that it supports GCP products and features that are in beta, while google does not. Fields and resources that are only present in google-beta will be marked as such in the shared provider documentation.</p> <p>Configure and Initialize GCP Beta Provider, similar to how we did it for GCP Provider in 1.3.3 Initialize Terraform update <code>provider.tf</code> and <code>main.tf</code> configuration files.</p> <pre><code>rm provider.tf\ncat &lt;&lt;EOF &gt;&gt; provider.tf\nterraform {\n  required_providers { \n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~&gt; 4.37.0\"\n    }\n    google-beta = {\n      source  = \"hashicorp/google-beta\"\n      version = \"~&gt; 4.37.0\"\n    }\n  }\n}\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt;  main.tf\nprovider \"google-beta\" {\n  project = var.gcp_project_id\n  region  = var.gcp_region\n}\nEOF\n</code></pre> <p>Initialize <code>google-beta</code> provider plugin:</p> <pre><code>terraform init\n</code></pre> <p>Success</p> <p>Terraform has been successfully initialized!</p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#152-enable-kubernetes-engine-api","title":"1.5.2 Enable Kubernetes Engine API","text":"<p>Kubernetes Engine API used to build and manages container-based applications, powered by the open source Kubernetes technology. Before starting  GKE cluster creation it is required to enable it.</p> <p>Task N5: Enable <code>container.googleapis.com</code> in <code>services.tf</code> file similar to what we already did in 2.2 Enable required GCP Services API. Make sure that service is set to <code>disable_on_destroy=false</code>, which  helps to prevent errors during redeployments of the system (aka immutable infra)</p> <pre><code>edit services.tf\nTODO\n</code></pre> <p>Solution:</p> <pre><code>edit services.tf\n</code></pre> <pre><code>resource \"google_project_service\" \"gke_api\" {\n  service = \"container.googleapis.com\"\n  disable_on_destroy = false\n}\n</code></pre> <p>Note</p> <p>Adding <code>disable_on_destroy=false</code> helps to prevent errors during redeployments of the system.</p> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Update Cloud Nat Configuration:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#153-create-a-private-gke-cluster-and-delete-default-node-pool","title":"1.5.3 Create a Private GKE Cluster and delete default node pool","text":"<p>Using Terraform resource google_container_cluster resource create a Regional, Private GKE cluster, with following characteristics:</p> <p>Cluster Configuration:</p> <ul> <li>Cluster name: <code>gke-$ORG-$PRODUCT-$ENV</code></li> <li>GKE Control plane is replicated across three zones of a region: <code>us-central1</code></li> <li>Private cluster with unrestricted access to the public endpoint:<ul> <li>Cluster Nodes access: Private Node GKE Cluster with Public API endpoint</li> <li>Cluster K8s API access: with unrestricted access to the public endpoint</li> </ul> </li> <li>Cluster Node Communication: <code>VPC Native</code></li> <li>Secondary pod range with name: <code>gke-standard-pods</code></li> <li>Secondary service range with name: <code>gke-standard-services</code></li> <li>GKE Release channel: <code>regular</code></li> <li>GKE master and node version: \"1.22.12-gke.300\"</li> <li>Terraform Provider: <code>google-beta</code></li> <li>Timeouts to finish creation of cluster and deletion of default node pool: 30M</li> <li>Features:<ul> <li>Enable Cilium based Networking DataplaneV2: <code>enable-dataplane-v2</code></li> <li>Configure Workload Identity Pool: <code>PROJECT_ID.svc.id.goog</code></li> <li>Enable HTTP Load-balancing Addon: <code>http_load_balancing</code></li> </ul> </li> </ul> <p>Terraform resource google_container_node_pool creates a <code>custom</code> GKE Node Pool.</p> <p>Custom Node Pool Configuration:</p> <ul> <li>The name of a GCE machine (VM) type: <code>e2-micro</code></li> <li>Node count: 1 per zone</li> <li>Node images: <code>Container-Optimized OS</code></li> <li>GKE Node Pool Boot Disk Size: \"100 Gb\"</li> </ul> <p>Note</p> <p>Why delete default node pool? The <code>default</code> node pools cause trouble with managing the cluster, when created with terraform as it is not part of the terraform lifecycle. GKE Architecture Best Practice recommends to delete <code>default</code> node pool and create a <code>custom</code> one instead and manage the node pools explicitly.</p> <p>Note</p> <p>Why define <code>Timeouts</code> for gke resource? Normally GKE creation takes few minutes. However, in our case we creating GKE Cluster, and then system cordon, drain and then destroy default node pool. This process may take 10-20 minutes and we want to make sure terraform will not time out during this time.</p> <p>Step 1: Let's define GKE resource first:</p> <pre><code>edit gke.tf\n</code></pre> <pre><code>resource \"google_container_cluster\" \"primary_cluster\" {\n  provider = google-beta\n\n  project = var.gcp_project_id\n\n  name               = format(\"gke-standard-%s-%s-%s\", var.org, var.product, var.environment)\n  min_master_version = var.kubernetes_version\n  network            = google_compute_network.vpc_network.self_link\n  subnetwork         = google_compute_subnetwork.gke_standard_subnet.self_link\n\n  location                    = var.gcp_region\n  logging_service             = var.logging_service\n  monitoring_service          = var.monitoring_service\n\n  remove_default_node_pool = true\n  initial_node_count       = 1\n\n  private_cluster_config {\n    enable_private_nodes   = var.enable_private_nodes\n    enable_private_endpoint = var.enable_private_endpoint\n    master_ipv4_cidr_block = var.master_ipv4_cidr_block\n  }\n\n  # Enable Dataplane V2\n  datapath_provider = \"ADVANCED_DATAPATH\"\n\n  release_channel {\n    channel = \"REGULAR\"\n  }\n\n\n  addons_config {\n    http_load_balancing {\n      disabled = var.disable_http_load_balancing\n    }\n\n  ip_allocation_policy {\n    cluster_secondary_range_name  = var.pods_cidr_name\n    services_secondary_range_name = var.services_cidr_name\n  }\n\n  timeouts {\n    create = \"30m\"\n    update = \"30m\"\n    delete = \"30m\"\n  }\n\n  workload_identity_config {\n    workload_pool  = \"${var.gcp_project_id}.svc.id.goog\"\n  }\n}\n</code></pre> <p>Step 2: Next define GKE cluster specific variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke_variables.tf\n\n# variables used to create GKE Cluster Control Plane\n\nvariable \"kubernetes_version\" {\n  default     = \"\"\n  type        = string\n  description = \"The GKE version of Kubernetes\"\n}\n\nvariable \"logging_service\" {\n  description = \"The logging service that the cluster should write logs to.\"\n  default     = \"logging.googleapis.com/kubernetes\"\n}\n\nvariable \"monitoring_service\" {\n  default     = \"monitoring.googleapis.com/kubernetes\"\n  description = \"The GCP monitoring service scope\"\n}\n\nvariable \"disable_http_load_balancing\" {\n  default     = false\n  description = \"Enable HTTP Load balancing GCP integration\"\n}\n\nvariable \"pods_range_name\" {\n  description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to pods\"\n  default     = \"\"\n}\n\nvariable \"services_range_name\" {\n  description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to services\"\n  default     = \"\"\n}\n\nvariable \"enable_private_nodes\" {\n  default     = false\n  description = \"Enable Private-IP Only GKE Nodes\"\n}\n\nvariable \"enable_private_endpoint\" {\n  default     = false\n  description = \"When true, the cluster's private endpoint is used as the cluster endpoint and access through the public endpoint is disabled.\"\n}\n\nvariable \"master_ipv4_cidr_block\" {\n  description = \"The ipv4 cidr block that the GKE masters use\"\n}\n\nvariable \"release_channel\" {\n  type        = string\n  default     = \"\"\n  description = \"The release channel of this cluster\"\n}\nEOF\n</code></pre> <p>Step 3: Define GKE cluster specific outputs:</p> <pre><code>edit outputs.tf\n</code></pre> <p>Add following outputs and save file:</p> <pre><code>output \"id\" {\n  value = \"${google_container_cluster.primary_cluster.id}\"\n}\noutput \"endpoint\" {\n  value = \"${google_container_cluster.primary_cluster.endpoint}\"\n}\noutput \"master_version\" {\n  value = \"${google_container_cluster.primary_cluster.master_version}\"\n}\n</code></pre> <p>Task N6: Complete <code>terraform.tfvars</code> with required values to GKE Cluster specified above:</p> <pre><code>edit terraform.tfvars\n</code></pre> <pre><code>//gke specific\nenable_private_nodes   = \"TODO\"  \nmaster_ipv4_cidr_block = \"TODO\"    # Using Table 1 `kubectl api range` for GKE Standard\nkubernetes_version     = \"TODO\"    # From GKE Cluster requirements\nrelease_channel        = \"TODO\"    # From GKE Cluster requirements\nEOF\n</code></pre> <p>Solution: </p> <pre><code>edit terraform.tfvars\n</code></pre> <pre><code>//gke standard specific\nenable_private_nodes   = \"true\"\nmaster_ipv4_cidr_block = \"172.16.0.0/28\"\nkubernetes_version     = \"1.22.12-gke.300\"\nrelease_channel        = \"REGULAR\"\n</code></pre> <p>In the next step, we going to create a <code>custom</code> GKE Node Pool.</p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#154-create-a-gke-custom-node-pool","title":"1.5.4 Create a GKE <code>custom</code> Node pool","text":"<p>Using google_container_node_pool resource create a <code>custom</code> GKE Node Pool with following characteristics:</p> <p>Node Pool Configuration:</p> <p>Custom Node Pool Configuration:</p> <ul> <li>The name of a GCE machine (VM) type: <code>e2-micro</code></li> <li>Node count: 1 per zone</li> <li>Node images: <code>Container-Optimized OS</code></li> <li>GKE Node Pool Boot Disk Size: \"100 Gb\"</li> </ul> <p>Step 1: Let's define GKE resource first:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke.tf\n\n#Node Pool Resource\nresource \"google_container_node_pool\" \"custom-node_pool\" {\n\n  name       = \"main-pool\"\n  location = var.gcp_region\n  project  = var.gcp_project_id\n  cluster    = google_container_cluster.primary_cluster.name\n  node_count = var.gke_pool_node_count\n  version    = var.kubernetes_version\n\n  node_config {\n    image_type   = var.gke_pool_image_type\n    disk_size_gb = var.gke_pool_disk_size_gb\n    disk_type    = var.gke_pool_disk_type\n    machine_type = var.gke_pool_machine_type\n  }\n\n  timeouts {\n    create = \"10m\"\n    delete = \"10m\"\n  }\n\n  lifecycle {\n    ignore_changes = [\n      node_count\n    ]\n  }\n}\nEOF\n</code></pre> <p>Step 2: Next define GKE cluster specific variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke_variables.tf\n\n#Node Pool specific variables\nvariable \"gke_pool_machine_type\" {\n  type = string\n}\nvariable \"gke_pool_node_count\" {\n  type = number\n}\nvariable \"gke_pool_disk_type\" {\n  type = string\n  default = \"pd-standard\"\n}\nvariable \"gke_pool_disk_size_gb\" {\n  type = string\n}\nvariable \"gke_pool_image_type\" {\n  type = string\n}\nEOF\n</code></pre> <p>Task 7 (Continued): Complete <code>terraform.tfvars</code> with required values to GKE Node Pool values specified above:</p> <pre><code>edit terraform.tfvars\n</code></pre> <pre><code>//pool specific\ngke_pool_node_count   = \"TODO\"\ngke_pool_image_type   = \"TODO\"\ngke_pool_disk_size_gb = \"TODO\"\ngke_pool_machine_type = \"TODO\"\n</code></pre> <p>Solution: </p> <pre><code>//pool specific\ngke_pool_node_count   = \"1\"\ngke_pool_image_type   = \"COS\"\ngke_pool_disk_size_gb = \"100\"\ngke_pool_machine_type = \"e2-micro\"\n</code></pre> <p>Step 3: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Step 4: Create GKE Cluster and Node Pool:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>google_container_cluster.primary_cluster: Creating...\n...\ngoogle_container_cluster.primary_cluster: Creation complete after 20m9s \ngoogle_container_node_pool.custom-node_pool: Creating...\ngoogle_container_node_pool.custom-node_pool: Creation complete after 2m10s\n</code></pre> <p>Note</p> <p>GKE Cluster Control plain and deletion of <code>default</code> Node Pool may take about 12 minutes, creation custom Node Pool another 6 minutes.</p> <p>Verify Cluster has been created:</p> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=dev\n</code></pre> <pre><code>gcloud container clusters list\ngcloud compute networks subnets describe gke-standard-$ORG-$PRODUCT-$ENV --region us-central1\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#155-update-gke-node-pool-to-support-auto-upgrade-and-auto-recovery-features","title":"1.5.5 Update GKE Node Pool to support Auto Upgrade and Auto Recovery features","text":"<p>Note</p> <p>GKE Master Nodes are managed by Google and get's upgraded automatically. Users can only specify Maintenance Window if they have preference for that process to occur (e.g. after busy hours). Users can however control Node Pool upgrade lifecycle. They can choose to do it themselves or with Auto Upgrade.</p> <p>Task N8: Using google_container_node_pool resource update node pool to turn-off <code>Auto Upgrade</code> and <code>Auto Repair</code> features that enabled by default.</p> <pre><code>edit gke.tf\nTODO\n</code></pre> <p>Solution: </p> <p>Replace channel release from <code>REGULAR</code> to <code>UNSPECIFIED</code> in edit <code>gke.tf</code> and turn off <code>auto_repair</code> and <code>auto_upgrade</code></p> <pre><code>edit gke.tf\n</code></pre> <pre><code>edit gke.tf\n</code></pre> <pre><code>  release_channel {\n    channel = \"UNSPECIFIED\"\n  }\n</code></pre> <pre><code>  management {\n    auto_repair  = false\n    auto_upgrade = false\n  }\n</code></pre> <p>Step 3: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>No errors.</p> <p>Step 4: Update GKE Cluster Node Pool configuration:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Summary</p> <p>Congrats! You've now learned how to deploy production grade GKE clusters.</p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#16-create-auto-mode-gke-cluster-with-terraform","title":"1.6 Create Auto Mode GKE Cluster with Terraform","text":"<p>Step 1: Define GKE Autopilot resource first:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke_auto.tf\nresource \"google_container_cluster\" \"auto_cluster\" {\n  provider = google-beta\n\n  project = var.gcp_project_id\n\n  name               = format(\"gke-auto-%s-%s-%s\", var.org, var.product, var.environment)\n  min_master_version = var.kubernetes_version\n  network            = google_compute_network.vpc_network.self_link\n  subnetwork         = google_compute_subnetwork.gke_auto_subnet.self_link\n\n  location                    = var.gcp_region\n  logging_service             = var.logging_service\n  monitoring_service          = var.monitoring_service\n\n# Enable Autopilot for this cluster\n  enable_autopilot = true\n\n# Private Autopilot GKE cluster\n  private_cluster_config {\n    enable_private_nodes   = var.enable_private_nodes\n    enable_private_endpoint = var.enable_private_endpoint\n    master_ipv4_cidr_block = var.auto_master_ipv4_cidr_block\n  }\n\n# Configuration options for the Release channel feature, which provide more control over automatic upgrades of your GKE clusters.\n  release_channel {\n    channel = \"REGULAR\"\n  }\n\n# Configuration of cluster IP allocation for VPC-native clusters\n  ip_allocation_policy {\n    cluster_secondary_range_name  = var.pods_auto_cidr_name\n    services_secondary_range_name = var.services_auto_cidr_name\n  }\n\n  timeouts {\n    create = \"20m\"\n    update = \"20m\"\n    delete = \"20m\"\n  }\n}\n</code></pre> <p>Step 2: Next define GKE cluster specific variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke_variables.tf\n\n# variables used to create GKE AutoPilot Cluster Control Plane\n\nvariable \"auto_master_ipv4_cidr_block\" {\n  description = \"The ipv4 cidr block that the GKE masters use\"\n}\nEOF\n</code></pre> <p>Step 3: Define GKE cluster specific outputs:</p> <pre><code>edit outputs.tf\n</code></pre> <p>Add following outputs and save file:</p> <pre><code>output \"autopilot_id\" {\n  value = \"${google_container_cluster.auto_cluster.id}\"\n}\noutput \"autopilot_endpoint\" {\n  value = \"${google_container_cluster.auto_cluster.endpoint}\"\n}\noutput \"autopilot_master_version\" {\n  value = \"${google_container_cluster.auto_cluster.master_version}\"\n}\n</code></pre> <p>Update tfvars</p> <pre><code>edit terraform.tfvars\n</code></pre> <pre><code>//gke autopilot specific\nauto_master_ipv4_cidr_block = \"172.16.0.16/28\"\n</code></pre> <p>Step 4: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Step 5: Create GKE Autopilot Cluster:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>google_container_cluster.auto_cluster: Still creating..\n...\ngoogle_container_cluster.auto_cluster: Creation complete after 7m58s \n</code></pre> <p>Verify Cluster has been created:</p> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=dev\n</code></pre> <pre><code>gcloud container clusters list\ngcloud compute networks subnets describe gke-standard-$ORG-$PRODUCT-$ENV --region us-central1\n</code></pre> <p>Task 9: Using google_container_cluster make sure that <code>Maintenance Windows</code> are set to occur Daily on weekdays from 9:00-17:00 UTC-4, but skip weekends, starting from October 2nd:</p> <pre><code>--maintenance-window-start 2022-10-02T09:00:00-04:00 \\\n--maintenance-window-end 2022-10-02T17:00:00-04:00 \\\n--maintenance-window-recurrence 'FREQ=WEEKLY;BYDAY=MO,TU,WE,TH,FR'\n</code></pre> <pre><code>edit gke_auto.tf\nTODO\n</code></pre> <p>Step 6: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Step 7: Create GKE Autopilot Cluster:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <pre><code>google_container_cluster.auto_cluster: Modifying... [id=projects/archy-notepad-dev-898/locations/us-central1/clusters/gke-auto-archy-notepad-dev]\ngoogle_container_cluster.auto_cluster: Modifications complete after 2s [id=projects/archy-notepad-dev-898/locations/us-central1/clusters/gke-auto-archy-notepad-dev]\n</code></pre> <p>Browse GKE UI and verify that GKE Autopilot is configured with Maintenance window.</p> <p>Success</p> <p>We can now create GKE Autopilot cluster </p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#17-optional-repeatable-infrastructure","title":"1.7 (Optional) Repeatable Infrastructure","text":"<p>When you doing IaC it is important to insure that you can both create and destroy resources consistently. This is especially important when doing CI/CD testing.</p> <p>Step 3: Destroy all resources:</p> <pre><code>terraform destroy -var-file terraform.tfvars\n</code></pre> <p>No errors.</p> <p>Step 4: Recreate all resources:</p> <pre><code>terraform plan -var-file terraform.tfvars\nterraform apply -var-file terraform.tfvars\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#18-create-documentation-for-terraform-code","title":"1.8  Create Documentation for terraform code","text":"<p>Documentation for your terraform code is an important part of IaC. Make sure all your variables have a good description!</p> <p>There are community tools that have been developed to make the documentation process smoother, in terms of documenting Terraform resources and requirements.Its good practice to also include a usage example snippet.</p> <p>Terraform-Docs is a good example of one tool that can generate some documentation based on the description argument of your Input Variables, Output Values, and from your required_providers configurations.</p> <p>Task N10: Create Terraform Documentation for your infrastructure.</p> <p>Step 1 Install the terraform-docs cli to your Google CloudShell environment: </p> <pre><code>curl -sSLo ./terraform-docs.tar.gz https://terraform-docs.io/dl/v0.16.0/terraform-docs-v0.16.0-$(uname)-amd64.tar.gz\ntar -xzf terraform-docs.tar.gz\nchmod +x terraform-docs\nsudo mv terraform-docs /usr/local/bin/\nterraform-docs\n</code></pre> <p>Generating terraform documentation with Terraform Docs:</p> <pre><code>cd ~/$student_name-notepad/ycit020_module4/foundation-infrastructure\nterraform-docs markdown . &gt; README.md\ncd ~/$student_name-notepad/ycit020_module4/notepad-infrastructure\nterraform-docs markdown . &gt; README.md\n</code></pre> <p>Verify created documentation:</p> <pre><code>edit README.md\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#19-commit-readme-doc-to-repository-and-share-it-with-instructorteacher","title":"1.9 Commit Readme doc to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>ycit020_module4</code> folder using the following Git commands:</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git add .\ngit commit -m \"TF manifests for Module 4 Assignment\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre> <p>Result</p> <p>Your instructor will be able to review you code and grade it.</p>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#110-cleanup","title":"1.10 Cleanup","text":"<p>We only going to cleanup GCP Service foundation layer, as we going to use GCP project in future.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module4/notepad-infrastructure\nterraform destroy -var-file terraform.tfvars\n</code></pre>"},{"location":"020_Module_4_Assignment_Production_GKE_terraform_solution/#2-workaround-for-project-quota-issue","title":"2. Workaround for Project Quota issue","text":"<p>If you see following error during project creation in foundation layer:</p> <pre><code>Error: Error setting billing account \"010BE6-CA1129-195D77\" for project \"projects/ayrat-notepad-dev-244\": googleapi: Error 400: Precondition check failed., failedPrecondition\n</code></pre> <p>This is due to our Billing account has quota of 5 projects per account.</p> <p>To solve this issue find all unused accounts:</p> <pre><code>gcloud beta billing projects list --billing-account $ACCOUNT_ID\n</code></pre> <p>And unlink them, so you have less then 5 projects per account:</p> <pre><code>gcloud beta billing projects unlink $PROJECT_ID\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/","title":"020 Module 4 sol Terraform GCP Foundation","text":"<p>Lab 3 Creating Production GKE Cluster via IaC using Terraform</p> <p>Objective:</p> <ul> <li>Learn Terraform Commands</li> <li>Learn GCP Terraform Provider</li> <li>Learn Terraform variables</li> <li>Store TF State in GCS buckets</li> <li>Learn how to create GCP resources with Terraform </li> </ul>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#working-with-git-and-iac","title":"Working with Git and IaC","text":"<p>Git-based workflows, merge requests and peer reviews create a level of documented transparency that is great for security teams and audits. They ensure every change is documented as well as the metadata surrounding the change, answering the why, who, when and what questions.</p> <p>In this set of exercises you will continue using <code>notepad</code> Google Source Repository that already contains you application code, <code>dockerfiles</code> and kubernetes manifests.</p> <p>We will use so called Monorepo approach, and store our terraform IAC configuration in the same repo as our application code and Kubernetes manifests.</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#add-your-iac-code-to-your-repository","title":"Add your IaC code to your repository","text":"<p>Step 1 Go into your personal Google Cloud Source Repository:</p> <pre><code>MY_REPO=your_student_id-notepad\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$MY_REPO\n</code></pre> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 2 Create <code>foundation-infrastructure</code> and <code>notepad-infrastructure</code> folders that will contains respective terraform configurations</p> <pre><code>cd ~/$MY_REPO\nmkdir foundation-infrastructure\nmkdir notepad-infrastructure\n</code></pre> <p>Step 3 Create a <code>README.md</code> file describing <code>foundation-infrastructure</code> folder purpose:  </p> <pre><code>cat &lt;&lt;EOF&gt; foundation-infrastructure/README.md\n\n# Creation of GCP Foundation Layer\n\nThis step, is focused on creating GCP Foundation: folders (if any) and service projects creation with their respictive GCS bucket to store terraform state and IAM Config.\n\nEOF\n</code></pre> <p>Step 4 Create a <code>README.md</code> file describing <code>notepad-infrastructure</code> folder purpose:  </p> <pre><code>cat &lt;&lt;EOF&gt; notepad-infrastructure/README.md\n# Creation of GCP Services Layer\n\nThis step, is focused on creating gcp services such as GKE, VPC and etc. inside of existing project\n\nEOF\n</code></pre> <p>Step 5 Create a .gitignore file in your working directory:</p> <pre><code>cat&lt;&lt;EOF&gt;&gt; .gitignore\n.terraform.*\n.terraform\nterraform.tfstate*\nEOF\n</code></pre> <p>Note</p> <p>Ignore files are used to tell <code>git</code> not to track files. You should also always include any files with credentials in a <code>gitignore</code> file.</p> <p>List created folder structure along with <code>gitignore</code>:</p> <pre><code>ls -ltra\n</code></pre> <p>Step 6 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"Terraform Folder structure for assignement 3\"\n</code></pre> <p>Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#1-build-gcp-foundation-layer","title":"1 Build GCP Foundation Layer","text":"<p>In this Lab we going to build GCP Foundation layer. As we learned in the class this includes Org Structure creation with Folders and Projects, creation IAM Roles and assigning them to users and groups in organization. This Layer usually build be DevOps or Infra team that.</p> <p>Since we don't have organization registered for each student, we going to skip creation of folders and IAM groups. We going to start from creation of a Project, deleting <code>default</code> VPC and configuring inside that project a <code>gcs</code> bucket that will be  used in the next terraform layer to store a Terraform state.</p> <p>Part 1: Create Terraform Configurations file to create's  GCP Foundation Layer:</p> <p>Layer 1 From existing  (we going to call it seeding) project that you currently use to store code in Google Cloud Source Repository:</p> <ul> <li>Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf</li> <li>Create a new <code>notepad-dev</code> Project<ul> <li>Delete Default VPC</li> </ul> </li> <li>Create a bucket in this project to store terraform state</li> </ul> <p>Part 2: Create Terraform Configurations file that create's  GCP Services Layer:</p> <p>Layer 2 From <code>notepad-dev</code> GCP Project:</p> <ul> <li>Enable Google Project Service APIs</li> <li>Create VPC (google_compute_network) and Subnet (google_compute_subnetwork)</li> <li>Create Cloud Nat (google_compute_router) and (google_compute_router_nat)</li> <li>Private GKE<ul> <li>Private Nodes with Public API Endpoint (for simplicity)</li> </ul> </li> </ul>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#11-installing-terraform","title":"1.1 Installing Terraform","text":"<p>GCP Cloud Shell comes with many common tools pre-installed including <code>terraform</code></p> <p>Verify and validate the version of Terraform that is installed:</p> <pre><code>terraform --version\n</code></pre> <p>If you want to use specific version of terraform or want to install terraform in you local machine use following link to download binary.</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#12-configure-gcp-credentials-for-terraform","title":"1.2 Configure GCP credentials for Terraform","text":"<p>If you would like to use terraform on GCP you have 2 options:</p> <p>1). Using you user credentials, great option for testing terraform from you laptop and for learning purposes.</p> <p>2). Using service account, great option if you going to use terraform with CI/CD system and fully automate Terraform deployment.</p> <p>In this Lab we going to use Option 1 - using user credentials.</p> <p>Step 1: In order to make requests against the GCP API, you need to authenticate to prove that it's you making the request.  The preferred method of provisioning resources with Terraform on your workstation is to use the Google Cloud SDK (Option 1) </p> <pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#13-initializing-terraform","title":"1.3 Initializing Terraform","text":"<p>Terraform relies on <code>providers</code> to interact with remote systems. Every resource type is implemented by a provider; without <code>providers</code>, Terraform can't manage any kind of infrastructure; in order for terraform to install and use a provider it must be declared.</p> <p>In this exercise you will declare and configure the Terraform provider(s) that will be used for the rest of the Lab.</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#131-declare-gcp-terraform-providers","title":"1.3.1 Declare GCP Terraform Providers","text":"<p>The <code>required_providers</code> block defines the providers terraform will use to provision resources and their source.</p> <ul> <li> <p><code>version</code>: The version argument is optional and is used to tell terraform to pick a particular version from the available versions</p> </li> <li> <p><code>source</code>: The source is the provider registry e.g. hashicorp/gcp is the short for registry.terraform.io/hashicorp/gcp</p> </li> </ul> <pre><code>cd ~/$MY_REPO/foundation-infrastructure/\ncat &lt;&lt; EOF&gt;&gt; provider.tf\nterraform {\n  required_providers {\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~&gt; 3.70.0\"\n    }\n  }\n}\nEOF\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#132-configure-the-gcp-terraform-provider","title":"1.3.2 Configure the GCP Terraform Provider","text":"<p>Providers often require configuration (like endpoint URLs or cloud regions) before they can be used. These configurations are defined by a <code>provider</code> block. Multiple provider configuration blocks can be declared for the same by adding the <code>alias</code> argument</p> <p>Set you current <code>PROJECT_ID</code> value here:</p> <pre><code>export PROJECT_ID=&lt;YOUR_PROJECT_ID&gt;\n</code></pre> <pre><code>cd ~/$MY_REPO/foundation-infrastructure/\ncat &lt;&lt; EOF&gt;&gt; main.tf\nprovider \"google\" {\n  alias   = \"gcp-provider\"\n  project = \"$PROJECT_ID\"\n  region  = \"us-central1\"\n}\nEOF\n</code></pre> <p>Result</p> <p>We configured <code>provider</code>, so that it can provision resource in specified gcp project in <code>us-central1</code> region.</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#133-initialize-gcp-terraform-provider","title":"1.3.3 Initialize GCP Terraform Provider","text":"<p>Now that you have declared and configured the GCP provider for terraform, initialize terraform:</p> <pre><code>cd ~/$MY_REPO/foundation-infrastructure/\nterraform init\n</code></pre> <p>Explore your directory. What has changed?</p> <pre><code>ls -ltra\n</code></pre> <p>Result</p> <p>We can see that new directory <code>.terraform</code> and <code>.terraform.lock.hcl</code> file.</p> <p>Extra!</p> <p>Investigate available providers in the Terraform Provider Registry</p> <p>Select another provider from the list, add it to your required providers, and to your main.tf</p> <p>Run terraform init to load your new provider!</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#14-terraform-variables","title":"1.4 Terraform Variables","text":"<p>Input variables are used to increase your Terraform configuration's flexibility by defining values that can be assigned to customize the configuration. They provide a consistent interface to change how a given configuration behaves. Input variables blocks have a defined format:</p> <p>Input variables blocks have a defined format:</p> <pre><code>variable \"variable_name\" {\n  type = \"The variable type eg. string , list , number\"\n  description = \"A description to understand what the variable will be used for\"\n  default = \"A default value can be provided, terraform will use this if no other value is provided at terraform apply\"\n}\n</code></pre> <p>Terraform CLI defines the following optional arguments for variable declarations:</p> <ul> <li>default - A default value which then makes the variable optional.</li> <li>type - This argument specifies what value types are accepted for the variable.</li> <li>description - This specifies the input variable's documentation.</li> <li>validation - A block to define validation rules, usually in addition to type constraints.</li> <li>sensitive - Limits Terraform UI output when the variable is used in configuration.</li> </ul> <p>In our above example for <code>main.tf</code> we can actually declare <code>project</code> and <code>region</code> as variables, so let's do it.</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#141-declare-input-variables","title":"1.4.1 Declare Input Variables","text":"<p>Variables are declared in a <code>variables.tf</code> file inside your terraform working directory.</p> <p>The label after the variable keyword is a name for the variable, which must be unique among all variables in the same module.  You can choose variable name based on you preference, or in some cases based on agreed in company naming convention. In our case we declaring variables names: <code>gcp_region</code> and <code>gcp_project_id</code></p> <p>We are going to declare a type as a  <code>string</code> variable for <code>gcp_project_id</code> and <code>gcp_region</code>.</p> <p>It is always good idea to specify a clear <code>description</code> for the variable for documentation purpose as well as code reusability and readability.</p> <p>Finally, let's configured <code>default</code> argument. In our case we going to use \"us-central1\" as a <code>default</code> for <code>gcp_region</code> and we going to keep <code>default</code> value empty for <code>gcp_project_id</code>.</p> <pre><code>cat &lt;&lt;EOF&gt; variables.tf\nvariable \"gcp_region\" {\n  type        = string\n  description = \"The GCP Region\"\n  default     = \"us-central1\"\n}\n\nvariable \"gcp_project_id\" {\n  type        = string\n  description = \"The GCP Seeding project ID\"\n  default     = \"\"\n}\nEOF\n</code></pre> <p>Note</p> <p>Explore other type of variables such as <code>number</code>, <code>bool</code> and type constructors such as: list(), set(), map()"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#142-using-variables","title":"1.4.2 Using Variables","text":"<p>Now that you have created your input variables, let's re-create <code>main.tf</code> that currently has the terraform configuration for GCP Provider.</p> <pre><code>rm main.tf\ncat &lt;&lt; EOF&gt;&gt; main.tf\nprovider \"google\" {\n  alias   = \"gcp-provider\"\n  project = var.gcp_project_id\n  region  = var.gcp_region\n}\nEOF\n</code></pre> <p>Test out your terraform configuration:</p> <pre><code>terraform plan\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#143-working-with-variables-files","title":"1.4.3 Working with Variables files.","text":"<p>Create a <code>terraform.tfvars</code> file to hold the values for your variables:</p> <pre><code>export gcp_project_id=&lt;YOUR_PROJECT_ID&gt;\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\ngcp_project_id = \"$PROJECT_ID\"\ngcp_region = \"us-central1\"\nEOF\n</code></pre> <p>Hint</p> <p>using different <code>env-name.tfvars</code> files you can create different set of terraform configuration for the same code. (e.g. code for dev, staging, prod)</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#144-validate-configuration-and-code-syntax","title":"1.4.4 Validate configuration and code syntax","text":"<p>Let's Validate configuration and code syntax that we've added so far.</p> <p>There are several tools that can analyze your Terraform code without running it, including:</p> <p>Terraform has build in command that you can use to check your Terraform syntax and types (a bit like a compiler):</p> <pre><code>terraform validate\n</code></pre> <p>Result</p> <p>Seems the code is legit so far</p> <p>Is your terraform easy to read and follow? Terraform has a built-in function to lint your configuration manifests for readability and best practice spacing:</p> <pre><code>terraform fmt --recursive\n</code></pre> <p>The <code>--recursive</code> flag asks the <code>fmt</code> command to traverse all of your terraform directories and format the .tf files it finds. It will report the files it changed as part of the return information of the command</p> <p>Hint</p> <p>Use the git diff command to see what was changed.</p> <p>Result</p> <p>We can see that <code>terraform.tfvars</code> file had some spacing that been fixed for better code readability.</p> <p>Extra</p> <p>Another cool tool that you can use along you terraform development is tflint - framework and each feature is provided by plugins, the key features are as follows:</p> <ul> <li>Find possible errors (like illegal instance types) for Major Cloud providers (AWS/Azure/GCP).</li> <li>Warn about deprecated syntax, unused declarations.</li> <li>Enforce best practices, naming conventions.</li> </ul> <p>Finally let's run <code>terraform plan</code>:</p> <pre><code>terraform plan\n</code></pre> <p>Output:</p> <pre><code>No changes. Your infrastructure matches the configuration.\n</code></pre> <p>Result</p> <p>We don't have any errors, however we don't have any resources created so far. Let's create a GCP project resource to start with!</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#15-create-gcp-project-using-terraform","title":"1.5 Create GCP project using Terraform","text":""},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#151-configure-google_project-resource","title":"1.5.1 Configure <code>google_project</code> resource","text":"<p><code>Resources</code> describe the infrastructure objects you want terraform to create. A resource block can be used to create any object such as virtual private cloud, security groups, DNS records etc.</p> <p>Let's create our first Terraform resource: GCP project.</p> <p>In order to accomplish this we going to use <code>google_project</code> resource, documented here</p> <p>In order to create a new Project we need to define following arguments:   * name - (Required) The display name of the project.   * project_id - (Required) The project ID. Changing this forces a new project to be created.   * billing_account - The alphanumeric ID of the billing account this project belongs to.</p> <p>First, Let's declare actual values for <code>name</code>, <code>project_id</code> and <code>billing_account</code> with variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\nvariable \"billing_account\" {\n  description = \"The billing account ID for this project\"\n}\n\nvariable \"project_name\" {\n  description = \"The human readable project name (min 4 letters)\"\n}\n\nvariable \"project_id\" {\n  description = \"The GCP project ID\"\n}\nEOF\n</code></pre> <p>Then, update <code>terraform.tfvars</code> variables files, with actual values for  <code>name</code>, <code>project_id</code> and <code>billing_account</code>:</p> <p>We going to use same naming structure for <code>PROJECT_ID</code> and <code>PROJECT_NAME</code> as in previous Assignment, in order to define your new GCP project:</p> <pre><code>export ORG=&lt;student-name&gt;\n</code></pre> <pre><code>export ORG=ayrat\nexport PRODUCT=notepad\nexport ENV=dev\nexport PROJECT_PREFIX=4  \nexport PROJECT_NAME=$ORG-$PRODUCT-$ENV\nexport PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX  # Project ID has to be unique\necho $PROJECT_NAME\necho $PROJECT_ID\n</code></pre> <p>In order to get Billing account run following command:</p> <pre><code>ACCOUNT_ID=$(gcloud alpha billing accounts list | grep Education | grep True |awk '{ print $1 }')\necho $ACCOUNT_ID\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\nbilling_account = \"$ACCOUNT_ID\"\nproject_name    = \"$PROJECT_NAME\"\nproject_id      = \"$PROJECT_ID\"\nEOF\n</code></pre> <p>Verify if everything looks good, and correct values has been set:</p> <pre><code>cat terraform.tfvars\n</code></pre> <p>Finally, let's define <code>google_project</code> resource in <code>project.tf</code> file, we will replace actual values for <code>name</code> and <code>billing_account</code> with variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; project.tf\nresource \"google_project\" \"project\" {\n  name                = var.project_name\n  billing_account     = var.billing_account\n  project_id          = var.project_id\n}\nEOF\n</code></pre> <p>Run plan command:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Result</p> <p><code>Plan: 1 to add, 0 to change, 0 to destroy.</code></p> <p>Note</p> <p>Take notice of plan command and see how some values that you declared in <code>*.tvfars</code> are visible and some values will <code>known after apply</code></p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Success</p> <p>We've created our first resource with terrraform</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#152-making-project-resource-immutable","title":"1.5.2 Making Project resource Immutable","text":"<p>Very often when you developing IaC, you need to destroy and recreate your resorces, e.g. for troubleshooting or creating a new resources using same config. Let's <code>destroy</code> our project and try to recreate it again.</p> <pre><code>terraform destroy -var-file terraform.tfvars\n</code></pre> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Failed</p> <p>What happened? Did you project been able to create? If not why ?</p> <p>If we want to make our infrastructure to be Immutable and fully automated, we need to make sure that we can destroy our service and recreate it any time the same way. In our case we can't do that because  <code>Project ID</code> always has to be unique. To tackle this problem we need to randomize our <code>Project ID</code>  creation within the terraform.</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#153-create-a-project-using-terraform-random-provider","title":"1.5.3 Create a Project using Terraform Random Provider","text":"<p>In order to create a GCP Project with Random name, we can use Terraform's random_integer resource with following arguments:</p> <ul> <li>max (Number) The maximum inclusive value of the range - 100</li> <li>min (Number) The minimum inclusive value of the range - 999</li> </ul> <pre><code>cat &lt;&lt;EOF &gt;&gt; random.tf\nresource \"random_integer\" \"id\" {\n  min = 100\n  max = 999\n}\nEOF\n</code></pre> <p><code>random_integer</code> resource doesn't belongs to Google Provider, it requires Hashicorp Random Provider to be initialized. Since it's native hashicorp provider we can skip the step of defining and configuring that provider, as it will be automatically initialized. </p> <pre><code>terraform init\n</code></pre> <p>Result</p> <ul> <li>Installing hashicorp/random v3.1.0...</li> <li>Installed hashicorp/random v3.1.0 (signed by HashiCorp)</li> </ul>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#153-create-a-project_id-name-using-local-values","title":"1.5.3 Create a Project_ID name using Local Values","text":"<p><code>Local</code> Values allow you to assign a name to an expression, so the expression can be used multiple times, without repeating it.</p> <p>We going to define value of <code>project_id</code> as local. And it will be a combination of <code>project_name</code> <code>-</code> <code>random_number</code>.</p> <p>Let's add local value for <code>project_id</code> and replace value <code>var.project_id</code> with <code>local.project_id</code>.</p> <pre><code>edit project.tf\n</code></pre> <p>Update code snippet of <code>project.tf</code> as following and save:</p> <pre><code>locals {\n  project_id     = \"${var.project_name}-${random_integer.id.result}\"\n}\n\nresource \"google_project\" \"project\" {\n  name                = var.project_name\n  billing_account     = var.billing_account\n  project_id          = local.project_id\n}\n</code></pre> <p>We can now remove <code>project_id</code> value from <code>terraform.tfvars</code>, as we going to randomly generate it using <code>locals</code> expression with <code>random_integer</code></p> <pre><code>edit terraform.tfvars\n</code></pre> <p>Remove <code>project_id</code> line and save.</p> <p>We can now also remove <code>project_id</code> variable from <code>variables.tf</code>:</p> <pre><code>rm variables.tf\ncat &lt;&lt;EOF &gt;&gt; variables.tf\nvariable \"gcp_region\" {\n  type        = string\n  description = \"The GCP Region\"\n  default     = \"us-central1\"\n}\n\nvariable \"gcp_project_id\" {\n  type        = string\n  description = \"The GCP Seeding project ID\"\n  default     = \"\"\n}\nvariable \"billing_account\" {\n  description = \"The billing account ID for this project\"\n}\n\nvariable \"project_name\" {\n  description = \"The human readable project name (min 4 letters)\"\n}\nEOF\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>  # random_integer.id will be created\n  + resource \"random_integer\" \"id\" {\n      + id     = (known after apply)\n      + max    = 999\n      + min    = 100\n      + result = (known after apply)\n    }\n</code></pre> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Result</p> <p>Project has been created with Random project_id</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#154-configure-terraform-output-for-gcp-project","title":"1.5.4 Configure Terraform Output for GCP Project","text":"<p><code>Outputs</code> provide return values of a Terraform state at any given time. So you can get for example values of what GCP resources like project, VMs, GKE cluster been created and their parametres.</p> <p><code>Outputs</code> can be used used to export structured data about resources. This data can be used to configure other parts of your infrastructure, or as a data source for another Terraform workspace. Outputs are also necessary to share data from a child module to your root module.</p> <p>Outputs follow a similar structure to variables:</p> <pre><code>output \"output_name\" {\n  description = \"A description to understand what information is provided by the output\"\n  value = \"An expression and/or resource_name.attribute\"\n  sensitive = \"Optional argument, marking an output sensitive will supress the value from plan/apply phases\"\n}\n</code></pre> <p>Let's declare GCP Project <code>output</code> values for <code>project_id</code> and <code>project_number</code>.</p> <p>You can find available <code>output</code> in each respective <code>resource</code> documents, under `Attributes Reference.</p> <p>For example for GCP project available outputs are:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"id\" {\n  value = google_project.project.project_id\n  description = \"GCP project ID\"\n}\n\noutput \"number\" {\n  value = google_project.project.number\n  description = \"GCP project number\"\n  sensitive = true\n}\nEOF\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>We can see that we have new changes to output:</p> <p>Output:</p> <pre><code>Changes to Outputs:\n  + id     = \"ayrat-notepad-dev-631\"\n  + number = (sensitive value)\n</code></pre> <p>Let's apply this changes:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>Apply complete! Resources: 0 added, 0 changed, 0 destroyed.\n</code></pre> <p>Let's now run <code>terraform output</code> command:</p> <pre><code>terraform output\n</code></pre> <p>Result</p> <p>We can see value of project <code>id</code>, however we don't see project number as it's been marked as sensitive.</p> <p>Note</p> <p><code>Outputs</code> can be useful when you want to provide results of terraform resource creation to CI/CD or next automation tool like <code>helm</code> to deploy application.</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#155-recreate-gcp-project-without-default-vpc","title":"1.5.5 Recreate GCP Project without Default VPC","text":"<p>One of the requirements for our solution is to create GCP project with <code>custom</code> VPC. However, when terraform creates GCP Project it creates <code>DEFAULT</code> vpc by default.</p> <pre><code>gcloud compute networks list --project ayrat-notepad-dev-631\n</code></pre> <p>Output:</p> <pre><code>NAME     SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4\ndefault  AUTO         REGIONAL\n</code></pre> <p>In order to remove automatically created <code>default</code> VPC, specify special attribute during <code>google_project</code> resource creation. Check documentation here and find this argument.</p> <p>Task N1: Find Attribute to remove <code>default</code> VPC during  <code>google_project</code> resource creation. Define in it <code>project.tf</code> and set it's value in <code>variables.tf</code> as <code>true</code> by default.</p> <p>Solution:</p> <p>Update code snippet of <code>project.tf</code> as following and save:</p> <pre><code>locals {\n  project_id     = \"${var.project_name}-${random_integer.id.result}\"\n}\n\nresource \"google_project\" \"project\" {\n  name                = var.project_name\n  billing_account     = var.billing_account\n  project_id          = local.project_id\n  auto_create_network = var.auto_create_network\n}\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\nvariable \"auto_create_network\" {\n  description = \"Automatically provision a default global VPC network\"\n  default     = false\n}\nEOF\n</code></pre> <p>After changes you need to re-create a project, as vpc get's deleted during project creation only.</p> <pre><code>terraform destroy -var-file terraform.tfvars\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\nterraform apply -var-file terraform.tfvars\n</code></pre> <p>Notice</p> <p>How long project is now getting created. This is due to after project creation, <code>default</code> vpc is being removed.</p> <p>Verify that <code>default</code> VPC has been deleted:</p> <pre><code>gcloud compute networks list --project ayrat-notepad-dev-631\n</code></pre> <p>Output:</p> <pre><code>Listed 0 items.\n</code></pre> <p>Success</p> <p>We now able to create a GCP project without <code>Default</code> VPC.</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#16-create-gcp-storage-bucket","title":"1.6 Create GCP Storage bucket","text":""},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#161-create-gcp-storage-bucket-in-new-gcp-project","title":"1.6.1 Create GCP Storage bucket in New GCP Project","text":"<p>Using reference doc for google_storage_bucket , let's create <code>google_storage_bucket</code> resource that will create MULTI_REGIONAL GCS bucket in newly created project. We going to give bucket <code>name</code>: <code>$ORG-notepad-dev-tfstate</code>, and we going to use this bucket to store Terraform state for GCP Service Layer.</p> <pre><code>cat &lt;&lt;EOF &gt;&gt;  bucket.tf\nresource \"google_storage_bucket\" \"state\" {\n  name          = var.bucket_name\n  project       = local.project_id\n  storage_class = var.storage_class\n\n  force_destroy = \"true\"\n}\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\nvariable \"bucket_name\" {\n  description = \"The name of the bucket.\"\n}\n\nvariable \"storage_class\" {\n  description = \n  default = \"MULTI_REGIONAL\"\n}\nEOF\n</code></pre> <p>Set variable that will be used to build name for a bucket:</p> <pre><code>export ORG=&lt;student-name&gt;\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\nbucket_name   = \"$ORG-notepad-dev-tfstate\"\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"bucket_name\" {\n  value = google_storage_bucket.state.name\n}\nEOF\n</code></pre> <p>Let's review the plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>And create google_storage_bucket resource:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify created bucket in GCP UI:</p> <pre><code>Storage -&gt; Cloud Storage\n</code></pre> <pre><code>gsutil ls -L -b gs://$ORG-notepad-dev-tfstate\n</code></pre> <p>Result</p> <p>GCS bucket for terraform state has been created</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#162-configure-versioning-on-gcp-storage-bucket","title":"1.6.2 Configure <code>versioning</code> on GCP Storage bucket.","text":"<p>Check if versioning enabled on the created bucket:</p> <pre><code>gsutil versioning get gs://$ORG-notepad-dev-tfstate\n</code></pre> <p>Result</p> <p>Versioning: Suspended</p> <p>It is highly recommended that if you going to use GCS bucket as Terraform storage backend you should enable Object Versioning on the GCS bucket to allow for state recovery in the case of accidental deletions and human error.</p> <p>Task N2: Using reference doc for google_storage_bucket  find and configure argument that enables gcs bucket versioning feature.</p> <p>Edit <code>bucket.tf</code> with correct argument.</p> <pre><code>edit bucket.tf\n</code></pre> <p>Solution:</p> <pre><code>resource \"google_storage_bucket\" \"state\" {\n  name          = var.bucket_name\n  project       = local.project_id\n  storage_class = var.storage_class\n\n  versioning {\n    enabled = false\n  }\n\n  force_destroy = \"true\"\n}\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>Plan: 0 to add, 1 to change, 0 to destroy.\n</code></pre> <p>Result</p> <p>Configuration for Versioning looks correct</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify versioning is ON:</p> <pre><code>gsutil versioning get gs://$ORG-notepad-dev-tfstate\n</code></pre> <p>Output:</p> <pre><code>Versioning Enabled\n</code></pre> <p>Result</p> <p>We've finished building the Foundation Layer. So far we able to accomplish following:</p> <ul> <li>Create structure: provider.tf, variable.tf, variables.tfvars, main.tf, output.tf</li> <li>Create a new <code>notepad-dev</code> Project</li> <li>Delete Default VPC</li> <li>Create a bucket in this project to store terraform state</li> </ul>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#2-build-gcp-services-layer","title":"2 Build GCP Services Layer","text":"<p>Once we finished building a GCP Foundation layer which is essentially our project, we can start building  GCP Services Layer inside that project.</p> <p>This second layer will configure following items:</p> <ul> <li>Enable Google Project Service APIs</li> <li>Create VPC (google_compute_network) and Subnet (google_compute_subnetwork)</li> <li>Create Cloud Nat (google_compute_router) and (google_compute_router_nat)</li> <li>Private GKE<ul> <li>Private Nodes with Public API Endpoint (for simplicity)</li> </ul> </li> </ul>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#21-initialize-terraform-for-gcp-services-layer","title":"2.1  Initialize Terraform for GCP Services Layer","text":""},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#211-define-and-configure-terraform-provider","title":"2.1.1 Define and configure terraform provider","text":"<p>Step 1: Take note of newly created GCP Project_ID and BUCKET_ID:</p> <pre><code>cd ~/$MY_REPO/foundation-infrastructure\n</code></pre> <pre><code>terraform output | grep 'id' |awk '{ print $3}'\nterraform output | grep 'bucket_name' |awk '{ print $3}'\n</code></pre> <p>Set it as variable:</p> <pre><code>export PROJECT_ID=$(terraform output | grep 'id' |awk '{ print $3}')\nexport BUCKET_ID=$(terraform output | grep 'bucket_name' |awk '{ print $3}')\necho $PROJECT_ID\necho $BUCKET_ID\n</code></pre> <p>Step 2: Declare the Terraform Provider for GCP Services Layer:</p> <p>We now going to switch to <code>notepad-infrastructure</code> where we going to create a new GCP service Layer terraform configuration:</p> <pre><code>cd ~/$MY_REPO/notepad-infrastructure\n</code></pre> <pre><code>cat &lt;&lt; EOF&gt;&gt; provider.tf\nterraform {\n  required_providers { \n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~&gt; 3.70.0\"\n    }\n  }\n}\nEOF\n</code></pre> <p>Step 4: Configure the Terraform Provider</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; main.tf\nprovider \"google\" {\n  project = var.gcp_project_id\n  region  = var.gcp_region\n}\nEOF\n</code></pre> <p>Step 5: Define variables:</p> <pre><code>cat &lt;&lt;EOF&gt; variables.tf\nvariable \"gcp_region\" {\n  type        = string\n  description = \"The GCP Region\"\n  default     = \"us-central1\"\n}\n\nvariable \"gcp_project_id\" {\n  type        = string\n  description = \"The newly created GCP project ID\"\n}\nEOF\n</code></pre> <p>Step 5: Set variables in <code>terraform.tfvars</code></p> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\ngcp_project_id = $PROJECT_ID\nEOF\n</code></pre> <p>Step 4: Now that you have declared and configured the GCP provider for terraform, initialize terraform:</p> <pre><code>terraform init\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#212-configure-terraform-state-backend","title":"2.1.2 Configure Terraform State backend","text":"<p>Terraform records information about what infrastructure it created in a Terraform state file. This information, or state, is stored by default in a <code>local</code> file named terraform.tfstate. This allows Terraform to compare what's in your configurations with what's in the state file, and determine what changes need to be applied.</p> <p>Local vs Remote backend: <code>Local</code> state files are the default for terraform. <code>Remote</code> state allows for collaboration between members of your team, for example multiple users or systems can deploy terraform configuration to the same environment as state is stored remotely and can be pulled <code>localy</code> during the execution. This however may create issues if 2 users running terraform <code>plan</code> and <code>apply</code> at the same time, however some remote backends including gcs provide <code>locking</code> feature that we covered and enabled in previous section <code>Configure versioning on GCP Storage bucket.</code></p> <p>In addition to that, <code>Remote</code> state is more secure storage of sensitive values that might be contained in variables or outputs.</p> <p>See documents forremote backend for reference:</p> <p>Step 1 Configure remote backend using gcs bucket:</p> <p>Let's configure a backend for your state, using the <code>gcs</code> bucket you previously created in Foundation Layer.</p> <p>Backends are configured with a nested backend block within the top-level terraform block While a backend can be declared anywhere, it is recommended to use a <code>backend.tf</code>.</p> <p>Since we running on GCP we going to use GCS remote backend. It stores the state as an object in a configurable prefix in a pre-existing bucket on Google Cloud Storage (GCS). This backend also supports state locking. The bucket must exist prior to configuring the backend.</p> <p>We going to use following arguments:</p> <ul> <li>bucket - (Required) The name of the GCS bucket. This name must be globally unique. For more information, see Bucket Naming Guidelines.</li> <li>prefix - (Optional) GCS prefix inside the bucket. Named states for workspaces are stored in an object called /.tfstate. <pre><code>cat &lt;&lt;EOF &gt;&gt; backend.tf\nterraform {\n  backend \"gcs\" {\n    bucket = $BUCKET_ID\n    prefix = \"state\"\n  }\n}\nEOF\n</code></pre> <p>Step 2  When you change, configure or unconfigure a backend, terraform must be re-initialized:</p> <pre><code>terraform init\n</code></pre> <p>Verify if <code>folder</code> state has been created in our bucket:</p> <pre><code>gsutil ls gs://$ORG-notepad-dev-tfstate\n</code></pre> <p>Summary</p> <p><code>state</code> Folder has been created in gcs bucket and terrafrom has been initialized with  <code>remote</code> backend</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#22-enable-required-gcp-services-api","title":"2.2 Enable required GCP Services API","text":"<p>Before we continue with creating GCP services like VPC, routers, Cloud Nat and GKE it is required to enable underlining GCP API services. When we creating a new project most of the services API's are disabled, and requires explicitly to be enabled. <code>google_project_service</code> resource allows management of a single API service for an existing Google Cloud Platform project.</p> <p>Let's enable <code>compute engine API</code> service with terraform:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; services.tf\nresource \"google_project_service\" \"compute\" {\n  service = \"compute.googleapis.com\"\n  disable_on_destroy = false\n}\nEOF\n</code></pre> <pre><code>terraform plan -var-file terraform.tfvars\nterraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify Compute Engine API service has been enabled:</p> <pre><code>gcloud services list\n</code></pre> <p>Result</p> <p>Compute Engine API is enabled as it is listed.</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#22-set-variables-to-define-standard-for-naming-convention","title":"2.2 Set variables to define standard for Naming Convention","text":"<p>As per terraform best practices, when you creating terraform resources, you need to follow naming convention, that is clear for you organization.</p> <ul> <li>Configuration objects should be named using underscores to delimit multiple words.</li> <li>Object's name should be named using dashes</li> </ul> <p>This practice ensures consistency with the naming convention for resource types, data source types, and other predefined values and helps prevent accidental deletion or outages:</p> <p>Example:</p> <pre><code># Good\nresource \"google_compute_instance\" \"web_server\" {\n  name = \u201cweb-server-$org-$app-$env\u201d\n  # ...\n}\n\n# Bad\nresource \u201cgoogle_compute_instance\u201d \u201cweb-server\u201d {\n  name =  \u201cweb-server\u201d\n  # \u2026\n</code></pre> <p>Create variables to define standard for Naming Convention:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\nvariable \"org\" {\n  type = string\n}\nvariable \"product\" {\n  type = string\n}\nvariable \"environment\" {\n  type = string\n}\nEOF\n</code></pre> <pre><code>export ORG=ayrat\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=dev\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\norg            = \"$ORG\"\nproduct        = \"$PRODUCT\"\nenvironment    = \"$ENV\"\nEOF\n</code></pre> <p>Review if created files are correct.</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#23-create-a-custom-mode-network-vpc-with-terraform","title":"2.3 Create a <code>custom mode</code> network (VPC) with terraform","text":"<p>Using google_compute_network resource create a VPC network with terraform.</p> <p>Task N3: Create <code>vpc.tf</code> file and define <code>custom mode</code> network (VPC) with following requirements:   * Description: <code>VPC that will be used by the GKE private cluster on the related project</code>   * <code>name</code> - vpc name with following pattern: \"vpc-$ORG-$PRODUCT-$ENV\"   * Subnet mode: <code>custom</code>   * Bgp routing mode: <code>regional</code>   * MTUs: <code>default</code></p> <p>Hint</p> <p>To create \"vpc-$ORG-$PRODUCT-$ENV\" name, use format function, and use <code>%s</code> to convert variables to string values.</p> <p>Define variables in <code>variables.tf</code> and <code>terraform.tfvars</code> if required.</p> <p>Define <code>Output</code> for:</p> <ul> <li>Generated VPC name: <code>google_compute_network.vpc_network.name</code></li> <li>self_link - The URI of the created resource.</li> </ul> <p>Solution:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; vpc.tf\nresource \"google_compute_network\" \"vpc_network\" {\n  name                    = format(\"vpc-%s-%s-%s\", var.org, var.product, var.environment)\n  routing_mode            = \"REGIONAL\"\n  description             = \"VPC that will be used by the GKE private cluster on the related project\"\n  auto_create_subnetworks = false\n}\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"vpc_name\" {\n  value = google_compute_network.vpc_network.name\n}\noutput \"vpc_selflink\" {\n  value = \"\\${google_compute_network.vpc_network.self_link}\"\n}\nEOF\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>  + create\n\nTerraform will perform the following actions:\n  # google_compute_network.vpc_network will be created\n  + resource \"google_compute_network\" \"vpc_network\" {\n      + auto_create_subnetworks         = false\n      + delete_default_routes_on_create = false\n      + description                     = \"VPC that will be used by the GKE private cluster on the related project\"\n      + gateway_ipv4                    = (known after apply)\n      + id                              = (known after apply)\n      + mtu                             = (known after apply)\n      + name                            = \"vpc-ayrat-notepad-dev\"\n      + project                         = (known after apply)\n      + routing_mode                    = \"REGIONAL\"\n      + self_link                       = (known after apply)\n</code></pre> <p>Create VPC:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify that <code>custom-mode</code> VPC has been created:</p> <pre><code>gcloud compute networks list\ngcloud compute networks describe $(gcloud compute networks list | grep CUSTOM |awk '{ print $1 }')\n</code></pre> <p>Output:</p> <pre><code>autoCreateSubnetworks: false\ndescription: VPC that will be used by the GKE private cluster on the related project\nkind: compute#network\nname: vpc-student_name-notepad-dev\nroutingConfig:\n  routingMode: REGIONAL\nselfLink: https://www.googleapis.com/compute/v1/projects/XXX\nx_gcloud_bgp_routing_mode: REGIONAL\nx_gcloud_subnet_mode: CUSTOM\n</code></pre> <p>Result</p> <p>VPC Network has been created, without auto subnets.</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#24-create-a-user-managed-subnet-with-terraform","title":"2.4 Create a <code>user-managed</code> subnet with terraform","text":"<p>Using google_compute_subnetwork resource create a <code>user-managed</code> subnet with terraform.</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; subnet.tf\nresource \"google_compute_subnetwork\" \"gke_private_subnet\" {\n  name                     = format(\"subnet-%s-%s-%s\", var.org, var.product, var.environment)\n  network                  = google_compute_network.vpc_network.self_link\n  region                   = var.gcp_region\n  project                  = var.gcp_project_id\n  ip_cidr_range            = var.network_cidr\n  secondary_ip_range {\n    range_name    = var.pods_cidr_name\n    ip_cidr_range = var.pods_cidr\n  }\n  secondary_ip_range {\n    range_name    = var.services_cidr_name\n    ip_cidr_range = var.services_cidr\n  }\n}\nEOF\n</code></pre> <p>Note</p> <p>Notice power of terraform outputs. Here we link <code>subnet</code> with our VPC <code>network</code> using <code>google_compute_network.vpc_network.self_link</code> <code>output</code> value of created <code>network</code> in previous step.</p> <p>Define variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; variables.tf\n\n# variables used to create VPC\n\nvariable \"network_cidr\" {\n  type = string\n}\nvariable \"pods_cidr\" {\n  type = string\n}\nvariable \"pods_cidr_name\" {\n  type = string\n}\nvariable \"services_cidr\" {\n  type = string\n}\nvariable \"services_cidr_name\" {\n  type = string\n}\nEOF\n</code></pre> <p>Define outputs:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"subnet_selflink\" {\n  value = \"\\${google_compute_subnetwork.gke_private_subnet.self_link}\"\n}\nEOF\n</code></pre> <p>Task N4: Update <code>terraform.tfvars</code> file values with following information:</p> <ul> <li>Node Range: See column <code>subnet</code> in above table for <code>dev</code> cluster</li> <li>Secondary service range with name: <code>services</code></li> <li>Secondary Ranges:<ul> <li>Service range name: <code>services</code></li> <li>Service range CIDR: See column <code>srv range</code> in above table for <code>dev</code> cluster</li> <li>Pods range name: <code>pods</code></li> <li>Pods range CIDR: See column <code>pod range</code> in above table for <code>dev</code> cluster</li> </ul> </li> </ul> <pre><code>env |   subnet      | pod range        | srv range      | kubectl api range\ndev | 10.128.1.0/26 | 172.0.0.0/18    | 172.10.0.0/21   | 172.16.0.0/28\nstg | 10.128.2.0/26 | 172.1.0.0/18    | 172.11.0.0/21   | 172.16.0.16/28\nprd | 10.128.3.0/26 | 172.2.0.0/18    | 172.12.0.0/21   |  172.16.0.32/28\n</code></pre> <p>Note</p> <p>Ranges must be with in Private (RFC1918) Address Space</p> <p>Solution:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\n#subnet vars\nnetwork_cidr       = \"10.128.1.0/26\"\npods_cidr          = \"172.0.0.0/18\"\npods_cidr_name     = \"pods\"\nservices_cidr      = \"172.10.0.0/21\"\nservices_cidr_name = \"services\"\nEOF\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Create VPC:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Review created subnet:</p> <pre><code>gcloud compute networks subnets list\ngcloud compute networks subnets describe $(gcloud compute networks subnets list | grep us-central1 |awk '{ print $1 }')  --region us-central1\n</code></pre> <p>Output:</p> <pre><code>enableFlowLogs: false\ngatewayAddress: 10.128.1.1\nipCidrRange: 10.128.1.0/26\nkind: compute#subnetwork\nlogConfig:\n  enable: false\nname: subnet-student_name-notepad-dev\nprivateIpGoogleAccess: false\nprivateIpv6GoogleAccess: DISABLE_GOOGLE_ACCESS\npurpose: PRIVATE\nsecondaryIpRanges:\n- ipCidrRange: 172.0.0.0/18\n  rangeName: pods\n- ipCidrRange: 172.10.0.0/21\n  rangeName: services\nstackType: IPV4_ONLY\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>Networking-&gt;VPC Networks -&gt; Click VPC network and check `Subnet` tab\n</code></pre> <p>Task N5: Update  <code>subnet.tf</code> so that  <code>google_compute_subnetwork</code> resource supports following features:</p> <pre><code>* Flow Logs\n  * Aggregation interval: 15 min\n  * Flow sampling: 0.1\n  * Metadata: \"INCLUDE_ALL_METADATA\"\n* Private IP Google Access\n</code></pre> <p>Solution:</p> <pre><code>  private_ip_google_access = true\n  log_config {\n    aggregation_interval = \"INTERVAL_15_MIN\"\n    flow_sampling        = 0.1\n    metadata             = \"INCLUDE_ALL_METADATA\"\n  }\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Create VPC:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#25-create-a-cloud-router","title":"2.5 Create a Cloud router","text":"<p>Create Cloud Router for <code>custom mode</code> network (VPC), in the same region as the instances that will use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway.</p> <p>Task N6: Define a <code>google_compute_router</code> inside <code>router.tf</code> that will be able to create a NAT router so the nodes can reach DockerHub and external APIs from private cluster, using following parameters:</p> <ul> <li>Create router for custom <code>vpc_network</code> created above with terraform</li> <li>Same project as VPC</li> <li>Same region as VPC</li> <li>Router name: <code>gke-net-router</code></li> <li>Local BGP Autonomous System Number (ASN): 64514</li> </ul> <p>Hint</p> <p>You can automatically recover vpc name from terraform output like this: <code>google_compute_network.vpc_network.self_link</code>.</p> <p>Solution:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; router.tf\nresource \"google_compute_router\" \"gke_net_router\" {\n  project = var.gcp_project_id\n  name    = \"gke-net-router\"\n  region  = var.gcp_region\n  network = google_compute_network.vpc_network.self_link\n  bgp {\n    asn = 64514\n  }\n}\nEOF\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Create Cloud Router:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify created Cloud Router:</p> <p>CLI:</p> <pre><code>gcloud compute routers list\ngcloud compute routers describe $(gcloud compute routers list | grep gke-net-router |awk '{ print $1 }')  --region us-central1\n</code></pre> <p>Output:</p> <pre><code>bgp:\n  advertiseMode: DEFAULT\n  asn: 64514\n  keepaliveInterval: 20\nkind: compute#router\nname: gke-net-router\n</code></pre> <p>UI:</p> <pre><code>Networking -&gt; Hybrid Connectivity -&gt; Cloud Routers\n</code></pre> <p>Result</p> <p>Router resource has been created for VPC Network</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#26-create-a-cloud-nat","title":"2.6 Create a Cloud Nat","text":"<p>Set up a simple Cloud Nat configuration using google_compute_router_nat resource, which will <code>automatically</code> allocates the necessary external IP addresses to provide NAT services to a region. </p> <p>When you use auto-allocation, Google Cloud reserves IP addresses in your project automatically. </p> <pre><code>cat &lt;&lt;EOF &gt;&gt; cloudnat.tf\nresource \"google_compute_router_nat\" \"gke_cloud_nat\" {\n  project                = var.gcp_project_id\n  name                   = \"gke-cloud-nat\"\n  router                 = google_compute_router.gke_net_router.name\n  region                 = var.gcp_region\n  nat_ip_allocate_option = \"AUTO_ONLY\"\n  source_subnetwork_ip_ranges_to_nat = \"ALL_SUBNETWORKS_ALL_IP_RANGES\"\n}\nEOF\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Create Cloud Router:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Verify created <code>Cloud Nat</code>:</p> <p>CLI:</p> <pre><code># List available Cloud Nat Routers\ngcloud compute routers nats list --router gke-net-router --router-region us-central1\n# Describe Cloud Nat Routers `gke-cloud-nat`:\ngcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1\n</code></pre> <p>Output:</p> <pre><code>enableEndpointIndependentMapping: true\nicmpIdleTimeoutSec: 30\nname: gke-cloud-nat\nnatIpAllocateOption: AUTO_ONLY\nsourceSubnetworkIpRangesToNat: ALL_SUBNETWORKS_ALL_IP_RANGES\ntcpEstablishedIdleTimeoutSec: 1200\ntcpTransitoryIdleTimeoutSec: 30\nudpIdleTimeoutSec: 30\n</code></pre> <p>UI:</p> <pre><code>Networking -&gt; Network Services -&gt; Cloud NAT\n</code></pre> <p>Result</p> <p>A NAT service created in a router</p> <p>Task N7: Additionally turn <code>ON</code> logging feature for <code>ALL</code> log types of communication for Cloud Nat</p> <pre><code>edit cloudnat.tf\n</code></pre> <p>Solution:</p> <p>Correctly add following configuration:</p> <pre><code>  log_config {\n    filter = \"ALL\"\n    enable = true\n  }\n</code></pre> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Update Cloud Nat Configuration:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>Apply complete! Resources: 0 added, 1 changed, 0 destroyed.\n</code></pre> <pre><code>gcloud compute routers nats describe gke-cloud-nat --router gke-net-router --router-region us-central1\n</code></pre> <p>Result</p> <p>Cloud Nat now supports Logging. Cloud NAT logging allows you to log NAT connections and errors. When Cloud NAT logging is enabled, one log entry can be generated for each of the following scenarios:</p> <ul> <li>When a network connection using NAT is created.</li> <li>When a packet is dropped because no port was available for NAT.</li> </ul>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#27-create-ssh-firewall-rules-default-allow-internal-and-default-allow-ssh","title":"2.7 Create SSH Firewall rules <code>default-allow-internal</code> and <code>default-allow-ssh</code>","text":"<p>Let's create SSH Firewall rule with name <code>allow-tcp-ssh-icmp-$ORG-$PRODUCT-$ENV</code> to allow SSH, ping, using google_compute_firewall resource.</p> <p>Reference: </p> <ul> <li>https://cloud.google.com/kubernetes-engine/docs/concepts/firewall-rules</li> <li>https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_firewall</li> </ul> <pre><code>cat &lt;&lt;EOF&gt;&gt; firewall.tf\nresource \"google_compute_firewall\" \"ssh-rule\" {\n  name =   format(\"allow-tcp-ssh-icmp-%s-%s-%s\", var.org, var.product, var.environment)\n  network = google_compute_network.vpc_network.self_link\n  allow {\n    protocol = \"tcp\"\n    ports = [\"22\"]\n  }\n  allow {\n    protocol = \"icmp\"\n  }\n}\nEOF\n</code></pre> <p>Review created firewall rules:</p> <pre><code>gcloud compute firewall-rules list\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>Networking-&gt;Firewalls\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#28-create-a-private-gke-cluster-and-delete-default-node-pool","title":"2.8 Create a Private GKE Cluster and delete default node pool","text":""},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#281-enable-gcp-beta-provider","title":"2.8.1 Enable GCP Beta Provider","text":"<p>In order to create a GKE cluster with terraform we will be leveraging google_container_cluster resource.</p> <p>Some of  <code>google_container_cluster</code> arguments, such VPC-Native networking mode, VPA, Istio, CSI Driver add-ons, requires  <code>google-beta</code> Provider.</p> <p>The <code>google-beta</code> provider is distinct from the <code>google</code> provider in that it supports GCP products and features that are in beta, while google does not. Fields and resources that are only present in google-beta will be marked as such in the shared provider documentation.</p> <p>Task N8: Configure and Initialize GCP Beta Provider, similar to how we did it for GCP Provider in 1.3.3 Initialize Terraform update <code>provider.tf</code> and <code>main.tf</code> configuration files.</p> <p>Solution:</p> <pre><code>rm provider.tf\ncat &lt;&lt;EOF &gt;&gt; provider.tf\nterraform {\n  required_providers { \n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~&gt; 3.70.0\"\n    }\n    google-beta = {\n      source  = \"hashicorp/google-beta\"\n      version = \"~&gt; 3.70.0\"\n    }\n  }\n}\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt;&gt;  main.tf\nprovider \"google-beta\" {\n  project = var.gcp_project_id\n  region  = var.gcp_region\n}\nEOF\n</code></pre> <p>Initialize <code>google-beta</code> provider plugin:</p> <pre><code>terraform init\n</code></pre> <p>Success</p> <p>Terraform has been successfully initialized!</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#282-enable-kubernetes-engine-api","title":"2.8.2 Enable Kubernetes Engine API","text":"<p>Kubernetes Engine API used to build and manages container-based applications, powered by the open source Kubernetes technology. Before starting  GKE cluster creation it is required to enable it.</p> <p>Task N9: Enable <code>container.googleapis.com</code> in <code>services.tf</code> file similar to what we already did in 2.2 Enable required GCP Services API.</p> <p>Solution:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; services.tf\nresource \"google_project_service\" \"gke_api\" {\n  service = \"container.googleapis.com\"\n  disable_on_destroy = false\n}\nEOF\n</code></pre> <p>Note</p> <p><code>disable_on_destroy=false</code> helps to prevent errors during redeployments of the system.</p> <p>Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Update Cloud Nat Configuration:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#283-create-a-private-gke-cluster-and-delete-default-node-pool","title":"2.8.3 Create a Private GKE Cluster and delete default node pool","text":"<p>Using google_container_cluster resource create a Regional, Private GKE cluster, with following characteristics:</p> <p>Cluster Configuration:</p> <ul> <li>Cluster name: <code>gke-$ORG-$PRODUCT-$ENV</code></li> <li>GKE Control plane is replicated across three zones of a region: <code>us-central1</code></li> <li>Private cluster with unrestricted access to the public endpoint:<ul> <li>Cluster Nodes access: Private Node GKE Cluster with Public API endpoint</li> <li>Cluster K8s API access: with unrestricted access to the public endpoint</li> </ul> </li> <li>Cluster Node Communication: <code>VPC Native</code></li> <li>Secondary pod range with name: <code>pods</code></li> <li>Secondary service range with name: <code>services</code></li> <li>GKE master and node version: \"1.20.8-gke.700\"</li> <li>Terraform Provider: <code>google-beta</code></li> <li>Timeouts: 30M</li> </ul> <p>Node Pool Configuration:</p> <ul> <li>VM size: <code>e2-small</code></li> <li>Node count: 1 per zone</li> <li>Node images: Container-Optimized OS</li> <li>The name of a GCE machine (VM) type: e2-small</li> </ul> <p>Note</p> <p>Why delete default node pool? The <code>default</code> node pools cause trouble with managing the cluster, when created with terraform as it is not part of the terraform lifecycle. GKE Architecture Best Practice recommends to delete <code>default</code> node pool and create a <code>custom</code> one instead and manage the node pools explicitly.</p> <p>Note</p> <p>Why define <code>Timeouts</code> for gke resource? Normally GKE creation takes few minutes. However, in our case we creating GKE Cluster, and then system cordon, drain and then destroy default node pool. This process may take 10-20 minutes and we want to make sure terraform will not time out during this time.</p> <p>Step 1: Let's define GKE resource first:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke.tf\nresource \"google_container_cluster\" \"primary_cluster\" {\n  provider = google-beta\n\n  project = var.gcp_project_id\n\n  name               = format(\"gke-%s-%s-%s\", var.org, var.product, var.environment)\n  min_master_version = var.kubernetes_version\n  network            = google_compute_network.vpc_network.self_link\n  subnetwork         = google_compute_subnetwork.gke_private_subnet.self_link\n\n  location                    = var.gcp_region\n  logging_service             = var.logging_service\n  monitoring_service          = var.monitoring_service\n\n  remove_default_node_pool = true\n  initial_node_count       = 1\n\n  private_cluster_config {\n    enable_private_nodes   = var.enable_private_nodes\n    enable_private_endpoint = var.enable_private_endpoint\n    master_ipv4_cidr_block = var.master_ipv4_cidr_block\n  }\n\n  network_policy {\n    enabled  = var.network_policy\n    provider = var.network_policy ? \"CALICO\" : \"PROVIDER_UNSPECIFIED\"\n  }\n\n  addons_config {\n    http_load_balancing {\n      disabled = var.disable_http_load_balancing\n    }\n\n    network_policy_config {\n      disabled = var.network_policy ? false : true\n    }\n  }\n\n  ip_allocation_policy {\n    cluster_secondary_range_name  = var.pods_range_name\n    services_secondary_range_name = var.services_range_name\n  }\n\n  timeouts {\n    create = \"30m\"\n    update = \"30m\"\n    delete = \"30m\"\n  }\n\n  workload_identity_config {\n    identity_namespace = \"${var.gcp_project_id}.svc.id.goog\"\n  }\n}\nEOF\n</code></pre> <p>Step 2: Next define GKE cluster specific variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke_variables.tf\n\nvariable \"kubernetes_version\" {\n  default     = \"\"\n  type        = string\n  description = \"The GKE version of Kubernetes\"\n}\n\nvariable \"logging_service\" {\n  description = \"The logging service that the cluster should write logs to.\"\n  default     = \"logging.googleapis.com/kubernetes\"\n}\n\nvariable \"monitoring_service\" {\n  default     = \"monitoring.googleapis.com/kubernetes\"\n  description = \"The GCP monitoring service scope\"\n}\n\nvariable \"disable_http_load_balancing\" {\n  default     = false\n  description = \"Enable HTTP Load balancing GCP integration\"\n}\n\nvariable \"network_policy\" {\n  description = \"Enable network policy addon\"\n  default     = true\n}\n\nvariable \"pods_range_name\" {\n  description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to pods\"\n  default     = \"\"\n}\n\nvariable \"services_range_name\" {\n  description = \"The pre-defined IP Range the Cluster should use to provide IP addresses to services\"\n  default     = \"\"\n}\n\nvariable \"enable_private_nodes\" {\n  default     = false\n  description = \"Enable Private-IP Only GKE Nodes\"\n}\n\nvariable \"enable_private_endpoint\" {\n  default     = false\n  description = \"When true, the cluster's private endpoint is used as the cluster endpoint and access through the public endpoint is disabled.\"\n}\n\nvariable \"master_ipv4_cidr_block\" {\n  description = \"The ipv4 cidr block that the GKE masters use\"\n}\nEOF\n</code></pre> <p>Step 3: Define GKE cluster specific outputs:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"id\" {\n  value = \"${google_container_cluster.primary_cluster.id}\"\n}\noutput \"endpoint\" {\n  value = \"${google_container_cluster.primary_cluster.endpoint}\"\n}\noutput \"master_version\" {\n  value = \"${google_container_cluster.primary_cluster.master_version}\"\n}\nEOF\n</code></pre> <p>Task N9: Complete <code>terraform.tfvars</code> with required values to GKE Cluster specified above:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\n//gke specific\nenable_private_nodes   = \"TODO\"\nmaster_ipv4_cidr_block = \"TODO\"\npods_range_name        = \"TODO\"\nservices_range_name    = \"TODO\"\nkubernetes_version     = \"TODO\"\nEOF\n</code></pre> <p>Solution: </p> <pre><code>//gke specific\nenable_private_nodes   = \"true\"\nmaster_ipv4_cidr_block = \"172.16.0.0/28\"\npods_range_name        = \"pods\"\nservices_range_name    = \"services\"\nkubernetes_version     = \"1.20.8-gke.700\"\n</code></pre> <p>In the next step, we going to create a <code>custom</code> GKE Node Pool.</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#284-create-a-gke-custom-node-pool","title":"2.8.4 Create a GKE <code>custom</code> Node pool","text":"<p>Using google_container_node_pool resource create a <code>custom</code> GKE Node Pool with following characteristics:</p> <p>Node Pool Configuration:</p> <ul> <li>VM size: <code>e2-small</code></li> <li>Node count: 1 per zone</li> <li>Node images: Container-Optimized OS</li> <li>The name of a GCE machine (VM) type: <code>e2-small</code></li> <li>GKE Node Pool Boot Disk Size: \"100 Gb\"</li> </ul> <p>Step 1: Let's define GKE resource first:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke.tf\n#Node Pool Resource\nresource \"google_container_node_pool\" \"custom-node_pool\" {\n  provider = google-beta\n\n  name       = \"main-pool\"\n  location = var.gcp_region\n  project  = var.gcp_project_id\n  cluster    = google_container_cluster.primary_cluster.name\n  node_count = var.gke_pool_node_count\n  version    = var.kubernetes_version\n\n  node_config {\n    image_type   = var.gke_pool_image_type\n    disk_size_gb = var.gke_pool_disk_size_gb\n    disk_type    = var.gke_pool_disk_type\n    machine_type = var.gke_pool_machine_type\n  }\n\n  timeouts {\n    create = \"10m\"\n    delete = \"10m\"\n  }\n\n  lifecycle {\n    ignore_changes = [\n      node_count\n    ]\n  }\n}\nEOF\n</code></pre> <p>Step 2: Next define GKE cluster specific variables:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; gke_variables.tf\n\n#Node Pool specific variables\nvariable \"gke_pool_machine_type\" {\n  type = string\n}\nvariable \"gke_pool_node_count\" {\n  type = number\n}\nvariable \"gke_pool_disk_type\" {\n  type = string\n  default = \"pd-standard\"\n}\nvariable \"gke_pool_disk_size_gb\" {\n  type = string\n}\nvariable \"gke_pool_image_type\" {\n  type = string\n}\nEOF\n</code></pre> <p>Task 9 (Continued): Complete <code>terraform.tfvars</code> with required values to GKE Node Pool values specified above:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; terraform.tfvars\n#pool specific\ngke_pool_node_count   = \"TODO\"\ngke_pool_image_type   = \"TODO\"\ngke_pool_disk_size_gb = \"TODO\"\ngke_pool_machine_type = \"TODO\"\nEOF\n</code></pre> <p>Solution: </p> <pre><code>#pool specific\ngke_pool_node_count   = \"1\"\ngke_pool_image_type   = \"COS\"\ngke_pool_disk_size_gb = \"100\"\ngke_pool_machine_type = \"e2-small\"\n</code></pre> <p>Step 3: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Step 4: Create GKE Cluster and Node Pool:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Output:</p> <pre><code>google_container_cluster.primary_cluster: Creating...\n...\ngoogle_container_cluster.primary_cluster: Creation complete after 20m9s \ngoogle_container_node_pool.custom-node_pool: Creating...\ngoogle_container_node_pool.custom-node_pool: Creation complete after 2m10s\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#285-update-gke-node-pool-to-support-auto-upgrade-and-auto-recovery-features","title":"2.8.5 Update GKE Node Pool to support Auto Upgrade and Auto Recovery features","text":"<p>Note</p> <p>GKE Master Nodes are managed by Google and get's upgraded automatically. Users can only specify Maintenance Window if they have preference for that process to occur (e.g. after busy hours). Users can however control Node Pool upgrade lifecycle. The can choose to do it themselves or with Auto Upgrade.</p> <p>Task N10: Using google_container_node_pool resource update node pool to support Auto Upgrade and Auto Recovery features.</p> <pre><code>edit gke.tf\n</code></pre> <p>Solution:</p> <pre><code>  management {\n    auto_repair  = true\n    auto_upgrade = true\n  }\n</code></pre> <p>Step 3: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>No errors.</p> <p>Step 4: Update GKE Cluster Node Pool configuration:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Summary</p> <p>Congrats! You've now learned how to deploy production grade GKE clusters.</p>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#29-optional-repeatable-infrastructure","title":"2.9 (Optional) Repeatable Infrastructure","text":"<p>When you doing IaC it is important to insure that you can both create and destroy resources consistently. This is especially important when doing CI/CD testing.</p> <p>Step 3: Destroy all resources:</p> <pre><code>terraform destroy -var-file terraform.tfvars\n</code></pre> <p>No errors.</p> <p>Step 4: Recreate all resources:</p> <pre><code>terraform plan -var-file terraform.tfvars\nterraform apply -var-file terraform.tfvars\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#3-create-documentation-for-terraform-code","title":"3. Create Documentation for terraform code","text":"<p>Documentation for your terraform code is an important part of IaC. Make sure all your variables have a good description!</p> <p>There are community tools that have been developed to make the documentation process smoother, in terms of documenting Terraform resources and requirements.Its good practice to also include a usage example snippet.</p> <p>Terraform-Docs is a good example of one tool that can generate some documentation based on the description argument of your Input Variables, Output Values, and from your required_providers configurations.</p> <p>Step 1 Install the terraform-docs cli to your Google CloudShell environment: </p> <pre><code>curl -Lo ./terraform-docs.tar.gz https://github.com/terraform-docs/terraform-docs/releases/download/v0.14.1/terraform-docs-v0.14.1-$(uname)-amd64.tar.gz\ntar -xzf terraform-docs.tar.gz\nrm terraform-docs.tar.gz\nchmod +x terraform-docs\nsudo mv terraform-docs /usr/local/bin/\nterraform-docs\n</code></pre> <p>Generating terraform documentation with Terraform Docs:</p> <pre><code>cd ~/$MY_REPO\ncd foundation-infrastructure\nterraform-docs markdown . &gt; README.md\ncd ../notepad-infrastructure\nterraform-docs markdown . &gt; README.md\n</code></pre> <p>Verify created documentation:</p> <pre><code>edit README.md\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#4-workaround-for-project-quota-issue","title":"4. Workaround for Project Quota issue","text":"<p>If you see following error during project creation in foundation layer:</p> <pre><code>Error: Error setting billing account \"010BE6-CA1129-195D77\" for project \"projects/ayrat-notepad-dev-244\": googleapi: Error 400: Precondition check failed., failedPrecondition\n</code></pre> <p>This is due to our Billing account has quota of 5 projects per account.</p> <p>To solve this issue find all unused accounts:</p> <pre><code>gcloud beta billing projects list --billing-account $ACCOUNT_ID\n</code></pre> <p>And unlink them, so you have less then 5 projects per account:</p> <pre><code>gcloud beta billing projects unlink $PROJECT_ID\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#5-commit-readme-doc-to-repository-and-share-it-with-instructorteacher","title":"5 Commit Readme doc to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>docs</code> folder using the following Git commands:</p> <pre><code>cd ~/$MY_REPO\n</code></pre> <pre><code>git add .\ngit commit -m \"Readme doc for Production GKE Creation using gcloud\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"020_Module_4_sol_Terraform_GCP_Foundation/#6-cleanup","title":"6 Cleanup","text":"<p>We only going to cleanup GCP Service foundation layer, as we going to use GCP project in future.</p> <pre><code>cd ~/$MY_REPO/notepad-infrastructure\nterraform destroy -var-file terraform.tfvars\n</code></pre>"},{"location":"020_Module_5_Assignment_CloudSQL_Database/","title":"Assignment 5 Deploying Cloud SQL Database and Connecting from GKE","text":"<p>Objective:</p> <ul> <li>In this lab, we will be focusing on deploying Cloud SQL instance first on GCP using Terraform </li> <li>Next we will look at configuration changes required in GKE deployment to connect to Cloud SQL instance</li> <li>Deploy a sample application in GKE which will be using Cloud SQL as the backend database </li> </ul>"},{"location":"020_Module_5_Assignment_CloudSQL_Database/#11-what-is-cloudsql","title":"1.1 What is CloudSQL?","text":"<p>CloudSQL is a Fully managed relational database service for MySQL, PostgreSQL, and SQL Server with rich extension collections, configuration flags, and developer ecosystems.</p>"},{"location":"020_Module_5_Assignment_CloudSQL_Database/#12-key-features","title":"1.2 Key features","text":"<p>Business continuity - Ensure business continuity with reliable, fault-tolerant and secure services managed by Google SRE team</p> <p>Automation - Automate database provisioning, storage capacity management, and other time-consuming tasks</p> <p>Observability - Database observability made easy for developers with Cloud SQL Insights</p> <p>Integration - Easy integration with existing apps and Google Cloud services like GKE and BigQuery</p>"},{"location":"020_Module_5_Assignment_CloudSQL_Database/#assignment","title":"Assignment","text":"<p>You will be performing the following steps in Cloud Shell. Launch Cloud Shell from Google Cloud Console</p> <p>Step 1 Cleanup any previous resources created in the project, including </p> <ul> <li> <p>Goto &gt; Kubernetes Engine &gt; Delete all clusters</p> </li> <li> <p>Goto &gt; VPC Networks &gt; Delete any custom VPC network</p> </li> <li> <p>Goto &gt; Cloud Router &gt; Delete any Router</p> </li> <li> <p>Goto &gt; Cloud NAT &gt; Delete any Cloud Nat Config</p> </li> </ul> <p>We will be creating all of these resources using Terraform configuration </p> <p>Step 2 Ensure that your cloud shell is set to the correct GCP Project </p> <pre><code>gcloud config set project &lt;PROJECT_ID&gt;\n</code></pre> <p>Activate all the api's which we will be using in the current assignment </p> <pre><code>gcloud services enable compute.googleapis.com sqladmin.googleapis.com \\\ncontainer.googleapis.com artifactregistry.googleapis.com cloudbuild.googleapis.com servicenetworking.googleapis.com\n</code></pre> <p>Step 3  Clone the following gitlab repository to your Cloud Shell, which going to use for our work:</p> <pre><code>git clone https://github.com/Cloud-Architects-Program/ycit020_2022.git\ncd ycit020_2022/cloud-sql/\n</code></pre> <p>Step 3 Understand the folder structure We have segregated the entire deployment and configuration into 3 three different folders </p> <ul> <li> <p><code>gke</code> </p> <ul> <li>represetnts the configuration requitred to spin up VPC Networks, Cloud NAT, Router and GKE AutoPilot configuration. In a typical organization, most of these would be done by the platform administration team as they plan the subnet allocation to be used for Cluster. They also set the allowed GKE features to be used     </li> </ul> </li> <li> <p><code>app-iac-folder</code> </p> <ul> <li> <p>represents all the Infra-Services that will be required to stand up the application. If your application requires CloudSQL, Memorystore/ Redis, Queueing system like Pub/Sub, Secret Manager, Alerting and Monitoring Configruration will all be declraed here </p> </li> <li> <p>In this case, all of the configuration was built using Terraform, but you can use any other tool like Cloud Connector, Pulumi, Gcloud commands to do these   </p> </li> </ul> </li> <li> <p><code>app-code-folder</code> </p> <ul> <li>has the Kubernetes Configs for your application deployment </li> <li><code>src</code> folder has the application code required to build the container image </li> </ul> </li> </ul> <p>Step 4 Deploy the GKE autopilot configuration using Terraform in a custom VPC</p> <pre><code>cd gke\n</code></pre> <p>Review the <code>terraform.tfvars</code> file for all the variable names used for cluster creation, network creation. Below are the sample values used in the Terraform</p> <pre><code>enable_private_nodes        = \"false\"  \nauto_master_ipv4_cidr_block = \"172.16.0.16/28\"        # IP Range to be used by Google Managed Kubernetes Master Nodes \nkubernetes_version          = \"1.22.12-gke.300\"       # From GKE Cluster requirements\nrelease_channel             = \"regular\"               # Release Channel Configuration\norg                         = \"&lt;STUDENT&gt;\"             # Values will be used for VPC and GKE creation\nproduct                     = \"notepad\"               # Values will be used for VPC and GKE creation\nenvironment                 = \"dev\"                   # Values will be used for VPC and GKE creation\ngcp_project_id              = \"&lt;PROJECT_ID&gt;\"          # GCP Project ID \ngcp_region                  = \"us-central1\"           # GCP Region \nnetwork_auto_cidr           = \"10.131.0.0/24\"         # GKE Node Range IP Address \npods_auto_cidr              = \"10.1.0.0/16\"           # GKE Pod Range IP Address\nservices_auto_cidr          = \"10.100.2.0/23\"         # GKE Service Range IP Address\n</code></pre> <p>Once the configuration has been updated to suitable values for <code>gcp_project_id</code> and <code>org</code>, you can perform the following steps </p> <pre><code>terraform init \nterraform plan \n</code></pre> <p>Review the terraform plan to check if there are any additional resources that are being modified or destroyed.</p> <pre><code>Plan: 5 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n  + endpoint              = (known after apply)\n  + id                    = (known after apply)\n  + master_version        = (known after apply)\n  + subnet_auto_selflink  = (known after apply)\n  + vpc_network_name      = (known after apply)\n  + vpc_network_self_link = (known after apply)\n</code></pre> <p>If all the previously deployed reources were all deleted, then the plan should only show new resources would be getting created. Upon verifying the plan output, you can apply the changes </p> <pre><code>terraform apply\n</code></pre> <p>Step 5 Connect to Autopilot cluster </p> <pre><code>clustername=$(terraform output gke_cluster_name | tr -d '\"')\ngcloud container clusters get-credentials $clustername --region us-central1\n</code></pre> <p>Once you have successfully connected to the cluster, you can perform the following commands as verification steps</p> <pre><code>kubectl get nodes  ## will display the node names\nkubectl get pods   ## will reutrn empty\n</code></pre> <p>Step 6 In this step you will be creating a MYSQL CloudSQL instance which will be used to host the database, using Terraform module. </p> <pre><code>cd ..\ncd app-iac-folder\n</code></pre> <p>Review the <code>cloud-sql.tf</code> configuration. </p> <ul> <li> <p>The <code>network</code> and <code>private-service-access</code> module are sed to create a Private network to host Cloud SQL database and a VPC Peering connection to the project </p> </li> <li> <p>The <code>additional_databases</code> section is used to define new databases to be used for application </p> </li> <li> <p>The <code>additional_user</code> section is used to provide username and password authentication for the database. </p> </li> </ul> <p>Important</p> <pre><code>Make sure to modify the password provided to your choice.\nRemember to update the `project_id` in the `terraform.tfvars` file.\n</code></pre> <p>Run the Terraform Script for Creating MySQL server and Provide the GCP service Account with Cloud SQL permissions</p> <pre><code>terraform init\nterraform plan \n</code></pre> <p>Review terraform output to see the resources that will be created </p> <pre><code>Plan: 16 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n  + mysql_conn             = (known after apply)\n  + mysql_user_pass        = (sensitive value)\n  + name                   = (known after apply)\n  + private_ip_address     = (known after apply)\n  + project_id             = \"&lt;PROJECT_ID&gt;\"\n  + public_ip_address      = (known after apply)\n  + reserved_range_address = (known after apply)\n  + reserved_range_name    = (known after apply)\n</code></pre> <p>Once you reivew the resources to be created and looks good, apply the terraform </p> <pre><code>terraform apply \n</code></pre> <p>Output:</p> <pre><code>mysql_conn = \"course-valavan:us-central1:example-mysql-private-a9aca803\"\nmysql_user_pass = &lt;sensitive&gt;\nname = \"example-mysql-private-a9aca803\"\nprivate_ip_address = \"10.34.0.2\"\nproject_id = \"&lt;PROJECT_ID&gt;\"\npublic_ip_address = \"x.x.x.x\"\nreserved_range_address = \"10.34.0.0\"\nreserved_range_name = \"google-managed-services-mysql-private-safer-a8bb20d40c\"\n</code></pre> <p>Verify the retuned SQL Connection ID , Public &amp; Private IP Address , VPC Services Peering range on the terraform output</p> <p>At this point, you can review the Cloud SQL instance created in Console. </p> <ul> <li> <p>Navigate to the Database Section to review the new databases created</p> </li> <li> <p>Navigate to the Users Section to review the username created to access the database </p> </li> </ul> <p>Step 7 Obtaining Service Account key file In the Terraform configuration, a Service account for the application with Cloud SQL client permissions are also created.  The configuration is listed in the <code>sa.tf</code> file.</p> <p>Navigate to <code>Service Accounts</code> Page in Console </p> <p></p> <p>Important</p> <pre><code>After this step, you will download the service account JSON key from Console to your computer. Please note the path where the file is downloaded. You will need it in the next steps**\n</code></pre> <p></p> <p>Step 8 Applicaiton build and deployment </p> <pre><code>cd ..\ncd app-code-folder\n</code></pre> <p>Create Artifact repo</p> <pre><code>gcloud artifacts repositories create gke-cloud-sql-repo \\\n--repository-format=docker \\\n--location=us-central1 \\\n--description=\"GKE Quickstart sample app\"\n</code></pre> <pre><code>gcloud auth configure-docker us-central1-docker.pkg.dev\n</code></pre> <p>Build the application image from Code and push it into the artifact repository. Replace the PROJECT_ID in the config below </p> <pre><code>gcloud builds submit --tag us-central1-docker.pkg.dev/&lt;PROJECT_ID&gt;/gke-cloud-sql-repo/gke-sql src/\n</code></pre> <p>Step 9 Create secret for DB - hostname, username, password, Service Account.</p> <p>Important</p> <p>In the Password field here, you will provide the same password used in the <code>cloudsql.tf</code> configuration file in the <code>app-iac-folder</code></p> <pre><code>kubectl create secret generic gke-cloud-sql-secrets \\\n--from-literal=database=quickstart-db \\\n--from-literal=username=quickstart-user \\\n--from-literal=password=PaSsWoRd\n</code></pre> <p>A successful output of the above step should have secret/gke-cloud-sql-secrets created. Copy the path for the downloaded service account key. In the next step, inject the service account key as a secret by poiting to the downloaded file. In the command below, replace the <code>key.json</code> with the actual path for the key</p> <pre><code>kubectl create secret generic sql-credentials --from-file=sql_credentials.json=~/Downloads/key.json\n</code></pre> <p>A successful output of the above step should have secret/sql-credentials created</p>"},{"location":"020_Module_5_Assignment_CloudSQL_Database/#task","title":"Task","text":"<p>Step 10 Modifying lines 34 and lines 75 in the <code>deployment.yaml</code> file to our desired configuration     </p> <p>Task 1</p> <ul> <li>Modify the image name on line 34, the image name should include an image from your project's artifact registry which you built on Step 8. You can find the image name in the artifact registry. You can copy the image name by hitting the <code>copy</code> link as shown in the image below</li> </ul> <p></p> <p>Task 2</p> <ul> <li>Modify the instances configuration on line 75. The instance name should represent the CloudSQL instance name you created in the previous step. You will find this in the Cloud SQL instance page. You will look for <code>connection name</code> as highlighted in the image</li> </ul> <p></p> <p>Step 11 As a final step, you can now run the deployment and service yaml </p> <pre><code>kubectl apply -f service-account.yaml\nkubectl apply -f deployment.yaml\n</code></pre> <p>You should be able to see the Deployement in the Kubernetes Cluster </p> <pre><code>kubectl get pods \nkubectl describe pods\n</code></pre> <p>Events sections will show if both containers started without any crashes</p> <pre><code>Events:\n  Type    Reason     Age   From                                   Message\n  ----    ------     ----  ----                                   -------\n  Normal  Scheduled  27s   gke.io/optimize-utilization-scheduler  Successfully assigned default/gke-cloud-sql-quickstart-54f464d4cc-rkkb8 to gk3-gke-auto-valavan-not-nap-i8jmtam0-15cdcc58-c9g6\n  Normal  Pulling    26s   kubelet                                Pulling image \"us-central1-docker.pkg.dev/course-valavan/gke-cloud-sql-repo/gke-sql:latest\"\n  Normal  Pulled     25s   kubelet                                Successfully pulled image \"us-central1-docker.pkg.dev/course-valavan/gke-cloud-sql-repo/gke-sql:latest\" in 243.900462ms\n  Normal  Created    25s   kubelet                                Created container gke-cloud-sql-app\n  Normal  Started    25s   kubelet                                Started container gke-cloud-sql-app\n  Normal  Pulling    25s   kubelet                                Pulling image \"gcr.io/cloudsql-docker/gce-proxy:latest\"\n  Normal  Pulled     24s   kubelet                                Successfully pulled image \"gcr.io/cloudsql-docker/gce-proxy:latest\" in 125.723593ms\n  Normal  Created    24s   kubelet                                Created container cloud-sql-proxy\n  Normal  Started    23s   kubelet                                Started container cloud-sql-proxy\n</code></pre> <p>To see the status of each container, you can use the following commands </p> <pre><code>PODNAME=$(kubectl get pods -o name)\nkubectl logs $PODNAME -c gke-cloud-sql-app\nkubectl logs $PODNAME -c cloud-sql-proxy\n</code></pre> <p>Finally, to make the application externally accessible, execute the following command</p> <pre><code>kubectl apply -f service.yaml\n</code></pre> <p>The service.yaml configuration also includes a Load balancer deployment in the backend Goto Load Balancer section to obtain the load balancer IP address </p> <pre><code>kubectl get svc  # will provide the external IP for Load Balancer\n</code></pre> <p>Sample Output</p> <pre><code>NAME                TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)        AGE\ngke-cloud-sql-app   LoadBalancer   10.100.2.214   34.67.178.40   80:31840/TCP   2m15s\n</code></pre> <p>Look for the EXTERNAL-IP section in the output above. You can open the  IP in a Web browser, which would load the application.</p> <p>Step 12 Assignment Submissions </p> <p>You should see the events for the pods with the following command:</p> <pre><code>kubectl get events --field-selector involvedObject.name=gke-cloud-sql-quickstart\n</code></pre> <p>Share the results with the instructor:</p> <pre><code>mkdir -p ~/$student_name-notepad/module5\ncd ~/$student_name-notepad/module5\n\n# post the commands \nkubectl describe pods &gt; k8s_pods_&lt;STUDENT&gt;.txt\nkubectl get all &gt; k8s_resources_&lt;STUDENT&gt;.txt\n</code></pre>"},{"location":"020_Module_5_Assignment_CloudSQL_Database_solution/","title":"Assignment 5 Deploying Cloud SQL Database and Connecting from GKE","text":"<p>Objective:</p> <ul> <li>In this lab, we will be focusing on deploying Cloud SQL instance first on GCP using Terraform </li> <li>Next we will look at configuration changes required in GKE deployment to connect to Cloud SQL instance</li> <li>Deploy a sample application in GKE which will be using Cloud SQL as the backend database </li> </ul>"},{"location":"020_Module_5_Assignment_CloudSQL_Database_solution/#11-what-is-cloudsql","title":"1.1 What is CloudSQL?","text":"<p>CloudSQL is a Fully managed relational database service for MySQL, PostgreSQL, and SQL Server with rich extension collections, configuration flags, and developer ecosystems.</p>"},{"location":"020_Module_5_Assignment_CloudSQL_Database_solution/#12-key-features","title":"1.2 Key features","text":"<p>Business continuity - Ensure business continuity with reliable, fault-tolerant and secure services managed by Google SRE team</p> <p>Automation - Automate database provisioning, storage capacity management, and other time-consuming tasks</p> <p>Observability - Database observability made easy for developers with Cloud SQL Insights</p> <p>Integration - Easy integration with existing apps and Google Cloud services like GKE and BigQuery</p>"},{"location":"020_Module_5_Assignment_CloudSQL_Database_solution/#assignment","title":"Assignment","text":"<p>You will be performing the following steps in Cloud Shell. Launch Cloud Shell from Google Cloud Console</p> <p>Step 1 Cleanup any previous resources created in the project, including </p> <ul> <li> <p>Goto &gt; Kubernetes Engine &gt; Delete all clusters</p> </li> <li> <p>Goto &gt; VPC Networks &gt; Delete any custom VPC network</p> </li> <li> <p>Goto &gt; Cloud Router &gt; Delete any Router</p> </li> <li> <p>Goto &gt; Cloud NAT &gt; Delete any Cloud Nat Config</p> </li> </ul> <p>We will be creating all of these resources using Terraform configuration </p> <p>Step 2 Ensure that your cloud shell is set to the correct GCP Project </p> <pre><code>gcloud config set project &lt;PROJECT_ID&gt;\n</code></pre> <p>Activate all the api's which we will be using in the current assignment </p> <pre><code>gcloud services enable compute.googleapis.com sqladmin.googleapis.com \\\ncontainer.googleapis.com artifactregistry.googleapis.com cloudbuild.googleapis.com servicenetworking.googleapis.com\n</code></pre> <p>Step 3  Clone the following gitlab repository to your Cloud Shell, which going to use for our work:</p> <pre><code>git clone https://github.com/Cloud-Architects-Program/ycit020_2022.git\ncd ycit020_2022/cloud-sql/\n</code></pre> <p>Step 3 Understand the folder structure We have segregated the entire deployment and configuration into 3 three different folders </p> <ul> <li> <p><code>gke</code> </p> <ul> <li>represetnts the configuration requitred to spin up VPC Networks, Cloud NAT, Router and GKE AutoPilot configuration. In a typical organization, most of these would be done by the platform administration team as they plan the subnet allocation to be used for Cluster. They also set the allowed GKE features to be used     </li> </ul> </li> <li> <p><code>app-iac-folder</code> </p> <ul> <li> <p>represents all the Infra-Services that will be required to stand up the application. If your application requires CloudSQL, Memorystore/ Redis, Queueing system like Pub/Sub, Secret Manager, Alerting and Monitoring Configruration will all be declraed here </p> </li> <li> <p>In this case, all of the configuration was built using Terraform, but you can use any other tool like Cloud Connector, Pulumi, Gcloud commands to do these   </p> </li> </ul> </li> <li> <p><code>app-code-folder</code> </p> <ul> <li>has the Kubernetes Configs for your application deployment </li> <li><code>src</code> folder has the application code required to build the container image </li> </ul> </li> </ul> <p>Step 4 Deploy the GKE autopilot configuration using Terraform in a custom VPC</p> <pre><code>cd gke\n</code></pre> <p>Review the <code>terraform.tfvars</code> file for all the variable names used for cluster creation, network creation. Below are the sample values used in the Terraform</p> <pre><code>enable_private_nodes        = \"false\"  \nauto_master_ipv4_cidr_block = \"172.16.0.16/28\"        # IP Range to be used by Google Managed Kubernetes Master Nodes \nkubernetes_version          = \"1.22.12-gke.300\"       # From GKE Cluster requirements\nrelease_channel             = \"regular\"               # Release Channel Configuration\norg                         = \"&lt;STUDENT&gt;\"             # Values will be used for VPC and GKE creation\nproduct                     = \"notepad\"               # Values will be used for VPC and GKE creation\nenvironment                 = \"dev\"                   # Values will be used for VPC and GKE creation\ngcp_project_id              = \"&lt;PROJECT_ID&gt;\"          # GCP Project ID \ngcp_region                  = \"us-central1\"           # GCP Region \nnetwork_auto_cidr           = \"10.131.0.0/24\"         # GKE Node Range IP Address \npods_auto_cidr              = \"10.1.0.0/16\"           # GKE Pod Range IP Address\nservices_auto_cidr          = \"10.100.2.0/23\"         # GKE Service Range IP Address\n</code></pre> <p>Once the configuration has been updated to suitable values for <code>gcp_project_id</code> and <code>org</code>, you can perform the following steps </p> <pre><code>terraform init \nterraform plan \n</code></pre> <p>Review the terraform plan to check if there are any additional resources that are being modified or destroyed.</p> <pre><code>Plan: 5 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n  + endpoint              = (known after apply)\n  + id                    = (known after apply)\n  + master_version        = (known after apply)\n  + subnet_auto_selflink  = (known after apply)\n  + vpc_network_name      = (known after apply)\n  + vpc_network_self_link = (known after apply)\n</code></pre> <p>If all the previously deployed reources were all deleted, then the plan should only show new resources would be getting created. Upon verifying the plan output, you can apply the changes </p> <pre><code>terraform apply\n</code></pre> <p>Step 5 Connect to Autopilot cluster </p> <pre><code>clustername=$(terraform output gke_cluster_name | tr -d '\"')\ngcloud container clusters get-credentials $clustername --region us-central1\n</code></pre> <p>Once you have successfully connected to the cluster, you can perform the following commands as verification steps</p> <pre><code>kubectl get nodes  ## will display the node names\nkubectl get pods   ## will reutrn empty\n</code></pre> <p>Step 6 In this step you will be creating a MYSQL CloudSQL instance which will be used to host the database, using Terraform module. </p> <pre><code>cd ..\ncd app-iac-folder\n</code></pre> <p>Review the <code>cloud-sql.tf</code> configuration. </p> <ul> <li> <p>The <code>network</code> and <code>private-service-access</code> module are used to create a Private network to host Cloud SQL database and a VPC Peering connection to the project </p> </li> <li> <p>The <code>additional_databases</code> section is used to define new databases to be used for application </p> </li> <li> <p>The <code>additional_user</code> section is used to provide username and password authentication for the database. </p> </li> </ul> <p>Important</p> <pre><code>Make sure to modify the password provided to your choice.\nRemember to update the `project_id` in the `terraform.tfvars` file.\n</code></pre> <p>Run the Terraform Script for Creating MySQL server and Provide the GCP service Account with Cloud SQL permissions</p> <pre><code>terraform init\nterraform plan \n</code></pre> <p>Review terraform output to see the resources that will be created </p> <pre><code>Plan: 16 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n  + mysql_conn             = (known after apply)\n  + mysql_user_pass        = (sensitive value)\n  + name                   = (known after apply)\n  + private_ip_address     = (known after apply)\n  + project_id             = \"&lt;PROJECT_ID&gt;\"\n  + public_ip_address      = (known after apply)\n  + reserved_range_address = (known after apply)\n  + reserved_range_name    = (known after apply)\n</code></pre> <p>Once you reivew the resources to be created and looks good, apply the terraform </p> <pre><code>terraform apply \n</code></pre> <p>Output:</p> <pre><code>mysql_conn = \"course-valavan:us-central1:example-mysql-private-a9aca803\"\nmysql_user_pass = &lt;sensitive&gt;\nname = \"example-mysql-private-a9aca803\"\nprivate_ip_address = \"10.34.0.2\"\nproject_id = \"&lt;PROJECT_ID&gt;\"\npublic_ip_address = \"x.x.x.x\"\nreserved_range_address = \"10.34.0.0\"\nreserved_range_name = \"google-managed-services-mysql-private-safer-a8bb20d40c\"\n</code></pre> <p>Verify the retuned SQL Connection ID , Public &amp; Private IP Address , VPC Services Peering range on the terraform output</p> <p>At this point, you can review the Cloud SQL instance created in Console. </p> <ul> <li> <p>Navigate to the Database Section to review the new databases created</p> </li> <li> <p>Navigate to the Users Section to review the username created to access the database </p> </li> </ul> <p>Step 7 Obtaining Service Account key file In the Terraform configuration, a Service account for the application with Cloud SQL client permissions are also created.  The configuration is listed in the <code>sa.tf</code> file.</p> <p>Navigate to <code>Service Accounts</code> Page in Console </p> <p></p> <p>Important</p> <pre><code>After this step, you will download the service account JSON key from Console to your computer. Please note the path where the file is downloaded. You will need it in the next steps**\n</code></pre> <p></p> <p>Step 8 Applicaiton build and deployment </p> <pre><code>cd ..\ncd app-code-folder\n</code></pre> <p>Create Artifact repo</p> <pre><code>gcloud artifacts repositories create gke-cloud-sql-repo \\\n--repository-format=docker \\\n--location=us-central1 \\\n--description=\"GKE Quickstart sample app\"\n</code></pre> <pre><code>gcloud auth configure-docker us-central1-docker.pkg.dev\n</code></pre> <p>Build the application image from Code and push it into the artifact repository. Replace the PROJECT_ID in the config below </p> <pre><code>gcloud builds submit --tag us-central1-docker.pkg.dev/&lt;PROJECT_ID&gt;/gke-cloud-sql-repo/gke-sql src/\n</code></pre> <p>Step 9 Create secret for DB - hostname, username, password, Service Account.</p> <p>Important</p> <p>In the Password field here, you will provide the same password used in the <code>cloudsql.tf</code> configuration file in the <code>app-iac-folder</code></p> <pre><code>kubectl create secret generic gke-cloud-sql-secrets \\\n--from-literal=database=quickstart-db \\\n--from-literal=username=quickstart-user \\\n--from-literal=password=PaSsWoRd\n</code></pre> <p>A successful output of the above step should have secret/gke-cloud-sql-secrets created. Copy the path for the downloaded service account key. In the next step, inject the service account key as a secret by poiting to the downloaded file. In the command below, replace the <code>key.json</code> with the actual path for the key</p> <pre><code>kubectl create secret generic sql-credentials --from-file=sql_credentials.json=~/Downloads/key.json\n</code></pre> <p>A successful output of the above step should have secret/sql-credentials created</p>"},{"location":"020_Module_5_Assignment_CloudSQL_Database_solution/#task","title":"Task","text":"<p>Step 10 Modifying lines 34 and lines 75 in the <code>deployment.yaml</code> file to our desired configuration     </p> <p>Task 1</p> <ul> <li>Modify the image name on line 34, the image name should include an image from your project's artifact registry which you built on Step 8. You can find the image name in the artifact registry. You can copy the image name by hitting the <code>copy</code> link as shown in the image below</li> </ul> <p></p> <p>Task 2</p> <ul> <li>Modify the instances configuration on line 75. The instance name should represent the CloudSQL instance name you created in the previous step. You will find this in the Cloud SQL instance page. You will look for <code>connection name</code> as highlighted in the image</li> </ul> <p></p> <p>SOLUTION</p> <ul> <li>This assignment has verification steps included as part of the setup itself</li> <li>Verification steps :  <ul> <li>Check if the GKE Cluster is created successfully through Terraform and verify in Console </li> <li>Ensure that the Cloud SQL with Private access is created through Terraform and verify the output in Console</li> <li>The Artifact Registry was created and the application image has been pushed to the artifact registry    Once all of the verification steps are completed, the last step is to create the deployment based with parameters from the above step - docker image and SQL connection information </li> </ul> </li> </ul> <p></p> <p>Step 11 As a final step, you can now run the deployment and service yaml </p> <pre><code>kubectl apply -f service-account.yaml\nkubectl apply -f deployment.yaml\n</code></pre> <p>You should be able to see the Deployement in the Kubernetes Cluster </p> <pre><code>kubectl get pods \nkubectl describe pods\n</code></pre> <p>Events sections will show if both containers started without any crashes</p> <pre><code>Events:\n  Type    Reason     Age   From                                   Message\n  ----    ------     ----  ----                                   -------\n  Normal  Scheduled  27s   gke.io/optimize-utilization-scheduler  Successfully assigned default/gke-cloud-sql-quickstart-54f464d4cc-rkkb8 to gk3-gke-auto-valavan-not-nap-i8jmtam0-15cdcc58-c9g6\n  Normal  Pulling    26s   kubelet                                Pulling image \"us-central1-docker.pkg.dev/course-valavan/gke-cloud-sql-repo/gke-sql:latest\"\n  Normal  Pulled     25s   kubelet                                Successfully pulled image \"us-central1-docker.pkg.dev/course-valavan/gke-cloud-sql-repo/gke-sql:latest\" in 243.900462ms\n  Normal  Created    25s   kubelet                                Created container gke-cloud-sql-app\n  Normal  Started    25s   kubelet                                Started container gke-cloud-sql-app\n  Normal  Pulling    25s   kubelet                                Pulling image \"gcr.io/cloudsql-docker/gce-proxy:latest\"\n  Normal  Pulled     24s   kubelet                                Successfully pulled image \"gcr.io/cloudsql-docker/gce-proxy:latest\" in 125.723593ms\n  Normal  Created    24s   kubelet                                Created container cloud-sql-proxy\n  Normal  Started    23s   kubelet                                Started container cloud-sql-proxy\n</code></pre> <p>To see the status of each container, you can use the following commands </p> <pre><code>PODNAME=$(kubectl get pods -o name)\nkubectl logs $PODNAME -c gke-cloud-sql-app\nkubectl logs $PODNAME -c cloud-sql-proxy\n</code></pre> <p>Finally, to make the application externally accessible, execute the following command</p> <pre><code>kubectl apply -f service.yaml\n</code></pre> <p>The service.yaml configuration also includes a Load balancer deployment in the backend Goto Load Balancer section to obtain the load balancer IP address </p> <pre><code>kubectl get svc  # will provide the external IP for Load Balancer\n</code></pre> <p>Sample Output</p> <pre><code>NAME                TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)        AGE\ngke-cloud-sql-app   LoadBalancer   10.100.2.214   34.67.178.40   80:31840/TCP   2m15s\n</code></pre> <p>Look for the EXTERNAL-IP section in the output above. You can open the  IP in a Web browser, which would load the application.</p> <p>Step 12 Assignment Submissions </p> <p>You should see the events for the pods with the following command:</p> <pre><code>kubectl get events --field-selector involvedObject.name=gke-cloud-sql-quickstart\n</code></pre> <p>Share the results with the instructor:</p> <pre><code>mkdir -p ~/$student_name-notepad/module5\ncd ~/$student_name-notepad/module5\n\n# post the commands \nkubectl describe pods &gt; k8s_pods_&lt;STUDENT&gt;.txt\nkubectl get all &gt; k8s_resources_&lt;STUDENT&gt;.txt\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation/","title":"Assignment 6 Learning Helm","text":"<p>Objective:</p> <ul> <li>Installing and Configuring the Helm Client</li> <li>Deploy Kubernetes apps with Helm Charts</li> <li>Learn Helm Commands</li> <li>Learn Helm Repository</li> <li>Create Helm chart</li> <li>Learn Helm Plugins</li> <li>Learn Helm File</li> </ul>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#prerequisite","title":"Prerequisite","text":"<p>Enable the needed services in the project</p> <pre><code>export PROJECT_ID=&lt;YOUR_PROJECT_ID&gt;\ngcloud config set project $PROJECT_ID\ngcloud services enable compute.googleapis.com cloudresourcemanager.googleapis.com container.googleapis.com servicenetworking.googleapis.com\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#locate-assignment-6","title":"Locate Assignment 6","text":"<p>Step 1  Clone <code>ycit020_2022</code> repo with Kubernetes manifests, which going to use for our work:</p> <pre><code>cd ~/ycit020_2022\n# Alternatively: cd ~ &amp; git clone https://github.com/Cloud-Architects-Program/ycit020_2022.git\ngit pull\ncd ~/ycit020_2022/assignment6_helm/\nls\n</code></pre> <p>Result</p> <p>You can see Kubernetes manifests and terraform configs</p> <p>Step 2 Go into your personal Google Cloud Source Repository:</p> <pre><code>MY_REPO=your_student_id-notepad\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$MY_REPO\n</code></pre> <p>Pull latest changes:</p> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 3 (Optional) If your terraform config from previous assignment is not working you can copy working config from <code>assignment6_helm</code> folder to your <code>notepad-infrastructure</code> folder.</p> <p>Step 4 Create exercise 1 folder for supporting applications</p> <pre><code>cd ~/$MY_REPO\nmkdir -p exercises/exercise1\n\ncat &lt;&lt;EOF&gt; exercises/exercise1/README.md\n# Setting up the infraestructure with Terraform\nEOF\n</code></pre> <p>Step 5 Copy Assignment 4 <code>deploy</code> folder to your repo:</p> <pre><code>cd ~/$MY_REPO/\ncp -r ~/ycit020_2022/assignment6_helm/deploy deploy\nls deploy\n</code></pre> <p>Result</p> <p>You should see <code>k8s-manifest</code> folder</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#reserve-static-ip-addresses","title":"Reserve Static IP addresses","text":"<p>In the next few assignments we might deploy many applications and so we will require expose them using Ingress.</p> <p>When you create an Ingress object, you get a stable external IP address that clients can use to access your Services and in turn, your running containers. The IP address is stable in the sense that it lasts for the lifetime of the Ingress object. If you delete your Ingress and create a new Ingress from the same manifest file, you are not guaranteed to get the same external IP address.</p> <p>For this application, we would like to have a permanent IP address that can persist even if the Ingress resource is recreated.</p> <p>For that you must reserve a <code>Regional</code> static <code>External</code> IP address and provide it teacher, so we can setup <code>A Record</code> on the DNS for your behalf.</p> <p>Reference: </p> <ol> <li>Reserving a static external IP address</li> <li>Terraform resource google_compute_address</li> </ol> <p>Step 1: Create folder for static ip terraform configuration:</p> <pre><code>cd ~/$MY_REPO/\nmkdir ip-infrastructure\n</code></pre> <p>Important</p> <p>The reason we creating a new folder for static_ip creation is because we don't want to destroy or delete this IP throughout our training.</p> <p>Step 1: Create Terraform configuration:</p> <p>Declare Provider:</p> <pre><code>cd ~/$MY_REPO/ip-infrastructure\ncat &lt;&lt; EOF&gt;&gt; provider.tf\nterraform {\n  required_providers {\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~&gt; 3.70.0\"\n    }\n  }\n}\nEOF\n</code></pre> <p>Configure global address resource:</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; static_ip.tf\nprovider \"google\" {\n  project = var.gcp_project_id\n}\n\nresource \"google_compute_address\" \"regional_external_ip\" {\n  provider      = google\n  name          = \"static-ingres-ip\"\n  address_type  = \"EXTERNAL\"\n  region        = \"us-central1\"\n}\nEOF\n</code></pre> <p>Configure variables:</p> <pre><code>cat &lt;&lt;EOF&gt; variables.tf\nvariable \"gcp_project_id\" {\n  type        = string\n  description = \"The GCP Seeding project ID\"\n  default     = \"\"\n}\nEOF\n</code></pre> <p>Task: Create a <code>.tfvars</code> file with your project_id:</p> <pre><code>gcp_project_id = \"$PROJECT_ID\"\n</code></pre> <p>Configure outputs:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"addresses\" {\n  description = \"Global IPv4 address for proxy load balancing to the nearest Ingress controller\"\n  value       = google_compute_address.regional_external_ip.address\n}\n\noutput \"name\" {\n  description = \"Static IP Name\"\n  value       = google_compute_address.regional_external_ip.name\n}\nEOF\n</code></pre> <p>Step 2: Apply Terraform configuration:</p> <p>Initialize:</p> <pre><code>terraform init\n</code></pre> <p>Plan and Deploy Infrastructure:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Result</p> <p>Plan: 1 to add, 0 to change, 0 to destroy.  Changes to Outputs: \u2003   + addresses = (known after apply) \u2003   + name      = \"static-ingres-ip\"</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Set variable:</p> <pre><code>cd ~/$MY_REPO/ip-infrastructure\nexport STATIC_IP_ADDRESS=$(terraform output -raw addresses )\nexport STATIC_IP_NAME=$(terraform output -raw name )\n</code></pre> <p>Create readme:</p> <pre><code>cat &lt;&lt;EOF&gt; README.md\n\nGenerated Static IP for Ingress:\n\n    * Name - $STATIC_IP_NAME # Used Ingress Manifest\n    * Address - $STATIC_IP_ADDRESS # Used to Configure DNS A Record\nEOF\n\ncp README.md ~/$MY_REPO/exercises/exercise1\n</code></pre> <p>Important</p> <p>Add information about your static_ip_address the tab \"Assignment 6\" in column C under your name in this spreadsheet: 2022 YCIT020 Student Project Info</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#commit-to-repository","title":"Commit to Repository","text":"<p>Step 1 Commit <code>ip-infrastructure</code> and <code>helm</code> folders using the following Git commands:</p> <pre><code>cd ~/$MY_REPO\ngit status\ngit add .\ngit commit -m \"adding documentation for ycit020 assignment 6\"\n</code></pre> <p>Step 2 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#1-install-and-configure-helm-client","title":"1 Install and Configure Helm Client","text":""},{"location":"020_Module_6_Assignment_Helm_Foundation/#11-install-helm-3","title":"1.1 Install Helm 3","text":"<p>GCP Cloud Shell comes with many common tools pre-installed including <code>helm</code>.</p> <p>Step 1: Verify and validate the version of <code>Helm</code> that is installed:</p> <pre><code>helm version\n</code></pre> <p>Output:</p> <pre><code>version.BuildInfo{Version:\"v3.9.3\", GitCommit:\"414ff28d4029ae8c8b05d62aa06c7fe3dee2bc58\", GitTreeState:\"clean\", GoVersion:\"go1.17.13\"}\n</code></pre> <p>Result</p> <p>Helm 3 is installed</p> <p>Note</p> <p>Until November 2020, two different major versions of Helm were actively maintained. The current stable major version of Helm is version 3. </p> <p>Note</p> <p>Helm follows a versioning convention known as Semantic Versioning (SemVer). In Semantic Versioning, the version number conveys meaning about what you can expect in the release. Because Helm follows this specification, users can expect certain things out of releases simply by carefully reading the version number. At its core, a semantic version has three numerical components and an optional stability marker (for alphas, betas, and release candidates). Here are some examples:</p> <ul> <li>v1.0.0</li> <li>v3.3.2</li> </ul> <p>SemVer represents format of  <code>X.Y.Z</code>, where <code>X</code> is a major version, <code>Y</code> is a minor version and <code>Z</code> is a patch release:</p> <ul> <li> <p>The major release number tends to be incremented infrequently. It indicates that major changes have been made to Helm, and that some of those changes may break compatibility with previous versions. The difference between Helm 2 and Helm 3 is substantial, and there is work necessary to migrate between the versions.</p> </li> <li> <p>The minor release number indicates feature additions. The difference between 3.2.0 and 3.3.0 might be that a few small new features were added. However, there are no breaking changes between versions. (With one caveat: a security fix might necessitate a breaking change, but we announce boldly when that is the case.)</p> </li> <li> <p>The patch release number indicates that only backward compatible bug fixes have been made between this release and the last one. It is always recommended to stay at the latest patch release.</p> </li> </ul> <p>(OPTIONAL) Step 2: If you want to use specific version of <code>helm</code> or want to install <code>helm</code> in you local machine (On macOS and Linux,) use following link to install Helm. </p> <p>The usual sequence of commands for installing this way is as follows:</p> <pre><code>$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n$ chmod +x get_helm.sh\n$ ./get_helm.sh\n</code></pre> <p>The preceding commands fetch the latest version of the <code>get_helm.sh</code> script, and then uses that to find and install the latest version of Helm 3.</p> <p>Alternatively, you can download latest binary of your choice here.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#13-deploy-gke-cluster","title":"1.3 Deploy GKE cluster","text":"<p>We going to reuse Terraform configuration to deploy our GKE Cluster. And we assume that <code>terraform.tfvars</code> has been properly configured already with required values.</p> <p>Step 1: Locate Terraform Configuration directory.</p> <pre><code>cd ~/$MY_REPO/notepad-infrastructure\n</code></pre> <p>Step 2: Modify your backend</p> <p>In case you are not using the output of the previous assignment, create a bucket to host your terraform state</p> <pre><code>gsutil mb gs://&lt;STUDENT_NAME&gt;-notepad-dev-tfstate\n</code></pre> <p>Task: Modify the <code>backend.tf</code> file to point to your newly created bucket</p> <p>Step 3: Initialize Terraform Providers</p> <pre><code>terraform init\n</code></pre> <p>Step 4:  Increase GKE Node Pool VM size</p> <p>Modify the <code>terraform.tfvars</code></p> <pre><code>edit terraform.tfvars\n</code></pre> <p>Update <code>gke_pool_machine_type</code> from <code>e2-small</code> to <code>e2-highcpu-4</code>, to support larger workloads. Also make sure to update the <code>gcp_project_id</code> and <code>org</code> if you are starting from the posted solution.</p> <p>Step 5: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Step 6: Create GKE Cluster and Node Pool:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Result</p> <p>GKE Clusters has been created</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#14-configure-helm","title":"1.4 Configure Helm","text":"<p>Helm interacts directly with the Kubernetes API server. For that reason, Helm needs to be able to connect to a Kubernetes cluster. Helm attempts to do this automatically by reading the same configuration files used by <code>kubectl</code>.</p> <p>Helm will try to find this information by reading the environment variable <code>$KUBECONFIG</code>.</p> <p>Step 1 Authenticate to the cluster.</p> <pre><code>export STUDENT_NAME=&lt;STUDENT_NAME&gt;\ngcloud container clusters get-credentials gke-$STUDENT_NAME-notepad-dev --region us-central1 \n</code></pre> <p>Step 2 Test that <code>kubectl</code> client connected to GKE cluster:</p> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>No resources found in default namespace.\n</code></pre> <p>Step 3 Test that helm client connected to GKE cluster:</p> <pre><code>helm list\n</code></pre> <p>Output:</p> <pre><code>NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION\n</code></pre> <p>Result</p> <p>As expected there are no Charts has been Installed on our cluster yet</p> <p>Summary</p> <p>Helm 3 has been installed and configured to work with our cluster. Let's deploy some charts!</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#2-basic-helm-chart-installation","title":"2 Basic Helm Chart Installation","text":""},{"location":"020_Module_6_Assignment_Helm_Foundation/#21-searching-chart","title":"2.1 Searching Chart","text":"<p>A Helm chart is a packaged application that can be installed into your Kubernetes cluster. During chart development, you will often just work with a chart that is stored on your local system and later pushed to GitHub.</p> <p>But when it comes to sharing charts, Helm describes a standard format for indexing and sharing information about Helm <code>charts</code>. A Helm chart <code>repository</code> is simply a set of files, reachable over the network, that conforms to the Helm specification for indexing packages.</p> <p>There are huge number of chart repositories on the internet. The easiest way to find the popular repositories is to use your web browser to navigate to the Artifact Hub. There you will find thousands of Helm charts, each hosted on an appropriate repository.</p> <p>Deprecation</p> <p>In the past all charts were located and maintained by Helm Kubernetes Community Git repositories, known as https://github.com/kubernetes/charts, and had 2 types:</p> <ul> <li>Stable</li> <li>Incubator</li> </ul> <p>All the charts were rendered in Web UI via Helm Hub page.</p> <p>While the central Git Repository to maintain charts was great idea, with fast growing Helm popularity, it become hard to impossible to manage and maintain it by small group of maintainers, as they had to aprove hundreds of PR per day and frustrating for chart contributors as they had to wait several weeks their PR to be reviewed and approved.</p> <p>As a result GitHub project for Helm <code>stable</code> and <code>incubator</code> charts as well as [Helm Hub] has been deprecated and archived. All the charts are now maintained by independent contributors in their subsequent repo's. (e.g vault, is under hashicorp/vault-helm repo) and the central place to find all active and official Charts can be foind in Artifact Hub.</p> <p>Important</p> <p>Helm 2 came with a Helm repository installed by default. The <code>stable</code> chart repository was at one time the official source of production-ready Helm charts. As we discussed above  <code>stable</code> chart repository has been now deprecated.</p> <p>In Helm 3, there is no <code>default</code> repository. Users are encouraged to use the Artifact Hub to find what they are looking for and then add their preferred repositories.</p> <p>Step 0: Setup exercise folder</p> <pre><code>mkdir -p ~/$MY_REPO/exercises/exercise2\ncd ~/$MY_REPO/exercises/exercise2\n\ncat &lt;&lt;EOF&gt; exercises/exercise1/README.md\n# Helm search and Install exercise\nEOF\n</code></pre> <p>Step 1: Helm provides native way to search charts from CLI in artifacthub.io</p> <p>Search for drupal chart:</p> <pre><code>helm search hub drupal\n</code></pre> <p>Output:</p> <pre><code>https://artifacthub.io/packages/helm/bitnami/drupal 10.2.30         9.2.3       One of the most versatile open source content m...\nhttps://artifacthub.io/packages/helm/cetic/drupal   0.1.0           1.16.0      Drupal is a free and open-source web content ma..\n</code></pre> <p>Result</p> <p>Search is a good way to find existing Helm packages</p> <p>Step 2: Use the link to navigate to Artifact Hub Drupal chart:</p> <pre><code>https://artifacthub.io/packages/helm/bitnami/drupal\n</code></pre> <p>Note</p> <p>Drupal is OSS content management systems that can be installed on k8s.</p> <p>Result: The Artifact Page opened with information about the chart, it's parameters and how to use the chart.</p> <p>Step 3: Review Github source Code of the chart:</p> <pre><code>https://github.com/bitnami/charts/tree/master/bitnami/drupal\n</code></pre> <p>Result: Github page with Drupal Helm Chart itself, where you can browse the <code>templates</code>, <code>values.yaml</code>, <code>Chart.yaml</code> to understand how this chart is actually working and if you have any issues with the chart this would be the right location to open the issue or send a PR with new feature if you decide to contribute.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#22-adding-a-chart-repository","title":"2.2 Adding a Chart Repository","text":"<p>Once you found a chart, it's logical to install it. However first step you need to do is to add a Chart Repository.</p> <p>Step 1: Adding a Helm chart is done with the <code>helm repo add</code> command:</p> <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\n</code></pre> <p>Note</p> <p>Bitnami is company well known to package application for Any Platforms and Cloud environments. With popularity of Helm, Bitnami developers were among the core contributors who designed the Helm repository system. They have contributed to the establishment of Helm\u2019s best practices for chart development and have written many of the most widely used charts. Bitnami is now part of VMware, provides IT organizations with an enterprise offering that is secure, compliant, continuously maintained and customizable to your organizational policies.</p> <p>Step 2: Now, we can verify that the Bitnami repository exists by running a <code>helm repo list</code> command:</p> <pre><code>helm repo list\n</code></pre> <p>Output:</p> <pre><code>NAME    URL                               \nbitnami https://charts.bitnami.com/bitnami\n</code></pre> <p>Result</p> <p>This command shows us all of the repositories installed for Helm. Right now, we see only the Bitnami repository that we just added.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#23-helm-chart-installation","title":"2.3 Helm Chart Installation","text":"<p>Step 1: At very minimum, installing a chart in Helm requires just 2 pieces of information: the <code>name</code> of the installation and the <code>chart</code> you want to install:</p> <pre><code>helm install mywebsite bitnami/drupal\n</code></pre> <p>Output:</p> <pre><code>NAME: mywebsite\nLAST DEPLOYED: Sun Aug  8 08:25:29 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\n*******************************************************************\n*** PLEASE BE PATIENT: Drupal may take a few minutes to install ***\n*******************************************************************\n\n1. Get the Drupal URL:\n\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        Watch the status with: 'kubectl get svc --namespace default -w mywebsite-drupal'\n\n  export SERVICE_IP=$(kubectl get svc --namespace default mywebsite-drupal --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\")\n  echo \"Drupal URL: http://$SERVICE_IP/\"\n\n2. Get your Drupal login credentials by running:\n\n  echo Username: user\n  echo Password: $(kubectl get secret --namespace default mywebsite-drupal -o jsonpath=\"{.data.drupal-password}\" | base64 --decode)\n</code></pre> <p>Result</p> <p>When using Helm, you will see that output for each installation. A good chart would provide helpful <code>notes</code> on how to connect to the deployed solution.</p> <p>Tip</p> <p>You can get <code>notes</code> information any time after helm installation using <code>helm get notes mywebsite</code> command.</p> <p>Step 2: Follow your Drupal Helm Chart <code>notes</code> Instruction to access Website</p> <pre><code>1. Get the Drupal URL:\n\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        Watch the status with: 'kubectl get svc --namespace test -w mywebsite-drupal'\n\n  export SERVICE_IP=$(kubectl get svc --namespace test mywebsite-drupal --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\")\n  echo \"Drupal URL: http://$SERVICE_IP/\"\n</code></pre> <p>Success</p> <p>We can access <code>My blog</code> website and login with provider user and password. You can start your own blog now, that runs on Kubernetes</p> <p>Step 3: List deployed Kubernetes resources:</p> <pre><code>kubectl get all \nkubectl get pvc\n</code></pre> <p>Summary</p> <p>Our Drupal website consist of MariaDB <code>statefulset</code>, Drupal <code>deployment</code>, 2 <code>pvc</code>s and  <code>services</code>. <code>Mywebsite</code> Drupal service is type <code>LoadBalancer</code> and that's how we able to access it.</p> <p>Step 4: List installed chart:</p> <pre><code>helm list\n</code></pre> <p>Output:</p> <pre><code>NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION\nmywebsite       default         1               2021-08-08 08:25:29.625862 -0400 EDT    deployed        drupal-10.2.30          9.2.3\n</code></pre> <p>Note</p> <p>Like other commands, helm list is <code>namespace</code> aware. By default, Helm uses the <code>namespace</code> your Kubernetes configuration file sets as the default. Usually this is the namespace named <code>default</code>.</p> <p>Step 5: Deploy same chart in namespace <code>test</code>: </p> <pre><code>kubectl create ns test\nhelm install --namespace test mywebsite bitnami/drupal --create-namespace\n</code></pre> <p>Tip</p> <p>By adding <code>--create-namespace</code>, indicates to Helm that we acknowledge that there may not be a namespace with that name already, and we just want one to be created.</p> <p>List deployed chart in namespace <code>test</code>: </p> <pre><code>helm list --namespace test\n</code></pre> <p>Summary</p> <p>In Helm 2, instance names were cluster-wide. You could only have an instance named <code>mywebsite</code> once per cluster. </p> <p>In Helm 3, naming has been changed. Now instance names are scoped to Kubernetes namespaces. We could install 2 instances named <code>mywebsite</code> as long as they each lived in a different namespace.</p> <p>Step 5: Submit the output into the assignments folder</p> <p>Commit the results to the repository:</p> <pre><code># make sure you are in exercise2 folder\nkubectl describe pods &gt; k8s_pods_$STUDENT_NAME_default.txt\nkubectl describe pods -n test &gt; k8s_pods_$STUDENT_NAME_test.txt\nkubectl get all -A &gt; k8s_resources_$STUDENT_NAME.txt\n\ngit add .\ngit commit -m \"helm search and install exercise\"\ngit push \n</code></pre> <p>Step 6: Cleanup Helm Drupal deployments using <code>helm uninstall</code> command:</p> <p>Uninstall the charts</p> <pre><code>helm uninstall mywebsite\nhelm uninstall mywebsite -n test \n</code></pre> <p>Remove the test namespace</p> <pre><code>kubectl delete ns test\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#3-advanced-helm-chart-installation","title":"3 Advanced Helm Chart Installation","text":""},{"location":"020_Module_6_Assignment_Helm_Foundation/#31-deploy-nginx-ingress-chart-with-custom-configuration","title":"3.1 Deploy NGINX Ingress Chart with Custom Configuration","text":"<p>Let's deploy another Helm application on our Kubernetes cluster: Ingress Nginx - is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer.</p> <p>In the previous classes we used Google implementation of Ingress - GKE Ingress for HTTP(S) Load Balancing. While this solutions provides managed Ingress experience and advanced features like Cloud Armor, DDoS protection and Identity aware proxy.</p> <p>Ingress Nginx Controller is popular solution and has a lot of features and integrations. If you want to deploy Kubernetes Application on different cloud providers or On-prem the same way, Nginx Ingress Controller becomes a default option.</p> <p>Our task is to configure Ingress Nginx using <code>type:LoadBalancer</code> with Regional Static IP configured in the section ### Reserve Static IP addresses. Additionally we want to enable metrics service that will fetch Prometheus monitoring metrics from ingress and disable admissionWebhooks configuration.</p> <p>Step 0: Setup exercise folder</p> <pre><code>mkdir -p ~/$MY_REPO/exercises/exercise3\ncd ~/$MY_REPO/exercises/exercise3\n\ncat &lt;&lt;EOF&gt; exercises/exercise3/README.md\n# Advanced Helm Chart Installation\nEOF\n</code></pre> <p>Step 1: Let's search <code>ingress-nginx</code> in artifacthub.io</p> <pre><code>helm search hub ingress-nginx\n</code></pre> <p>Output:</p> <pre><code>URL                                                     CHART VERSION   APP VERSION     DESCRIPTION                                       \nhttps://artifacthub.io/packages/helm/ingress-ng...      4.3.0           1.4.0           Ingress controller for Kubernetes using NGINX a...\nhttps://artifacthub.io/packages/helm/ingress-ng...      4.1.0           1.2.0           Ingress controller for Kubernetes using NGINX a...\n</code></pre> <p>Result</p> <p>We going to select the first <code>ingress-nginx</code> chart that is maintained by Kubernetes Community</p> <p>Step 2: Review Hub Page details about this chart and locate information how to add repository:</p> <pre><code>https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx\n</code></pre> <p>Result: The Artifact Page opened with information about the chart, it's parameters, and how to use the chart itself</p> <p>Step 3: Add <code>ingress-nginx</code>  Repo:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update  \n</code></pre> <p>Step 4: Oftentimes, searching is a useful way to find not only what <code>charts</code> can be installed, but what <code>versions</code> are available:</p> <pre><code>helm search repo ingress-nginx --versions | head\n</code></pre> <p>Output:</p> <pre><code>NAME                            CHART VERSION   APP VERSION     DESCRIPTION                                       \ningress-nginx/ingress-nginx     4.3.0           1.4.0           Ingress controller for Kubernetes using NGINX a...\ningress-nginx/ingress-nginx     4.2.5           1.3.1           Ingress controller for Kubernetes using NGINX a...\n</code></pre> <p>Note</p> <p>By default, Helm tries to install the latest <code>stable</code> release of a chart, but you can override this behavior and install a specific version of a chart. Thus it is often useful to see not just the summary info for a chart, but exactly which versions exist for a chart. Every new version of the chart can be bring fixes and new changes, so for production use it's better to go with tested version and pin installation version.</p> <p>Summary</p> <p>We going with latest listed official version of the chart <code>4.3.0</code> of <code>ingress-nginx</code> Chart.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#32-download-and-inspect-chart-locally","title":"3.2 Download and inspect Chart locally","text":"<p>Step 1: Pull <code>ingress-nginx</code> Helm Chart of specific version to Local filesystem:</p> <pre><code># ensure you are in exercise3 folder\nhelm pull ingress-nginx/ingress-nginx --version 4.3.0\ntar -xvzf  ingress-nginx-4.3.0.tgz\n</code></pre> <p>Step 2:  See the tree structure of the chart</p> <pre><code>sudo apt-get install tree\ntree -L 2 ingress-nginx\n</code></pre> <p>Output:</p> <pre><code>ingress-nginx\n\u251c\u2500\u2500 CHANGELOG.md\n\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 ci\n\u2502   \u251c\u2500\u2500 controller-custom-ingressclass-flags.yaml\n\u2502   \u251c\u2500\u2500 daemonset-customconfig-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-customnodeport-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-extra-modules.yaml\n\u2502   \u251c\u2500\u2500 daemonset-headers-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-internal-lb-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-nodeport-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-podannotations-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-tcp-udp-configMapNamespace-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-tcp-udp-portNamePrefix-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-tcp-udp-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-tcp-values.yaml\n\u2502   \u251c\u2500\u2500 deamonset-default-values.yaml\n\u2502   \u251c\u2500\u2500 deamonset-metrics-values.yaml\n\u2502   \u251c\u2500\u2500 deamonset-psp-values.yaml\n\u2502   \u251c\u2500\u2500 deamonset-webhook-and-psp-values.yaml\n\u2502   \u251c\u2500\u2500 deamonset-webhook-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-autoscaling-behavior-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-autoscaling-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-customconfig-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-customnodeport-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-default-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-extra-modules.yaml\n\u2502   \u251c\u2500\u2500 deployment-headers-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-internal-lb-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-metrics-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-nodeport-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-podannotations-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-psp-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-tcp-udp-configMapNamespace-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-tcp-udp-portNamePrefix-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-tcp-udp-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-tcp-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-webhook-and-psp-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-webhook-extraEnvs-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-webhook-resources-values.yaml\n\u2502   \u2514\u2500\u2500 deployment-webhook-values.yaml\n\u251c\u2500\u2500 OWNERS\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 README.md.gotmpl\n\u251c\u2500\u2500 templates\n\u2502   \u251c\u2500\u2500 admission-webhooks\n\u2502   \u251c\u2500\u2500 clusterrolebinding.yaml\n\u2502   \u251c\u2500\u2500 clusterrole.yaml\n\u2502   \u251c\u2500\u2500 controller-configmap-addheaders.yaml\n\u2502   \u251c\u2500\u2500 controller-configmap-proxyheaders.yaml\n\u2502   \u251c\u2500\u2500 controller-configmap-tcp.yaml\n\u2502   \u251c\u2500\u2500 controller-configmap-udp.yaml\n\u2502   \u251c\u2500\u2500 controller-configmap.yaml\n\u2502   \u251c\u2500\u2500 controller-daemonset.yaml\n\u2502   \u251c\u2500\u2500 controller-deployment.yaml\n\u2502   \u251c\u2500\u2500 controller-hpa.yaml\n\u2502   \u251c\u2500\u2500 controller-ingressclass.yaml\n\u2502   \u251c\u2500\u2500 controller-keda.yaml\n\u2502   \u251c\u2500\u2500 controller-poddisruptionbudget.yaml\n\u2502   \u251c\u2500\u2500 controller-prometheusrules.yaml\n\u2502   \u251c\u2500\u2500 controller-psp.yaml\n\u2502   \u251c\u2500\u2500 controller-rolebinding.yaml\n\u2502   \u251c\u2500\u2500 controller-role.yaml\n\u2502   \u251c\u2500\u2500 controller-serviceaccount.yaml\n\u2502   \u251c\u2500\u2500 controller-service-internal.yaml\n\u2502   \u251c\u2500\u2500 controller-service-metrics.yaml\n\u2502   \u251c\u2500\u2500 controller-servicemonitor.yaml\n\u2502   \u251c\u2500\u2500 controller-service-webhook.yaml\n\u2502   \u251c\u2500\u2500 controller-service.yaml\n\u2502   \u251c\u2500\u2500 controller-wehbooks-networkpolicy.yaml\n\u2502   \u251c\u2500\u2500 default-backend-deployment.yaml\n\u2502   \u251c\u2500\u2500 default-backend-hpa.yaml\n\u2502   \u251c\u2500\u2500 default-backend-poddisruptionbudget.yaml\n\u2502   \u251c\u2500\u2500 default-backend-psp.yaml\n\u2502   \u251c\u2500\u2500 default-backend-rolebinding.yaml\n\u2502   \u251c\u2500\u2500 default-backend-role.yaml\n\u2502   \u251c\u2500\u2500 default-backend-serviceaccount.yaml\n\u2502   \u251c\u2500\u2500 default-backend-service.yaml\n\u2502   \u251c\u2500\u2500 dh-param-secret.yaml\n\u2502   \u251c\u2500\u2500 _helpers.tpl\n\u2502   \u251c\u2500\u2500 NOTES.txt\n\u2502   \u2514\u2500\u2500 _params.tpl\n\u2514\u2500\u2500 values.yaml\n</code></pre> <p>Result</p> <p>Typical structure of the helm chart, where:</p> <ul> <li>The <code>Chart.yaml</code> file contains metadata and some functionality controls for the chart</li> <li><code>templates</code> folder contains all <code>yaml</code> manifests in Go templates and used to generate Kubernetes manifests</li> <li><code>values.yaml</code> contains <code>default</code> values applied during the rendering.</li> <li>The <code>NOTES.txt</code> file is a special template. When a chart is installed, the NOTES.txt template is rendered and displayed rather than being installed into a cluster.</li> </ul>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#33-helm-template","title":"3.3 Helm Template","text":"<p>As a reminder our goal it customize Nginx deployment to configure Ingress Nginx using <code>type:LoadBalancer</code> with Regional Static IP configured in the section ### Reserve Static IP addresses. Additionally we want to enable metrics service that will fetch Prometheus monitoring metrics from ingress and disable admissionWebhooks configuration.</p> <p>Step 1: Review <code>values.yaml</code></p> <pre><code>cd ~/$MY_REPO/exercises/exercise3/ingress-nginx\nedit values.yaml\n</code></pre> <p>Result</p> <p>It is pretty confusing at this point to understand what this chart is going to deploy, it would of been great to render to YAML format first.</p> <p>Step 2: Execute <code>helm template</code> command and store output in <code>render1.yaml</code></p> <pre><code>helm template ingress-nginx/ingress-nginx --version 4.3.0  &gt; render1.yaml\n</code></pre> <p>Note</p> <p>During helm template, Helm never contacts a remote Kubernetes server, hence the chart only has access to default Kubernetes kinds. The template command always acts like an installation.</p> <p>Review rendered values:</p> <pre><code>edit render1.yaml\n</code></pre> <pre><code>grep \"Source\" render1.yaml\n</code></pre> <p>Result</p> <p><code>helm template</code> designed to isolate the template rendering process of Helm from the installation.  The template command performs following phases of Helm Lifecycle:</p> <ul> <li>loads the chart</li> <li>determines the values</li> <li>renders the templates</li> <li>formats to YAML</li> </ul> <p>Step 3: Let's first create a configuration that will disable <code>admissionWebhooks</code>.</p> <p>Review looking in <code>values.yaml</code> for admissionWebhooks configuration: </p> <pre><code>cd ~/$MY_REPO/exercises/exercise3/ingress-nginx\ngrep -A7 admissionWebhooks values.yaml\n</code></pre> <p>Output:</p> <pre><code>  admissionWebhooks:\n    annotations: {}\n    # ignore-check.kube-linter.io/no-read-only-rootfs: \"This deployment needs write access to root filesystem\".\n\n    ## Additional annotations to the admission webhooks.\n    ## These annotations will be added to the ValidatingWebhookConfiguration and\n    ## the Jobs Spec of the admission webhooks.\n    enabled: true\n</code></pre> <p>Result</p> <p>Note that admissionWebhooks is enabled by default ( <code>enabled: true</code> )</p> <p>Let's create <code>custom_values.yaml</code> file with  admissionWebhooks disabled</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; custom_values.yaml\ncontroller:\n  admissionWebhooks:\n    enabled: false\nEOF\n</code></pre> <p>Execute <code>helm template</code> and store output in new file <code>render2.yaml</code>:</p> <pre><code>helm template ingress-nginx/ingress-nginx --version 4.3.0 --values custom_values.yaml   &gt; render2.yaml\n</code></pre> <p>Now comparing this 2 render files you can see the what change in configuration has resulted in the rendered file:</p> <pre><code>diff render1.yaml render2.yaml\n</code></pre> <p>This is the YAML files that will be generated after this change:</p> <pre><code>grep \"Source\" render2.yaml\n</code></pre> <p>Output:</p> <pre><code># Source: ingress-nginx/templates/controller-serviceaccount.yaml\n# Source: ingress-nginx/templates/controller-configmap.yaml\n# Source: ingress-nginx/templates/clusterrole.yaml\n# Source: ingress-nginx/templates/clusterrolebinding.yaml\n# Source: ingress-nginx/templates/controller-role.yaml\n# Source: ingress-nginx/templates/controller-rolebinding.yaml\n# Source: ingress-nginx/templates/controller-service.yaml\n# Source: ingress-nginx/templates/controller-deployment.yaml\n# Source: ingress-nginx/templates/controller-ingressclass.yaml\n</code></pre> <p>Step 4: Now, let's add a custom Configuration to <code>controller-service.yaml</code> to use Static <code>loadBalancerIP</code>,  we've configured in step ### Reserve Static IP addresses. First we need to locate  the correct service parameters:</p> <pre><code>grep -A20 \"service:\" values.yaml \n</code></pre> <p>Output</p> <p>Shows that we have 4 different services in values file</p> <p>Let's narrow our search:</p> <pre><code>grep -A20 \"service:\" values.yaml | grep  \"List of IP address\"\n</code></pre> <p>Output:</p> <pre><code>    ## List of IP addresses at which the controller services are available\n      ## List of IP addresses at which the stats-exporter service is available\n    ## List of IP addresses at which the default backend service is available\n</code></pre> <p>Output:</p> <pre><code>  service:\n    enabled: true\n\n    annotations: {}\n    labels: {}\n    # clusterIP: \"\"\n\n    ## List of IP addresses at which the controller services are available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    # loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n</code></pre> <p>Result</p> <p>It seems controller is first <code>service:</code> in <code>values.yaml</code> and currently is  it has <code>loadBalancerIP</code> section commented</p> <p>Step 4: Let's add <code>loadBalancerIP</code> IP configuration to <code>custom_values.yaml</code></p> <p>Set variable:</p> <pre><code>cd ~/$MY_REPO/ip-infrastructure\nexport STATIC_IP_ADDRESS=$(terraform output -raw addresses )\necho $STATIC_IP_ADDRESS\n</code></pre> <p>Add custom values:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise3/ingress-nginx\ncat &lt;&lt; EOF&gt;&gt; custom_values.yaml\n  service:\n    loadBalancerIP: $STATIC_IP_ADDRESS\nEOF\n</code></pre> <p>Our final <code>custom_values.yaml</code> should look as following:</p> <pre><code>cat custom_values.yaml\ncontroller:\n  admissionWebhooks:\n    enabled: false\n  service:\n    loadBalancerIP: 3?.X.X.X\n</code></pre> <p>Execute <code>helm template</code> and store output in new file <code>render3.yaml</code>:</p> <pre><code>helm template ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0  &gt; render3.yaml\n</code></pre> <p>Show the difference in rendered file:</p> <pre><code>diff render2.yaml render3.yaml\n</code></pre> <p>Output:</p> <pre><code>&gt;   loadBalancerIP: 3?.X.X.X\n</code></pre> <p>Ensure that this change is in fact belongs to <code>controller-service.yaml</code>:</p> <pre><code>grep -C16  \"loadBalancerIP:\" render3.yaml\n</code></pre> <p>Result</p> <p>We configured correct service that will Ingress Controller service with <code>type: Loadbalancer</code> and  static IP with previously generated with terraform.</p> <p>Summary</p> <p><code>helm template</code> is great tool to render you charts to YAML.</p> <p>Tip</p> <p>You can use Helm as packaging you charts only. And then use  <code>helm template</code> to generate actual YAMLs and apply them with <code>kubectl apply</code></p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#34-dry-runs","title":"3.4 Dry-Runs","text":"<p>Before applying configuration to Kubernetes Cluster it is good idea to Dry-Run it for Errors. This is especially important if you doing Upgrades.</p> <p>The dry-run feature provides Helm users a way to debug the output of a chart before it is sent on to Kubernetes. With all of the templates rendered,  you can inspect exactly what would have been submitted to your cluster. And with the release data, you can verify that the release would have been created as you expected.</p> <p>Here is some of the <code>dry-run</code> working principals:</p> <ul> <li><code>--dry-run</code> mixes non-YAML information with the rendered templates. This means the data has to be cleaned up before being sent to tools like kubectl.</li> <li>A <code>--dry-run</code> on upgrade can produce different YAML output than a --dry-run on install, and this can be confusing.</li> <li>It contacts the Kubernetes API server for validation, which means Helm has to have Kubernetes credentials even if it is just used to --dry-run a release.</li> <li>It also inserts information into the template engine that is cluster-specific. Because of this, the output of some rendering processes may be cluster-specific.</li> </ul> <p>Difference between <code>dry-run</code> and <code>template</code> is that the <code>dry-run</code> contacts Kubernetes API. It is good idea to use <code>dry-run</code> prior deployment and upgrade as it creates mutated output, while <code>template</code> doesn't contacts API and can to pure rendering.</p> <p>Step 1: Execute our deployment with <code>dry-run</code> command:</p> <pre><code>helm install ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --dry-run\n</code></pre> <p>Output:</p> <pre><code>NAME: ingress-nginx\nLAST DEPLOYED: Mon Oct 24 17:37:50 2022\nNAMESPACE: default\nSTATUS: pending-install\nREVISION: 1\nTEST SUITE: None\nHOOKS:\nMANIFEST:\n---\n# Source: ingress-nginx/templates/controller-serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    helm.sh/chart: \n</code></pre> <p>At the top of the output, it will print some information about the release in our case it tells what phase of the release it is in (pending-install), and the revision number. Next, after the informational block, all of the rendered templates are dumped to standard output. Finally, at the bottom of the dry-run output, Helm prints the user-oriented release notes:</p> <p>Note</p> <p><code>--dry-run</code> dumps the output validates, but doesn't deploy actual chart.</p> <p>Step 2: Finally let's deploy Nginx Ingress Charts with our parameters in <code>ingress-nginx</code> namespace</p> <pre><code>kubectl create ns ingress-nginx\nhelm install ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx\n</code></pre> <p>Step 3: Verify Helm Deployment:</p> <pre><code>helm list --namespace ingress-nginx\n</code></pre> <p>Follow Installation note and verify Ingress-Controller Service:</p> <pre><code>kubectl --namespace ingress-nginx get services -o wide ingress-nginx-controller\n</code></pre> <p>Output:</p> <pre><code>NAME                       TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                      AGE\ningress-nginx-controller   LoadBalancer   172.10.0.213   35.192.101.76   80:30930/TCP,443:30360/TCP   3m9s\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#35-deploy-drupal-using-nginx-ingress-controller","title":"3.5 Deploy Drupal using Nginx Ingress Controller","text":"<p>Let's test our newly configured <code>ingress-nginx-controller</code> and expose <code>drupal</code> application using ingress instead of <code>LoadBalancer</code>. </p> <p>Check if STATIC_IP_ADDRESS variable is set:</p> <pre><code>echo $STATIC_IP_ADDRESS\n</code></pre> <p>If not, set it with you Static IP address value. Then proceed to create a YAML file for Drupal's ingress controller:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise3\nkubectl delete pvc data-mywebsite-mariadb-0\ncat &lt;&lt; EOF&gt;&gt; drupal_ing_values.yaml\ningress:\n  annotations: {kubernetes.io/ingress.class: \"nginx\"}\n  enabled: true\n  hostname: $STATIC_IP_ADDRESS.nip.io\n  path: /\n  pathType: ImplementationSpecific\nEOF\n</code></pre> <p>Note</p> <p>Take note of the domain you use here as you'll need this later for the assignment submission</p> <pre><code>helm install  mywebsite bitnami/drupal --values drupal_ing_values.yaml \n</code></pre> <pre><code>1. Get the Drupal URL:\n\n  You should be able to access your new Drupal installation through\n\n  http://3?.X.X.X.nip.io/\n</code></pre> <p>Success</p> <p>Drupal site is now accessible via Ingress. Our Nginx Ingress Controller has been setup correctly!</p> <p>Uninstall Drupal:</p> <pre><code>helm uninstall mywebsite\nkubectl delete pvc data-mywebsite-mariadb-0\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#36-optional-using-set-values-and-upgrading-charts","title":"3.6 (Optional)  Using  <code>--set</code> values and Upgrading Charts","text":"<p>In addition to <code>--value</code> option, there is a second flag that can be used to add individual parameters to an <code>install</code> or <code>upgrade</code>.  The <code>--set</code> flag takes one or more values directly. They do not need to be stored in a YAML file.</p> <p>Step 1: Let's update our <code>ingress-nginx</code> release with new parameter <code>--set controller.image.pullPolicy=Always</code>, but first we want to render template with and without parameter to see what the change will be applied:</p> <p>Generate a version without <code>--set</code> :</p> <pre><code>cd ~/$MY_REPO/exercises/exercise3/ingress-nginx\nhelm template ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx  &gt; render4.yaml\n</code></pre> <p>Generate a version with <code>--set</code>:</p> <pre><code>helm template ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx --set controller.image.pullPolicy=Always &gt; render5.yaml\n</code></pre> <p>Compare both:</p> <pre><code>diff render4.yaml render5.yaml\n</code></pre> <p>Output:</p> <pre><code>&lt;           imagePullPolicy: IfNotPresent\n---\n&gt;           imagePullPolicy: Always\n</code></pre> <p>Step 2: Let's apply update to our <code>ingress-nginx</code> release. For that we going to use <code>helm upgrade</code> command:</p> <pre><code>helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx --set controller.image.pullPolicy=Always \n</code></pre> <p>Verify that upgrade was successful:</p> <pre><code>helm list --namespace ingress-nginx\n</code></pre> <p>Output:</p> <pre><code>NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION\ningress-nginx   ingress-nginx   2               2022-10-24 18:03:33.541452261 -0400 EDT deployed        ingress-nginx-4.3.0     1.4.0  \n</code></pre> <p>Result</p> <p>We can see that Revision change to <code>2</code></p> <p>Note</p> <p>When we talk about upgrading in Helm, we talk about upgrading an installation, not a chart. An installation is a particular instance of a chart in your cluster.  When you run <code>helm install</code>, it creates the installation. To modify that installation, use <code>helm upgrade</code>. This is an important distinction to make in the present context because upgrading an installation can consist of two different kinds of changes:</p> <pre><code>* You can upgrade the version of the chart\n* You can upgrade the configuration of the installation\n</code></pre> <p>In this case we upgrading the configuration of the installation.</p> <p>Extra</p> <p>If you interested upgrade chart version of ingress-nginx to the next available release or <code>3.34.0</code>, give it a try.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#37-optional-listing-releases-history-rollbacks","title":"3.7 (Optional)  Listing Releases, History, Rollbacks","text":"<p>Step 1: Let's see if we can upgrade our release with wrong value <code>pullPolicy=NoSuchPolicy</code> that doesn't exist on Kubernetes. We will first run in <code>dry-run</code> mode</p> <pre><code>helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx --set controller.image.pullPolicy=NoSuchPolicy --dry-run\n</code></pre> <p>Result</p> <p>Release \"ingress-nginx\" has been upgraded. Happy Helming!</p> <p>Let's now apply same config without <code>--dry-run</code> mode:</p> <pre><code>helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx --set controller.image.pullPolicy=NoSuchPolicy\n</code></pre> <p>Output:</p> <pre><code>Error: UPGRADE FAILED: cannot patch \"ingress-nginx-controller\" with kind Deployment: Deployment.apps \"ingress-nginx-controller\" is invalid: spec.template.spec.containers[0].imagePullPolicy: Unsupported value: \"NoSuchPolicy\": supported values: \"Always\", \"IfNotPresent\", \"Never\"\n</code></pre> <p>Failed</p> <p>As the error message indicates, a pull policy cannot be set to NoSuchPolicy. This error came from the Kubernetes API server, which means Helm submitted the manifest,  and Kubernetes rejected it. So our release should be in a failed state.</p> <p>Verify with <code>helm list</code> to confirm <code>failed</code> state:</p> <pre><code>helm list --namespace ingress-nginx\n</code></pre> <p>Output:</p> <pre><code>NAME            NAMESPACE       REVISION        UPDATED                                 STATUS  CHART                APP VERSION\ningress-nginx   ingress-nginx   3               2022-10-24 18:07:29.108706605 -0400 EDT failed  ingress-nginx-4.3.0  1.4.0\n</code></pre> <p>Step 2: List all releases with <code>helm history</code>:</p> <pre><code>helm history ingress-nginx -n ingress-nginx\n</code></pre> <pre><code>REVISION        UPDATED                         STATUS          CHART                   APP VERSION     DESCRIPTION                                                                                                                                                                                                                                                                                  \n1               Mon Oct 24 17:39:36 2022        superseded      ingress-nginx-4.3.0     1.4.0           Install complete                                                                                                                                                                                                                                                                             \n2               Mon Oct 24 18:03:33 2022        deployed        ingress-nginx-4.3.0     1.4.0           Upgrade complete                                                                                                                                                                                                                                                                             \n3               Mon Oct 24 18:07:29 2022        failed          ingress-nginx-4.3.0     1.4.0           Upgrade \"ingress-nginx\" failed: cannot patch \"ingress-nginx-controller\" with kind Deployment: Deployment.apps \"ingress-nginx-controller\" is invalid: spec.template.spec.containers[0].imagePullPolicy: Unsupported value: \"NoSuchPolicy\": supported values: \"Always\", \"IfNotPresent\", \"Never\"\n</code></pre> <p>Info</p> <p>During the life cycle of a release, it can pass through several different statuses. Here they are, approximately in the order you would likely see them:</p> <ul> <li> <p><code>pending-install</code> Before sending the manifests to Kubernetes, Helm claims the installation by creating a release (marked version 1) whose status is set to pending-install.</p> </li> <li> <p><code>deployed</code> As soon as Kubernetes accepts the manifest from Helm, Helm updates the release record, marking it as deployed.</p> </li> <li> <p><code>pending-upgrade</code> When a Helm upgrade is begun, a new release is created for an installation (e.g., v2), and its status is set to pending-upgrade.</p> </li> <li> <p><code>superseded</code> When an upgrade is run, the last deployed release is updated, marked as superseded, and the newly upgraded release is changed from pending-upgrade to deployed.</p> </li> <li> <p><code>pending-rollback</code> If a rollback is created, a new release (e.g., v3) is created, and its status is set to pending-rollback until Kubernetes accepts the release manifest. Then it is marked deployed and the last release is marked superseded.</p> </li> <li> <p><code>uninstalling</code> When a helm uninstall is executed, the most recent release is read and then its status is changed to uninstalling.</p> </li> <li> <p><code>uninstalled</code> If history is preserved during deletion, then when the helm uninstall is complete, the last release\u2019s status is changed to uninstalled.</p> </li> <li> <p><code>failed</code> Finally, if during any operation, Kubernetes rejects a manifest submitted by Helm, Helm will mark that release failed.</p> </li> </ul> <p>Step 3: Rollback release with <code>helm rollback</code>.</p> <p>From the error, we know that the release failed because we supplied an invalid image pull policy. So of course we could correct this by simply running another helm upgrade.  But imagine a case where the cause of error was not readily available. Rather than leave the application in a failed state while diagnosing the problem, it would be nice to  simply revert back to the release that worked before.</p> <pre><code>helm rollback ingress-nginx 2 -n ingress-nginx\nhelm history ingress-nginx -n ingress-nginx\nhelm list\n</code></pre> <pre><code>4               Tue Aug 10 11:16:24 2021        deployed        ingress-nginx-3.35.0    0.48.1          Rollback to 2  \n---\ningress-nginx   ingress-nginx   4               2021-08-10 11:16:24.424371 -0400 EDT    deployed        ingress-nginx-3.35.0    0.48.1  \n</code></pre> <pre><code>Rollback was a success! Happy Helming!\n</code></pre> <p>Success</p> <p>This command tells Helm to fetch the ingress-nginx version 2 release, and resubmit that manifest to Kubernetes.  A rollback does not restore to a previous snapshot of the cluster. Helm does not track enough information to do that. What it does is resubmit the previous configuration,  and Kubernetes attempts to reset the resources to match.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#38-optional-upgrade-release-with-helm-diff-plugin","title":"3.8 (Optional) Upgrade Release with Helm <code>Diff</code> Plugin","text":"<p>While Helm provides a lot of functionality out of the box. Community contributes a lot of great functionality via helm plugins.</p> <p>Plugins allow you to add extra functionality to Helm and integrate seamlessly with the CLI, making them a popular choice for users with unique workflow requirements.  There are a number of third-party plugins available online for common use cases, such as secrets management. In addition, plugins are incredibly easy to build on your own for unique, one-off tasks.</p> <p>Helm plugins are external tools that are accessible directly from the Helm CLI. They allow you to add custom subcommands to Helm without making any modifications to Helm\u2019s Go source code.</p> <p>Many third-party plugins are made open source and publicly available on GitHub. Many of these plugins use the \u201chelm-plugin\u201d tag/topic to make them easy to find. Refer to the documentation for Helm plugins on GitHub</p> <p>Let's Upgrade our running Ingress Nginx chart with metrics service that will enable Prometheus to scrape metrics from our Nginx.</p> <p>Step 1: Update <code>custom_values.yaml</code> with following information:</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; custom_values.yaml\n  metrics:\n    enabled: true\nEOF\n</code></pre> <p>Step 2: So far we've used <code>helm template</code> to render and compare manifests. However most of the time you might not be able to do it during upgrades. However you want to have a clear understanding what will be upgraded if you change something.</p> <p>That's where Helm Diff Plugin can be handy.  Helm Diff plugin giving your a preview of what a helm upgrade would change. It basically generates a diff between the latest deployed version of a release and a <code>helm upgrade --dry-run</code></p> <p>Install Helm Diff Plugin:</p> <pre><code>helm plugin install https://github.com/databus23/helm-diff\n</code></pre> <pre><code>helm diff upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx\n</code></pre> <p>Output:</p> <pre><code>-           imagePullPolicy: Always\n+           imagePullPolicy: IfNotPresent\n+               protocol: TCP\n+             - name: metrics\n+               containerPort: 1025\n+ # Source: ingress-nginx/templates/controller-service-metrics.yaml\n+ apiVersion: v1\n+ kind: Service\n...\n</code></pre> <p>Result</p> <p>Amazing! This looks like <code>terraform plan</code> but for helm :) </p> <p>Important</p> <p>Another discovery from above printout is that since we forgot to add <code>--set imagePullPolicy</code> command, our value will be reverted with upgrade. This is really important to understand as you configuration maybe lost if you use <code>--set</code></p> <p>Step 3: Let's upgrade our <code>ingress-nginx</code> release:</p> <pre><code>helm  upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx\n helm list -n ingress-nginx\n</code></pre> <p>Success</p> <p>Our <code>ingress-nginx</code> is ready to run. Going forward we going to always deploy this chart.</p> <p>Summary</p> <p>So far we've learned:</p> <ul> <li> <p>How to customize helm chart deployment using <code>--set</code> and <code>--values</code> using values.yaml</p> </li> <li> <p>Using <code>values.yaml</code> preferable mehtod in order to achieve reproducible deployments</p> </li> <li> <p>How to upgrade and rollback releases</p> </li> <li> <p>Helm template and dry-run</p> </li> <li> <p>Helm Diff Plugin</p> </li> </ul>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#39-add-results-to-repository","title":"3.9 Add results to repository","text":"<pre><code>cd ~/$MY_REPO/exercises/exercise3\ngit add 'render*.yaml'\n\ngit commit -m \"exercise 3 - advanced search and install\"\ngit push\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#4-creating-notepad-helm-charts","title":"4 Creating NotePad Helm Charts","text":"<p>So far with learned how to deploy existing charts from the Artifact Hub.</p> <p>However sometimes you or your company needs to build software that require's to be distributed and shared externally, as well as have lifecycle and release management. Helm Charts becomes are valuable option for that, specifically if you application is based of containers!</p> <p>Imagine that our solution is Go-based NotePad has first customer that wants to deploy it on their system and they requested delivery via Helm Charts that available on GCP based Helm Repository. To achieve such task we going to create 2 Charts:</p> <pre><code>* `gowebapp-mysql` chart\n* `gowebapp` chart\n</code></pre> <p>And store them in Google Artifact Registry that provides Helm 3 OCI Repository.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#41-design-gowebapp-mysql-chart","title":"4.1 Design <code>gowebapp-mysql</code> chart","text":""},{"location":"020_Module_6_Assignment_Helm_Foundation/#411-create-gowebapp-mysql-chart","title":"4.1.1  Create <code>gowebapp-mysql</code> chart","text":"<p>As scary as it sounds creating a new basic helm chart is 5 minute thing! We going to learn 2 quick methods to create helm charts, that will help you to save time and get started with helm quickly.</p> <p>Helm includes the <code>helm create</code> command to make it easy for you to create a chart of your own, and it\u2019s a great way to get started.</p> <p>The create command creates a chart for you, with all the required chart structure and files. These files are documented to help you understand what is needed, and the templates it provides  showcase multiple Kubernetes manifests working together to deploy an application. In addition, you can install and test this chart right out of the box.</p> <p>Step 0 Create the exercise folder for the <code>NotePad</code> Helm Chart</p> <pre><code>mkdir -p ~/$MY_REPO/exercises/exercise4\ncd ~/$MY_REPO/exercises/exercise4\n\ncat &lt;&lt;EOF&gt; helm/README.md\n# Helm Chart, HelmFiles and values to deploy \\`NotePad\\` application.\nEOF\n</code></pre> <p>Step 1: Create a new chart:</p> <p>Create the chart:</p> <pre><code>helm create gowebapp-mysql\n</code></pre> <p>Inspect the new structure:</p> <pre><code>cd gowebapp-mysql\ntree\n</code></pre> <p>Output:</p> <pre><code>\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 charts\n\u251c\u2500\u2500 templates\n\u2502   \u251c\u2500\u2500 NOTES.txt\n\u2502   \u251c\u2500\u2500 _helpers.tpl\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 hpa.yaml\n\u2502   \u251c\u2500\u2500 ingress.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u251c\u2500\u2500 serviceaccount.yaml\n\u2502   \u2514\u2500\u2500 tests\n\u2502       \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 values.yaml\n</code></pre> <p>Result</p> <p>This command creates a new Nginx chart, with a name of your choice, following best practices for a chart layout. Since Kubernetes clusters can have different methods to expose an application, this chart makes the way Nginx is exposed to network traffic configurable so it can be exposed in a wide variety of clusters. The chart has following structure:</p> <ul> <li>The <code>Chart.yaml</code> file contains metadata and some functionality controls for the chart</li> <li>The <code>charts</code> folder currently empty may container charts dependency of the top level chart. For example we could of make our <code>gowebapp-mysql</code> as dependency chart for <code>gowebapp</code></li> <li><code>templates</code> folder contains all <code>yaml</code> manifests in Go templates and used to generate Kubernetes manifests</li> <li><code>values.yaml</code> contains <code>default</code> values applied during the rendering.</li> <li>The <code>NOTES.txt</code> file is a special template. When a chart is installed, the NOTES.txt template is rendered and displayed rather than being installed into a cluster.</li> <li><code>_helpers.tpl</code> - helper templates for your other templates (e.g creating same labers across all charts). Files with <code>_</code> are not rendered to Kubernetes object definitions, but are available everywhere within other chart templates for use.</li> </ul> <p>Here we going to show a quick way to create a Helm chart without adding any templating.</p> <p>Step 2: Delete <code>templates</code> folder and create empty one:</p> <pre><code>rm -rf templates\nmkdir templates\ncd templates\n</code></pre> <p>Step 3: Copy existing <code>gowebapp-mysql</code> manifests to <code>templates</code> folder:</p> <pre><code>cp ~/$MY_REPO/deploy/k8s-manifests/gowebapp-mysql-{pvc,service,deployment}.yaml .\ncp ~/$MY_REPO/deploy/k8s-manifests/secret-mysql.yaml .              \n</code></pre> <p>Step 4: Lint Helm Chart:</p> <p>When developing charts, especially when working with YAML templates, it can be easy to make a mistake or miss something.  To help you catch errors, bugs, style issues, and other suspicious elements, the Helm client includes a linter.  This linter can be used during chart development and as part of any testing processes.</p> <p>To use the linter, use the <code>lint</code> command on a chart as a directory or a packaged archive:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise4\nhelm lint gowebapp-mysql\n</code></pre> <p>Output:</p> <pre><code>==&gt; Linting gowebapp-mysql\n[INFO] Chart.yaml: icon is recommended\n\n1 chart(s) linted, 0 chart(s) failed\n</code></pre> <p>Step 5:  Install Helm Chart locally:</p> <p>Create <code>dev</code> namespace:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise4/gowebapp-mysql\nkubectl create ns dev\nkubectl config set-context --current --namespace=dev\n</code></pre> <p>Render the manifest locally and compare to original manifests:</p> <pre><code>helm template gowebapp-mysql . &gt; render.yaml\n</code></pre> <p>Inspect the <code>render.yaml</code> file that was generated. Install the chart:</p> <pre><code>helm install gowebapp-mysql .\n</code></pre> <p>Output:</p> <pre><code>NAME: gowebapp-mysql\nLAST DEPLOYED: Mon Oct 24 18:45:17 2022\nNAMESPACE: dev\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <pre><code>helm list\nkubectl get all\nkubectl get pvc\n</code></pre> <p>Success</p> <p>We've deployed our own helm chart locally. While we haven't used templating of the chart at all, we have a helm chart that can be installed and upgraded using helm release features.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#412-add-results-to-repository","title":"4.1.2 Add results to repository","text":"<pre><code>cd ~/$MY_REPO/exercises/exercise4\ngit add .\n\ngit commit -m \"exercise 4 - simple helm files\"\ngit push\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#42-design-gowebapp-chart","title":"4.2  Design <code>gowebapp</code> chart.","text":""},{"location":"020_Module_6_Assignment_Helm_Foundation/#421-create-gowebapp-chart","title":"4.2.1  Create <code>gowebapp</code> chart","text":"<p>We going to use another method to deploy our second chart <code>gowebapp</code>. In this case we going to use <code>nginx</code> template provided by the tool. </p> <p>Task N2: Create <code>gowebapp</code> Helm chart:</p> <pre><code>* Configure `Deployment` template in `values.yaml`\n* Configure `Service` template in `values.yaml`\n* Disable `Service account` template in `values.yaml`\n* Configure `Ingress` template in `values.yaml`:\n    * ingress.class: \"nginx\"\n    * path: \"/\"\n    * pathType: \"ImplementationSpecific\"\n    * host: $STATIC_IP_ADDRESS.nip.io\n* Templatize the ConfigMap Resource\n* Ensure the chart is deployable\n* Ensure `gowebapp` in browser\n</code></pre> <pre><code>mkdir -p ~/$MY_REPO/exercises/exercise5\ncd ~/$MY_REPO/exercises/exercise5\n\ncat &lt;&lt;EOF&gt; helm/README.md\n# N2 exercise for Helm Chart, HelmFiles and values to deploy \\`NotePad\\` application.\nEOF\n</code></pre> <p>Step 1 Create a new chart:</p> <p>Create the new chart and inspect it</p> <pre><code>helm create gowebapp\ncd gowebapp\ntree\n</code></pre> <p>Output:</p> <pre><code>\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 charts\n\u251c\u2500\u2500 templates\n\u2502   \u251c\u2500\u2500 NOTES.txt\n\u2502   \u251c\u2500\u2500 _helpers.tpl\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 hpa.yaml\n\u2502   \u251c\u2500\u2500 ingress.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u251c\u2500\u2500 serviceaccount.yaml\n\u2502   \u2514\u2500\u2500 tests\n\u2502       \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 values.yaml\n</code></pre> <pre><code>edit values.yaml\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#422-template-the-deployment","title":"4.2.2 Template the Deployment","text":"<p>Ensure you are working with the correct chart in <code>exercise5</code> folder.</p> <p>Execute the following tasks:</p> <p>Task 1 Update the replicaCount value to 2 in the <code>values.yaml</code> file:</p> <p>Task 2 Update the repository and tag section to point to your <code>gowebapp</code> docker image in the <code>values.yaml</code> file:</p> <p>Task 3 Update the resources section in the <code>values.yaml</code> file to include the resource <code>requests</code> and <code>limits</code> the gowebapp application needs:</p> <p>Task 4 Add the livness and readiness probes to the deployment.</p> <p>Hint</p> <p>If you need additional help see the reference <code>gowebapp-deployment.yaml</code>:   <code>cat ~/$MY_REPO/deploy/k8s-manifests/gowebapp-deployment.yaml</code></p> <p>Task 5 Notice that the <code>deployment.yaml</code> file does not have an environment variable section for <code>secrets</code>, so let's add one. For this chart we will assume that this section is optional based on whether or not a secrets section exist in the Values.yaml file.</p> <p>Task 5-a Include env delcarion in the deployment template:</p> <pre><code>edit gowebapp/templates/deployment.yaml\n</code></pre> <p>And add following code snippet in the appropriate location:</p> <pre><code>{{- if .Values.secrets }}\n          - env:\n            - name: {{.Values.secrets.name}}\n              valueFrom:\n                secretKeyRef:\n                  name: {{.Values.secrets.secretReference.name}}\n                  key: {{.Values.secrets.secretReference.key}}\n{{- end}}\n</code></pre> <p>Task 5-b Include a section in the <code>values.yaml</code> file:</p> <p>Add following snippet:</p> <pre><code>secrets:\n  enabled: true\n  name: DB_PASSWORD\n  secretReference:\n    name: mysql\n    key: password\n</code></pre> <p>Task 6 For this lab, we will include the <code>volumes</code> and <code>volumeMounts</code> sections without templating, so just copy the required sections to the appropriate location in the <code>deployment.yaml</code> template.</p> <p>Task 7  Render the Chart locally and compare Deployment to original <code>gowebapp-deployment.yaml</code> manifests:</p> <pre><code>helm template gowebapp-mysql . &gt; render.yaml\n</code></pre> <p>Hint</p> <p>If you need additional help see the reference <code>gowebapp-deployment.yaml</code>:   <code>cat ~/$MY_REPO/deploy/k8s-manifests/gowebapp-deployment.yaml</code></p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#423-template-the-service","title":"4.2.3 Template the Service","text":"<p>Ensure you are working with <code>exercise5</code> chart.</p> <p>Step 1: The <code>service.yaml</code> template doesn't have an <code>annotation</code> section, so modify the template to add an <code>annotation</code> section, that looks following:</p> <pre><code>edit gowebapp/templates/service.yaml\n</code></pre> <pre><code>      {{- with .Values.annotations }}\n      annotations:\n        {{- toYaml . | nindent 4 }}\n      {{- end }}\n</code></pre> <p>Task: Next, modify the <code>values.yaml</code> file to allow chart users to add annotations for the service. Make sure to use the right section in the <code>values.yaml</code> file, based on how you modified your <code>/templates/service.yaml</code> file.</p> <p>Reference documentation: </p> <pre><code>* https://helm.sh/docs/chart_template_guide/control_structures/#modifying-scope-using-with\n</code></pre> <p>Step 2: Under the <code>service:</code> section in the <code>values.yaml</code> file, update the service <code>port:</code> to 9000.</p> <p>Step 3 In the <code>values.yaml</code> file, update the service type to <code>NodePort</code></p> <p>Step 4  Render the Chart locally and compare Service to original <code>gowebapp-service.yaml</code> manifests:</p> <pre><code>helm template gowebapp-mysql . &gt; render.yaml\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#424-disable-the-service-account","title":"4.2.4 Disable the Service account","text":"<p>Ensure you are working with <code>exercise5</code> chart.</p> <p>Task 1 Update the <code>values.yaml</code> file to <code>disable</code> the service account creation for the <code>gowebapp</code> deployment.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#425-template-the-ingress-resource","title":"4.2.5 Template the Ingress Resource","text":"<p>Ensure you are working with <code>exercise5</code> chart.</p> <p>Task 1 <code>enable</code> the ingress the <code>values.yaml</code> file and configure it according requirements: :</p> <pre><code>* Expose `gowebapp` using `Ingress` resource:\n    * ingress.class: \"nginx\"\n    * path: \"/\"\n    * pathType: \"ImplementationSpecific\"\n    * host: $STATIC_IP_ADDRESS.nip.io\n</code></pre> <p>Step 2  Render the Chart locally and verify if any issues:</p> <pre><code>helm template gowebapp-mysql . &gt; render.yaml\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#426-templatize-the-configmap-resource","title":"4.2.6 Templatize the ConfigMap Resource","text":"<p>Ensure you are working with <code>exercise5</code> chart.</p> <p>Step 1  Create and templatize <code>configmap</code> resource for our <code>gowebapp</code> that provides connection to Mysql:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise5/gowebapp/templates/\ncat &lt;&lt;EOF&gt;&gt; configmap.yaml\nkind: ConfigMap \napiVersion: v1 \nmetadata:\n  name: {{ .Values.configMap.name }}\ndata:\n  webapp-config-json: |-\n{{ .Files.Get \"config.json\" | indent 4 }}\nEOF\n</code></pre> <p>Store the <code>config.json</code> inside the chart repository:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise5/gowebapp\ncat &lt;&lt;EOF&gt;&gt; config.json\n{\n    \"Database\": {\n        \"Type\": \"MySQL\",\n        \"Bolt\": {\n            \"Path\": \"gowebapp.db\"\n        },\n        \"MongoDB\": {\n            \"URL\": \"127.0.0.1\",\n            \"Database\": \"gowebapp\"\n        },\n        \"MySQL\": {\n            \"Username\": \"root\",\n            \"Password\": \"rootpasswd\",\n            \"Name\": \"gowebapp\",\n            \"Hostname\": \"gowebapp-mysql\",\n            \"Port\": 3306,\n            \"Parameter\": \"?parseTime=true\"\n        }\n    },\n    \"Email\": {\n        \"Username\": \"\",\n        \"Password\": \"\",\n        \"Hostname\": \"\",\n        \"Port\": 25,\n        \"From\": \"\"\n    },\n    \"Recaptcha\": {\n        \"Enabled\": false,\n        \"Secret\": \"\",\n        \"SiteKey\": \"\"\n    },\n    \"Server\": {\n        \"Hostname\": \"\",\n        \"UseHTTP\": true,\n        \"UseHTTPS\": false,\n        \"HTTPPort\": 80,\n        \"HTTPSPort\": 443,\n        \"CertFile\": \"tls/server.crt\",\n        \"KeyFile\": \"tls/server.key\"\n    },\n    \"Session\": {\n        \"SecretKey\": \"@r4B?EThaSEh_drudR7P_hub=s#s2Pah\",\n        \"Name\": \"gosess\",\n        \"Options\": {\n            \"Path\": \"/\",\n            \"Domain\": \"\",\n            \"MaxAge\": 28800,\n            \"Secure\": false,\n            \"HttpOnly\": true\n        }\n    },\n    \"Template\": {\n        \"Root\": \"base\",\n        \"Children\": [\n            \"partial/menu\",\n            \"partial/footer\"\n        ]\n    },\n    \"View\": {\n        \"BaseURI\": \"/\",\n        \"Extension\": \"tmpl\",\n        \"Folder\": \"template\",\n        \"Name\": \"blank\",\n        \"Caching\": true\n    }\n}\nEOF\n</code></pre> <p>Note: you may need to update the secret value.</p> <p>And finally add following snippet inside <code>values.yaml</code>:</p> <pre><code>edit gowebapp/values.yaml\n</code></pre> <pre><code>configMap:\n  name: gowebapp\n</code></pre> <p>Step 2  Render the Chart locally and verify if any issues:</p> <pre><code>helm template gowebapp-mysql . &gt; render.yaml\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#427-deploy-gowebapp","title":"4.2.7 Deploy <code>gowebapp</code>","text":"<p>Before deployment make sure test chart with <code>dry-run</code>, <code>template</code> and <code>lint</code> the chart</p> <p>Lint: </p> <pre><code>cd ~/$MY_REPO/exercises/exercise5\nhelm lint gowebapp\n</code></pre> <p>Install:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise5/gowebapp\nhelm install gowebapp .\n</code></pre> <p>Output:</p> <pre><code>NAME: gowebapp\nLAST DEPLOYED: Tue Oct 24 19:05:44 2022\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <pre><code>helm ls\nkubectl get all\nkubectl get pvc\nkubectl get ing\n</code></pre> <p>Access <code>gowebapp</code> with Ingress</p> <p>Success</p> <p>You've now became a chart developer! And learned, how to create basic helm charts with 2 methods. You can farther customize the chart as needed and add more templates as you go. The next step would be to look contribute back to Helm community, use charts in Artificatory, find issue or add missing feature, help grow help project!</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#428-add-results-to-repository","title":"4.2.8 Add results to repository","text":"<pre><code>cd ~/$MY_REPO/exercises/exercise5\ngit add .\n\ngit commit -m \"exercise 5 - helm files\"\ngit push\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation/#5-submit-assignement","title":"5 Submit assignement","text":"<p>Step 1 Ensure you pushed the changes to the repository</p> <pre><code>cd ~/$MY_REPO\ngit status \ngit log -n 5 \n</code></pre> <p>If any files are missing be sure to add it prior to submission.</p> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre> <p>Step 3 Uninstall chart <code>gowebapp</code> and <code>gowebapp-mysql</code> chart</p> <pre><code>helm install gowebapp\nhelm uninstall gowebapp-mysql\n</code></pre> <p>Step 4 Resize GKE Cluster to <code>0</code> nodes, to avoid charges:</p> <pre><code>cd ~/$MY_REPO/notepad-infrastructure\n</code></pre> <pre><code>edit terraform.tfvars\n</code></pre> <p>And set <code>gke_pool_node_count</code>   = \"0\"</p> <p>Step 5: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Step 6: Shutdown all Nodes in GKE Cluster Node Pool:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Result</p> <p>GKE Clusters has been scale down to 0 nodes.</p> <p>That is it, make sure to share your repository with your instructor.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/","title":"Assignment 6 Learning Helm","text":"<p>Objective:</p> <ul> <li>Installing and Configuring the Helm Client</li> <li>Deploy Kubernetes apps with Helm Charts</li> <li>Learn Helm Commands</li> <li>Learn Helm Repository</li> <li>Create Helm chart</li> <li>Learn Helm Plugins</li> <li>Learn Helm File</li> </ul>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#prerequisite","title":"Prerequisite","text":"<p>Enable the needed services in the project</p> <pre><code>export PROJECT_ID=&lt;YOUR_PROJECT_ID&gt;\ngcloud config set project $PROJECT_ID\ngcloud services enable compute.googleapis.com cloudresourcemanager.googleapis.com container.googleapis.com servicenetworking.googleapis.com\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#locate-assignment-6","title":"Locate Assignment 6","text":"<p>Step 1  Clone <code>ycit020_2022</code> repo with Kubernetes manifests, which going to use for our work:</p> <pre><code>cd ~/ycit020_2022\n# Alternatively: cd ~ &amp; git clone https://github.com/Cloud-Architects-Program/ycit020_2022.git\ngit pull\ncd ~/ycit020_2022/assignment6_helm/\nls\n</code></pre> <p>Result</p> <p>You can see Kubernetes manifests and terraform configs</p> <p>Step 2 Go into your personal Google Cloud Source Repository:</p> <pre><code>MY_REPO=your_student_id-notepad\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$MY_REPO\n</code></pre> <p>Pull latest changes:</p> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 3 (Optional) If your terraform config from previous assignment is not working you can copy working config from <code>assignment6_helm</code> folder to your <code>notepad-infrastructure</code> folder.</p> <p>Step 4 Create exercise 1 folder for supporting applications</p> <pre><code>cd ~/$MY_REPO\nmkdir -p exercises/exercise1\n\ncat &lt;&lt;EOF&gt; exercises/exercise1/README.md\n# Setting up the infraestructure with Terraform\nEOF\n</code></pre> <p>Step 5 Copy Assignment 4 <code>deploy</code> folder to your repo:</p> <pre><code>cd ~/$MY_REPO/\ncp -r ~/ycit020_2022/assignment6_helm/deploy deploy\nls deploy\n</code></pre> <p>Result</p> <p>You should see <code>k8s-manifest</code> folder</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#reserve-static-ip-addresses","title":"Reserve Static IP addresses","text":"<p>In the next few assignments we might deploy many applications and so we will require expose them using Ingress.</p> <p>When you create an Ingress object, you get a stable external IP address that clients can use to access your Services and in turn, your running containers. The IP address is stable in the sense that it lasts for the lifetime of the Ingress object. If you delete your Ingress and create a new Ingress from the same manifest file, you are not guaranteed to get the same external IP address.</p> <p>For this application, we would like to have a permanent IP address that can persist even if the Ingress resource is recreated.</p> <p>For that you must reserve a <code>Regional</code> static <code>External</code> IP address and provide it teacher, so we can setup <code>A Record</code> on the DNS for your behalf.</p> <p>Reference: </p> <ol> <li>Reserving a static external IP address</li> <li>Terraform resource google_compute_address</li> </ol> <p>Step 1: Create folder for static ip terraform configuration:</p> <pre><code>cd ~/$MY_REPO/\nmkdir ip-infrastructure\n</code></pre> <p>Important</p> <p>The reason we creating a new folder for static_ip creation is because we don't want to destroy or delete this IP throughout our training.</p> <p>Step 1: Create Terraform configuration:</p> <p>Declare Provider:</p> <pre><code>cd ~/$MY_REPO/ip-infrastructure\ncat &lt;&lt; EOF&gt;&gt; provider.tf\nterraform {\n  required_providers {\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~&gt; 3.70.0\"\n    }\n  }\n}\nEOF\n</code></pre> <p>Configure global address resource:</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; static_ip.tf\nprovider \"google\" {\n  project = var.gcp_project_id\n}\n\nresource \"google_compute_address\" \"regional_external_ip\" {\n  provider      = google\n  name          = \"static-ingres-ip\"\n  address_type  = \"EXTERNAL\"\n  region        = \"us-central1\"\n}\nEOF\n</code></pre> <p>Configure variables:</p> <pre><code>cat &lt;&lt;EOF&gt; variables.tf\nvariable \"gcp_project_id\" {\n  type        = string\n  description = \"The GCP Seeding project ID\"\n  default     = \"\"\n}\nEOF\n</code></pre> <p>Task: Create a <code>.tfvars</code> file with your project_id:</p> <pre><code>gcp_project_id = \"$PROJECT_ID\"\n</code></pre> <p>Configure outputs:</p> <pre><code>cat &lt;&lt;EOF &gt;&gt; outputs.tf\noutput \"addresses\" {\n  description = \"Global IPv4 address for proxy load balancing to the nearest Ingress controller\"\n  value       = google_compute_address.regional_external_ip.address\n}\n\noutput \"name\" {\n  description = \"Static IP Name\"\n  value       = google_compute_address.regional_external_ip.name\n}\nEOF\n</code></pre> <p>Step 2: Apply Terraform configuration:</p> <p>Initialize:</p> <pre><code>terraform init\n</code></pre> <p>Plan and Deploy Infrastructure:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Result</p> <p>Plan: 1 to add, 0 to change, 0 to destroy.  Changes to Outputs: \u2003   + addresses = (known after apply) \u2003   + name      = \"static-ingres-ip\"</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Set variable:</p> <pre><code>cd ~/$MY_REPO/ip-infrastructure\nexport STATIC_IP_ADDRESS=$(terraform output -raw addresses )\nexport STATIC_IP_NAME=$(terraform output -raw name )\n</code></pre> <p>Create readme:</p> <pre><code>cat &lt;&lt;EOF&gt; README.md\n\nGenerated Static IP for Ingress:\n\n    * Name - $STATIC_IP_NAME # Used Ingress Manifest\n    * Address - $STATIC_IP_ADDRESS # Used to Configure DNS A Record\nEOF\n\ncp README.md ~/$MY_REPO/exercises/exercise1\n</code></pre> <p>Important</p> <p>Add information about your static_ip_address the tab \"Assignment 6\" in column C under your name in this spreadsheet: 2022 YCIT020 Student Project Info</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#commit-to-repository","title":"Commit to Repository","text":"<p>Step 1 Commit <code>ip-infrastructure</code> and <code>helm</code> folders using the following Git commands:</p> <pre><code>cd ~/$MY_REPO\ngit status\ngit add .\ngit commit -m \"adding documentation for ycit020 assignment 6\"\n</code></pre> <p>Step 2 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#1-install-and-configure-helm-client","title":"1 Install and Configure Helm Client","text":""},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#11-install-helm-3","title":"1.1 Install Helm 3","text":"<p>GCP Cloud Shell comes with many common tools pre-installed including <code>helm</code>.</p> <p>Step 1: Verify and validate the version of <code>Helm</code> that is installed:</p> <pre><code>helm version\n</code></pre> <p>Output:</p> <pre><code>version.BuildInfo{Version:\"v3.9.3\", GitCommit:\"414ff28d4029ae8c8b05d62aa06c7fe3dee2bc58\", GitTreeState:\"clean\", GoVersion:\"go1.17.13\"}\n</code></pre> <p>Result</p> <p>Helm 3 is installed</p> <p>Note</p> <p>Until November 2020, two different major versions of Helm were actively maintained. The current stable major version of Helm is version 3. </p> <p>Note</p> <p>Helm follows a versioning convention known as Semantic Versioning (SemVer). In Semantic Versioning, the version number conveys meaning about what you can expect in the release. Because Helm follows this specification, users can expect certain things out of releases simply by carefully reading the version number. At its core, a semantic version has three numerical components and an optional stability marker (for alphas, betas, and release candidates). Here are some examples:</p> <ul> <li>v1.0.0</li> <li>v3.3.2</li> </ul> <p>SemVer represents format of  <code>X.Y.Z</code>, where <code>X</code> is a major version, <code>Y</code> is a minor version and <code>Z</code> is a patch release:</p> <ul> <li> <p>The major release number tends to be incremented infrequently. It indicates that major changes have been made to Helm, and that some of those changes may break compatibility with previous versions. The difference between Helm 2 and Helm 3 is substantial, and there is work necessary to migrate between the versions.</p> </li> <li> <p>The minor release number indicates feature additions. The difference between 3.2.0 and 3.3.0 might be that a few small new features were added. However, there are no breaking changes between versions. (With one caveat: a security fix might necessitate a breaking change, but we announce boldly when that is the case.)</p> </li> <li> <p>The patch release number indicates that only backward compatible bug fixes have been made between this release and the last one. It is always recommended to stay at the latest patch release.</p> </li> </ul> <p>(OPTIONAL) Step 2: If you want to use specific version of <code>helm</code> or want to install <code>helm</code> in you local machine (On macOS and Linux,) use following link to install Helm. </p> <p>The usual sequence of commands for installing this way is as follows:</p> <pre><code>$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n$ chmod +x get_helm.sh\n$ ./get_helm.sh\n</code></pre> <p>The preceding commands fetch the latest version of the <code>get_helm.sh</code> script, and then uses that to find and install the latest version of Helm 3.</p> <p>Alternatively, you can download latest binary of your choice here.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#13-deploy-gke-cluster","title":"1.3 Deploy GKE cluster","text":"<p>We going to reuse Terraform configuration to deploy our GKE Cluster. And we assume that <code>terraform.tfvars</code> has been properly configured already with required values.</p> <p>Step 1: Locate Terraform Configuration directory.</p> <pre><code>cd ~/$MY_REPO/notepad-infrastructure\n</code></pre> <p>Step 2: Modify your backend</p> <p>In case you are not using the output of the previous assignment, create a bucket to host your terraform state</p> <pre><code>gsutil mb gs://&lt;STUDENT_NAME&gt;-notepad-dev-tfstate\n</code></pre> <p>Task: Modify the <code>backend.tf</code> file to point to your newly created bucket</p> <p>Solution</p> <p>Your backend.tf file should look similar to:</p> <pre><code>    terraform {\n        backend \"gcs\" {\n        bucket = \"&lt;STUDENT_NAME&gt;-notepad-dev-tfstate\"\n        prefix = \"state\"\n    }\n}\n</code></pre> <p>Step 3: Initialize Terraform Providers</p> <pre><code>terraform init\n</code></pre> <p>Step 4:  Increase GKE Node Pool VM size</p> <p>Modify the <code>terraform.tfvars</code></p> <pre><code>edit terraform.tfvars\n</code></pre> <p>Update <code>gke_pool_machine_type</code> from <code>e2-small</code> to <code>e2-highcpu-4</code>, to support larger workloads. Also make sure to update the <code>gcp_project_id</code> and <code>org</code> if you are starting from the posted solution.</p> <p>Step 5: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Step 6: Create GKE Cluster and Node Pool:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Result</p> <p>GKE Clusters has been created</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#14-configure-helm","title":"1.4 Configure Helm","text":"<p>Helm interacts directly with the Kubernetes API server. For that reason, Helm needs to be able to connect to a Kubernetes cluster. Helm attempts to do this automatically by reading the same configuration files used by <code>kubectl</code>.</p> <p>Helm will try to find this information by reading the environment variable <code>$KUBECONFIG</code>.</p> <p>Step 1 Authenticate to the cluster.</p> <pre><code>export STUDENT_NAME=&lt;STUDENT_NAME&gt;\ngcloud container clusters get-credentials gke-$STUDENT_NAME-notepad-dev --region us-central1 \n</code></pre> <p>Step 2 Test that <code>kubectl</code> client connected to GKE cluster:</p> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>No resources found in default namespace.\n</code></pre> <p>Step 3 Test that helm client connected to GKE cluster:</p> <pre><code>helm list\n</code></pre> <p>Output:</p> <pre><code>NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION\n</code></pre> <p>Result</p> <p>As expected there are no Charts has been Installed on our cluster yet</p> <p>Summary</p> <p>Helm 3 has been installed and configured to work with our cluster. Let's deploy some charts!</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#2-basic-helm-chart-installation","title":"2 Basic Helm Chart Installation","text":""},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#21-searching-chart","title":"2.1 Searching Chart","text":"<p>A Helm chart is a packaged application that can be installed into your Kubernetes cluster. During chart development, you will often just work with a chart that is stored on your local system and later pushed to GitHub.</p> <p>But when it comes to sharing charts, Helm describes a standard format for indexing and sharing information about Helm <code>charts</code>. A Helm chart <code>repository</code> is simply a set of files, reachable over the network, that conforms to the Helm specification for indexing packages.</p> <p>There are huge number of chart repositories on the internet. The easiest way to find the popular repositories is to use your web browser to navigate to the Artifact Hub. There you will find thousands of Helm charts, each hosted on an appropriate repository.</p> <p>Deprecation</p> <p>In the past all charts were located and maintained by Helm Kubernetes Community Git repositories, known as https://github.com/kubernetes/charts, and had 2 types:</p> <ul> <li>Stable</li> <li>Incubator</li> </ul> <p>All the charts were rendered in Web UI via Helm Hub page.</p> <p>While the central Git Repository to maintain charts was great idea, with fast growing Helm popularity, it become hard to impossible to manage and maintain it by small group of maintainers, as they had to aprove hundreds of PR per day and frustrating for chart contributors as they had to wait several weeks their PR to be reviewed and approved.</p> <p>As a result GitHub project for Helm <code>stable</code> and <code>incubator</code> charts as well as [Helm Hub] has been deprecated and archived. All the charts are now maintained by independent contributors in their subsequent repo's. (e.g vault, is under hashicorp/vault-helm repo) and the central place to find all active and official Charts can be foind in Artifact Hub.</p> <p>Important</p> <p>Helm 2 came with a Helm repository installed by default. The <code>stable</code> chart repository was at one time the official source of production-ready Helm charts. As we discussed above  <code>stable</code> chart repository has been now deprecated.</p> <p>In Helm 3, there is no <code>default</code> repository. Users are encouraged to use the Artifact Hub to find what they are looking for and then add their preferred repositories.</p> <p>Step 0: Setup exercise folder</p> <pre><code>mkdir -p ~/$MY_REPO/exercises/exercise2\ncd ~/$MY_REPO/exercises/exercise2\n\ncat &lt;&lt;EOF&gt; exercises/exercise1/README.md\n# Helm search and Install exercise\nEOF\n</code></pre> <p>Step 1: Helm provides native way to search charts from CLI in artifacthub.io</p> <p>Search for drupal chart:</p> <pre><code>helm search hub drupal\n</code></pre> <p>Output:</p> <pre><code>https://artifacthub.io/packages/helm/bitnami/drupal 10.2.30         9.2.3       One of the most versatile open source content m...\nhttps://artifacthub.io/packages/helm/cetic/drupal   0.1.0           1.16.0      Drupal is a free and open-source web content ma..\n</code></pre> <p>Result</p> <p>Search is a good way to find existing Helm packages</p> <p>Step 2: Use the link to navigate to Artifact Hub Drupal chart:</p> <pre><code>https://artifacthub.io/packages/helm/bitnami/drupal\n</code></pre> <p>Note</p> <p>Drupal is OSS content management systems that can be installed on k8s.</p> <p>Result: The Artifact Page opened with information about the chart, it's parameters and how to use the chart.</p> <p>Step 3: Review Github source Code of the chart:</p> <pre><code>https://github.com/bitnami/charts/tree/master/bitnami/drupal\n</code></pre> <p>Result: Github page with Drupal Helm Chart itself, where you can browse the <code>templates</code>, <code>values.yaml</code>, <code>Chart.yaml</code> to understand how this chart is actually working and if you have any issues with the chart this would be the right location to open the issue or send a PR with new feature if you decide to contribute.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#22-adding-a-chart-repository","title":"2.2 Adding a Chart Repository","text":"<p>Once you found a chart, it's logical to install it. However first step you need to do is to add a Chart Repository.</p> <p>Step 1: Adding a Helm chart is done with the <code>helm repo add</code> command:</p> <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\n</code></pre> <p>Note</p> <p>Bitnami is company well known to package application for Any Platforms and Cloud environments. With popularity of Helm, Bitnami developers were among the core contributors who designed the Helm repository system. They have contributed to the establishment of Helm\u2019s best practices for chart development and have written many of the most widely used charts. Bitnami is now part of VMware, provides IT organizations with an enterprise offering that is secure, compliant, continuously maintained and customizable to your organizational policies.</p> <p>Step 2: Now, we can verify that the Bitnami repository exists by running a <code>helm repo list</code> command:</p> <pre><code>helm repo list\n</code></pre> <p>Output:</p> <pre><code>NAME    URL                               \nbitnami https://charts.bitnami.com/bitnami\n</code></pre> <p>Result</p> <p>This command shows us all of the repositories installed for Helm. Right now, we see only the Bitnami repository that we just added.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#23-helm-chart-installation","title":"2.3 Helm Chart Installation","text":"<p>Step 1: At very minimum, installing a chart in Helm requires just 2 pieces of information: the <code>name</code> of the installation and the <code>chart</code> you want to install:</p> <pre><code>helm install mywebsite bitnami/drupal\n</code></pre> <p>Output:</p> <pre><code>NAME: mywebsite\nLAST DEPLOYED: Sun Aug  8 08:25:29 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\n*******************************************************************\n*** PLEASE BE PATIENT: Drupal may take a few minutes to install ***\n*******************************************************************\n\n1. Get the Drupal URL:\n\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        Watch the status with: 'kubectl get svc --namespace default -w mywebsite-drupal'\n\n  export SERVICE_IP=$(kubectl get svc --namespace default mywebsite-drupal --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\")\n  echo \"Drupal URL: http://$SERVICE_IP/\"\n\n2. Get your Drupal login credentials by running:\n\n  echo Username: user\n  echo Password: $(kubectl get secret --namespace default mywebsite-drupal -o jsonpath=\"{.data.drupal-password}\" | base64 --decode)\n</code></pre> <p>Result</p> <p>When using Helm, you will see that output for each installation. A good chart would provide helpful <code>notes</code> on how to connect to the deployed solution.</p> <p>Tip</p> <p>You can get <code>notes</code> information any time after helm installation using <code>helm get notes mywebsite</code> command.</p> <p>Step 2: Follow your Drupal Helm Chart <code>notes</code> Instruction to access Website</p> <pre><code>1. Get the Drupal URL:\n\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        Watch the status with: 'kubectl get svc --namespace test -w mywebsite-drupal'\n\n  export SERVICE_IP=$(kubectl get svc --namespace test mywebsite-drupal --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\")\n  echo \"Drupal URL: http://$SERVICE_IP/\"\n</code></pre> <p>Success</p> <p>We can access <code>My blog</code> website and login with provider user and password. You can start your own blog now, that runs on Kubernetes</p> <p>Step 3: List deployed Kubernetes resources:</p> <pre><code>kubectl get all \nkubectl get pvc\n</code></pre> <p>Summary</p> <p>Our Drupal website consist of MariaDB <code>statefulset</code>, Drupal <code>deployment</code>, 2 <code>pvc</code>s and  <code>services</code>. <code>Mywebsite</code> Drupal service is type <code>LoadBalancer</code> and that's how we able to access it.</p> <p>Step 4: List installed chart:</p> <pre><code>helm list\n</code></pre> <p>Output:</p> <pre><code>NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION\nmywebsite       default         1               2021-08-08 08:25:29.625862 -0400 EDT    deployed        drupal-10.2.30          9.2.3\n</code></pre> <p>Note</p> <p>Like other commands, helm list is <code>namespace</code> aware. By default, Helm uses the <code>namespace</code> your Kubernetes configuration file sets as the default. Usually this is the namespace named <code>default</code>.</p> <p>Step 5: Deploy same chart in namespace <code>test</code>: </p> <pre><code>kubectl create ns test\nhelm install --namespace test mywebsite bitnami/drupal --create-namespace\n</code></pre> <p>Tip</p> <p>By adding <code>--create-namespace</code>, indicates to Helm that we acknowledge that there may not be a namespace with that name already, and we just want one to be created.</p> <p>List deployed chart in namespace <code>test</code>: </p> <pre><code>helm list --namespace test\n</code></pre> <p>Summary</p> <p>In Helm 2, instance names were cluster-wide. You could only have an instance named <code>mywebsite</code> once per cluster. </p> <p>In Helm 3, naming has been changed. Now instance names are scoped to Kubernetes namespaces. We could install 2 instances named <code>mywebsite</code> as long as they each lived in a different namespace.</p> <p>Step 5: Submit the output into the assignments folder</p> <p>Commit the results to the repository:</p> <pre><code># make sure you are in exercise2 folder\nkubectl describe pods &gt; k8s_pods_$STUDENT_NAME_default.txt\nkubectl describe pods -n test &gt; k8s_pods_$STUDENT_NAME_test.txt\nkubectl get all -A &gt; k8s_resources_$STUDENT_NAME.txt\n\ngit add .\ngit commit -m \"helm search and install exercise\"\ngit push \n</code></pre> <p>Step 6: Cleanup Helm Drupal deployments using <code>helm uninstall</code> command:</p> <p>Uninstall the charts</p> <pre><code>helm uninstall mywebsite\nhelm uninstall mywebsite -n test \n</code></pre> <p>Remove the test namespace</p> <pre><code>kubectl delete ns test\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#3-advanced-helm-chart-installation","title":"3 Advanced Helm Chart Installation","text":""},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#31-deploy-nginx-ingress-chart-with-custom-configuration","title":"3.1 Deploy NGINX Ingress Chart with Custom Configuration","text":"<p>Let's deploy another Helm application on our Kubernetes cluster: Ingress Nginx - is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer.</p> <p>In the previous classes we used Google implementation of Ingress - GKE Ingress for HTTP(S) Load Balancing. While this solutions provides managed Ingress experience and advanced features like Cloud Armor, DDoS protection and Identity aware proxy.</p> <p>Ingress Nginx Controller is popular solution and has a lot of features and integrations. If you want to deploy Kubernetes Application on different cloud providers or On-prem the same way, Nginx Ingress Controller becomes a default option.</p> <p>Our task is to configure Ingress Nginx using <code>type:LoadBalancer</code> with Regional Static IP configured in the section ### Reserve Static IP addresses. Additionally we want to enable metrics service that will fetch Prometheus monitoring metrics from ingress and disable admissionWebhooks configuration.</p> <p>Step 0: Setup exercise folder</p> <pre><code>mkdir -p ~/$MY_REPO/exercises/exercise3\ncd ~/$MY_REPO/exercises/exercise3\n\ncat &lt;&lt;EOF&gt; exercises/exercise3/README.md\n# Advanced Helm Chart Installation\nEOF\n</code></pre> <p>Step 1: Let's search <code>ingress-nginx</code> in artifacthub.io</p> <pre><code>helm search hub ingress-nginx\n</code></pre> <p>Output:</p> <pre><code>URL                                                     CHART VERSION   APP VERSION     DESCRIPTION                                       \nhttps://artifacthub.io/packages/helm/ingress-ng...      4.3.0           1.4.0           Ingress controller for Kubernetes using NGINX a...\nhttps://artifacthub.io/packages/helm/ingress-ng...      4.1.0           1.2.0           Ingress controller for Kubernetes using NGINX a...\n</code></pre> <p>Result</p> <p>We going to select the first <code>ingress-nginx</code> chart that is maintained by Kubernetes Community</p> <p>Step 2: Review Hub Page details about this chart and locate information how to add repository:</p> <pre><code>https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx\n</code></pre> <p>Result: The Artifact Page opened with information about the chart, it's parameters, and how to use the chart itself</p> <p>Step 3: Add <code>ingress-nginx</code>  Repo:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update  \n</code></pre> <p>Step 4: Oftentimes, searching is a useful way to find not only what <code>charts</code> can be installed, but what <code>versions</code> are available:</p> <pre><code>helm search repo ingress-nginx --versions | head\n</code></pre> <p>Output:</p> <pre><code>NAME                            CHART VERSION   APP VERSION     DESCRIPTION                                       \ningress-nginx/ingress-nginx     4.3.0           1.4.0           Ingress controller for Kubernetes using NGINX a...\ningress-nginx/ingress-nginx     4.2.5           1.3.1           Ingress controller for Kubernetes using NGINX a...\n</code></pre> <p>Note</p> <p>By default, Helm tries to install the latest <code>stable</code> release of a chart, but you can override this behavior and install a specific version of a chart. Thus it is often useful to see not just the summary info for a chart, but exactly which versions exist for a chart. Every new version of the chart can be bring fixes and new changes, so for production use it's better to go with tested version and pin installation version.</p> <p>Summary</p> <p>We going with latest listed official version of the chart <code>4.3.0</code> of <code>ingress-nginx</code> Chart.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#32-download-and-inspect-chart-locally","title":"3.2 Download and inspect Chart locally","text":"<p>Step 1: Pull <code>ingress-nginx</code> Helm Chart of specific version to Local filesystem:</p> <pre><code># ensure you are in exercise3 folder\nhelm pull ingress-nginx/ingress-nginx --version 4.3.0\ntar -xvzf  ingress-nginx-4.3.0.tgz\n</code></pre> <p>Step 2:  See the tree structure of the chart</p> <pre><code>sudo apt-get install tree\ntree -L 2 ingress-nginx\n</code></pre> <p>Output:</p> <pre><code>ingress-nginx\n\u251c\u2500\u2500 CHANGELOG.md\n\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 ci\n\u2502   \u251c\u2500\u2500 controller-custom-ingressclass-flags.yaml\n\u2502   \u251c\u2500\u2500 daemonset-customconfig-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-customnodeport-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-extra-modules.yaml\n\u2502   \u251c\u2500\u2500 daemonset-headers-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-internal-lb-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-nodeport-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-podannotations-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-tcp-udp-configMapNamespace-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-tcp-udp-portNamePrefix-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-tcp-udp-values.yaml\n\u2502   \u251c\u2500\u2500 daemonset-tcp-values.yaml\n\u2502   \u251c\u2500\u2500 deamonset-default-values.yaml\n\u2502   \u251c\u2500\u2500 deamonset-metrics-values.yaml\n\u2502   \u251c\u2500\u2500 deamonset-psp-values.yaml\n\u2502   \u251c\u2500\u2500 deamonset-webhook-and-psp-values.yaml\n\u2502   \u251c\u2500\u2500 deamonset-webhook-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-autoscaling-behavior-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-autoscaling-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-customconfig-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-customnodeport-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-default-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-extra-modules.yaml\n\u2502   \u251c\u2500\u2500 deployment-headers-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-internal-lb-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-metrics-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-nodeport-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-podannotations-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-psp-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-tcp-udp-configMapNamespace-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-tcp-udp-portNamePrefix-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-tcp-udp-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-tcp-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-webhook-and-psp-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-webhook-extraEnvs-values.yaml\n\u2502   \u251c\u2500\u2500 deployment-webhook-resources-values.yaml\n\u2502   \u2514\u2500\u2500 deployment-webhook-values.yaml\n\u251c\u2500\u2500 OWNERS\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 README.md.gotmpl\n\u251c\u2500\u2500 templates\n\u2502   \u251c\u2500\u2500 admission-webhooks\n\u2502   \u251c\u2500\u2500 clusterrolebinding.yaml\n\u2502   \u251c\u2500\u2500 clusterrole.yaml\n\u2502   \u251c\u2500\u2500 controller-configmap-addheaders.yaml\n\u2502   \u251c\u2500\u2500 controller-configmap-proxyheaders.yaml\n\u2502   \u251c\u2500\u2500 controller-configmap-tcp.yaml\n\u2502   \u251c\u2500\u2500 controller-configmap-udp.yaml\n\u2502   \u251c\u2500\u2500 controller-configmap.yaml\n\u2502   \u251c\u2500\u2500 controller-daemonset.yaml\n\u2502   \u251c\u2500\u2500 controller-deployment.yaml\n\u2502   \u251c\u2500\u2500 controller-hpa.yaml\n\u2502   \u251c\u2500\u2500 controller-ingressclass.yaml\n\u2502   \u251c\u2500\u2500 controller-keda.yaml\n\u2502   \u251c\u2500\u2500 controller-poddisruptionbudget.yaml\n\u2502   \u251c\u2500\u2500 controller-prometheusrules.yaml\n\u2502   \u251c\u2500\u2500 controller-psp.yaml\n\u2502   \u251c\u2500\u2500 controller-rolebinding.yaml\n\u2502   \u251c\u2500\u2500 controller-role.yaml\n\u2502   \u251c\u2500\u2500 controller-serviceaccount.yaml\n\u2502   \u251c\u2500\u2500 controller-service-internal.yaml\n\u2502   \u251c\u2500\u2500 controller-service-metrics.yaml\n\u2502   \u251c\u2500\u2500 controller-servicemonitor.yaml\n\u2502   \u251c\u2500\u2500 controller-service-webhook.yaml\n\u2502   \u251c\u2500\u2500 controller-service.yaml\n\u2502   \u251c\u2500\u2500 controller-wehbooks-networkpolicy.yaml\n\u2502   \u251c\u2500\u2500 default-backend-deployment.yaml\n\u2502   \u251c\u2500\u2500 default-backend-hpa.yaml\n\u2502   \u251c\u2500\u2500 default-backend-poddisruptionbudget.yaml\n\u2502   \u251c\u2500\u2500 default-backend-psp.yaml\n\u2502   \u251c\u2500\u2500 default-backend-rolebinding.yaml\n\u2502   \u251c\u2500\u2500 default-backend-role.yaml\n\u2502   \u251c\u2500\u2500 default-backend-serviceaccount.yaml\n\u2502   \u251c\u2500\u2500 default-backend-service.yaml\n\u2502   \u251c\u2500\u2500 dh-param-secret.yaml\n\u2502   \u251c\u2500\u2500 _helpers.tpl\n\u2502   \u251c\u2500\u2500 NOTES.txt\n\u2502   \u2514\u2500\u2500 _params.tpl\n\u2514\u2500\u2500 values.yaml\n</code></pre> <p>Result</p> <p>Typical structure of the helm chart, where:</p> <ul> <li>The <code>Chart.yaml</code> file contains metadata and some functionality controls for the chart</li> <li><code>templates</code> folder contains all <code>yaml</code> manifests in Go templates and used to generate Kubernetes manifests</li> <li><code>values.yaml</code> contains <code>default</code> values applied during the rendering.</li> <li>The <code>NOTES.txt</code> file is a special template. When a chart is installed, the NOTES.txt template is rendered and displayed rather than being installed into a cluster.</li> </ul>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#33-helm-template","title":"3.3 Helm Template","text":"<p>As a reminder our goal it customize Nginx deployment to configure Ingress Nginx using <code>type:LoadBalancer</code> with Regional Static IP configured in the section ### Reserve Static IP addresses. Additionally we want to enable metrics service that will fetch Prometheus monitoring metrics from ingress and disable admissionWebhooks configuration.</p> <p>Step 1: Review <code>values.yaml</code></p> <pre><code>cd ~/$MY_REPO/exercises/exercise3/ingress-nginx\nedit values.yaml\n</code></pre> <p>Result</p> <p>It is pretty confusing at this point to understand what this chart is going to deploy, it would of been great to render to YAML format first.</p> <p>Step 2: Execute <code>helm template</code> command and store output in <code>render1.yaml</code></p> <pre><code>helm template ingress-nginx/ingress-nginx --version 4.3.0  &gt; render1.yaml\n</code></pre> <p>Note</p> <p>During helm template, Helm never contacts a remote Kubernetes server, hence the chart only has access to default Kubernetes kinds. The template command always acts like an installation.</p> <p>Review rendered values:</p> <pre><code>edit render1.yaml\n</code></pre> <pre><code>grep \"Source\" render1.yaml\n</code></pre> <p>Result</p> <p><code>helm template</code> designed to isolate the template rendering process of Helm from the installation.  The template command performs following phases of Helm Lifecycle:</p> <ul> <li>loads the chart</li> <li>determines the values</li> <li>renders the templates</li> <li>formats to YAML</li> </ul> <p>Step 3: Let's first create a configuration that will disable <code>admissionWebhooks</code>.</p> <p>Review looking in <code>values.yaml</code> for admissionWebhooks configuration: </p> <pre><code>cd ~/$MY_REPO/exercises/exercise3/ingress-nginx\ngrep -A7 admissionWebhooks values.yaml\n</code></pre> <p>Output:</p> <pre><code>  admissionWebhooks:\n    annotations: {}\n    # ignore-check.kube-linter.io/no-read-only-rootfs: \"This deployment needs write access to root filesystem\".\n\n    ## Additional annotations to the admission webhooks.\n    ## These annotations will be added to the ValidatingWebhookConfiguration and\n    ## the Jobs Spec of the admission webhooks.\n    enabled: true\n</code></pre> <p>Result</p> <p>Note that admissionWebhooks is enabled by default ( <code>enabled: true</code> )</p> <p>Let's create <code>custom_values.yaml</code> file with  admissionWebhooks disabled</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; custom_values.yaml\ncontroller:\n  admissionWebhooks:\n    enabled: false\nEOF\n</code></pre> <p>Execute <code>helm template</code> and store output in new file <code>render2.yaml</code>:</p> <pre><code>helm template ingress-nginx/ingress-nginx --version 4.3.0 --values custom_values.yaml   &gt; render2.yaml\n</code></pre> <p>Now comparing this 2 render files you can see the what change in configuration has resulted in the rendered file:</p> <pre><code>diff render1.yaml render2.yaml\n</code></pre> <p>This is the YAML files that will be generated after this change:</p> <pre><code>grep \"Source\" render2.yaml\n</code></pre> <p>Output:</p> <pre><code># Source: ingress-nginx/templates/controller-serviceaccount.yaml\n# Source: ingress-nginx/templates/controller-configmap.yaml\n# Source: ingress-nginx/templates/clusterrole.yaml\n# Source: ingress-nginx/templates/clusterrolebinding.yaml\n# Source: ingress-nginx/templates/controller-role.yaml\n# Source: ingress-nginx/templates/controller-rolebinding.yaml\n# Source: ingress-nginx/templates/controller-service.yaml\n# Source: ingress-nginx/templates/controller-deployment.yaml\n# Source: ingress-nginx/templates/controller-ingressclass.yaml\n</code></pre> <p>Step 4: Now, let's add a custom Configuration to <code>controller-service.yaml</code> to use Static <code>loadBalancerIP</code>,  we've configured in step ### Reserve Static IP addresses. First we need to locate  the correct service parameters:</p> <pre><code>grep -A20 \"service:\" values.yaml \n</code></pre> <p>Output</p> <p>Shows that we have 4 different services in values file</p> <p>Let's narrow our search:</p> <pre><code>grep -A20 \"service:\" values.yaml | grep  \"List of IP address\"\n</code></pre> <p>Output:</p> <pre><code>    ## List of IP addresses at which the controller services are available\n      ## List of IP addresses at which the stats-exporter service is available\n    ## List of IP addresses at which the default backend service is available\n</code></pre> <p>Output:</p> <pre><code>  service:\n    enabled: true\n\n    annotations: {}\n    labels: {}\n    # clusterIP: \"\"\n\n    ## List of IP addresses at which the controller services are available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    # loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n</code></pre> <p>Result</p> <p>It seems controller is first <code>service:</code> in <code>values.yaml</code> and currently is  it has <code>loadBalancerIP</code> section commented</p> <p>Step 4: Let's add <code>loadBalancerIP</code> IP configuration to <code>custom_values.yaml</code></p> <p>Set variable:</p> <pre><code>cd ~/$MY_REPO/ip-infrastructure\nexport STATIC_IP_ADDRESS=$(terraform output -raw addresses )\necho $STATIC_IP_ADDRESS\n</code></pre> <p>Add custom values:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise3/ingress-nginx\ncat &lt;&lt; EOF&gt;&gt; custom_values.yaml\n  service:\n    loadBalancerIP: $STATIC_IP_ADDRESS\nEOF\n</code></pre> <p>Our final <code>custom_values.yaml</code> should look as following:</p> <pre><code>cat custom_values.yaml\ncontroller:\n  admissionWebhooks:\n    enabled: false\n  service:\n    loadBalancerIP: 3?.X.X.X\n</code></pre> <p>Execute <code>helm template</code> and store output in new file <code>render3.yaml</code>:</p> <pre><code>helm template ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0  &gt; render3.yaml\n</code></pre> <p>Show the difference in rendered file:</p> <pre><code>diff render2.yaml render3.yaml\n</code></pre> <p>Output:</p> <pre><code>&gt;   loadBalancerIP: 3?.X.X.X\n</code></pre> <p>Ensure that this change is in fact belongs to <code>controller-service.yaml</code>:</p> <pre><code>grep -C16  \"loadBalancerIP:\" render3.yaml\n</code></pre> <p>Result</p> <p>We configured correct service that will Ingress Controller service with <code>type: Loadbalancer</code> and  static IP with previously generated with terraform.</p> <p>Summary</p> <p><code>helm template</code> is great tool to render you charts to YAML.</p> <p>Tip</p> <p>You can use Helm as packaging you charts only. And then use  <code>helm template</code> to generate actual YAMLs and apply them with <code>kubectl apply</code></p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#34-dry-runs","title":"3.4 Dry-Runs","text":"<p>Before applying configuration to Kubernetes Cluster it is good idea to Dry-Run it for Errors. This is especially important if you doing Upgrades.</p> <p>The dry-run feature provides Helm users a way to debug the output of a chart before it is sent on to Kubernetes. With all of the templates rendered,  you can inspect exactly what would have been submitted to your cluster. And with the release data, you can verify that the release would have been created as you expected.</p> <p>Here is some of the <code>dry-run</code> working principals:</p> <ul> <li><code>--dry-run</code> mixes non-YAML information with the rendered templates. This means the data has to be cleaned up before being sent to tools like kubectl.</li> <li>A <code>--dry-run</code> on upgrade can produce different YAML output than a --dry-run on install, and this can be confusing.</li> <li>It contacts the Kubernetes API server for validation, which means Helm has to have Kubernetes credentials even if it is just used to --dry-run a release.</li> <li>It also inserts information into the template engine that is cluster-specific. Because of this, the output of some rendering processes may be cluster-specific.</li> </ul> <p>Difference between <code>dry-run</code> and <code>template</code> is that the <code>dry-run</code> contacts Kubernetes API. It is good idea to use <code>dry-run</code> prior deployment and upgrade as it creates mutated output, while <code>template</code> doesn't contacts API and can to pure rendering.</p> <p>Step 1: Execute our deployment with <code>dry-run</code> command:</p> <pre><code>helm install ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --dry-run\n</code></pre> <p>Output:</p> <pre><code>NAME: ingress-nginx\nLAST DEPLOYED: Mon Oct 24 17:37:50 2022\nNAMESPACE: default\nSTATUS: pending-install\nREVISION: 1\nTEST SUITE: None\nHOOKS:\nMANIFEST:\n---\n# Source: ingress-nginx/templates/controller-serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    helm.sh/chart: \n</code></pre> <p>At the top of the output, it will print some information about the release in our case it tells what phase of the release it is in (pending-install), and the revision number. Next, after the informational block, all of the rendered templates are dumped to standard output. Finally, at the bottom of the dry-run output, Helm prints the user-oriented release notes:</p> <p>Note</p> <p><code>--dry-run</code> dumps the output validates, but doesn't deploy actual chart.</p> <p>Step 2: Finally let's deploy Nginx Ingress Charts with our parameters in <code>ingress-nginx</code> namespace</p> <pre><code>kubectl create ns ingress-nginx\nhelm install ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx\n</code></pre> <p>Step 3: Verify Helm Deployment:</p> <pre><code>helm list --namespace ingress-nginx\n</code></pre> <p>Follow Installation note and verify Ingress-Controller Service:</p> <pre><code>kubectl --namespace ingress-nginx get services -o wide ingress-nginx-controller\n</code></pre> <p>Output:</p> <pre><code>NAME                       TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                      AGE\ningress-nginx-controller   LoadBalancer   172.10.0.213   35.192.101.76   80:30930/TCP,443:30360/TCP   3m9s\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#35-deploy-drupal-using-nginx-ingress-controller","title":"3.5 Deploy Drupal using Nginx Ingress Controller","text":"<p>Let's test our newly configured <code>ingress-nginx-controller</code> and expose <code>drupal</code> application using ingress instead of <code>LoadBalancer</code>. </p> <p>Check if STATIC_IP_ADDRESS variable is set:</p> <pre><code>echo $STATIC_IP_ADDRESS\n</code></pre> <p>If not, set it with you Static IP address value. Then proceed to create a YAML file for Drupal's ingress controller:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise3\nkubectl delete pvc data-mywebsite-mariadb-0\ncat &lt;&lt; EOF&gt;&gt; drupal_ing_values.yaml\ningress:\n  annotations: {kubernetes.io/ingress.class: \"nginx\"}\n  enabled: true\n  hostname: $STATIC_IP_ADDRESS.nip.io\n  path: /\n  pathType: ImplementationSpecific\nEOF\n</code></pre> <p>Note</p> <p>Take note of the domain you use here as you'll need this later for the assignment submission</p> <pre><code>helm install  mywebsite bitnami/drupal --values drupal_ing_values.yaml \n</code></pre> <pre><code>1. Get the Drupal URL:\n\n  You should be able to access your new Drupal installation through\n\n  http://3?.X.X.X.nip.io/\n</code></pre> <p>Success</p> <p>Drupal site is now accessible via Ingress. Our Nginx Ingress Controller has been setup correctly!</p> <p>Uninstall Drupal:</p> <pre><code>helm uninstall mywebsite\nkubectl delete pvc data-mywebsite-mariadb-0\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#36-optional-using-set-values-and-upgrading-charts","title":"3.6 (Optional)  Using  <code>--set</code> values and Upgrading Charts","text":"<p>In addition to <code>--value</code> option, there is a second flag that can be used to add individual parameters to an <code>install</code> or <code>upgrade</code>.  The <code>--set</code> flag takes one or more values directly. They do not need to be stored in a YAML file.</p> <p>Step 1: Let's update our <code>ingress-nginx</code> release with new parameter <code>--set controller.image.pullPolicy=Always</code>, but first we want to render template with and without parameter to see what the change will be applied:</p> <p>Generate a version without <code>--set</code> :</p> <pre><code>cd ~/$MY_REPO/exercises/exercise3/ingress-nginx\nhelm template ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx  &gt; render4.yaml\n</code></pre> <p>Generate a version with <code>--set</code>:</p> <pre><code>helm template ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx --set controller.image.pullPolicy=Always &gt; render5.yaml\n</code></pre> <p>Compare both:</p> <pre><code>diff render4.yaml render5.yaml\n</code></pre> <p>Output:</p> <pre><code>&lt;           imagePullPolicy: IfNotPresent\n---\n&gt;           imagePullPolicy: Always\n</code></pre> <p>Step 2: Let's apply update to our <code>ingress-nginx</code> release. For that we going to use <code>helm upgrade</code> command:</p> <pre><code>helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx --set controller.image.pullPolicy=Always \n</code></pre> <p>Verify that upgrade was successful:</p> <pre><code>helm list --namespace ingress-nginx\n</code></pre> <p>Output:</p> <pre><code>NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION\ningress-nginx   ingress-nginx   2               2022-10-24 18:03:33.541452261 -0400 EDT deployed        ingress-nginx-4.3.0     1.4.0  \n</code></pre> <p>Result</p> <p>We can see that Revision change to <code>2</code></p> <p>Note</p> <p>When we talk about upgrading in Helm, we talk about upgrading an installation, not a chart. An installation is a particular instance of a chart in your cluster.  When you run <code>helm install</code>, it creates the installation. To modify that installation, use <code>helm upgrade</code>. This is an important distinction to make in the present context because upgrading an installation can consist of two different kinds of changes:</p> <pre><code>* You can upgrade the version of the chart\n* You can upgrade the configuration of the installation\n</code></pre> <p>In this case we upgrading the configuration of the installation.</p> <p>Extra</p> <p>If you interested upgrade chart version of ingress-nginx to the next available release or <code>3.34.0</code>, give it a try.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#37-optional-listing-releases-history-rollbacks","title":"3.7 (Optional)  Listing Releases, History, Rollbacks","text":"<p>Step 1: Let's see if we can upgrade our release with wrong value <code>pullPolicy=NoSuchPolicy</code> that doesn't exist on Kubernetes. We will first run in <code>dry-run</code> mode</p> <pre><code>helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx --set controller.image.pullPolicy=NoSuchPolicy --dry-run\n</code></pre> <p>Result</p> <p>Release \"ingress-nginx\" has been upgraded. Happy Helming!</p> <p>Let's now apply same config without <code>--dry-run</code> mode:</p> <pre><code>helm upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx --set controller.image.pullPolicy=NoSuchPolicy\n</code></pre> <p>Output:</p> <pre><code>Error: UPGRADE FAILED: cannot patch \"ingress-nginx-controller\" with kind Deployment: Deployment.apps \"ingress-nginx-controller\" is invalid: spec.template.spec.containers[0].imagePullPolicy: Unsupported value: \"NoSuchPolicy\": supported values: \"Always\", \"IfNotPresent\", \"Never\"\n</code></pre> <p>Failed</p> <p>As the error message indicates, a pull policy cannot be set to NoSuchPolicy. This error came from the Kubernetes API server, which means Helm submitted the manifest,  and Kubernetes rejected it. So our release should be in a failed state.</p> <p>Verify with <code>helm list</code> to confirm <code>failed</code> state:</p> <pre><code>helm list --namespace ingress-nginx\n</code></pre> <p>Output:</p> <pre><code>NAME            NAMESPACE       REVISION        UPDATED                                 STATUS  CHART                APP VERSION\ningress-nginx   ingress-nginx   3               2022-10-24 18:07:29.108706605 -0400 EDT failed  ingress-nginx-4.3.0  1.4.0\n</code></pre> <p>Step 2: List all releases with <code>helm history</code>:</p> <pre><code>helm history ingress-nginx -n ingress-nginx\n</code></pre> <pre><code>REVISION        UPDATED                         STATUS          CHART                   APP VERSION     DESCRIPTION                                                                                                                                                                                                                                                                                  \n1               Mon Oct 24 17:39:36 2022        superseded      ingress-nginx-4.3.0     1.4.0           Install complete                                                                                                                                                                                                                                                                             \n2               Mon Oct 24 18:03:33 2022        deployed        ingress-nginx-4.3.0     1.4.0           Upgrade complete                                                                                                                                                                                                                                                                             \n3               Mon Oct 24 18:07:29 2022        failed          ingress-nginx-4.3.0     1.4.0           Upgrade \"ingress-nginx\" failed: cannot patch \"ingress-nginx-controller\" with kind Deployment: Deployment.apps \"ingress-nginx-controller\" is invalid: spec.template.spec.containers[0].imagePullPolicy: Unsupported value: \"NoSuchPolicy\": supported values: \"Always\", \"IfNotPresent\", \"Never\"\n</code></pre> <p>Info</p> <p>During the life cycle of a release, it can pass through several different statuses. Here they are, approximately in the order you would likely see them:</p> <ul> <li> <p><code>pending-install</code> Before sending the manifests to Kubernetes, Helm claims the installation by creating a release (marked version 1) whose status is set to pending-install.</p> </li> <li> <p><code>deployed</code> As soon as Kubernetes accepts the manifest from Helm, Helm updates the release record, marking it as deployed.</p> </li> <li> <p><code>pending-upgrade</code> When a Helm upgrade is begun, a new release is created for an installation (e.g., v2), and its status is set to pending-upgrade.</p> </li> <li> <p><code>superseded</code> When an upgrade is run, the last deployed release is updated, marked as superseded, and the newly upgraded release is changed from pending-upgrade to deployed.</p> </li> <li> <p><code>pending-rollback</code> If a rollback is created, a new release (e.g., v3) is created, and its status is set to pending-rollback until Kubernetes accepts the release manifest. Then it is marked deployed and the last release is marked superseded.</p> </li> <li> <p><code>uninstalling</code> When a helm uninstall is executed, the most recent release is read and then its status is changed to uninstalling.</p> </li> <li> <p><code>uninstalled</code> If history is preserved during deletion, then when the helm uninstall is complete, the last release\u2019s status is changed to uninstalled.</p> </li> <li> <p><code>failed</code> Finally, if during any operation, Kubernetes rejects a manifest submitted by Helm, Helm will mark that release failed.</p> </li> </ul> <p>Step 3: Rollback release with <code>helm rollback</code>.</p> <p>From the error, we know that the release failed because we supplied an invalid image pull policy. So of course we could correct this by simply running another helm upgrade.  But imagine a case where the cause of error was not readily available. Rather than leave the application in a failed state while diagnosing the problem, it would be nice to  simply revert back to the release that worked before.</p> <pre><code>helm rollback ingress-nginx 2 -n ingress-nginx\nhelm history ingress-nginx -n ingress-nginx\nhelm list\n</code></pre> <pre><code>4               Tue Aug 10 11:16:24 2021        deployed        ingress-nginx-3.35.0    0.48.1          Rollback to 2  \n---\ningress-nginx   ingress-nginx   4               2021-08-10 11:16:24.424371 -0400 EDT    deployed        ingress-nginx-3.35.0    0.48.1  \n</code></pre> <pre><code>Rollback was a success! Happy Helming!\n</code></pre> <p>Success</p> <p>This command tells Helm to fetch the ingress-nginx version 2 release, and resubmit that manifest to Kubernetes.  A rollback does not restore to a previous snapshot of the cluster. Helm does not track enough information to do that. What it does is resubmit the previous configuration,  and Kubernetes attempts to reset the resources to match.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#38-optional-upgrade-release-with-helm-diff-plugin","title":"3.8 (Optional) Upgrade Release with Helm <code>Diff</code> Plugin","text":"<p>While Helm provides a lot of functionality out of the box. Community contributes a lot of great functionality via helm plugins.</p> <p>Plugins allow you to add extra functionality to Helm and integrate seamlessly with the CLI, making them a popular choice for users with unique workflow requirements.  There are a number of third-party plugins available online for common use cases, such as secrets management. In addition, plugins are incredibly easy to build on your own for unique, one-off tasks.</p> <p>Helm plugins are external tools that are accessible directly from the Helm CLI. They allow you to add custom subcommands to Helm without making any modifications to Helm\u2019s Go source code.</p> <p>Many third-party plugins are made open source and publicly available on GitHub. Many of these plugins use the \u201chelm-plugin\u201d tag/topic to make them easy to find. Refer to the documentation for Helm plugins on GitHub</p> <p>Let's Upgrade our running Ingress Nginx chart with metrics service that will enable Prometheus to scrape metrics from our Nginx.</p> <p>Step 1: Update <code>custom_values.yaml</code> with following information:</p> <pre><code>cat &lt;&lt; EOF&gt;&gt; custom_values.yaml\n  metrics:\n    enabled: true\nEOF\n</code></pre> <p>Step 2: So far we've used <code>helm template</code> to render and compare manifests. However most of the time you might not be able to do it during upgrades. However you want to have a clear understanding what will be upgraded if you change something.</p> <p>That's where Helm Diff Plugin can be handy.  Helm Diff plugin giving your a preview of what a helm upgrade would change. It basically generates a diff between the latest deployed version of a release and a <code>helm upgrade --dry-run</code></p> <p>Install Helm Diff Plugin:</p> <pre><code>helm plugin install https://github.com/databus23/helm-diff\n</code></pre> <pre><code>helm diff upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx\n</code></pre> <p>Output:</p> <pre><code>-           imagePullPolicy: Always\n+           imagePullPolicy: IfNotPresent\n+               protocol: TCP\n+             - name: metrics\n+               containerPort: 1025\n+ # Source: ingress-nginx/templates/controller-service-metrics.yaml\n+ apiVersion: v1\n+ kind: Service\n...\n</code></pre> <p>Result</p> <p>Amazing! This looks like <code>terraform plan</code> but for helm :) </p> <p>Important</p> <p>Another discovery from above printout is that since we forgot to add <code>--set imagePullPolicy</code> command, our value will be reverted with upgrade. This is really important to understand as you configuration maybe lost if you use <code>--set</code></p> <p>Step 3: Let's upgrade our <code>ingress-nginx</code> release:</p> <pre><code>helm  upgrade ingress-nginx ingress-nginx/ingress-nginx --values custom_values.yaml --version 4.3.0 --namespace ingress-nginx\n helm list -n ingress-nginx\n</code></pre> <p>Success</p> <p>Our <code>ingress-nginx</code> is ready to run. Going forward we going to always deploy this chart.</p> <p>Summary</p> <p>So far we've learned:</p> <ul> <li> <p>How to customize helm chart deployment using <code>--set</code> and <code>--values</code> using values.yaml</p> </li> <li> <p>Using <code>values.yaml</code> preferable mehtod in order to achieve reproducible deployments</p> </li> <li> <p>How to upgrade and rollback releases</p> </li> <li> <p>Helm template and dry-run</p> </li> <li> <p>Helm Diff Plugin</p> </li> </ul>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#39-add-results-to-repository","title":"3.9 Add results to repository","text":"<pre><code>cd ~/$MY_REPO/exercises/exercise3\ngit add 'render*.yaml'\n\ngit commit -m \"exercise 3 - advanced search and install\"\ngit push\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#4-creating-notepad-helm-charts","title":"4 Creating NotePad Helm Charts","text":"<p>So far with learned how to deploy existing charts from the Artifact Hub.</p> <p>However sometimes you or your company needs to build software that require's to be distributed and shared externally, as well as have lifecycle and release management. Helm Charts becomes are valuable option for that, specifically if you application is based of containers!</p> <p>Imagine that our solution is Go-based NotePad has first customer that wants to deploy it on their system and they requested delivery via Helm Charts that available on GCP based Helm Repository. To achieve such task we going to create 2 Charts:</p> <pre><code>* `gowebapp-mysql` chart\n* `gowebapp` chart\n</code></pre> <p>And store them in Google Artifact Registry that provides Helm 3 OCI Repository.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#41-design-gowebapp-mysql-chart","title":"4.1 Design <code>gowebapp-mysql</code> chart","text":""},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#411-create-gowebapp-mysql-chart","title":"4.1.1  Create <code>gowebapp-mysql</code> chart","text":"<p>As scary as it sounds creating a new basic helm chart is 5 minute thing! We going to learn 2 quick methods to create helm charts, that will help you to save time and get started with helm quickly.</p> <p>Helm includes the <code>helm create</code> command to make it easy for you to create a chart of your own, and it\u2019s a great way to get started.</p> <p>The create command creates a chart for you, with all the required chart structure and files. These files are documented to help you understand what is needed, and the templates it provides  showcase multiple Kubernetes manifests working together to deploy an application. In addition, you can install and test this chart right out of the box.</p> <p>Step 0 Create the exercise folder for the <code>NotePad</code> Helm Chart</p> <pre><code>mkdir -p ~/$MY_REPO/exercises/exercise4\ncd ~/$MY_REPO/exercises/exercise4\n\ncat &lt;&lt;EOF&gt; helm/README.md\n# Helm Chart, HelmFiles and values to deploy \\`NotePad\\` application.\nEOF\n</code></pre> <p>Step 1: Create a new chart:</p> <p>Create the chart:</p> <pre><code>helm create gowebapp-mysql\n</code></pre> <p>Inspect the new structure:</p> <pre><code>cd gowebapp-mysql\ntree\n</code></pre> <p>Output:</p> <pre><code>\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 charts\n\u251c\u2500\u2500 templates\n\u2502   \u251c\u2500\u2500 NOTES.txt\n\u2502   \u251c\u2500\u2500 _helpers.tpl\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 hpa.yaml\n\u2502   \u251c\u2500\u2500 ingress.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u251c\u2500\u2500 serviceaccount.yaml\n\u2502   \u2514\u2500\u2500 tests\n\u2502       \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 values.yaml\n</code></pre> <p>Result</p> <p>This command creates a new Nginx chart, with a name of your choice, following best practices for a chart layout. Since Kubernetes clusters can have different methods to expose an application, this chart makes the way Nginx is exposed to network traffic configurable so it can be exposed in a wide variety of clusters. The chart has following structure:</p> <ul> <li>The <code>Chart.yaml</code> file contains metadata and some functionality controls for the chart</li> <li>The <code>charts</code> folder currently empty may container charts dependency of the top level chart. For example we could of make our <code>gowebapp-mysql</code> as dependency chart for <code>gowebapp</code></li> <li><code>templates</code> folder contains all <code>yaml</code> manifests in Go templates and used to generate Kubernetes manifests</li> <li><code>values.yaml</code> contains <code>default</code> values applied during the rendering.</li> <li>The <code>NOTES.txt</code> file is a special template. When a chart is installed, the NOTES.txt template is rendered and displayed rather than being installed into a cluster.</li> <li><code>_helpers.tpl</code> - helper templates for your other templates (e.g creating same labers across all charts). Files with <code>_</code> are not rendered to Kubernetes object definitions, but are available everywhere within other chart templates for use.</li> </ul> <p>Here we going to show a quick way to create a Helm chart without adding any templating.</p> <p>Step 2: Delete <code>templates</code> folder and create empty one:</p> <pre><code>rm -rf templates\nmkdir templates\ncd templates\n</code></pre> <p>Step 3: Copy existing <code>gowebapp-mysql</code> manifests to <code>templates</code> folder:</p> <pre><code>cp ~/$MY_REPO/deploy/k8s-manifests/gowebapp-mysql-{pvc,service,deployment}.yaml .\ncp ~/$MY_REPO/deploy/k8s-manifests/secret-mysql.yaml .              \n</code></pre> <p>Step 4: Lint Helm Chart:</p> <p>When developing charts, especially when working with YAML templates, it can be easy to make a mistake or miss something.  To help you catch errors, bugs, style issues, and other suspicious elements, the Helm client includes a linter.  This linter can be used during chart development and as part of any testing processes.</p> <p>To use the linter, use the <code>lint</code> command on a chart as a directory or a packaged archive:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise4\nhelm lint gowebapp-mysql\n</code></pre> <p>Output:</p> <pre><code>==&gt; Linting gowebapp-mysql\n[INFO] Chart.yaml: icon is recommended\n\n1 chart(s) linted, 0 chart(s) failed\n</code></pre> <p>Step 5:  Install Helm Chart locally:</p> <p>Create <code>dev</code> namespace:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise4/gowebapp-mysql\nkubectl create ns dev\nkubectl config set-context --current --namespace=dev\n</code></pre> <p>Render the manifest locally and compare to original manifests:</p> <pre><code>helm template gowebapp-mysql . &gt; render.yaml\n</code></pre> <p>Inspect the <code>render.yaml</code> file that was generated. Install the chart:</p> <pre><code>helm install gowebapp-mysql .\n</code></pre> <p>Output:</p> <pre><code>NAME: gowebapp-mysql\nLAST DEPLOYED: Mon Oct 24 18:45:17 2022\nNAMESPACE: dev\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <pre><code>helm list\nkubectl get all\nkubectl get pvc\n</code></pre> <p>Success</p> <p>We've deployed our own helm chart locally. While we haven't used templating of the chart at all, we have a helm chart that can be installed and upgraded using helm release features.</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#412-add-results-to-repository","title":"4.1.2 Add results to repository","text":"<pre><code>cd ~/$MY_REPO/exercises/exercise4\ngit add .\n\ngit commit -m \"exercise 4 - simple helm files\"\ngit push\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#42-design-gowebapp-chart","title":"4.2  Design <code>gowebapp</code> chart.","text":""},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#421-create-gowebapp-chart","title":"4.2.1  Create <code>gowebapp</code> chart","text":"<p>We going to use another method to deploy our second chart <code>gowebapp</code>. In this case we going to use <code>nginx</code> template provided by the tool. </p> <p>Task N2: Create <code>gowebapp</code> Helm chart:</p> <pre><code>* Configure `Deployment` template in `values.yaml`\n* Configure `Service` template in `values.yaml`\n* Disable `Service account` template in `values.yaml`\n* Configure `Ingress` template in `values.yaml`:\n    * ingress.class: \"nginx\"\n    * path: \"/\"\n    * pathType: \"ImplementationSpecific\"\n    * host: $STATIC_IP_ADDRESS.nip.io\n* Templatize the ConfigMap Resource\n* Ensure the chart is deployable\n* Ensure `gowebapp` in browser\n</code></pre> <pre><code>mkdir -p ~/$MY_REPO/exercises/exercise5\ncd ~/$MY_REPO/exercises/exercise5\n\ncat &lt;&lt;EOF&gt; helm/README.md\n# N2 exercise for Helm Chart, HelmFiles and values to deploy \\`NotePad\\` application.\nEOF\n</code></pre> <p>Step 1 Create a new chart:</p> <p>Create the new chart and inspect it</p> <pre><code>helm create gowebapp\ncd gowebapp\ntree\n</code></pre> <p>Output:</p> <pre><code>\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 charts\n\u251c\u2500\u2500 templates\n\u2502   \u251c\u2500\u2500 NOTES.txt\n\u2502   \u251c\u2500\u2500 _helpers.tpl\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 hpa.yaml\n\u2502   \u251c\u2500\u2500 ingress.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u251c\u2500\u2500 serviceaccount.yaml\n\u2502   \u2514\u2500\u2500 tests\n\u2502       \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 values.yaml\n</code></pre> <pre><code>edit values.yaml\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#422-template-the-deployment","title":"4.2.2 Template the Deployment","text":"<p>Ensure you are working with the correct chart in <code>exercise5</code> folder.</p> <p>Execute the following tasks:</p> <p>Task 1 Update the replicaCount value to 2 in the <code>values.yaml</code> file:</p> <p>Solution</p> <p>On the file <code>values.yaml</code>:</p> <pre><code>replicaCount: 2\n</code></pre> <p>Task 2 Update the repository and tag section to point to your <code>gowebapp</code> docker image in the <code>values.yaml</code> file:</p> <p>Solution</p> <p>On the file <code>values.yaml</code>:</p> <pre><code>image:\n    repository: gcr.io/${PROJECT_ID}/gowebapp\n    pullPolicy: IfNotPresent\n    # Overrides the image tag whose default is the chart appVersion.\n    tag: \"v3\"\n</code></pre> <p>Task 3 Update the resources section in the <code>values.yaml</code> file to include the resource <code>requests</code> and <code>limits</code> the gowebapp application needs:</p> <p>Solution</p> <p>On the file <code>values.yaml</code>:</p> <pre><code>resources:\n  requests:\n    cpu: \"20m\"\n    memory: \"10M\"\n  limits: \n    cpu: \"500m\"\n    memory: \"2G\"\n</code></pre> <p>Task 4 Add the livness and readiness probes to the deployment.</p> <p>Hint</p> <p>If you need additional help see the reference <code>gowebapp-deployment.yaml</code>:   <code>cat ~/$MY_REPO/deploy/k8s-manifests/gowebapp-deployment.yaml</code></p> <p>Solution</p> <p>There are multiple ways of doing this, ideally you should create an  entry in your <code>values.yaml</code> file:</p> <pre><code>liveness:\n    path: /register\n    port: 80\nreadiness:\n    path: /register\n    port: 80\n</code></pre> <p>Then reference it on your <code>deployment.yaml</code> file:</p> <pre><code>livenessProbe:\n    httpGet:\n      path: {{ .Values.liveness.path }} # was /\n      port: {{ .Values.liveness.port }} # was http\n  readinessProbe:\n    httpGet:\n      path: {{ .Values.readiness.path }} # was /\n      port: {{ .Values.readiness.port }} # was http\n</code></pre> <p>Task 5 Notice that the <code>deployment.yaml</code> file does not have an environment variable section for <code>secrets</code>, so let's add one. For this chart we will assume that this section is optional based on whether or not a secrets section exist in the Values.yaml file.</p> <p>Task 5-a Include env delcarion in the deployment template:</p> <pre><code>edit gowebapp/templates/deployment.yaml\n</code></pre> <p>And add following code snippet in the appropriate location:</p> <pre><code>{{- if .Values.secrets }}\n          - env:\n            - name: {{.Values.secrets.name}}\n              valueFrom:\n                secretKeyRef:\n                  name: {{.Values.secrets.secretReference.name}}\n                  key: {{.Values.secrets.secretReference.key}}\n{{- end}}\n</code></pre> <p>Task 5-b Include a section in the <code>values.yaml</code> file:</p> <p>Add following snippet:</p> <pre><code>secrets:\n  enabled: true\n  name: DB_PASSWORD\n  secretReference:\n    name: mysql\n    key: password\n</code></pre> <p>Task 6 For this lab, we will include the <code>volumes</code> and <code>volumeMounts</code> sections without templating, so just copy the required sections to the appropriate location in the <code>deployment.yaml</code> template.</p> <p>Solution</p> <p>There are multiple ways of doing this, volume and volumeMounts seldom change, it's ok to put directly in the <code>deployment.yaml</code> but it's also perfectly valid to add it to the <code>values.yaml</code>. </p> <p>For this example the task suggests the change directly on <code>deployment.yaml</code>, under <code>container</code> entry inside containers tag add the following:</p> <pre><code>volumeMounts:\n  - name: config-volume\n    mountPath: /go/src/gowebapp/config\n</code></pre> <p>Under the <code>spec</code> tag enter the following: </p> <pre><code>volumes: \n- name: config-volume\n    configMap:\n    name: gowebapp\n    items:\n    - key: webapp-config-json\n        path: config.json\n</code></pre> <p>Task 7  Render the Chart locally and compare Deployment to original <code>gowebapp-deployment.yaml</code> manifests:</p> <pre><code>helm template gowebapp-mysql . &gt; render.yaml\n</code></pre> <p>Hint</p> <p>If you need additional help see the reference <code>gowebapp-deployment.yaml</code>:   <code>cat ~/$MY_REPO/deploy/k8s-manifests/gowebapp-deployment.yaml</code></p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#423-template-the-service","title":"4.2.3 Template the Service","text":"<p>Ensure you are working with <code>exercise5</code> chart.</p> <p>Step 1: The <code>service.yaml</code> template doesn't have an <code>annotation</code> section, so modify the template to add an <code>annotation</code> section, that looks following:</p> <pre><code>edit gowebapp/templates/service.yaml\n</code></pre> <pre><code>      {{- with .Values.annotations }}\n      annotations:\n        {{- toYaml . | nindent 4 }}\n      {{- end }}\n</code></pre> <p>Task: Next, modify the <code>values.yaml</code> file to allow chart users to add annotations for the service. Make sure to use the right section in the <code>values.yaml</code> file, based on how you modified your <code>/templates/service.yaml</code> file.</p> <p>Reference documentation: </p> <pre><code>* https://helm.sh/docs/chart_template_guide/control_structures/#modifying-scope-using-with\n</code></pre> <p>Solution</p> <p>On <code>values.yaml</code> file:</p> <pre><code>annotations:\n    - run: gowebapp \n    - app: gowebapp # more commonly used label\n</code></pre> <p>Step 2: Under the <code>service:</code> section in the <code>values.yaml</code> file, update the service <code>port:</code> to 9000.</p> <p>Step 3 In the <code>values.yaml</code> file, update the service type to <code>NodePort</code></p> <p>Step 4  Render the Chart locally and compare Service to original <code>gowebapp-service.yaml</code> manifests:</p> <pre><code>helm template gowebapp-mysql . &gt; render.yaml\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#424-disable-the-service-account","title":"4.2.4 Disable the Service account","text":"<p>Ensure you are working with <code>exercise5</code> chart.</p> <p>Task 1 Update the <code>values.yaml</code> file to <code>disable</code> the service account creation for the <code>gowebapp</code> deployment.</p> <p>Solution</p> <p>On <code>values.yaml</code> file:</p> <pre><code>serviceAccount:\n    # Specifies whether a service account should be created\n    create: false # was true\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#425-template-the-ingress-resource","title":"4.2.5 Template the Ingress Resource","text":"<p>Ensure you are working with <code>exercise5</code> chart.</p> <p>Task 1 <code>enable</code> the ingress the <code>values.yaml</code> file and configure it according requirements: :</p> <pre><code>* Expose `gowebapp` using `Ingress` resource:\n    * ingress.class: \"nginx\"\n    * path: \"/\"\n    * pathType: \"ImplementationSpecific\"\n    * host: $STATIC_IP_ADDRESS.nip.io\n</code></pre> <p>Solution</p> <p>On <code>values.yaml</code> file:</p> <pre><code>ingress:\n    enabled: true # was false\n    className: \"nginx\" # was \"\"\n    annotations:\n        - kubernetes.io/ingress.class: nginx\n    hosts:\n        - host: $STATIC_IP_ADDRESS.nip.io\n        paths:\n            - path: /\n            pathType: ImplementationSpecific\n</code></pre> <p>Step 2  Render the Chart locally and verify if any issues:</p> <pre><code>helm template gowebapp-mysql . &gt; render.yaml\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#426-templatize-the-configmap-resource","title":"4.2.6 Templatize the ConfigMap Resource","text":"<p>Ensure you are working with <code>exercise5</code> chart.</p> <p>Step 1  Create and templatize <code>configmap</code> resource for our <code>gowebapp</code> that provides connection to Mysql:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise5/gowebapp/templates/\ncat &lt;&lt;EOF&gt;&gt; configmap.yaml\nkind: ConfigMap \napiVersion: v1 \nmetadata:\n  name: {{ .Values.configMap.name }}\ndata:\n  webapp-config-json: |-\n{{ .Files.Get \"config.json\" | indent 4 }}\nEOF\n</code></pre> <p>Store the <code>config.json</code> inside the chart repository:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise5/gowebapp\ncat &lt;&lt;EOF&gt;&gt; config.json\n{\n    \"Database\": {\n        \"Type\": \"MySQL\",\n        \"Bolt\": {\n            \"Path\": \"gowebapp.db\"\n        },\n        \"MongoDB\": {\n            \"URL\": \"127.0.0.1\",\n            \"Database\": \"gowebapp\"\n        },\n        \"MySQL\": {\n            \"Username\": \"root\",\n            \"Password\": \"rootpasswd\",\n            \"Name\": \"gowebapp\",\n            \"Hostname\": \"gowebapp-mysql\",\n            \"Port\": 3306,\n            \"Parameter\": \"?parseTime=true\"\n        }\n    },\n    \"Email\": {\n        \"Username\": \"\",\n        \"Password\": \"\",\n        \"Hostname\": \"\",\n        \"Port\": 25,\n        \"From\": \"\"\n    },\n    \"Recaptcha\": {\n        \"Enabled\": false,\n        \"Secret\": \"\",\n        \"SiteKey\": \"\"\n    },\n    \"Server\": {\n        \"Hostname\": \"\",\n        \"UseHTTP\": true,\n        \"UseHTTPS\": false,\n        \"HTTPPort\": 80,\n        \"HTTPSPort\": 443,\n        \"CertFile\": \"tls/server.crt\",\n        \"KeyFile\": \"tls/server.key\"\n    },\n    \"Session\": {\n        \"SecretKey\": \"@r4B?EThaSEh_drudR7P_hub=s#s2Pah\",\n        \"Name\": \"gosess\",\n        \"Options\": {\n            \"Path\": \"/\",\n            \"Domain\": \"\",\n            \"MaxAge\": 28800,\n            \"Secure\": false,\n            \"HttpOnly\": true\n        }\n    },\n    \"Template\": {\n        \"Root\": \"base\",\n        \"Children\": [\n            \"partial/menu\",\n            \"partial/footer\"\n        ]\n    },\n    \"View\": {\n        \"BaseURI\": \"/\",\n        \"Extension\": \"tmpl\",\n        \"Folder\": \"template\",\n        \"Name\": \"blank\",\n        \"Caching\": true\n    }\n}\nEOF\n</code></pre> <p>Note: you may need to update the secret value.</p> <p>And finally add following snippet inside <code>values.yaml</code>:</p> <pre><code>edit gowebapp/values.yaml\n</code></pre> <pre><code>configMap:\n  name: gowebapp\n</code></pre> <p>Step 2  Render the Chart locally and verify if any issues:</p> <pre><code>helm template gowebapp-mysql . &gt; render.yaml\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#427-deploy-gowebapp","title":"4.2.7 Deploy <code>gowebapp</code>","text":"<p>Before deployment make sure test chart with <code>dry-run</code>, <code>template</code> and <code>lint</code> the chart</p> <p>Lint: </p> <pre><code>cd ~/$MY_REPO/exercises/exercise5\nhelm lint gowebapp\n</code></pre> <p>Install:</p> <pre><code>cd ~/$MY_REPO/exercises/exercise5/gowebapp\nhelm install gowebapp .\n</code></pre> <p>Output:</p> <pre><code>NAME: gowebapp\nLAST DEPLOYED: Tue Oct 24 19:05:44 2022\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <pre><code>helm ls\nkubectl get all\nkubectl get pvc\nkubectl get ing\n</code></pre> <p>Access <code>gowebapp</code> with Ingress</p> <p>Success</p> <p>You've now became a chart developer! And learned, how to create basic helm charts with 2 methods. You can farther customize the chart as needed and add more templates as you go. The next step would be to look contribute back to Helm community, use charts in Artificatory, find issue or add missing feature, help grow help project!</p>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#428-add-results-to-repository","title":"4.2.8 Add results to repository","text":"<pre><code>cd ~/$MY_REPO/exercises/exercise5\ngit add .\n\ngit commit -m \"exercise 5 - helm files\"\ngit push\n</code></pre>"},{"location":"020_Module_6_Assignment_Helm_Foundation_solution/#5-submit-assignement","title":"5 Submit assignement","text":"<p>Step 1 Ensure you pushed the changes to the repository</p> <pre><code>cd ~/$MY_REPO\ngit status \ngit log -n 5 \n</code></pre> <p>If any files are missing be sure to add it prior to submission.</p> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre> <p>Step 3 Uninstall chart <code>gowebapp</code> and <code>gowebapp-mysql</code> chart</p> <pre><code>helm install gowebapp\nhelm uninstall gowebapp-mysql\n</code></pre> <p>Step 4 Resize GKE Cluster to <code>0</code> nodes, to avoid charges:</p> <pre><code>cd ~/$MY_REPO/notepad-infrastructure\n</code></pre> <pre><code>edit terraform.tfvars\n</code></pre> <p>And set <code>gke_pool_node_count</code>   = \"0\"</p> <p>Step 5: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Step 6: Shutdown all Nodes in GKE Cluster Node Pool:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Result</p> <p>GKE Clusters has been scale down to 0 nodes.</p> <p>That is it, make sure to share your repository with your instructor.</p>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases/","title":"Istio Policies","text":"<p>Objective:</p> <ul> <li>Install Anthos Service Mesh (Managed Istio)</li> <li>Deploy bookshelf application and enable anthos proxy sidecar injection</li> <li>Deploy Istio policies and verify funcationality </li> </ul>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases/#1-create-gke-cluster-on-gcp","title":"1. Create GKE Cluster on GCP","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com \\\nanthos.googleapis.com container.googleapis.com \\\ncompute.googleapis.com monitoring.googleapis.com \\\nlogging.googleapis.com cloudtrace.googleapis.com \\\niamcredentials.googleapis.com\n</code></pre> <p>Step 2 Export variable names to be used in the lab</p> <pre><code>ZONE1=us-central1-b\nPROJECT=&lt;PROJECT_ID&gt;\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 3 nodes:</p> <pre><code>gcloud compute networks create default \\\n    --subnet-mode=auto \\\n    --bgp-routing-mode=regional \\\n    --mtu=1460\n</code></pre> <p>WORKLOAD_POOL will be used to enable Workload Identity, which is the recommended way to safely access Google Cloud services from GKE applications. MESH_ID will be used to set the mesh_id label on the cluster, which is required for metrics to get displayed on the Anthos Service Mesh Dashboard in the Cloud Console. If no MESH_ID is provided, a deafult ID based on project ID is used for creation</p> <pre><code>gcloud container clusters create central-gke-cluster \\\n--project=${PROJECT} --zone=${ZONE1} \\\n--machine-type=e2-standard-2 --num-nodes=3 \\\n--scopes=cloud-platform \\\n--workload-pool=${PROJECT}.svc.id.goog\n</code></pre> <p>Modify the cluster admin permissions to your user ID. This will allow the user to install Anthos Service Mesh on to the cluster  </p> <pre><code>kubectl create clusterrolebinding cluster-admin-binding   --clusterrole=cluster-admin   --user=&lt;USER_EMAIL&gt;\n</code></pre>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases/#2-deploy-anthos-service-mesh-on-the-cluster","title":"2. Deploy Anthos Service Mesh on the cluster","text":"<p>This installation guide install Anthos Service Mesh on the GKE Cluster. Installing ASM triggers a set of managed services to be active on the GCP backend, including the ASM Service Mesh Dashboard.  </p> <p>Google provides a tool, asmcli, which allows you to install or upgrade Anthos Service Mesh. <code>asmcli</code> will configure your project and cluster as follows:</p> <ul> <li>Grant you the required Identity and Access Management (IAM) permissions on your Google Cloud project.</li> <li>Enable the required Google APIs on your Cloud project.</li> <li>Set a label on the cluster that identifies the mesh.</li> <li>Create a service account that lets data plane components, such as the sidecar proxy, securely access your project's data and resources.</li> <li>Register the cluster to the fleet if it isn't already registered.</li> </ul> <p>Download the latest version of Anthos Mesh CLI</p> <pre><code>curl https://storage.googleapis.com/csm-artifacts/asm/asmcli &gt; asmcli\nchmod +x asmcli\n</code></pre> <p>Connect to the Anthos Cluster</p> <pre><code>gcloud container clusters get-credentials central-gke-cluster --zone us-central1-b --project &lt;PROJECT_ID&gt;\n</code></pre> <p>Step 2 Run Pre-verification steps on the cluster </p> <pre><code>./asmcli validate \\\n--project_id $PROJECT \\\n--cluster_name central-gke-cluster \\\n--cluster_location $ZONE1 \\\n--fleet_id $PROJECT\n</code></pre> <p>Note</p> <p>Please ignore any validation errors as in this case, the validation is confirming that the cluster has all necessary components to install ASM, not that ASM was installed correctly.</p> <p>Step 3 Install Anthos Service Mesh   The following command will install Anthos Service Mesh. The --enable_all flag allows the script to enable the required Google APIs, set Identity and Access Management permissions, and make the required updates to your cluster, which includes enabling GKE Workload Identity:</p> <pre><code>./asmcli install \\\n  --project_id $PROJECT \\\n  --cluster_name central-gke-cluster \\\n  --cluster_location $ZONE1 \\\n  --fleet_id $PROJECT \\\n  --option legacy-default-ingressgateway \\\n  --enable_all \\\n  --ca mesh_ca\n</code></pre> <p>Note</p> <p>Wait a few seconds for the Anthos Service Mesh install to complete.</p> <pre><code>kubectl get pods -n istio-system\n</code></pre> <p>The isito-system namespace created, will be used in the further steps to deploy Istio Ingress Gateway </p> <p>Step 4 Obtain the revision version for ASM identified in the label.</p> <pre><code>REVISION=$(kubectl get deploy -n istio-system -l app=istiod -o jsonpath={.items[*].metadata.labels.'istio\\.io\\/rev'}'{\"\\n\"}')\n</code></pre> <p>Step 5 Istio Ingress Gateway install Anthos Service Mesh gives you the option to deploy and manage gateways as part of your service mesh. A gateway describes a load balancer operating at the edge of the mesh receiving incoming or outgoing HTTP/TCP connections. Gateways are Envoy proxies that provide you with fine-grained control over traffic entering and leaving the mesh.</p> <p>The Default ASM configuration doesnt include the installation of Istio Ingress Gateway. Hence we will manually create the Istio Ingress Gateway configuration. </p> <pre><code>GATEWAY_NS=istio-gateway\nkubectl create namespace $GATEWAY_NS\nkubectl label namespace $GATEWAY_NS istio.io/rev=${REVISION} --overwrite\n</code></pre> <p>Output</p> <pre><code>    &gt; namespace/istio-gateway labeled\n</code></pre> <p>Anthos Service Mesh uses sidecar proxies to enhance network security, reliability, and observability. With Anthos Service Mesh, these functions are abstracted away from an application's primary container and implemented in a common out-of-process proxy delivered as a separate container in the same Pod.</p> <p>Step 6 Next we will download configurations for Istio Ingress gateway and start configuration </p> <pre><code>git clone https://github.com/GoogleCloudPlatform/anthos-service-mesh-packages\nkubectl apply -n $GATEWAY_NS -f anthos-service-mesh-packages/samples/gateways/istio-ingressgateway\n</code></pre> <p>Info</p> <p>The above steps installs the Istio Ingress Gateway </p> <pre><code>* `istiod` - contains components such as `Citadel` and `Pilot`\n* `istio-ingressgateway` Istio Ingress Gateway\n</code></pre>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases/#3-deploy-application-to-the-cluster","title":"3. Deploy application to the cluster:","text":"<p>In the next section, we will deploy the sample BookInfo application on the GKE cluster    </p> <pre><code>kubectl label namespace default istio-injection- istio.io/rev=$REVISION --overwrite\ngit clone https://github.com/istio/istio.git\n</code></pre> <p>Review the following BookShelf sample application Deployment and service configuration </p> <pre><code>cat istio/samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre> <p>Apply the Kubernetes manifests to the cluster</p> <pre><code>kubectl apply -f istio/samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre> <p>Output:</p> <pre><code>service/details created\nserviceaccount/bookinfo-details created\ndeployment.apps/details-v1 created\nservice/ratings created\nserviceaccount/bookinfo-ratings created\ndeployment.apps/ratings-v1 created\nservice/reviews created\nserviceaccount/bookinfo-reviews created\ndeployment.apps/reviews-v1 created\ndeployment.apps/reviews-v2 created\ndeployment.apps/reviews-v3 created\nservice/productpage created\nserviceaccount/bookinfo-productpage created\ndeployment.apps/productpage-v1 created\n</code></pre> <p>Notice that the Deployments should have two containers per pod, of which one is the istio-proxy</p> <pre><code>kubectl get pods\n</code></pre> <p>To review the list of services created, we can run the following command</p> <pre><code>kubectl get services\n</code></pre> <p>To test if the services are runnning, we can access the homepage of the deployment. In this case, we are accessing the Product Page from Rating Pods </p> <pre><code>kubectl exec -it $(kubectl get pod -l app=ratings \\\n-o jsonpath='{.items[0].metadata.name}') \\\n-c ratings -- curl productpage:9080/productpage | grep -o \"&lt;title&gt;.*&lt;/title&gt;\"\n</code></pre> <p>Step 3 Next, we will work on making the service externally accessible</p> <p>Verify that Istio Ingressgateway using external Load Balancer:</p> <pre><code>cat istio/samples/bookinfo/networking/bookinfo-gateway.yaml\nkubectl apply -f istio/samples/bookinfo/networking/bookinfo-gateway.yaml\nkubectl get gateway \n</code></pre> <p>To access the external access, it is important to understand that all the traffic goes through the istio-ingressgateway Load Balancer</p> <pre><code>kubectl get svc istio-ingressgateway -n istio-system\n</code></pre> <p>Output</p> <pre><code>NAME                   TYPE           CLUSTER-IP    EXTERNAL-IP     Ports\nistio-ingressgateway   LoadBalancer   10.32.1.153   35.192.101.76   15021:32714/TCP,80:30042/TCP,443:31889/TCP,...  62s\n</code></pre> <pre><code>export GATEWAY_URL=&lt;EXTERNAL-IP&gt;   \ncurl -I http://${GATEWAY_URL}/productpage\n</code></pre> <p>You can also open the link in the browser to see the same page</p> <p>Refresh the page several times.</p> <p>Notice how you see three different versions of reviews, since we have not yet used Istio to control the version routing.</p> <p>There are three different book review services being called in a round-robin style:</p> <ul> <li>no stars</li> <li>black stars</li> <li>red stars</li> </ul>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases/#4-generate-traffic-to-the-application-using-seige","title":"4. Generate traffic to the application using seige","text":"<p>Step 1 Install the seige application and run load tests for 5 mins </p> <pre><code>sudo apt install siege\nsiege http://${GATEWAY_URL}/productpage\n</code></pre> <p>Stop the tests by using <code>Ctrl+C</code></p> <p>Step 2 Verify output in the Console</p> <ul> <li> <p>Anthos Service Mesh Page</p> </li> <li> <p>Observe the Services, Connection and Latency in the Table / Topology View</p> <p></p> </li> </ul>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases/#5-exploring-istio-policies","title":"5. Exploring Istio Policies","text":""},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases/#51-istio-security-mtls-configuration","title":"5.1 Istio Security &amp; mTLS configuration","text":"<p>We will be enforcing strict mTLS between the services in this step by making configuration changes </p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\nname: \"default\"\nnamespace: \"istio-system\"\nspec:\nmtls:\n    mode: STRICT\nEOF\n</code></pre> <p>Create 3 new namespaces for testing purposes and two of those will have anthos proxy enabled and one of them will not have any proxy config. </p> <pre><code>kubectl create ns foo\nkubectl create ns bar\nkubectl create ns legacy\n\nkubectl label namespace foo istio-injection- istio.io/rev=$REVISION --overwrite\nkubectl label namespace bar istio-injection- istio.io/rev=$REVISION --overwrite\n</code></pre> <p>Any traffic that is originating from the anthos proxy is authenticated adnd will be allowed in the cluster, whereas any deployment with no proxy will not be allowed to communicated with deployments that have the proxy. </p> <p>In this step, we will be deploying two sample pods per namespace </p> <pre><code>kubectl apply -f  istio/samples/httpbin/httpbin.yaml -n foo\nkubectl apply -f  istio/samples/sleep/sleep.yaml -n foo\n\nkubectl apply -f  istio/samples/httpbin/httpbin.yaml -n bar\nkubectl apply -f  istio/samples/sleep/sleep.yaml -n bar\n\nkubectl apply -f istio/samples/httpbin/httpbin.yaml -n legacy\nkubectl apply -f istio/samples/sleep/sleep.yaml -n legacy\n</code></pre> <p>Run the following authentication Tests </p> <pre><code>kubectl exec \"$(kubectl get pod -l app=sleep -n bar -o jsonpath={.items..metadata.name})\" -c sleep -n bar -- curl http://httpbin.foo:8000/ip -s -o /dev/null -w \"%{http_code}\\n\"\n\nfor from in \"foo\" \"bar\" \"legacy\"; do for to in \"foo\" \"bar\" \"legacy\"; do kubectl exec \"$(kubectl get pod -l app=sleep -n ${from} -o jsonpath={.items..metadata.name})\" -c sleep -n ${from} -- curl -s \"http://httpbin.${to}:8000/ip\" -s -o /dev/null -w \"sleep.${from} to httpbin.${to}: %{http_code}\\n\"; done; done\n</code></pre> <p>We can observe that any traffic that originated from namespaces that doesnt have the Istio proxy enabled will not have a successful response as indicated below </p> <p>Delete all the temp configration created for testing </p> <pre><code>kubectl delete peerauthentication -n istio-system default\nkubectl delete ns legacy\n</code></pre>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases/#52-assignment","title":"5.2 Assignment","text":"<p>Task 1 For the assignment you will be primarily working on deploying Istio Authorization Policy.  Navigate to assignment submission repo</p> <pre><code>mkdir -p ~/$student_name-notepad/module8\ncd ~/$student_name-notepad/module8\n</code></pre> <p>For the assignment tasks </p> <ul> <li> <p>Use the two namespaces <code>foo</code> &amp; <code>bar</code> and the deployments created in the previous step for this task</p> </li> <li> <p>You will not modify the existing deployments in both namespace</p> </li> <li> <p>You wil be creating an authorization policy for both namespaces that will not allow     communication outside of its own namespace </p> </li> <li> <p>Create the policy with the filename <code>istio-policy-1.yaml</code></p> <p><code>kubectl apply -f istio-policy-1.yaml</code></p> </li> </ul> <p>You can run the following command to test output of the Istio authorization policy  </p> <pre><code>for from in \"foo\" \"bar\"; do for to in \"foo\" \"bar\"; do kubectl exec \"$(kubectl get pod -l app=sleep -n ${from} -o jsonpath={.items..metadata.name})\" -c sleep -n ${from} -- curl -s \"http://httpbin.${to}:8000/ip\" -s -o /dev/null -w \"sleep.${from} to httpbin.${to}: %{http_code}\\n\"; done; done\n</code></pre> <p>Task 2 In this task, you will use IP allow access list to allow traiffc to ingress gateway from only certain IPs.</p> <ul> <li> <p>You will use the <code>Istio Authorization Policy</code> configuration to allow traffic only from your cloud console IP. You will be applying the policy to the istio-system namespace so that it applies to all the deployments </p> </li> <li> <p>Create the policy with the filename <code>istio-policy-2.yaml</code></p> </li> <li> <p><code>kubectl apply -f istio-policy-2.yaml</code></p> </li> </ul> <p>You can use the following command to test access to the page based on the IP allow list </p> <pre><code>curl -I http://${GATEWAY_URL}/productpage -s -o /dev/null -w \"%{http_code}\\n\"\n</code></pre> <p>Submission the assignement to the git repo</p> <pre><code>cd ~/$student_name-notepad\ngit add .\ngit commit -m \"TF manifests for Module 8 Assignment\"\n</code></pre>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases/#6-cleanup","title":"6 Cleanup","text":"<p>Delete GKE cluster:</p> <pre><code>gcloud container clusters delete central-gke-clusuter --region $ZONE1\n</code></pre>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases_solution/","title":"Istio Policies","text":"<p>Objective:</p> <ul> <li>Install Anthos Service Mesh (Managed Istio)</li> <li>Deploy bookshelf application and enable anthos proxy sidecar injection</li> <li>Deploy Istio policies and verify funcationality </li> </ul>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases_solution/#1-create-gke-cluster-on-gcp","title":"1. Create GKE Cluster on GCP","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com \\\nanthos.googleapis.com container.googleapis.com \\\ncompute.googleapis.com monitoring.googleapis.com \\\nlogging.googleapis.com cloudtrace.googleapis.com \\\niamcredentials.googleapis.com\n</code></pre> <p>Step 2 Export variable names to be used in the lab</p> <pre><code>ZONE1=us-central1-b\nPROJECT=&lt;PROJECT_ID&gt;\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 3 nodes:</p> <pre><code>gcloud compute networks create default \\\n    --subnet-mode=auto \\\n    --bgp-routing-mode=regional \\\n    --mtu=1460\n</code></pre> <p>WORKLOAD_POOL will be used to enable Workload Identity, which is the recommended way to safely access Google Cloud services from GKE applications. MESH_ID will be used to set the mesh_id label on the cluster, which is required for metrics to get displayed on the Anthos Service Mesh Dashboard in the Cloud Console. If no MESH_ID is provided, a deafult ID based on project ID is used for creation</p> <pre><code>gcloud container clusters create central-gke-cluster \\\n--project=${PROJECT} --zone=${ZONE1} \\\n--machine-type=e2-standard-2 --num-nodes=3 \\\n--scopes=cloud-platform \\\n--workload-pool=${PROJECT}.svc.id.goog\n</code></pre> <p>Modify the cluster admin permissions to your user ID. This will allow the user to install Anthos Service Mesh on to the cluster  </p> <pre><code>kubectl create clusterrolebinding cluster-admin-binding   --clusterrole=cluster-admin   --user=&lt;USER_EMAIL&gt;\n</code></pre>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases_solution/#2-deploy-anthos-service-mesh-on-the-cluster","title":"2. Deploy Anthos Service Mesh on the cluster","text":"<p>This installation guide install Anthos Service Mesh on the GKE Cluster. Installing ASM triggers a set of managed services to be active on the GCP backend, including the ASM Service Mesh Dashboard.  </p> <p>Google provides a tool, asmcli, which allows you to install or upgrade Anthos Service Mesh. <code>asmcli</code> will configure your project and cluster as follows:</p> <ul> <li>Grant you the required Identity and Access Management (IAM) permissions on your Google Cloud project.</li> <li>Enable the required Google APIs on your Cloud project.</li> <li>Set a label on the cluster that identifies the mesh.</li> <li>Create a service account that lets data plane components, such as the sidecar proxy, securely access your project's data and resources.</li> <li>Register the cluster to the fleet if it isn't already registered.</li> </ul> <p>Download the latest version of Anthos Mesh CLI</p> <pre><code>curl https://storage.googleapis.com/csm-artifacts/asm/asmcli &gt; asmcli\nchmod +x asmcli\n</code></pre> <p>Connect to the Anthos Cluster</p> <pre><code>gcloud container clusters get-credentials central-gke-cluster --zone us-central1-b --project &lt;PROJECT_ID&gt;\n</code></pre> <p>Step 2 Run Pre-verification steps on the cluster </p> <pre><code>./asmcli validate \\\n--project_id $PROJECT \\\n--cluster_name central-gke-cluster \\\n--cluster_location $ZONE1 \\\n--fleet_id $PROJECT\n</code></pre> <p>Note</p> <p>Please ignore any validation errors as in this case, the validation is confirming that the cluster has all necessary components to install ASM, not that ASM was installed correctly.</p> <p>Step 3 Install Anthos Service Mesh   The following command will install Anthos Service Mesh. The --enable_all flag allows the script to enable the required Google APIs, set Identity and Access Management permissions, and make the required updates to your cluster, which includes enabling GKE Workload Identity:</p> <pre><code>./asmcli install \\\n  --project_id $PROJECT \\\n  --cluster_name central-gke-cluster \\\n  --cluster_location $ZONE1 \\\n  --fleet_id $PROJECT \\\n  --option legacy-default-ingressgateway \\\n  --enable_all \\\n  --ca mesh_ca\n</code></pre> <p>Note</p> <p>Wait a few seconds for the Anthos Service Mesh install to complete.</p> <pre><code>kubectl get pods -n istio-system\n</code></pre> <p>The isito-system namespace created, will be used in the further steps to deploy Istio Ingress Gateway </p> <p>Step 4 Obtain the revision version for ASM identified in the label.</p> <pre><code>REVISION=$(kubectl get deploy -n istio-system -l app=istiod -o jsonpath={.items[*].metadata.labels.'istio\\.io\\/rev'}'{\"\\n\"}')\n</code></pre> <p>Step 5 Istio Ingress Gateway install Anthos Service Mesh gives you the option to deploy and manage gateways as part of your service mesh. A gateway describes a load balancer operating at the edge of the mesh receiving incoming or outgoing HTTP/TCP connections. Gateways are Envoy proxies that provide you with fine-grained control over traffic entering and leaving the mesh.</p> <p>The Default ASM configuration doesnt include the installation of Istio Ingress Gateway. Hence we will manually create the Istio Ingress Gateway configuration. </p> <pre><code>GATEWAY_NS=istio-gateway\nkubectl create namespace $GATEWAY_NS\nkubectl label namespace $GATEWAY_NS istio.io/rev=${REVISION} --overwrite\n</code></pre> <p>Output</p> <pre><code>    &gt; namespace/istio-gateway labeled\n</code></pre> <p>Anthos Service Mesh uses sidecar proxies to enhance network security, reliability, and observability. With Anthos Service Mesh, these functions are abstracted away from an application's primary container and implemented in a common out-of-process proxy delivered as a separate container in the same Pod.</p> <p>Step 6 Next we will download configurations for Istio Ingress gateway and start configuration </p> <pre><code>git clone https://github.com/GoogleCloudPlatform/anthos-service-mesh-packages\nkubectl apply -n $GATEWAY_NS -f anthos-service-mesh-packages/samples/gateways/istio-ingressgateway\n</code></pre> <p>Info</p> <p>The above steps installs the Istio Ingress Gateway </p> <pre><code>* `istiod` - contains components such as `Citadel` and `Pilot`\n* `istio-ingressgateway` Istio Ingress Gateway\n</code></pre>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases_solution/#3-deploy-application-to-the-cluster","title":"3. Deploy application to the cluster:","text":"<p>In the next section, we will deploy the sample BookInfo application on the GKE cluster    </p> <pre><code>kubectl label namespace default istio-injection- istio.io/rev=$REVISION --overwrite\ngit clone https://github.com/istio/istio.git\n</code></pre> <p>Review the following BookShelf sample application Deployment and service configuration </p> <pre><code>cat istio/samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre> <p>Apply the Kubernetes manifests to the cluster</p> <pre><code>kubectl apply -f istio/samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre> <p>Output:</p> <pre><code>service/details created\nserviceaccount/bookinfo-details created\ndeployment.apps/details-v1 created\nservice/ratings created\nserviceaccount/bookinfo-ratings created\ndeployment.apps/ratings-v1 created\nservice/reviews created\nserviceaccount/bookinfo-reviews created\ndeployment.apps/reviews-v1 created\ndeployment.apps/reviews-v2 created\ndeployment.apps/reviews-v3 created\nservice/productpage created\nserviceaccount/bookinfo-productpage created\ndeployment.apps/productpage-v1 created\n</code></pre> <p>Notice that the Deployments should have two containers per pod, of which one is the istio-proxy</p> <pre><code>kubectl get pods\n</code></pre> <p>To review the list of services created, we can run the following command</p> <pre><code>kubectl get services\n</code></pre> <p>To test if the services are runnning, we can access the homepage of the deployment. In this case, we are accessing the Product Page from Rating Pods </p> <pre><code>kubectl exec -it $(kubectl get pod -l app=ratings \\\n-o jsonpath='{.items[0].metadata.name}') \\\n-c ratings -- curl productpage:9080/productpage | grep -o \"&lt;title&gt;.*&lt;/title&gt;\"\n</code></pre> <p>Step 3 Next, we will work on making the service externally accessible</p> <p>Verify that Istio Ingressgateway using external Load Balancer:</p> <pre><code>cat istio/samples/bookinfo/networking/bookinfo-gateway.yaml\nkubectl apply -f istio/samples/bookinfo/networking/bookinfo-gateway.yaml\nkubectl get gateway \n</code></pre> <p>To access the external access, it is important to understand that all the traffic goes through the istio-ingressgateway Load Balancer</p> <pre><code>kubectl get svc istio-ingressgateway -n istio-system\n</code></pre> <p>Output</p> <pre><code>NAME                   TYPE           CLUSTER-IP    EXTERNAL-IP     Ports\nistio-ingressgateway   LoadBalancer   10.32.1.153   35.192.101.76   15021:32714/TCP,80:30042/TCP,443:31889/TCP,...  62s\n</code></pre> <pre><code>export GATEWAY_URL=&lt;EXTERNAL-IP&gt;   \ncurl -I http://${GATEWAY_URL}/productpage\n</code></pre> <p>You can also open the link in the browser to see the same page</p> <p>Refresh the page several times.</p> <p>Notice how you see three different versions of reviews, since we have not yet used Istio to control the version routing.</p> <p>There are three different book review services being called in a round-robin style:</p> <ul> <li>no stars</li> <li>black stars</li> <li>red stars</li> </ul>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases_solution/#4-generate-traffic-to-the-application-using-seige","title":"4. Generate traffic to the application using seige","text":"<p>Step 1 Install the seige application and run load tests for 5 mins </p> <pre><code>sudo apt install siege\nsiege http://${GATEWAY_URL}/productpage\n</code></pre> <p>Stop the tests by using <code>Ctrl+C</code></p> <p>Step 2 Verify output in the Console</p> <ul> <li> <p>Anthos Service Mesh Page</p> </li> <li> <p>Observe the Services, Connection and Latency in the Table / Topology View</p> <p></p> </li> </ul>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases_solution/#5-exploring-istio-policies","title":"5. Exploring Istio Policies","text":""},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases_solution/#51-istio-security-mtls-configuration","title":"5.1 Istio Security &amp; mTLS configuration","text":"<p>We will be enforcing strict mTLS between the services in this step by making configuration changes </p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\nname: \"default\"\nnamespace: \"istio-system\"\nspec:\nmtls:\n    mode: STRICT\nEOF\n</code></pre> <p>Create 3 new namespaces for testing purposes and two of those will have anthos proxy enabled and one of them will not have any proxy config. </p> <pre><code>kubectl create ns foo\nkubectl create ns bar\nkubectl create ns legacy\n\nkubectl label namespace foo istio-injection- istio.io/rev=$REVISION --overwrite\nkubectl label namespace bar istio-injection- istio.io/rev=$REVISION --overwrite\n</code></pre> <p>Any traffic that is originating from the anthos proxy is authenticated adnd will be allowed in the cluster, whereas any deployment with no proxy will not be allowed to communicated with deployments that have the proxy. </p> <p>In this step, we will be deploying two sample pods per namespace </p> <pre><code>kubectl apply -f  istio/samples/httpbin/httpbin.yaml -n foo\nkubectl apply -f  istio/samples/sleep/sleep.yaml -n foo\n\nkubectl apply -f  istio/samples/httpbin/httpbin.yaml -n bar\nkubectl apply -f  istio/samples/sleep/sleep.yaml -n bar\n\nkubectl apply -f istio/samples/httpbin/httpbin.yaml -n legacy\nkubectl apply -f istio/samples/sleep/sleep.yaml -n legacy\n</code></pre> <p>Run the following authentication Tests </p> <pre><code>kubectl exec \"$(kubectl get pod -l app=sleep -n bar -o jsonpath={.items..metadata.name})\" -c sleep -n bar -- curl http://httpbin.foo:8000/ip -s -o /dev/null -w \"%{http_code}\\n\"\n\nfor from in \"foo\" \"bar\" \"legacy\"; do for to in \"foo\" \"bar\" \"legacy\"; do kubectl exec \"$(kubectl get pod -l app=sleep -n ${from} -o jsonpath={.items..metadata.name})\" -c sleep -n ${from} -- curl -s \"http://httpbin.${to}:8000/ip\" -s -o /dev/null -w \"sleep.${from} to httpbin.${to}: %{http_code}\\n\"; done; done\n</code></pre> <p>We can observe that any traffic that originated from namespaces that doesnt have the Istio proxy enabled will not have a successful response as indicated below </p> <p>Delete all the temp configration created for testing </p> <pre><code>kubectl delete peerauthentication -n istio-system default\nkubectl delete ns legacy\n</code></pre>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases_solution/#52-assignment","title":"5.2 Assignment","text":"<p>Task 1 For the assignment you will be primarily working on deploying Istio Authorization Policy.  Navigate to assignment submission repo</p> <pre><code>mkdir -p ~/$student_name-notepad/module8\ncd ~/$student_name-notepad/module8\n</code></pre> <p>For the assignment tasks </p> <ul> <li> <p>Use the two namespaces <code>foo</code> &amp; <code>bar</code> and the deployments created in the previous step for this task</p> </li> <li> <p>You will not modify the existing deployments in both namespace</p> </li> <li> <p>You wil be creating an authorization policy for both namespaces that will not allow     communication outside of its own namespace </p> </li> <li> <p>Create the policy with the filename <code>istio-policy-1.yaml</code></p> <p><code>kubectl apply -f istio-policy-1.yaml</code></p> </li> </ul> <p>You can run the following command to test output of the Istio authorization policy  </p> <pre><code>for from in \"foo\" \"bar\"; do for to in \"foo\" \"bar\"; do kubectl exec \"$(kubectl get pod -l app=sleep -n ${from} -o jsonpath={.items..metadata.name})\" -c sleep -n ${from} -- curl -s \"http://httpbin.${to}:8000/ip\" -s -o /dev/null -w \"sleep.${from} to httpbin.${to}: %{http_code}\\n\"; done; done\n</code></pre> <p>SOLUTION The following YAML template provides instruction to the istio pilot what namespaces are allowed to communicate with the namespace for which the policy is applied. Istio will convert the configuration into envoy proxy acls, which gets pushed to the istio sidecars running next to the deployment.  - Generally, the deployments or services under a namespace are maintained by a single team. In a mult-tenant GKE environment, where multiple teams are depoloying the services, as a measure of trust, you want to allow services within the namespace to talk to each other and by default prevent other services from other namespace trying to interact with the deployed service.  - If there is communication required with other namespaces, then you can allow them by explicity creating new authorization policies on top of the default policies created for all namespaces, which prevent unnecessary exposure of services.</p> <pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: foo-isolation\n  namespace: foo\nspec:\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        namespaces: [\"foo\"]\n---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: foo-isolation\n  namespace: bar\nspec:\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        namespaces: [\"bar\"]\n</code></pre> <p>Task 2 In this task, you will use IP allow access list to allow traiffc to ingress gateway from only certain IPs.</p> <ul> <li>You will use the <code>Istio Authorization Policy</code> configuration to allow traffic only from your cloud console IP.  You can get the cloud console IP using the following command <code>curl ipinfo.io/ip</code></li> </ul> <p>You will be applying the policy to the istio-system namespace so that it applies to all the deployments </p> <ul> <li> <p>Create the policy with the filename <code>istio-policy-2.yaml</code></p> </li> <li> <p><code>kubectl apply -f istio-policy-2.yaml</code></p> </li> </ul> <p>You can use the following command to test access to the page based on the IP allow list </p> <pre><code>curl -I http://${GATEWAY_URL}/productpage -s -o /dev/null -w \"%{http_code}\\n\"\n</code></pre> <p>SOLUTION The following YAML template provides instruction to the istio ingress gateway to allow traffic only from the selected IP address blocks listed in the configuration. All other IP blocks will be denied access to the all namespaces behind the Istio ingress gateway. If you are hosting some applicaitons that are meant to be allowed only from your corp/office network, then this kind of policy can be used to achieve that.</p> <pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: ingress-policy\n  namespace: istio-system\nspec:\n  selector:\n    matchLabels:\n      app: istio-ingressgateway\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        ipBlocks: [\"5.6.7.0/24\"]\n</code></pre> <p>Submission the assignement to the git repo</p> <pre><code>cd ~/$student_name-notepad\ngit add .\ngit commit -m \"TF manifests for Module 8 Assignment\"\n</code></pre>"},{"location":"020_Module_8_Assignment_ASM%28Istio%29_use_cases_solution/#6-cleanup","title":"6 Cleanup","text":"<p>Delete GKE cluster:</p> <pre><code>gcloud container clusters delete central-gke-clusuter --region $ZONE1\n</code></pre>"},{"location":"020_ass2/","title":"1 GKE Deployment","text":"<p>Objective:</p> <ul> <li>Review process of creation Custom VPC</li> <li>Review process of VPC subnet planning</li> <li>Review creation of Cloud Nat</li> <li>Review process of creation Private GKE</li> <li>Automating Process using Terraform</li> </ul>"},{"location":"020_ass2/#prepare-lab-environment","title":"Prepare Lab Environment","text":""},{"location":"Istio%20Demo/","title":"Istio Demo","text":"<p>Lab 4 Learning Istio</p>"},{"location":"Istio%20Demo/#7-install-istio-on-gke","title":"7 Install Istio on GKE","text":"<p>Currently Istio community developing following options of Istio deployment on Kubernetes Clusters:</p> <ul> <li>Install with Istioctl installation via <code>istioctl</code> command line tool used to showcase Istio functionality. Using the CLI, we generate a YAML file with all Istio resources and then deploy it to the Kubernetes cluster</li> <li>Istio Operator Install Takes installation of Istio to the next level as it's managing not only Istio installation but overall lifecicle of the Istio deployment including Upgrades and configurations.</li> <li>Install with Helm (alpha) Allows to farther simplify Istio deployment and it's customization.  </li> </ul>"},{"location":"Istio%20Demo/#10-prerequisite","title":"1.0 Prerequisite","text":"<p>Scaleup cluster back to 3 nodes and Update VPC firewall rules to enable <code>auto-injection</code> and the <code>istioctl version</code> and <code>istioctl ps</code> commands.</p> <p>References:</p> <ol> <li>Opening ports on a private cluster</li> <li>Istio deployment on GKE Private Clusters</li> </ol> <p>Step 1: Locate Terraform Configuration directory.</p> <pre><code>cd ~/$MY_REPO/notepad-infrastructure\n</code></pre> <pre><code>edit terraform.tfvars\n</code></pre> <p>And set <code>gke_pool_node_count</code>   = \"0\"</p> <p>Step 2:  Create new firewall rule for source range (master-ipv4-cidr) of the cluster, that will open required ports for Istio installation:</p> <pre><code>cat &lt;&lt;EOF&gt; istio_firewall.tf\nresource \"google_compute_firewall\" \"istio_specific\" {\n  name =   format(\"allow-istio-in-privategke-%s-%s-%s\", var.org, var.product, var.environment)\n  network = google_compute_network.vpc_network.self_link\n  source_ranges = [\"172.16.0.0/28\"]\n  allow {\n    protocol = \"tcp\"\n    ports = [\"10250\", \"443\", \"15017\", \"15014\", \"8080\"]\n  }\n}\nEOF\n</code></pre> <p>Step 3: Review TF Plan:</p> <pre><code>terraform plan -var-file terraform.tfvars\n</code></pre> <p>Step 4: Scale up GKE Cluster Node Pool and update firewall rules for required range:</p> <pre><code>terraform apply -var-file terraform.tfvars\n</code></pre> <p>Result</p> <p>GKE Clusters has been scalled to 3 nodes and has firewall rules opened</p> <p>Step 5: Verify firewall rule:</p> <pre><code>gcloud compute firewall-rules list --filter=\"name~allow-istio-in-privategke*\"\n</code></pre> <p>Output:</p> <pre><code>NAME                                            NETWORK                   DIRECTION  PRIORITY  ALLOW                                           DENY  DISABLED\nallow-istio-in-privategke-$student-notepad-dev  vpc-$student-notepad-dev  INGRESS    1000      tcp:10250,tcp:443,tcp:15017,tcp:15014,tcp:8080        False\n</code></pre>"},{"location":"Istio%20Demo/#71-deploy-istio-using-custom-resources","title":"7.1 Deploy Istio using Custom Resources","text":"<p>This installation guide uses the istioctl command line tool to provide rich customization of the Istio control plane and of the sidecars for the Istio data plane.</p> <p>Download and extract the latest release:</p> <pre><code>curl -L https://istio.io/downloadIstio | sh -\ncd istio-1.11.0\nexport PATH=$PWD/bin:$PATH\n</code></pre> <p>Success</p> <p>The above command will fetch Istio packages and untar them in the same folder.</p> <pre><code>tree -L 1\n</code></pre> <pre><code>.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 manifest.yaml\n\u251c\u2500\u2500 manifests\n\u251c\u2500\u2500 samples\n\u2514\u2500\u2500 tools\n</code></pre> <p>Note</p> <p>Istio installation directory contains:</p> <ul> <li><code>bin</code> directory contains <code>istioctl</code> client binary</li> <li><code>manifests</code> Installation Profiles, configurations and Helm Charts</li> <li><code>samples</code> directory contains sample applications deployment</li> <li><code>tools</code>  directory contains auto-completion tooling and etc.</li> </ul> <p>Step 2 Deploy Istio Custom Resource Definitions (CRDs)</p> <p>Istio extends Kubernetes using Custom Resource Definitions (CRDs).  CRDs allow registration of new/non-default Kubernetes resources. When Istio CRDs are deployed, Istio\u2019s objects are registered as Kubernetes objects, providing a highly integrated experience with  Kubernetes as a deployment platform and thus allowing Kubernetes to store configuration of Istio features such as routing, security and telemetry and etc.</p> <p>We going to install using using <code>demo</code> configuration profile. The <code>demo</code> configuration profile allows to experiment with most of Istio features with modest resource requirements.  Since it enables high levels of tracing and access logging, it is not suitable for production use cases.</p> <pre><code>istioctl install --set profile=demo -y\n</code></pre> <p>Output:</p> <pre><code>\u2714 Istio core installed\n\u2714 Istiod installed\n\u2714 Egress gateways installed\n\u2714 Ingress gateways installed\n\u2714 Installation complete\nThank you for installing Istio 1.11.  Please take a few minutes to tell us about your install/upgrade experience!  \n</code></pre> <p>Note</p> <p>Wait a few seconds for the CRDs to be committed in the Kubernetes API-server.</p> <p>Step 3 Verify Istio CRDs successfully applied to Kubernetes Cluster.</p> <pre><code>kubectl get crds | grep istio\n</code></pre> <pre><code>authorizationpolicies.security.istio.io          2021-08-16T19:38:16Z\ndestinationrules.networking.istio.io             2021-08-16T19:38:16Z\nenvoyfilters.networking.istio.io                 2021-08-16T19:38:16Z\ngateways.networking.istio.io                     2021-08-16T19:38:17Z\nistiooperators.install.istio.io                  2021-08-16T19:38:17Z\npeerauthentications.security.istio.io            2021-08-16T19:38:17Z\nrequestauthentications.security.istio.io         2021-08-16T19:38:17Z\nserviceentries.networking.istio.io               2021-08-16T19:38:17Z\nsidecars.networking.istio.io                     2021-08-16T19:38:17Z\ntelemetries.telemetry.istio.io                   2021-08-16T19:38:18Z\nvirtualservices.networking.istio.io              2021-08-16T19:38:18Z\nworkloadentries.networking.istio.io              2021-08-16T19:38:18Z\nworkloadgroups.networking.istio.io               2021-08-16T19:38:19Z\n</code></pre> <p>Info</p> <p>Above CRDs will be avaialable as a new Kubernetes Resources and stored in Kubernetes ETCD database. The Kubernetes API will represent these new resources as endpoints that  can be used as other native Kubernetes object (such as Pod, Services) levereging  kubectl, RBAC and other features and  admission controllers of Kubernetes.</p> <p>Step 4 Count total number of Installed CRDs:</p> <pre><code>kubectl get crds | grep istio | wc -l\n</code></pre> <p>Note</p> <p>CRDs count will vary based on Istio version and profile deployed.</p> <p>Step 5 Verify that Istio control plane has been installed successfully.</p> <pre><code>kubectl get pods -n istio-system\n</code></pre> <p>Output:</p> <pre><code>istio-egressgateway-9dc6cbc49-5wkqt     1/1     Running   0          2m54s\nistio-ingressgateway-7975cdb749-xjc5g   1/1     Running   0          2m54s\nistiod-77b4d7b55d-j6kb5                 1/1     Running   0          3m9s\n</code></pre> <p>Info</p> <p>Istio control-plane include following components:</p> <ul> <li><code>istiod</code> - contains components such as <code>Citadel</code> and <code>Pilot</code></li> <li><code>istio-ingressgateway</code> Istio Ingress Gateway</li> <li><code>istio-ingressgateway</code> Istio Egress Gateway</li> </ul> <p>Step 6 Verify installation with <code>istioctl</code> CLI:</p> <pre><code>istioctl version\n</code></pre> <pre><code>istioctl verify-install\n</code></pre> <p>Output:</p> <pre><code>Checked 13 custom resource definitions\nChecked 3 Istio Deployments\n\u2714 Istio is installed and verified successfully\n</code></pre> <p>Step 5  Deploy  <code>Bookinfo</code> sample application:</p> <pre><code>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME                              READY   STATUS    RESTARTS   AGE\ndetails-v1-79f774bdb9-cjj5b       1/1     Running   0          12s\nproductpage-v1-6b746f74dc-sxdxz   1/1     Running   0          10s\nratings-v1-b6994bb9-jd985         1/1     Running   0          11s\nreviews-v1-545db77b95-5kjr4       1/1     Running   0          11s\nreviews-v2-7bf8c9648f-5v2s9       1/1     Running   0          11s\nreviews-v3-84779c7bbc-5khlt       1/1     Running   0          11s\n</code></pre> <p>Note</p> <p>This is a typical Kubernetes deployment, and has nothing specific to Istio. Each Pod has 1 container.</p> <p>Step 6 Remove <code>Bookinfo</code> sample application:</p> <pre><code>kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre> <p>Step X: Manual Sidecar injection:</p> <pre><code>cat &lt;&lt;EOF &gt; go-web-app-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webk8sbirthday-v2\n  labels:\n    run: webk8sbirthday-v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: webk8sbirthday-v2\n  template:\n    metadata:\n      labels:\n        run: webk8sbirthday-v2\n    spec:\n      containers:\n      - name: k8s-demo\n        image: archyufa/webk8sbirthday:v2\n        ports:\n        - name: port\n          containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: webk8sbirthday-v2\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 8080\n    protocol: TCP\n    name: http\n  selector:\n    run: webk8sbirthday-v2\nEOF\n</code></pre> <pre><code>kubectl apply -f go-web-app-deployment.yaml\n</code></pre> <pre><code>kubectl port-forward --namespace default svc/webk8sbirthday-v2 8080:80\n</code></pre> <pre><code>kubectl delete -f go-web-app-deployment.yaml\n</code></pre> <p>Step 7 Enable manual Envoy sidecar injection.</p> <p>Using <code>istioctl kube-inject</code> manually inject Envoy containers in your application pods before deploying them.</p> <pre><code>istioctl kube-inject -f go-web-app-deployment.yaml | kubectl apply -f -\n</code></pre> <p>Alternatively, injection can be done using local copies of the configuration.</p> <pre><code>kubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath='{.data.config}' &gt; inject-config.yaml\nkubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath='{.data.values}' &gt; inject-values.yaml\nkubectl -n istio-system get configmap istio -o=jsonpath='{.data.mesh}' &gt; mesh-config.yaml\n</code></pre> <p>Run kube-inject over the input file and deploy.</p> <p>Info</p> <p>Optionally manual injection can also load configuration from local custom files. See example below:</p> <p><code>$ istioctl kube-inject --injectConfigFile inject-config.yaml --meshConfigFile mesh-config.yaml --filename go-web-app-deployment.yaml --valuesFile inject-values.yaml --output go-web-app-deployment-injected.yaml</code></p> <pre><code>grep \"image:\" -A1 go-web-app-deployment-injected.yaml\n</code></pre> <p>Step 7 Enable automatic Envoy sidecar injection.</p> <p>Currently there are 2 method to inject <code>Envoy</code> sidecar inside Istio Mesh:</p> <ul> <li>Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment.</li> <li>Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update.</li> </ul> <p>Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later:</p> <pre><code>kubectl label namespace default istio-injection=enabled\n</code></pre> <p>Step 8 Deploy  <code>Bookinfo</code> sample application on Istio with Auto Sidecar Injection</p> <pre><code>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre> <p>Step 8 Verify deployed application:</p> <p>The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it.</p> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME                              READY   STATUS    RESTARTS   AGE\ndetails-v1-79f774bdb9-zqjbg       2/2     Running   0          13m\nproductpage-v1-6b746f74dc-dj959   2/2     Running   0          12m\nratings-v1-b6994bb9-29n4w         2/2     Running   0          12m\nreviews-v1-545db77b95-p9pgf       2/2     Running   0          12m\nreviews-v2-7bf8c9648f-4ltkg       2/2     Running   0          12m\nreviews-v3-84779c7bbc-vhsmw       2/2     Running   0          12m\n</code></pre> <p>Note</p> <p>Each Pod  has now 2 containers. One application container and another is <code>istio-proxy</code> sidecar container.</p> <p>Step 9 Access UI <code>productpage</code> via console:</p> <pre><code>kubectl get services\n</code></pre> <p>In Gcloud console execute <code>kubectl port-forward</code>:</p> <pre><code>kubectl port-forward --namespace default svc/productpage 8080:9080\n</code></pre> <p>Click <code>Web-Preview</code> button in Gcloud, and select preview on port <code>8080</code> Click <code>Normal user</code> URL.</p> <p>Success</p> <p>We can access <code>Bookinfo</code> application configured with Istio and Envoy sidecar using private IP via tunnel</p> <p>Step 10 Remove <code>Bookinfo</code> sample application:</p> <pre><code>kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre> <p>Step 11 Uninstall Istio</p> <pre><code>istioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f -\nkubectl delete namespace istio-system\nkubectl label namespace default istio-injection-\n</code></pre>"},{"location":"Istio%20Demo/#72-deploy-istio-using-istio-operator","title":"7.2 Deploy Istio using Istio Operator","text":"<p>Step 1: Deploy the Istio operator:</p> <pre><code>istioctl operator init\n</code></pre> <p>Note</p> <p>This command runs the operator by creating the following resources in the <code>istio-operator</code> namespace:</p> <ul> <li>The operator custom resource definition</li> <li>The operator controller deployment</li> <li>A service to access operator metrics</li> <li>Necessary Istio operator RBAC rules</li> </ul> <p>Step 2: To install the Istio demo configuration profile using the operator, run the following command:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nmetadata:\n  namespace: istio-system\n  name: example-istiocontrolplane\nspec:\n  profile: demo\nEOF\n</code></pre> <p>Note</p> <p>The Istio operator controller begins the process of installing Istio within 90 seconds of the creation of the IstioOperator resource. The Istio installation completes within 120 seconds.</p> <p>Step 3 Verify Operator resource status on Kubernetes Cluster:</p> <pre><code>kubectl get istiooperators  -n istio-system \n</code></pre> <p>Output:</p> <pre><code>NAMESPACE      NAME                        REVISION   STATUS    AGE\nistio-system   example-istiocontrolplane              HEALTHY   25m\n</code></pre> <pre><code>kubectl describe istiooperators example-istiocontrolplane -n istio-system\n</code></pre> <pre><code>Spec:\n  Profile:  demo\nStatus:\n  Component Status:\n    Base:\n      Status:  HEALTHY\n    Egress Gateways:\n      Status:  HEALTHY\n    Ingress Gateways:\n      Status:  HEALTHY\n    Pilot:\n      Status:  HEALTHY\n  Status:      HEALTHY\nEvents:        &lt;none&gt;\n</code></pre> <pre><code>kubectl get pods -n istio-system\n</code></pre> <p>Output:</p> <pre><code>istio-egressgateway-9dc6cbc49-5wkqt     1/1     Running   0          2m54s\nistio-ingressgateway-7975cdb749-xjc5g   1/1     Running   0          2m54s\nistiod-77b4d7b55d-j6kb5                 1/1     Running   0          3m9s\n</code></pre> <p>Info</p> <p>Istio control-plane include following components:</p> <ul> <li><code>istiod</code> - contains components such as <code>Citadel</code> and <code>Pilot</code></li> <li><code>istio-ingressgateway</code> Istio Ingress Gateway</li> <li><code>istio-ingressgateway</code> Istio Egress Gateway</li> </ul> <p>Verify installation with <code>istioctl</code> CLI:</p> <pre><code>istioctl verify-install\n</code></pre> <p>Step 5 Enable automatic Envoy sidecar injection.</p> <p>Currently there are 2 method to inject <code>Envoy</code> sidecar inside Istio Mesh:</p> <ul> <li>Manual sidecar injection - modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding/Updating/Removing the sidecar requires modifying the entire deployment.</li> <li>Automatic sidecar injection via a Mutating Admission Webhook - The Deployment resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update.</li> </ul> <p>Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later:</p> <pre><code>kubectl label namespace default istio-injection=enabled\n</code></pre> <p>Step 8 Deploy  <code>Bookinfo</code> sample application on Istio with Auto Sidecar Injection</p> <pre><code>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre> <p>Step 8 Verify deployed application:</p> <p>The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it.</p> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME                              READY   STATUS    RESTARTS   AGE\ndetails-v1-79f774bdb9-zqjbg       2/2     Running   0          13m\nproductpage-v1-6b746f74dc-dj959   2/2     Running   0          12m\nratings-v1-b6994bb9-29n4w         2/2     Running   0          12m\nreviews-v1-545db77b95-p9pgf       2/2     Running   0          12m\nreviews-v2-7bf8c9648f-4ltkg       2/2     Running   0          12m\nreviews-v3-84779c7bbc-vhsmw       2/2     Running   0          12m\n</code></pre> <p>Summary</p> <p>We've leaned to deploy Istio Control plane using <code>istioctl</code> cli and using Istio Operator. We also deployed regular k8s application on the namespace marked with auto injection!</p> <p>Step 9 Cleanup and Uninstall Istio Operator</p> <pre><code>kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre> <pre><code>kubectl delete istiooperators.install.istio.io -n istio-system example-istiocontrolplane\n</code></pre> <p>Note</p> <p>Wait until Istio is uninstalled - this may take some time.</p> <p>Then you can remove the Istio operator for the old revision by running the following command:</p> <pre><code>istioctl operator remove\n</code></pre>"},{"location":"Module_5_Lab_Docker_basics/","title":"Module 5 Lab Docker basics","text":"<p>Objective:</p> <ul> <li>Practice to run Docker containers</li> <li>Learn to build docker images using Dockerfiles</li> <li>Learn Docker Networking</li> </ul>"},{"location":"Module_5_Lab_Docker_basics/#1-setup-environment-and-install-docker","title":"1 Setup Environment and Install Docker","text":""},{"location":"Module_5_Lab_Docker_basics/#11-create-an-instance-with-gcloud","title":"1.1 Create an instance with gcloud","text":"<p>Rather than using the Google Cloud Console to create a virtual machine instance, you can use the command line tool <code>gcloud</code>, which is pre-installed in Google Cloud Shell. Cloud Shell is a Debian-based virtual machine loaded with all the development tools you\u2019ll need (gcloud, git, and others) and offers a persistent 5GB home directory.</p> <p>Note</p> <p>If you want to try this on your own machine in the future, read the gcloud command line tool guide.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can create a new virtual machine instance from the command line by using <code>gcloud</code> (feel free to use another zone closer to you):</p> <p>Step 1 Create a new virtual machine instance on GCE</p> <pre><code>gcloud config set project &lt;set_you_project_id&gt;\n</code></pre> <pre><code>gcloud compute instances create docker-nginx --zone us-central1-c\n</code></pre> <p>Output:</p> <pre><code>Created [...docker-nginx].\nNAME     ZONE           MACHINE_TYPE  PREEMPTIBLE INTERNAL_IP EXTERNAL_IP    STATUS\ndocker-nginx  us-central1-c  n1-standard-1             10.240.X.X  X.X.X.X        RUNNING\n</code></pre> <p>The instance created has these default values: - The latest Debian 10 (buster) image. - The n1-standard-1 machine type. - A root persistent disk with the same name as the instance; the disk is automatically attached to the instance.</p> <p>Note</p> <p>You can set the default region and zones that <code>gcloud</code> uses if you are always working within one region/zone and you don\u2019t want to append the --<code>zone</code> flag every time. Do this by running these commands:</p> <p><code>gcloud config set compute/zone ... gcloud config set compute/region ...</code></p> <p>Step 2 SSH into your instance using <code>gcloud</code> as well. </p> <pre><code>gcloud compute ssh docker-nginx --zone us-central1-c\n</code></pre> <p>Output:</p> <pre><code>WARNING: The public SSH key file for gcloud does not exist.\nWARNING: The private SSH key file for gcloud does not exist.\nWARNING: You do not have an SSH key for gcloud.\nWARNING: [/usr/bin/ssh-keygen] will be executed to generate a key.\nThis tool needs to create the directory \n[/home/gcpstaging306_student/.ssh] before being able to generate SSH \nKeys.\n</code></pre> <p>Now you\u2019ll type <code>Y</code> to continue.</p> <pre><code>Do you want to continue? (Y/n)\n</code></pre> <p>Enter through the passphrase section to leave the passphrase empty. </p> <pre><code>Generating public/private rsa key pair.\nEnter passphrase (empty for no passphrase)\n</code></pre>"},{"location":"Module_5_Lab_Docker_basics/#12-install-docker-engine","title":"1.2 Install Docker Engine","text":"<p>We need to first set up the Docker repository. To set up the repository:</p> <p>Step 1 Update the <code>apt</code> package index:</p> <pre><code>sudo apt-get update\n</code></pre> <p>Step 2 Then install packages to allow \u201capt\u201d to use a repository over HTTPS:</p> <pre><code>sudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n</code></pre> <p>Step 3  Add Docker\u2019s official GPG key:</p> <pre><code>curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n</code></pre> <p>Step 4 Set up the stable repository</p> <pre><code>echo \\\n  \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre> <p>Step 5 Install Docker Engine:</p> <pre><code>sudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n</code></pre> <p>Step 6 Verify Docker Version has been deployed:</p> <pre><code>docker version\n</code></pre> <p>Verify what is the latest Docker release</p> <p>Step 6 Confirm that your installation was successful by running Hello World! Container.</p> <pre><code>sudo docker run hello-world\n</code></pre> <p>Step 7 If it is successful, you will see the following output:</p> <pre><code>Unable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\nb8dfde127a29: Pull complete\nDigest: sha256:f2266cbfc127c960fd30e76b7c792dc23b588c0db76233517e1891a4e357d519\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n</code></pre> <p>Step 8  Exit from <code>VM</code></p> <pre><code>exit\n</code></pre> <p>Step 9  Delete <code>VM Instance</code> to avoid extra cost:</p> <pre><code>gcloud compute instances delete docker-nginx --zone us-central1-c\n</code></pre> <p>Now you\u2019ll type <code>Y</code> to continue.</p> <pre><code>Do you want to continue? (Y/n)\n</code></pre> <p>Summary</p> <p>Now you know how to deploy Docker on Linux VM. </p> <p>Task</p> <p>Find out how to deploy Docker on Mac or Windows?</p>"},{"location":"Module_5_Lab_Docker_basics/#2-working-with-docker-cli","title":"2 Working with Docker CLI","text":""},{"location":"Module_5_Lab_Docker_basics/#21-show-running-containers","title":"2.1 Show running containers","text":"<p>Going forward we going to run Docker commands directly from Google Cloud Console. This is possible because Docker is installed on Cloud Console VM for you convenience.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p> <p>Step 1 Create a docker container <code>hello-world</code>:</p> <pre><code>docker run hello-world\n</code></pre> <p>Step 2 Run docker ps to show running containers:</p> <pre><code>docker ps\n</code></pre> <p>Result</p> <p>The output shows that there are no running containers at the moment.</p> <p>Step 2 Use the command docker <code>ps -a</code> to list all containers including the ones has been stopped:</p> <pre><code>docker ps -a\n</code></pre> <p>Output:</p> <pre><code>CONTAINER ID IMAGE       COMMAND  CREATED        STATUS             PORTS  NAMES\n6e6db2a24a8e hello-world \"/hello\" 15 minutes ago Exited (0) 15 min  dreamy_nobel\n</code></pre> <p>Review the collumns <code>CONTAINER ID</code>, <code>STATUS</code>, <code>COMMAND</code>, <code>PORTS</code>, <code>NAMES</code>.</p> <p>In the previous section we started one container and the command docker <code>ps -a</code> shows it as <code>Exited</code>.</p> <p>Note</p> <p>You can name your own containers with --name when you use docker run. If you do not provide a name, Docker will generate a random one like the one you have.</p> <p>Question</p> <p>Why Docker names are random? How docker containers named?</p> <p>Step 3 Let\u2019s run the command <code>docker images</code> to show all the images on your local system:</p> <pre><code>docker images\n</code></pre> <p>As you see, there is only one image that was downloaded from the Docker Hub.</p>"},{"location":"Module_5_Lab_Docker_basics/#23-specify-a-container-main-process","title":"2.3 Specify a container main process","text":"<p>Step 1 Let\u2019s run our own \"hello world\" container. For that we will use the official Ubuntu image:</p> <pre><code>docker run ubuntu /bin/echo 'Hello world'\n</code></pre> <p>Output:</p> <pre><code>Unable to find image 'ubuntu:latest' locally\nlatest: Pulling from library/ubuntu\n...\nStatus: Downloaded newer image for ubuntu:latest\nHello world\n</code></pre> <p>As you see, Docker downloaded the image ubuntu because it was not on the local machine.</p> <p>Step 2 Let\u2019s run the command <code>docker images</code> again:</p> <pre><code>docker images\n</code></pre> <p>Output:</p> <pre><code>REPOSITORY          TAG                 IMAGE ID            CREATED        SIZE\nubuntu              latest              42118e3df429        11 days ago  124.8 MB\nhello-world         latest              c54a2cc56cbb        4 weeks ago  1.848 kB\n</code></pre> <p>Step 3 If you run the same \"hello world\" container again, Docker will use a local copy of the image:</p> <pre><code>docker run ubuntu /bin/echo 'Hello world'\n</code></pre> <p>Output:</p> <pre><code>Hello world\n</code></pre> <p>Question</p> <p>Compare Ubuntu Docker image with ISO image or with Cloud VM image.</p> <ul> <li>Why the size is so different ?</li> </ul> <p>Summary</p> <p>Pulling docker images from Docker Hub takes sometime. This time depends on:</p> <ul> <li>How large is the image?</li> <li>How fast is the network to Internet ?</li> </ul> <p>However, it is still much faster than booting traditional OS with Ubuntu on VM.</p> <p>If image already pulled on local host it takes fraction of a second to start a container.</p> <p>Running application in docker containers considered as a best practice for running CI/CD pipelines as it considerably faster than using VMs and reduce time for deploying a test environments.</p>"},{"location":"Module_5_Lab_Docker_basics/#23-specify-an-image-version","title":"2.3 Specify an image version","text":"<p>Step 1 As you see, Docker has downloaded the ubuntu:latest image. You can see Ubuntu version by running the following command:</p> <pre><code>docker run ubuntu /bin/cat /etc/issue.net\n</code></pre> <p>Output:</p> <pre><code>Ubuntu 22.04 LTS\n</code></pre> <p>Let\u2019s say you need a previous Ubuntu LTS release. In this case, you can specify the version you need:</p> <pre><code>docker run ubuntu:14.04 /bin/cat /etc/issue.net\n</code></pre> <p>Output:</p> <pre><code>Unable to find image 'ubuntu:14.04' locally\n14.04: Pulling from library/ubuntu\n...\nStatus: Downloaded newer image for ubuntu:14.04\nUbuntu 14.04.4 LTS\n</code></pre> <p>Step 2 The <code>docker images</code> command should show that we have 3 Ubuntu images downloaded locally:</p> <pre><code>docker images\n</code></pre> <p>Output:</p> <pre><code>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nubuntu              latest              42118e3df429        11 days ago         124.8 MB\nubuntu              14.04               0ccb13bf1954        11 days ago         188 MB\nhello-world         latest              c54a2cc56cbb        4 weeks ago         1.848 kB\n</code></pre> <p>Tip</p> <p>Running CI/CD pipeline with Docker using <code>latest</code> tag considered as a Bad Practice.</p> <p>Instead consider using:</p> <ul> <li>Versioning</li> <li><code>SHA</code> tagging.</li> </ul>"},{"location":"Module_5_Lab_Docker_basics/#24-run-an-interactive-container","title":"2.4 Run an interactive container","text":"<p>Step 1 Let\u2019s use the <code>ubuntu</code> image to run an interactive bash session and inspect what is running inside our docker image.</p> <p>To achive that we going to use  <code>-i</code> and <code>-t</code> flags.</p> <p>The  <code>-i</code> is shorthand for <code>--interactive</code>, which instructs Docker to keep <code>stdin</code> open so that we can send commands to the sprocess.</p> <p>The <code>-t</code> flag is short for <code>--tty</code> and allocates a <code>pseudo-TTY</code> or terminal inside of the session.</p> <pre><code>docker run -it ubuntu /bin/bash\nroot@17d8bdeda98e:/#\n</code></pre> <p>Result</p> <p>We get a  bash shell prompt inside of the container.</p> <p>Note</p> <p>Bash prompt is not availabe for all docker images.</p> <p>Step 2 Let's print the system information of the latest <code>Ubuntu</code> image:</p> <pre><code>root@17d8bdeda98e:/#  uname -a\nLinux 17d8bdeda98e 3.19.0-31-generic ...\n</code></pre> <p>Step 3 Let's verify what Ubuntu version is run by <code>latest</code> image of ubuntu:</p> <pre><code>root@17d8bdeda98e:/#  lsb_release -a\nbash: lsb_release: command not found\n</code></pre> <p>Failure</p> <p>Why the standard Ubuntu command that checks version of OS is not working as expeced ?</p> <p>Step 4 Let's verify Ubuntu version using alternative way by checking <code>/etc/lsb-release</code> file.</p> <pre><code>root@8cbcbd0fe8d2:/# cat /etc/lsb-release\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=16.04\nDISTRIB_CODENAME=xenial\nDISTRIB_DESCRIPTION=\"Ubuntu 16.04.3 LTS\"\n</code></pre> <p>Step 5 Let's compare the number of executable binaries availabe inside of the docker image versus Cloud VM that we running our class environment. First, run <code>ls</code> command on  <code>/bin</code> and <code>/usr/bin</code> directories inside of the running ubuntu container as well as <code>dpkg --list</code> command that shows total number of installed packages:</p> <pre><code>root@8cbcbd0fe8d2:/# ls /bin | wc -l\n294\nroot@8cbcbd0fe8d2:/# ls /usr/bin | wc -l\nxx\nroot@eb11cd0b4106:/# dpkg --list | wc -l\nxx\n</code></pre> <p>Step 6 Use the <code>exit</code> command or press <code>Ctrl-D</code> to exit the interactive bash session back to Cloud VM.</p> <pre><code>root@eb11cd0b4106:/# exit\n</code></pre> <p>Step 7 Now run <code>ls</code> command on <code>/bin</code> and <code>/usr/bin</code> directories on Cloud VM that we using as our class environment:</p> <pre><code>cca-user@userx-docker-vm:~$ ls /bin | wc -l\n1173\ncca-user@userx-docker-vm:~$ ls /usr/bin | wc -l\n660\ncca-user@userx-docker-vm:~$ dpkg --list | wc -l\n463\n</code></pre> <p>Result</p> <p>Official Docker container has much less binaries and packages installed vs Ubuntu Cloud Image.</p> <p>Summary</p> <p>Some of the use cases running docker containers in <code>interactive mode</code> are:</p> <ul> <li>Troubleshooting containerized applications</li> <li>Deploying and running containerized application on the existing production systems without affecting it.</li> </ul> <p>We've also learned that an official Docker \"minimal\" ubuntu image, does not include <code>lsb_release</code> command, as well as many other commands and packages that can be found in Official Ubuntu ISO image. The docker images are ment to contain only required core system commands and functions to make Images as light as possible. That say you can still install required packages using <code>apt-get install</code>, however this may increase size of docker image considerably.</p> <p>Hint</p> <p>While Docker <code>Ubuntu</code> image we used so far or Docker <code>Centos</code> image are very familiar to users and can be good starting point for learning docker containers. Using them in production or development considered as a Bad Practice.</p> <p>This is due those images still considered as <code>heavy</code> and potentially contain a lot more valnurabilities compare to specialized images.</p> <p>To reduce image pull time from docker hub and follow the best secuirity practices consider using specialized images that works well with you underlining code (Node image for NodeJS applications and etc.). Examples of specialized images are:</p> <ul> <li>Alpine Linux</li> <li>Node</li> <li>Atomic</li> </ul> <p>In fact, not so long ago all the official Docker Images in Docker-Hub has been moved to use Alpine Image.</p> <p>Step 8 Finally let\u2019s check that when the shell process has finished, the container stops:</p> <pre><code>docker ps\n</code></pre>"},{"location":"Module_5_Lab_Docker_basics/#25-run-a-container-in-a-background","title":"2.5 Run a container in a background","text":"<p>Now we know how to connect to running container and execute commands in it. However in most cases you just want run a container in a background so it can do a specific action.</p> <p>Step 1 Run a container in a background using the <code>-d</code> command line argument:</p> <pre><code>docker run -d ubuntu /bin/sh -c \"while true; do date; echo hello world; sleep 1; done\"\n</code></pre> <p>Result</p> <p>Command should return the container ID.</p> <p>Step 2 Let\u2019s use the <code>docker ps</code> command to see running containers:</p> <pre><code>docker ps\n</code></pre> <pre><code> CONTAINER ID IMAGE  COMMAND                  CREATED        STATUS  PORTS NAMES\nac231579e57f ubuntu \"/bin/sh -c 'while tr\"   1 minute ago   Up 11 minute  evil_golick\n</code></pre> <p>Note</p> <p>Container id is going to  be different in your case</p> <p>Hint</p> <p>Instead of using full <code>container-id</code> when building commands, it is possible simply type first few characters of container-id, to make things nice and easy.</p> <p>Step 3 Let\u2019s use <code>container-id</code> to show the container standard output:</p> <pre><code>docker logs &lt;container-id&gt;\n</code></pre> <pre><code>Thu Jan 26 00:23:45 UTC 2017\nhello world\nThu Jan 26 00:23:46 UTC 2017\nhello world\nThu Jan 26 00:23:47 UTC 2017\nhello world\n...\n</code></pre> <p>As you can see, in the <code>docker ps</code> command output, the auto generated container name is <code>evil_golick</code> (your container can have a different name).</p> <p>Step 4 Now, instead of using docker <code>contaier-id</code> use container name to show the container standard output:</p> <pre><code>docker logs &lt;name&gt;\n</code></pre> <pre><code>Thu Jan 26 00:23:51 UTC 2017\nhello world\nThu Jan 26 00:23:52 UTC 2017\nhello world\nThu Jan 26 00:23:53 UTC 2017\nhello world\n...\n</code></pre> <p>Step 5 Finally, let\u2019s stop our container:</p> <pre><code>docker stop &lt;name&gt;\n</code></pre> <p>Step 6 Check, that there are no running containers:</p> <pre><code>docker ps\n</code></pre> <p>Summary</p> <p><code>docker logs</code> is a very usefull command to troubleshoot containers, and going to be used very often both for Docker and Kubernertes troubleshooting.</p>"},{"location":"Module_5_Lab_Docker_basics/#26-accessing-containers-from-the-internet","title":"2.6 Accessing Containers from the Internet","text":"<p>Step 1 Let\u2019s run a simple web application. We will use the existing image training/webapp, which contains a Python Flask application:</p> <pre><code>docker run -d -P training/webapp python app.py\n</code></pre> <pre><code>...\nStatus: Downloaded newer image for training/webapp:latest\n6e88f42d3d853762edcbfe1fe73fdc5c48865275bc6df759b83b0939d5bd2456\n</code></pre> <p>In the command above we specified the main process (python app.py), the <code>-d</code> command line argument, which tells Docker to run the container in the background. The <code>-P</code> command line argument tells Docker to map any required network ports inside our container to our host. This allows us to access the web application in the container.</p> <p>Step 2 Use the <code>docker ps</code> command to list running containers:</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID IMAGE           COMMAND         CREATED       STATUS       PORTS                   NAMES\n6e88f42d3d85 training/webapp \"python app.py\" 3 minutes ago Up 3 minutes 0.0.0.0:32768-&gt;5000/tcp determined_torvalds\n</code></pre> <p>The PORTS column contains the mapped ports. In our case, Docker has exposed port 5000 (the default Python Flask port) on port 32768 (can be different in your case).</p> <p>Step 3 The <code>docker port</code> command shows the exposed port. We will use the container name (determined_torvalds in the example above, it can be different in your case):</p> <pre><code>docker port &lt;name&gt; 5000\n0.0.0.0:32768\n</code></pre> <p>Step 4 Let\u2019s check that we can access the web application exposed port:</p> <pre><code>curl http://localhost:&lt;port&gt;/\n</code></pre> <p>Result</p> <p><code>Hello world!</code></p> <p>Step 5 Let\u2019s stop our web application for now:</p> <pre><code>docker stop &lt;name&gt;\n</code></pre> <p>Step 6 We want to manually specify the local port to expose (-p argument). Let\u2019s use the standard HTTP port 8080. We also want to specify the container name (--name argument):</p> <pre><code>docker run -d -p 8080:5000 --name webapp training/webapp python app.py\n</code></pre> <p>Step 7 Let\u2019s check that the port 8080 is exposed:</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID IMAGE           COMMAND         CREATED       STATUS       PORTS                NAMES\n249476631f7d training/webapp \"python app.py\" 1 minute  ago Up 1 minute  0.0.0.0:80-&gt;5000/tcp webapp\n</code></pre> <pre><code>curl http://localhost/\n</code></pre> <p>Result</p> <p><code>Hello world!</code></p> <p>Step 4 Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Result</p> <p>Our web-app can be accessed from Internet!</p>"},{"location":"Module_5_Lab_Docker_basics/#27-restart-a-container","title":"2.7 Restart a container","text":"<p>Step 1 Let\u2019s stop the container with web application:</p> <pre><code>docker stop webapp\n</code></pre> <p>The main process inside of the container will receive SIGTERM, and after a grace period, SIGKILL.</p> <p>Step 2 You can start the container later using the <code>docker start</code> command:</p> <pre><code>docker start webapp\n</code></pre> <p>Step 3 Check that the web application works:</p> <p></p> <p>Step 4 You also can restart the running container using the docker restart command.</p> <pre><code>docker restart webapp\n</code></pre> <p>Step 4  Run <code>docker ps</code> command and check  <code>STATUS</code> field:</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID    IMAGE             COMMAND           CREATED           STATUS              \n6e400179070f    training/webapp   \"python app.py\"   25 minutes ago    Up 3 seconds\n</code></pre>"},{"location":"Module_5_Lab_Docker_basics/#28-ensuring-container-uptime","title":"2.8 Ensuring Container Uptime","text":"<p>Docker considers any containers to exit with a non-zero exit code to have crashed. By default a crashed container will remain stopped.</p> <p>Step 1 Start the container that outputs a message and then exits with code 1 to simulate a crash.</p> <pre><code>docker run -d --name restart-default scrapbook/docker-restart-example\n</code></pre> <pre><code>docker ps -a | grep restart-default\n</code></pre> <pre><code>CONTAINER ID  IMAGE                             CREATED       STATUS               NAMES\nc854289d2f39  scrapbook/docker-restart-example  5 seconds ago Exited 3 sec ago    restart-default\n$ docker logs restart-default\nSun Sep 17 20:34:55 UTC 2017 Booting up...\n</code></pre> <p>Result</p> <p>Container crushed and exited.</p> <p>However, there are several ways to ensure that you container up and running even if it\u2019s restarts.</p> <p>Step 2 The option <code>--restart=on-failure</code>: allows you to say how many times Docker should try again:</p> <pre><code>docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example\n</code></pre> <pre><code>docker logs restart-3\n</code></pre> <pre><code>Thu Apr 20 14:01:27 UTC 2017 Booting up...\nThu Apr 20 14:01:28 UTC 2017 Booting up...\nThu Apr 20 14:01:29 UTC 2017 Booting up...\nThu Apr 20 14:01:31 UTC 2017 Booting up...\n</code></pre> <p>Step 3 Finally, Docker can always restart a failed container. In this case, Docker will keep trying until the container is explicitly told to stop.</p> <pre><code>docker run -d --name restart-always --restart=always scrapbook/docker-restart-example\ndocker logs restart-always\n</code></pre> <p>Step 4  After sometime stop running docker container, as it will be keep failing and starting again:</p> <pre><code>docker stop restart-always\n</code></pre>"},{"location":"Module_5_Lab_Docker_basics/#29-inspect-a-container","title":"2.9 Inspect a container","text":"<p>Step 1 You can use the <code>docker inspect</code> command to see the configuration and status information for the specified container:</p> <pre><code>docker inspect webapp\n</code></pre> <pre><code>[\n    {\n        \"Id\": \"249476631f7d...\",\n        \"Created\": \"2016-08-02T23:42:56.932135327Z\",\n        \"Path\": \"python\",\n        \"Args\": [\n            \"app.py\"\n        ],\n        \"State\": {\n            \"Status\": \"running\",\n            \"Running\": true,\n            \"Paused\": false,\n            \"Restarting\": false,\n            \"OOMKilled\": false,\n            \"Dead\": false,\n            \"Pid\": 16055,\n            \"ExitCode\": 0,\n            \"Error\": \"\",\n            ...\n</code></pre> <p>Step 2 You can specify a filter (-f command line argument) to show only specific elements. For example:</p> <pre><code>docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp\n</code></pre> <pre><code>172.17.0.2\n</code></pre> <p>The command returns the IP address of the container.</p>"},{"location":"Module_5_Lab_Docker_basics/#210-interacting-with-containers","title":"2.10 Interacting with containers","text":"<p>In some cases using <code>docker log</code> is not enough to undertand issues and you want to login inside of running VM. Also sometimes you package you applicaiton and in order to run it you need to login inside of container and execute and leave it running in background. Below provded few ways to interacting with containers that can help to achive descrined use cases.</p>"},{"location":"Module_5_Lab_Docker_basics/#2101-detach-from-interactive-container","title":"2.10.1 Detach from Interactive container","text":"<p>In Module, <code>1.4 Run an interactive container</code> we run an <code>Ubuntu</code> container with <code>-it</code> flag and able directly login inside of the container to interact with it, however after we exited contianer using <code>Ctrl-D</code> or <code>exit</code> command container stopped. However you can exit from <code>Interactive mode</code> without stoping a container. Let's demonstrate how this works:</p> <p>Step 1 Start Ubunu container in interactive  mode:</p> <pre><code>docker run -it ubuntu /bin/bash\n</code></pre> <p>Step 2 Run <code>watch date</code> command inside running container in order to exit <code>date</code> command every 2 seconds.</p> <pre><code>root@1d688a9f4ed4:/# watch date\n</code></pre> <p>Step 3 Detach from a container and leave it running using the <code>CTRL-p</code> <code>CTRL-q</code> key sequence.</p> <p>Step 4 Verify that Ubuntu container is still running:</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID  IMAGE   COMMAND      CREATED        STATUS        NAMES\n1d688a9f4ed4  ubuntu  \"/bin/bash\"  1 minutes ago  Up 1 minutes  admiring_lovelace\n</code></pre> <p>Result</p> <p>Great you were able to detach from Docker container without stopping it, while it is executing a process in it. What about attaching back to container ?</p> <p>Important</p> <p><code>CTRL-p</code> <code>CTRL-q</code>  sequence key only works if docker contaienr started with <code>-it</code> command!</p>"},{"location":"Module_5_Lab_Docker_basics/#2112-attach-to-a-container","title":"2.11.2 Attach to a container","text":"<p>Now let's get back and <code>attach</code> to our running Ubuntu image. For that docker provides <code>docker attach</code> command.</p> <pre><code>docker attach &lt;container name&gt;\n</code></pre> <pre><code>Every 2.0s: date                                                                                                                    Mon Sep 18 00:08:57 2017\n</code></pre> <p>Summary</p> <p><code>docker attach</code> attaches your contairs terminal\u2019s standard input, output, and error (or any combination of the 3) to a running container. This allows you to view its ongoing output or to control it interactively, as though the commands were running directly in your terminal.</p>"},{"location":"Module_5_Lab_Docker_basics/#2113-execute-a-process-in-a-container","title":"2.11.3 Execute a process in a container","text":"<p>Step 1 Let verify if webapp container is still running</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID IMAGE           COMMAND         CREATED       STATUS       PORTS                NAMES\n249476631f7d training/webapp \"python app.py\" 1 minute  ago Up 1 minute  0.0.0.0:80-&gt;5000/tcp webapp\n</code></pre> <p>If not running start it with following command: <code>$ docker run -d -p 80:5000 --name webapp training/webapp python app.py</code> other wise skip to next step.</p> <p>Step 2 Use the docker exec command to execute a command in the running container. For example:</p> <pre><code>docker exec webapp ps aux\n</code></pre> <pre><code>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.2  0.0  52320 17384 ?        Ss   00:11   0:00 python app.py\nroot        26  0.0  0.0  15572  2104 ?        Rs   00:12   0:00 ps aux\n</code></pre> <p>The same command with the <code>-it</code> command line argument can be used to run an interactive session in the container:</p> <pre><code>docker exec -it webapp bash\nroot@249476631f7d:/opt/webapp# ps auxw\nps auxw\n</code></pre> <pre><code>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.0  0.0  52320 17384 ?        Ss   00:11   0:00 python app.py\nroot        32  0.0  0.0  18144  3064 ?        Ss   00:14   0:00 bash\nroot        47  0.0  0.0  15572  2076 ?        R+   00:16   0:00 ps auxw\n</code></pre> <p>Step 2 Use the <code>exit</code> command or press <code>Ctrl-D</code> to exit the interactive bash session:</p> <pre><code>root@249476631f7d:/opt/webapp# exit\n</code></pre> <p>Summary</p> <p><code>docker exec</code> is one of the most usefull docker commands used for troubleshooting containers.</p>"},{"location":"Module_5_Lab_Docker_basics/#212-copy-files-tofrom-container","title":"2.12 Copy files to/from container","text":"<p>The <code>docker cp</code> command allows you to copy files from the container to the local machine or from the local file system to the container. This command works for a running or stopped container.</p> <p>Step 1 Let\u2019s copy the container\u2019s app.py file to the local machine:</p> <pre><code>docker cp webapp:/opt/webapp/app.py .\n</code></pre> <p>Step 2 Edit the local app.py file. For example, change the line return 'Hello '+provider+'!' to return 'Hello '+provider+'!!!'. Copy the modified file back and restart the container:</p> <pre><code>docker cp app.py webapp:/opt/webapp/\n\ndocker restart webapp\n</code></pre> <p>Step 3 Check that the modified web application works::</p> <pre><code>curl http://localhost/\n</code></pre> <p>Result</p> <p>`Hello world!!!``</p>"},{"location":"Module_5_Lab_Docker_basics/#212-remove-containers","title":"2.12 Remove containers","text":"<p>Now let's clean up the environment and at the same time learn how delete containers.</p> <p>Step 1 First list running containers:</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID IMAGE           COMMAND         CREATED       STATUS       PORTS                NAMES\n81c4c66baaf9 training/webapp \"python app.py\" 1 minute  ago Up 1 minute  0.0.0.0:80-&gt;5000/tcp webapp\n</code></pre> <p>Step 2 Than try to delete running container using <code>docker rm &lt;container_id&gt;</code></p> <pre><code>docker rm $container_id\n</code></pre> <pre><code>Error response from daemon: You cannot remove a running container 81c4c66baaf9. Stop the container before attempting removal or force remove.\n</code></pre> <p>Failure</p> <p>Docker containers needs to be first stopped or deleted using <code>--force</code> flag.</p> <pre><code>docker rm $container_id -f\n</code></pre> <p>Alternatively, you can run <code>stop</code> and <code>rm</code> in sequence:</p> <pre><code>docker stop 81c4c66baaf9\ndocker rm 81c4c66baaf9\n</code></pre>"},{"location":"Module_5_Lab_Docker_basics/#3-building-images-with-dockerfile","title":"3 Building Images with DockerFile","text":"<p>In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask (http://flask.pocoo.org/), a microframework for Python. Our application for each request will display a random picture from the defined set.</p> <p>The code for this application is also available in GitHub:</p> <pre><code>https://github.com/Cloud-Architects-Program/ycit019_2022/tree/main/Module5/flask-app\n</code></pre>"},{"location":"Module_5_Lab_Docker_basics/#31-overview-dockerfile-creation","title":"3.1 Overview DOCKERFILE Creation","text":"<p>Step 1 Clone git repo on you laptop:</p> <pre><code>git clone https://github.com/Cloud-Architects-Program/ycit019_2022\ncd ~/ycit019/Module5/flask-app/\n</code></pre> <p>Step 2 In this directory, we see following files:</p> <pre><code>flask-app/\n    Dockerfile\n    app.py\n    requirements.txt\n    templates/\n        index.html\n</code></pre> <p>Step 3 Let\u2019s review file app.py with the following content:</p> <pre><code>from flask import Flask, render_template\nimport random\n\napp = Flask(__name__)\n\n# list of cat images\nimages = [\n    \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\",\n    \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\",\n    \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\",\n    \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\",\n    \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\",\n    \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\",\n    \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\",\n    \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\",\n    \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\",\n    \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\"\n]\n\n@app.route('/')\ndef index():\n    url = random.choice(images)\n    return render_template('index.html', url=url)\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\")\n</code></pre> <p>Step 4 Below is the content of requirements.txt file:</p> <pre><code>Flask==2.0.0\n</code></pre> <p>Step 5 Under directory templates observe index.html with the following content:</p> <pre><code>&lt;html&gt;\n  &lt;head&gt;\n    &lt;style type=\"text/css\"&gt;\n      body {\n        background: black;\n        color: white;\n      }\n      div.container {\n        max-width: 500px;\n        margin: 100px auto;\n        border: 20px solid white;\n        padding: 10px;\n        text-align: center;\n      }\n      h4 {\n        text-transform: uppercase;\n      }\n    &lt;/style&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div class=\"container\"&gt;\n      &lt;h4&gt;Cat Gif of the day&lt;/h4&gt;\n      &lt;img src=\"{{url}}\" /&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Step 6 Let\u2019s review content of the Dockerfile:</p> <pre><code># Official Python Alpine Base image using Simple Tags\n# Image contains Python 3 and pip pre-installed, so no need to install them\n\nFROM python:3.9.5-alpine3.12\n\n# Specify Working directory\nWORKDIR /usr/src/app\n\n# COPY requirements.txt /usr/src/app/\nCOPY requirements.txt ./\n\n# Install Python Flask used by the Python app\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy files required for the app to run\nCOPY app.py ./\nCOPY templates/index.html ./templates/\n\n# Make a record that the port number the container should be expose is:\nEXPOSE 5000\n\n# run the application\nCMD [\"python\", \"./app.py\"]\n</code></pre>"},{"location":"Module_5_Lab_Docker_basics/#32-build-a-docker-image","title":"3.2 Build a Docker image","text":"<p>Step 1 Now let\u2019s build our Docker image. In the command below, replace  with your user name. This user name should be the same as you created when you registered on Docker Hub. <pre><code>docker build -t &lt;user-name&gt;/myfirstapp .\n</code></pre> <p>Result</p> <p>Image has been buit </p> <p>Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below:</p> <pre><code>docker images\n</code></pre> <p>Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000:</p> <pre><code>docker run -dp 8080:5000 --name myfirstapp &lt;user-name&gt;/myfirstapp\n</code></pre> <p>Step 4 Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Step 5 Stop the container and remove it:</p> <pre><code>docker rm -f myfirstapp\n</code></pre> <p>Summary</p> <p>We've learned a lot of docker commands which are very handy to know both when using Docker and Kubernetes. Next we will learn how to create Docker networks.</p>"},{"location":"Module_5_Lab_Docker_basics/#4-docker-networking","title":"4 Docker Networking","text":""},{"location":"Module_5_Lab_Docker_basics/#41-docker-networking-basics","title":"4.1 Docker Networking Basics","text":"<p>Step 1: The Docker Network Command The <code>docker network</code> command is the main command for configuring and managing container networks. Run the <code>docker network</code> command from the first terminal.</p> <pre><code>docker network\n</code></pre> <pre><code>Usage:  docker network COMMAND\n\nManage networks\n\nOptions:\n      --help   Print usage\n\nCommands:\n  connect     Connect a container to a network\n  create      Create a network\n  disconnect  Disconnect a container from a network\n  inspect     Display detailed information on one or more networks\n  ls          List networks\n  prune       Remove all unused networks\n  rm          Remove one or more networks\n\nRun 'docker network COMMAND --help' for more information on a command.\n</code></pre> <p>The command output shows how to use the command as well as all of the <code>docker network</code> sub-commands. As you can see from the output, the <code>docker network</code> command allows you to create new networks, list existing networks, inspect networks, and remove networks. It also allows you to connect and disconnect containers from networks.</p> <p>Step 2 Run a <code>docker network ls</code> command to view existing container networks on the current Docker host.</p> <pre><code>docker network ls\n</code></pre> <pre><code>NETWORK ID          NAME                DRIVER              SCOPE\n3430ad6f20bf        bridge              bridge              local\na7449465c379        host                host                local\n06c349b9cc77        none                null                local\n</code></pre> <p>The output above shows the container networks that are created as part of a standard installation of Docker.</p> <p>New networks that you create will also show up in the output of the <code>docker network ls</code> command.</p> <p>You can see that each network gets a unique <code>ID</code> and <code>NAME</code>. Each network is also associated with a single driver. Notice that the \"bridge\" network and the \"host\" network have the same name as their respective drivers.</p> <p>Step 3: The <code>docker network inspect</code> command is used to view network configuration details. These details include; name, ID, driver, IPAM driver, subnet info, connected containers, and more.</p> <p>Use <code>docker network inspect &lt;network&gt;</code> to view configuration details of the container networks on your Docker host. The command below shows the details of the network called <code>bridge</code>.</p> <pre><code>docker network inspect bridge\n</code></pre> <pre><code>[\n    {\n        \"Name\": \"bridge\",\n        \"Id\": \"3430ad6f20bf1486df2e5f64ddc93cc4ff95d81f59b6baea8a510ad500df2e57\",\n        \"Created\": \"2017-04-03T16:49:58.6536278Z\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.17.0.0/16\",\n                    \"Gateway\": \"172.17.0.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Containers\": {},\n        \"Options\": {\n            \"com.docker.network.bridge.default_bridge\": \"true\",\n            \"com.docker.network.bridge.enable_icc\": \"true\",\n            \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n            \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n            \"com.docker.network.bridge.name\": \"docker0\",\n            \"com.docker.network.driver.mtu\": \"1500\"\n        },\n        \"Labels\": {}\n    }\n]\n</code></pre> <p>Note</p> <p>The syntax of the <code>docker network inspect</code> command is <code>docker network inspect &lt;network&gt;</code>, where <code>&lt;network&gt;</code> can be either network name or network ID. In the example above we are showing the configuration details for the network called \"bridge\". Do not confuse this with the \"bridge\" driver.</p> <p>Step 4 Now, list Docker supported network driver plugins. For that run <code>docker info</code> command, that  shows a lot of interesting information about a Docker installation.</p> <p>Run the <code>docker info</code> command and locate the list of network plugins.</p> <pre><code>docker info\n</code></pre> <pre><code>Containers: 0\n Running: 0\n Paused: 0\n Stopped: 0\nImages: 0\nServer Version: 17.03.1-ee-3\nStorage Driver: aufs\n&lt;Snip&gt;\nPlugins:\n Volume: local\n Network: bridge host macvlan null overlay\nSwarm: inactive\nRuntimes: runc\n&lt;Snip&gt;\n</code></pre> <p>The output above shows the bridge, host,macvlan, null, and overlay drivers.</p> <p>Summary</p> <p>We've quickly reviewed available docker networking commands as well as found what drivers current docker setup supports.</p>"},{"location":"Module_5_Lab_Docker_basics/#42-default-bridge-network","title":"4.2 Default bridge network","text":"<p>Every clean installation of Docker comes with a pre-built network called Default bridge network. Let's explore in more details how it works.</p> <p>Step 1 Verify this with the <code>docker network ls</code>.</p> <pre><code>docker network ls\n</code></pre> <pre><code>NETWORK ID          NAME                DRIVER              SCOPE\n3430ad6f20bf        bridge              bridge              local\na7449465c379        host                host                local\n06c349b9cc77        none                null                local\n</code></pre> <p>Result</p> <p>The output above shows that the bridge network is associated with the bridge driver. It's important to note that the network and the driver are connected, but they are not the same. In this example the network and the driver have the same name - but they are not the same thing!</p> <p>The output above also shows that the bridge network is scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the bridge driver - the bridge driver provides single-host networking.</p> <p>All networks created with the bridge driver are based on a Linux bridge (a.k.a. a virtual switch).</p> <p>Step 5 Start webapp in Default bridge network</p> <pre><code>docker run -d -p 80:5000 --name webapp training/webapp python app.py\n</code></pre> <p>Step 6 Check that the webapp and db containers are running:</p> <p>Command:</p> <pre><code>docker ps\n</code></pre>"},{"location":"Module_5_Lab_Docker_basics/#43-user-defined-private-networks","title":"4.3 User-defined Private Networks","text":"<p>So far we\u2019ve learned how Docker networking works with Docker default bridge network. With the introduction of user-defined networking in Docker 1.9, it is now possible to create multiple Docker bridges to allow network segregation within the same host or multi-host networking to allow communicate Docker containers between hosts.</p> <p>The commands are available through the Docker Engine CLI are:</p> <pre><code>docker network create\ndocker network connect\ndocker network ls\ndocker network rm\ndocker network disconnect\ndocker network inspect\n</code></pre> <p>Let's demonstrate how to create a custom bridge network.</p> <p>Step 1 By default, Docker runs containers in the bridge network. You may want to isolate one or more containers in a separate network. Let\u2019s create a new network:</p> <pre><code>docker network create my-network \\\n-d bridge \\\n--subnet 172.19.0.0/16\n</code></pre> <p>The <code>-d</code> bridge command line argument specifies the bridge network driver and the <code>--subnet</code> command line argument specifies the network segment in CIDR format. If you do not specify a subnet when creating a network, then Docker assigns a subnet automatically, so it is a good idea to specify a subnet to avoid potential conflicts with the existing networks.</p> <p>Below are some other options that are available with the bridge Driver:</p> <ul> <li> <p>com.docker.network.bridge.enable_ip_masquerade: This instructs the Docker   host to hide or masquerade all containers in this network behind the Docker   host's interfaces if the container attempts to route off the local host .</p> </li> <li> <p>com.docker.network.bridge.name: This is the name you wish to give to the   bridge.</p> </li> <li> <p>com.docker.network.bridge.enable_icc: This turns on or off Inter-Container   Connectivity (ICC) mode for the bridge.</p> </li> <li> <p>com.docker.network.bridge.host_binding_ipv4: This defines the host interface   that should be used for port binding.</p> </li> <li> <p>com.docker.network.driver.mtu: This sets MTU for containers attached to this   bridge.</p> </li> </ul> <p>Step 2 To check that the new network is created, execute docker network ls:</p> <pre><code>docker network ls\n</code></pre> <pre><code>NETWORK ID          NAME                DRIVER              SCOPE\nd428e49e4869        bridge              bridge              local\n0d1f78528cc5        host                host                local\n56ef0481820d        my-network          bridge              local\n4a07cef84617        none                null                local\n</code></pre> <p>Step 3 Let\u2019s inspect the new network:</p> <pre><code>docker network inspect my-network\n</code></pre> <pre><code>[\n    {\n        \"Name\": \"my-network\",\n        \"Id\": \"56ef0481820d...\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": {},\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.19.0.0/16\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Containers\": {},\n        \"Options\": {},\n        \"Labels\": {}\n    }\n]\n</code></pre> <p>Step 4 As expected, there are no containers connected to the my-network. Let\u2019s recreate the db container in the my-network:</p> <pre><code>docker rm -f db\n</code></pre> <pre><code>docker run -d --network=my-network --name db training/postgres\n</code></pre> <p>Step 5 Inspect the <code>my-network</code>again:</p> <pre><code>docker network inspect my-network\n</code></pre> <p>Output:</p> <pre><code>    \"Containers\": {\n        \"93af62cdab64...\": {\n            \"Name\": \"db\",\n            \"EndpointID\": \"b1e8e314cff0...\",\n            \"MacAddress\": \"02:42:ac:12:00:02\",\n            \"IPv4Address\": \"172.19.0.2/16\",\n            \"IPv6Address\": \"\"\n        }\n    },\n...\n</code></pre> <p>As you see, the <code>db</code> container is connected to the my-network and has 172.19.0.2 address.</p> <p>Step 6 Let\u2019s start an interactive session in the db container and ping the IP address of the webapp again:</p> <p>Note</p> <p>Quick reminder how to locate webapp ip:</p> <p><code>docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp</code></p> <pre><code>docker exec -it db bash\n</code></pre> <p>Once inside of container run:</p> <pre><code>root@c3afff20019a:/# ping -c 1 172.17.0.3\nPING 172.17.0.3 (172.17.0.3) 56(84) bytes of data.\n\n--- 172.17.0.3 ping statistics ---\n1 packets transmitted, 0 received, 100% packet loss, time 0ms\n</code></pre> <p>As expected, the webapp container is no longer accessible from the db container, because they are connected to different networks.</p> <p>Summary</p> <p>Using <code>Multi-host networking</code> provides network isolation within a Docker host via network namepsaces. This is can be used if you want to deploy different applications on same host for isolation or resource duplicate prevention.</p> <p>Step 7 Let\u2019s connect the webapp container to the my-network:</p> <pre><code>docker network connect my-network webapp\n</code></pre> <p>Step 8 Check that the webapp container now is connected to the my-network:</p> <pre><code>docker network inspect my-network\n</code></pre> <p>Output:</p> <pre><code>...\n    \"Containers\": {\n        \"62ed4a627356...\": {\n            \"Name\": \"webapp\",\n            \"EndpointID\": \"ae95b0103bbc...\",\n            \"MacAddress\": \"02:42:ac:12:00:03\",\n            \"IPv4Address\": \"172.19.0.3/16\",\n            \"IPv6Address\": \"\"\n        },\n        \"93af62cdab64...\": {\n            \"Name\": \"db\",\n            \"EndpointID\": \"b1e8e314cff0...\",\n            \"MacAddress\": \"02:42:ac:12:00:02\",\n            \"IPv4Address\": \"172.19.0.2/16\",\n            \"IPv6Address\": \"\"\n        }\n    },\n...\n</code></pre> <p>The output shows that two containers are connected to the my-network and the webapp container has 172.19.0.3 address in that network.</p> <p>Step 9 Check that the webapp container is accessible from the db container using its new IP address:</p> <pre><code>docker exec -it db bash\n</code></pre> <pre><code>root@c3afff20019a:/# ping -c 1 172.19.0.3\nPING 172.19.0.3 (172.19.0.3) 56(84) bytes of data.\n64 bytes from 172.19.0.3: icmp_seq=1 ttl=64 time=0.136 ms\n\n--- 172.19.0.3 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms\n</code></pre> <p>Success</p> <p>As expected containers can communicate with each other.</p> <p>Step 10 You can now remove the existing container. You should stop the container before removing it. Alternatively you can use the -f command line argument:</p> <pre><code>docker rm -f webapp\ndocker rm -f db\ndocker network rm  my-network\n</code></pre> <p>Hint</p> <p>Use below command to delete running containers in bulk:</p> <p><code>docker rm -f $(docker ps -q)</code></p> <p>Summary</p> <p>It is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses</p>"},{"location":"Module_6_Lab_Advanced_Docker/","title":"Labs","text":"<p>Lab 4 Managing Docker Images</p> <p>Objective:</p> <ul> <li>Docker Storage</li> <li>Learn to build docker images using Dockerfiles.</li> <li>Store images in Docker Hub</li> <li>Learn alternative registry solutions (GCR)</li> </ul>"},{"location":"Module_6_Lab_Advanced_Docker/#prepare-lab-environment","title":"Prepare Lab Environment","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p>"},{"location":"Module_6_Lab_Advanced_Docker/#1-persistant-volumes","title":"1 Persistant Volumes","text":""},{"location":"Module_6_Lab_Advanced_Docker/#11-storage-driver","title":"1.1 Storage driver","text":"<p>We've discussed several Storage drivers (graphdrivers) during the class. Let's find out what graphdriver is running in our Lab environment.</p> <pre><code>docker info | grep  Storage\n</code></pre> <pre><code>Storage Driver: overlay2\n</code></pre> <p>Result</p> <p>Our Classroom is running <code>auoverlay2fs</code> storage driver.</p> <p>Summary</p> <p>Systems runnng Ubuntu or Debian ,going to run <code>overlay2</code> storage driver by default. Prior to that it was  <code>aufs</code> storage driver</p>"},{"location":"Module_6_Lab_Advanced_Docker/#12-persisting-data-using-volumes","title":"1.2 Persisting Data Using Volumes","text":"<p>Docker Volumes are created and assigned when containers are started. Data Volumes allow you to map a host directory to a container for sharing data.</p> <p>This mapping is bi-directional. It allows data stored on the host to be accessed from within the container. It also means data saved by the process inside the container is persisted on the host.</p>"},{"location":"Module_6_Lab_Advanced_Docker/#121-create-and-manage-volumes","title":"1.2.1  Create and manage volumes","text":"<p>Step 1 Create a volume:</p> <pre><code>docker volume create --name my-vol\n</code></pre> <p>Step 2 List volumes:</p> <pre><code>docker volume ls\n</code></pre> <pre><code>Output:\nlocal               my-vol\n</code></pre> <p>Step 3 Inspect a volume:</p> <pre><code>docker volume inspect my-vol\n</code></pre> <pre><code>[\n    {\n        \"Driver\": \"local\",\n        \"Labels\": {},\n        \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\",\n        \"Name\": \"my-vol\",\n        \"Options\": {},\n        \"Scope\": \"local\"\n    }\n]\n</code></pre> <p>Step 3 Add some data to the <code>Mountpoint</code> of the volume:</p> <pre><code>sudo touch  /var/lib/docker/volumes/my-vol/_data/test_vol\nsudo ls  /var/lib/docker/volumes/my-vol/_data/\n</code></pre> <p>Step 4 Create a container <code>busybox</code> alpine image and attach created <code>my-vol</code> volume in to it:</p> <pre><code>docker run -it -v my-vol:/world busybox\n</code></pre> <pre><code>/ # ls /world\ntest_vol\n/ #\n</code></pre> <p>Result</p> <p>Volume is mounted and <code>test_vol</code> file is under <code>/world</code> folder as expected</p> <p>Step 5  Try to delete the volume:</p> <pre><code>docker volume rm  my-vol\n</code></pre> <pre><code>Error response from daemon: unable to remove volume: remove my-vol: volume is in use - [6ef3055b516b306847150af8fcea796c02cd90578967802ac29c39d3a2c90102]\n</code></pre> <p>Failure</p> <p>Deleting container that is attached is not permited. However you can delete with <code>-f</code> option</p> <p>Step 5  Busybox container stopped, howerver it is not deleted. Let's locate stopped <code>busybox</code> container and delete it:</p> <pre><code>docker ps -a | grep busybox\n</code></pre> <pre><code>docker rm $docker_id\n</code></pre> <p>Step 6 You can now delete <code>my-vol</code></p> <p>Note</p> <p>Volume is still avaiable if needed to be reattached any time</p> <pre><code>docker volume ls\ndocker volume rm my-vol\ndocker volume ls\n</code></pre> <p>Summary</p> <p>Volumes can be craeted and managed separately from containers.</p>"},{"location":"Module_6_Lab_Advanced_Docker/#122-start-a-container-with-a-volume","title":"1.2.2 Start a container with a volume","text":"<p>If you start a container with a volume that does not yet exist, Docker creates the volume for you.</p> <p>Step 1 Add a data volume to a container:</p> <pre><code>docker run -d -P --name webapp -v /webapp training/webapp python app.py\n</code></pre> <p>Result</p> <p>Command started a new container and created a new volume inside the container at /webapp.</p> <p>Step 2 Locate the volume on the host using the docker inspect command:</p> <pre><code> docker inspect webapp | grep -A9 Mounts\n ```\n **Output:**\n ```       \"Mounts\": [\n            {\n                \"Type\": \"volume\",\n                \"Name\": \"39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\",\n                \"Source\": \"/var/lib/docker/volumes/39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d/_data\",\n                \"Destination\": \"/webapp\",\n                \"Driver\": \"local\",\n                \"Mode\": \"\",\n                \"RW\": true,\n                \"Propagation\": \"\"\n</code></pre> <p>Step 3 List container</p> <pre><code>docker volume ls\n</code></pre> <p>Output:</p> <pre><code>DRIVER              VOLUME NAME\nlocal               39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\n</code></pre> <p>Step 5 Alternatively, you can specify a host directory you want to use as a data volume:</p> <pre><code>mkdir db\n\ndocker run -d --name db -v ~/db:/db training/postgres\n</code></pre> <p>Step 2 Start an interactive session in the db container and create a new file in the /db directory:</p> <pre><code>docker exec -it db bash\n</code></pre> <p>Type inside docker containers console:</p> <pre><code>root@9a7a4fbcc929:/# cd /db\n\nroot@9a7a4fbcc929:/db# touch hello_from_db_container\n\nroot@9a7a4fbcc929:/db# exit\n</code></pre> <p>Step 4 Check that the local db directory contains the new file:</p> <pre><code>ls db\nhello_from_db_container\n</code></pre> <p>Step 5 Check that the data volume is persistent. Remove the db container:</p> <pre><code>docker rm -f db\n</code></pre> <p>Step 6 Create the db container again:</p> <pre><code>docker run -d --name db -v ~/db:/db training/postgres\n</code></pre> <p>Step 7 Check that its /db directory contains the hello_from_db_container file:</p> <pre><code>docker exec -it db bash\n</code></pre> <p>Run commands inside container:</p> <pre><code>root@47a60c01590e:/# ls /db\nhello_from_db_container\nroot@47a60c01590e:/# exit\n</code></pre>"},{"location":"Module_6_Lab_Advanced_Docker/#2-distributing-docker-images-with-container-registry","title":"2 Distributing Docker images with Container Registry","text":"<p>In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask (http://flask.pocoo.org/), a microframework for Python. Our application for each request will display a random picture from the defined set.</p> <p>In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay.</p> <p>The code for this application is also available in GitHub:</p> <pre><code>https://github.com/Cloud-Architects-Program/ycit019_2022/tree/main/Module5/flask-app\n</code></pre>"},{"location":"Module_6_Lab_Advanced_Docker/#11-create-dockerfile","title":"1.1 Create DOCKERFILE","text":"<p>Step 1 Clone git repo on you laptop:</p> <pre><code>git clone https://github.com/Cloud-Architects-Program/ycit019_2022\ncd ~/ycit019_2022/Module5/flask-app/\ngit pull\n</code></pre> <p>Step 2 In this directory, we see following files:</p> <pre><code>flask-app/\n    Dockerfile\n    app.py\n    requirements.txt\n    templates/\n        index.html\n</code></pre> <p>Step 3 Let\u2019s review file app.py with the following content:</p> <pre><code>from flask import Flask, render_template\nimport random\n\napp = Flask(__name__)\n\n# list of cat images\nimages = [\n    \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\",\n    \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\",\n    \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\",\n    \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\",\n    \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\",\n    \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\",\n    \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\",\n    \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\",\n    \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\",\n    \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\"\n]\n\n@app.route('/')\ndef index():\n    url = random.choice(images)\n    return render_template('index.html', url=url)\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\")\n</code></pre> <p>Step 4 Below is the content of requirements.txt file:</p> <pre><code>Flask==2.0.0\n</code></pre> <p>Step 5 Under directory templates observe index.html with the following content:</p> <pre><code>&lt;html&gt;\n  &lt;head&gt;\n    &lt;style type=\"text/css\"&gt;\n      body {\n        background: black;\n        color: white;\n      }\n      div.container {\n        max-width: 500px;\n        margin: 100px auto;\n        border: 20px solid white;\n        padding: 10px;\n        text-align: center;\n      }\n      h4 {\n        text-transform: uppercase;\n      }\n    &lt;/style&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div class=\"container\"&gt;\n      &lt;h4&gt;Cat Gif of the day&lt;/h4&gt;\n      &lt;img src=\"{{url}}\" /&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Step 6 Let\u2019s review content of the Dockerfile:</p> <pre><code># Official Python Alpine Base image using Simple Tags\n# Image contains Python 3 and pip pre-installed, so no need to install them\n\nFROM python:3.9.5-alpine3.12\n\n# Specify Working directory\nWORKDIR /usr/src/app\n\n# COPY requirements.txt /usr/src/app/\nCOPY requirements.txt ./\n\n# Install Python Flask used by the Python app\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy files required for the app to run\nCOPY app.py ./\nCOPY templates/index.html ./templates/\n\n# Make a record that the port number the container should be expose is:\nEXPOSE 5000\n\n# run the application\nCMD [\"python\", \"./app.py\"]\n</code></pre>"},{"location":"Module_6_Lab_Advanced_Docker/#12-build-a-docker-image","title":"1.2 Build a Docker image","text":"<p>Step 1 Now let\u2019s build our Docker image. In the command below, replace  with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step to your own Docker Hub. <pre><code>docker build -t &lt;Docker-hub-user-name&gt;/myfirstapp .\n</code></pre> <p>Result</p> <p>Image has been buit </p> <p>Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below:</p> <pre><code>docker images\n</code></pre> <p>Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000:</p> <pre><code>docker run -dp 8080:5000 --name myfirstapp &lt;Docker-hub-user-name&gt;/myfirstapp\n</code></pre> <p>Step 4 Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Step 5 Stop the container and remove it:</p> <pre><code>docker rm -f myfirstapp\n</code></pre>"},{"location":"Module_6_Lab_Advanced_Docker/#122-publish-docker-image-to-docker-hub","title":"1.2.2 Publish Docker Image to Docker Hub","text":"<p>One of the most popular way to share and work with you images is to push them to the Docker Hub.</p> <p>Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images.</p> <p>Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here. Make a note of your username and password.</p> <p>Step 2 Log in to your local machine.</p> <pre><code>docker login\n</code></pre> <p>Step 3 Now, publish your image to docker Hub.</p> <pre><code>docker push &lt;Docker-hub-user-name&gt;/myfirstapp\n</code></pre> <p>Step 4 Login to https://hub.docker.com and verify simage and tags.</p> <p>Result</p> <p>Image been pushed and can be observed in Docker Hub, with the tag latest.</p> <p>Step 5 It is also possible to specify a custom tag for image prior to push it to the registry</p> <p>Note Image Tag of the created <code>myfirstapp</code>:</p> <pre><code>docker images\n</code></pre> <p>Modify <code>$docker_image_tag</code> with <code>myfirstapp:v1</code> image tag value:</p> <pre><code>docker tag $docker_image_tag &lt;Docker-hub-user-name&gt;/myfirstapp:v1\ndocker push &lt;Docker-hub-user-name&gt;/myfirstapp:v1\n</code></pre> <p>Result</p> <p>Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1</p> <p>Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands:</p> <pre><code>docker pull &lt;Docker-hub-user-name&gt;/myfirstapp:latest\n\ndocker pull &lt;Docker-hub-user-name&gt;/myfirstapp:v1\n</code></pre> <p>Result</p> <p>Images stored locally</p> <pre><code>docker images\n</code></pre> <p>Output:</p> <pre><code>myfirstapp       v1       f50f9524513f    1 hour ago   22 MB\n\nmyfirstapp       latest   f50f9524513f    1 hour ago   22 MB\n</code></pre> <p>Finally run images with specific tag:</p> <pre><code>docker run &lt;Docker-hub-user-name&gt;/myfirstapp:v1\n</code></pre>"},{"location":"Module_6_Lab_Advanced_Docker/#123-pushing-images-to-gcrio","title":"1.2.3 Pushing images to gcr.io","text":"<p>In a similar manner we need to tag the image to prepare it to be pushed to gcr.io. We just need to change the registry, which is for gcr.io formatted as gcr.io/PROJECT_ID.</p> <p>Step 1 Get the Project ID:</p> <pre><code>PROJECT_ID=$(gcloud config get-value project)\n</code></pre> <p>Step 2 Enable the required APIs:</p> <pre><code>gcloud services enable containerregistry.googleapis.com\n</code></pre> <p>Step 3 Tag the image:</p> <p>Modify <code>$docker_image_tag</code> with <code>myfirstapp:v1</code> image tag value:</p> <pre><code>docker tag $docker_image_tag gcr.io/${PROJECT_ID}/myfirstapp:v1\n</code></pre> <p>Push the image to gcr.io:</p> <pre><code>docker push gcr.io/${PROJECT_ID}/myfirstapp:v1\n</code></pre> <p>Step 4  Login to GCP console -&gt; Container Registry -&gt; Images</p> <p>Result</p> <p>Docker images has been pushed to GCR registry</p>"},{"location":"Module_6_Lab_Advanced_Docker/#123-pushing-images-to-local-repository","title":"1.2.3 Pushing images to Local Repository","text":"<p>First, we need to spin up a local docker registry. This could be a use case if you want to deploy basic registry On-Prem. This registry will luck security features such as Authentication, SSL, scanning. If you interested to use Enterprise ready solution On-Prem consider: Jfrog Artifactory, RedHa's Clair, Docker Enterprise or open source CNCF project Harbor.</p> <p>Step 1 Deploy local registry</p> <pre><code>docker run -d -p 5000:5000 --name registry registry:2.7.1\n</code></pre> <p>Step 2 In order to upload an image to a registry, we need to tag it properly</p> <p>Modify <code>$docker_image_tag</code> with <code>myfirstapp:v1</code> image tag value:</p> <pre><code>docker tag $docker_image_tag localhost:5000/myfirstapp:v1\n</code></pre> <p>Step 3 Now that we have an image tagged correctly, we can push it to our local registry</p> <pre><code>docker push localhost:5000/myfirstapp:v1\n</code></pre> <p>Step 4 Let\u2019s now delete the local image, and pull it again from the local registry</p> <p>To delete the image, we need to first remove the container that depends on that image. Run <code>docker ps</code> and get the Container_ID for the container that uses  myfirstapp:v1 Kill and delete that container by running the following command, but make sure to replace CONTAINER_ID, with the actual ID.</p> <pre><code>docker rm CONTAINER_ID\n</code></pre> <p>Result: The command will print back the container ID, which is an indication it was successful.</p> <p>Step 5 Run docker images to validate</p> <pre><code>docker images\n</code></pre> <p>Step 6 Now we can delete the docker image</p> <pre><code>docker rmi localhost:5000/myfirstapp:v1\n</code></pre> <p>Step 7 Although the image is deleted locally, it is still in the registry and we can pull it back, or use it to deploy containers.</p> <pre><code>docker run -dp 8080:5000 --name myfirstapp localhost:5000/myfirstapp:v1\n</code></pre> <p>Run <code>docker images</code> again to check how the image is available locally again.</p> <pre><code>docker images\n</code></pre> <p>Step 8 Cleanup:</p> <pre><code>docker rm -f myfirstapp\n</code></pre>"},{"location":"Module_6_Lab_Advanced_Docker/#3-follow-docker-best-practices","title":"3 Follow Docker Best Practices","text":""},{"location":"Module_6_Lab_Advanced_Docker/#31-inspecting-dockerfiles-with-dockle","title":"3.1 Inspecting Dockerfiles with <code>dockle</code>","text":"<p><code>Dockle</code> - Container Image Linter for Security, Helping build the Best-Practice Docker Image, Easy to start</p> <p><code>Dockle</code> helps you:</p> <ul> <li>Build Best Practice Docker images</li> <li>Build secure Docker images</li> <li>Checkpoints includes CIS Benchmarks</li> </ul> <p>Step 1  Install Dockle</p> <pre><code>$ VERSION=$(\n curl --silent \"https://api.github.com/repos/goodwithtech/dockle/releases/latest\" | \\\n grep '\"tag_name\":' | \\\n sed -E 's/.*\"v([^\"]+)\".*/\\1/' \\\n) &amp;&amp; curl -L -o dockle.deb https://github.com/goodwithtech/dockle/releases/download/v${VERSION}/dockle_${VERSION}_Linux-64bit.deb\n$ sudo dpkg -i dockle.deb &amp;&amp; rm dockle.deb\n</code></pre> <p>Step 2 Experiment with existing applications we've created in the class:</p> <pre><code>$ dockle [YOUR_IMAGE_NAME]\n</code></pre> <p>e.g.</p> <pre><code>dockle archy/myfirstapp\n</code></pre> <p>output:</p> <pre><code>WARN    - CIS-DI-0001: Create a user for the container\n    * Last user should not be root\nWARN    - DKL-DI-0006: Avoid latest tag\n    * Avoid 'latest' tag\nINFO    - CIS-DI-0005: Enable Content trust for Docker\n    * export DOCKER_CONTENT_TRUST=1 before docker pull/build\nINFO    - CIS-DI-0006: Add HEALTHCHECK instruction to the container image\n    * not found HEALTHCHECK statement\nINFO    - DKL-LI-0003: Only put necessary files\n    * Suspicious directory : tmp\n</code></pre>"},{"location":"Module_6_Lab_Advanced_Docker/#32-scan-images-with-trivy","title":"3.2 Scan images with Trivy","text":"<p>Trivy (tri pronounced like trigger, vy pronounced like envy) is a simple and comprehensive vulnerability scanner for containers and other artifacts. A software vulnerability is a glitch, flaw, or weakness present in the software or in an Operating System. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and application dependencies (Bundler, Composer, npm, yarn, etc.). Trivy is easy to use. Just install the binary and you're ready to scan. All you need to do for scanning is to specify a target such as an image name of the container.</p> <p>Step 1  Install Trivy</p> <pre><code>sudo apt-get install wget apt-transport-https gnupg lsb-release\nwget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -\necho deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list\nsudo apt-get update\nsudo apt-get install trivy\n</code></pre> <p>Step 2 Specify an image name (and a tag).</p> <pre><code>$ trivy image [YOUR_IMAGE_NAME]\n</code></pre> <p>For example:</p> <pre><code>$ trivy image python:3.4-alpine\n</code></pre> <pre><code>2019-05-16T01:20:43.180+0900    INFO    Updating vulnerability database...\n2019-05-16T01:20:53.029+0900    INFO    Detecting Alpine vulnerabilities...\n\npython:3.4-alpine3.9 (alpine 3.9.2)\n===================================\nTotal: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)\n\n+---------+------------------+----------+-------------------+---------------+--------------------------------+\n| LIBRARY | VULNERABILITY ID | SEVERITY | INSTALLED VERSION | FIXED VERSION |             TITLE              |\n+---------+------------------+----------+-------------------+---------------+--------------------------------+\n| openssl | CVE-2019-1543    | MEDIUM   | 1.1.1a-r1         | 1.1.1b-r1     | openssl: ChaCha20-Poly1305     |\n|         |                  |          |                   |               | with long nonces               |\n+---------+------------------+----------+-------------------+---------------+--------------------------------+\n</code></pre> <p>Step 3  Explore local images in your environment. </p>"},{"location":"Module_6_Lab_Advanced_Docker/#4-docker-compose","title":"4 Docker Compose","text":"<p>In this module, will guide you through the process of building a multi-container application using docker compose. The application code is available at GitHub: https://github.com/Cloud-Architects-Program/ycit019_2022</p>"},{"location":"Module_6_Lab_Advanced_Docker/#41-deploy-guestbook-app-with-compose","title":"4.1 Deploy Guestbook app with Compose","text":"<p>Let\u2019s build another application. This time we going to create famous Guestbook application.</p> <p>Guestbook consists of three services. A redis-master node, a set of redis-slave that can be scaled and find the redis-master via its DNS name. And a PHP frontend that exposes itself on port 80. The resulting application allows you to leave short messages which are stored in the redis cluster.</p> <p>Step 1 Change directory to the guestbook</p> <pre><code>cd ~/ycit019_2022/Module6/guestbook/\nls\n</code></pre> <p>Step 2 Let\u2019s review the docker-guestbook.yml file</p> <pre><code>version: \"2\"\n\nservices:\n redis-master:\n   image: gcr.io/google_containers/redis:e2e\n   ports:\n     - \"6379\"\n redis-slave:\n   image: gcr.io/google_samples/gb-redisslave:v1\n   ports:\n     - \"6379\"\n   environment:\n     - GET_HOSTS_FROM=dns\n frontend:\n   image: gcr.io/google-samples/gb-frontend:v4\n   ports:\n     - \"80:80\"\n   environment:\n     - GET_HOSTS_FROM=dns\n</code></pre> <p>Step 3 Let\u2019s run docker-guestbook.yml with compose</p> <pre><code>export LD_LIBRARY_PATH=/usr/local/lib\ndocker-compose -f docker-guestbook.yml up -d\n</code></pre> <pre><code>Creating network \"examples_default\" with the default driver\nCreating examples_redis-slave_1\nCreating examples_frontend_1\nCreating examples_redis-master_1\n</code></pre> <p>Note</p> <p><code>-d</code> -  Detached mode: Run containers in the background, print new container names.</p> <p><code>-f</code> -  Specify an alternate compose file (default: docker-compose.yml)</p> <p>Step 4 Check that all containers are running:</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID        IMAGE                                    COMMAND\nd1006d1beee5        gcr.io/google-samples/gb-frontend:v4     \"apache2-foreground\"\nfb3a15fde23f        gcr.io/google_containers/redis:e2e       \"redis-server /etc...\"\n326b94d4cdd7        gcr.io/google_samples/gb-redisslave:v1   \"/entrypoint.sh /b...\"\n</code></pre> <p>Step 5  Test the application locally</p> <p>Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Success</p> <p>Nice you now have compose stuck up and running!</p> <p>Step 6 Cleanup environment:</p> <pre><code>docker-compose -f docker-guestbook.yml down\n</code></pre> <pre><code>Stopping guestbook_frontend_1 ... done\nStopping guestbook_redis-master_1 ... done\nStopping guestbook_redis-slave_1 ... done\nRemoving guestbook_frontend_1 ... done\nRemoving guestbook_redis-master_1 ... done\nRemoving guestbook_redis-slave_1 ... done\nRemoving network guestbook_default\n</code></pre> <p>Congratulations</p> <p>You are now docker expert! We were able to start microservices application with docker compose.</p> <p>Summary</p> <p>We've learned how to use docker-compose v2.</p> <p>In the assignement you will be using docker-compose v3</p> <p>Read the Docker-Compose documentation on new syntax.</p>"},{"location":"add_mat_5_docker_basic/","title":"Additional Materials","text":""},{"location":"add_mat_5_docker_basic/#official-documentation","title":"Official Documentation","text":"<ul> <li>Docker Documentation \u2014 Official Docker documentation, from installation to advanced usage.</li> <li>Docker Hub \u2014 Repository of ready-to-use Docker images.</li> </ul>"},{"location":"add_mat_5_docker_basic/#articles-tutorials","title":"Articles &amp; Tutorials","text":"<ul> <li>What is Docker? \u2014 Explanation of container basics.</li> <li>Docker Tutorials by DigitalOcean \u2014 Beginner-friendly tutorial series.</li> </ul>"},{"location":"add_mat_5_docker_basic/#videos","title":"Videos","text":"<ul> <li>Docker Tutorial for Beginners - Full Course \u2014 Full-length video course on Docker.</li> <li>Docker Explained \u2014 Short explanation of Docker concepts.</li> </ul>"},{"location":"add_mat_5_docker_basic/#interactive-labs","title":"Interactive Labs","text":"<ul> <li>Play with Docker \u2014 Online Docker playground and interactive labs.</li> </ul>"},{"location":"add_mat_5_docker_basic/#books","title":"Books","text":"<ul> <li>Docker Deep Dive by Nigel Poulton \u2014 Great book for deep understanding of Docker.</li> <li>The Docker Book by James Turnbull \u2014 Practical Docker guide.</li> </ul>"},{"location":"add_mat_6_docker_adv/","title":"Additional Materials","text":""},{"location":"add_mat_6_docker_adv/#official-documentation","title":"Official Documentation","text":"<ul> <li>Docker Networking \u2014 Official guide to Docker networking concepts and drivers.</li> <li>Docker Storage \u2014 Persistent storage options and drivers.</li> <li>Docker Registry \u2014 Hosting and distributing container images.</li> <li>Dockerfile Best Practices \u2014 Guidelines for creating efficient images.</li> <li>Docker Compose \u2014 Managing multi-container applications.</li> </ul>"},{"location":"add_mat_6_docker_adv/#articles-tutorials","title":"Articles &amp; Tutorials","text":"<ul> <li>Trivy Quickstart \u2014 Container image vulnerability scanning tutorial.</li> <li>Kaniko \u2014 Building container images in unprivileged environments.</li> <li>Cloud Native Buildpacks \u2014 Automating container image builds.</li> <li>Docker Compose Getting Started \u2014 Hands-on guide for multi-container apps.</li> </ul>"},{"location":"add_mat_6_docker_adv/#videos","title":"Videos","text":"<ul> <li>Docker Networking Deep Dive \u2014 In-depth explanation of Docker networking.</li> <li>Docker Compose Tutorial \u2014 Using Docker Compose step by step.</li> </ul>"},{"location":"add_mat_6_docker_adv/#interactive-labs","title":"Interactive Labs","text":"<ul> <li>Play with Docker \u2014 Online Docker playground and interactive labs.</li> </ul>"},{"location":"add_mat_6_docker_adv/#books","title":"Books","text":"<ul> <li>Docker in Practice (2nd Edition) by Ian Miell and Aidan Hobson Sayers \u2014 Advanced techniques, automation strategies, and production-grade Docker practices.</li> <li>Using Docker by Adrian Mouat \u2014 Practical Docker guide with advanced concepts.</li> </ul>"},{"location":"assignment5_dockerbas/","title":"1 Containerize Applications","text":"<p>Objective:</p> <ul> <li>Review process of containerizing of applications</li> <li>Review creation of Docker Images</li> <li>Review build image process</li> <li>Review process to launch containers with docker cli</li> </ul>"},{"location":"assignment5_dockerbas/#1-prepare-lab-environment","title":"1  Prepare Lab Environment","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p>"},{"location":"assignment5_dockerbas/#2-build-and-deploy-gowebapp-application","title":"2 Build and Deploy gowebapp application","text":""},{"location":"assignment5_dockerbas/#21-overview-of-the-sample-application","title":"2.1 Overview of the Sample Application","text":"<p>This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes:</p> <ul> <li>gowebapp</li> </ul> <p>This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page.</p> <p>Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL.</p> <p>Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises.</p> <p>For more details about the internal design and implementation of the Go web application, see code/README.md.</p> <ul> <li>gowebapp-mysql</li> </ul> <p>This directory contains the schema file used to setup the backing MySQL database for the Go web application.</p>"},{"location":"assignment5_dockerbas/#23-build-dockers-image-for-frontend-application","title":"2.3 Build Dockers image for frontend application","text":"<p>Step 1 Locate and review the go source code:</p> <pre><code>cd ~/$student_name-notepad/Mod5_assignment/\n</code></pre> <p>Result</p> <p>Two folders with go app and mysql config has been reviewed.</p> <p>Step 2 Write <code>Dockerfile</code> for your frontend application</p> <pre><code>cd ~/$student_name-notepad/Mod5_assignment/gowebapp\n</code></pre> <p>Modify a file named <code>Dockerfile</code> in this directory for the frontend Go app. Use Cloud Editor or editor of you choice</p> <pre><code>edit Dockerfile\n</code></pre> <p>The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands:</p> <pre><code>#TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang`\n#https://hub.docker.com/_/golang/\n#https://docs.docker.com/engine/reference/builder/#from\n\n\n#TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels.\n#MAINTAINER should be you student e-mail.\n\n#TODO --- Define a version label for this image\n#https://docs.docker.com/engine/reference/builder/#label\n\nEXPOSE 80\n\nENV GOPATH=/go\n\n#TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp\n#https://docs.docker.com/engine/reference/builder/#copy\n\nWORKDIR $GOPATH/src/gowebapp/\n\nRUN go get &amp;&amp; go install\n\n#TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts\n#https://docs.docker.com/engine/reference/builder/#entrypoint\n</code></pre> <p>Step 4  Build gowebapp Docker image locally</p> <p>Build the  image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message.</p> <pre><code>#TODO  Build image `&lt;your-github-user&gt;/gowebapp:v1\n</code></pre>"},{"location":"assignment5_dockerbas/#24-build-docker-image-for-backend-application","title":"2.4 Build Docker image for backend application","text":"<p>Step 1 Locate folder with mysql config</p> <pre><code>cd ~/$student_name-notepad/Mod5_assignment/gowebapp-mysql\n</code></pre> <p>Step 2 Write Dockerfile for your backend application</p> <p>Create a file named <code>Dockerfile</code> in this directory for the backend MySQL database application. Use Cloud Editor or editor of you choice.</p> <pre><code>edit Dockerfile\n</code></pre> <p>The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands:</p> <pre><code>#TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image\n#https://hub.docker.com/_/mysql/\n#https://docs.docker.com/engine/reference/builder/#from\n\n#TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels.\n#MAINTAINER should be you student e-mail.\n\nLABEL gowebapp-mysql \"v1\"\n\n#TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up\n#https://hub.docker.com/_/mysql/\n#https://docs.docker.com/engine/reference/builder/#copy\n</code></pre> <p>Step 2 Build gowebapp-mysql Docker image locally</p> <pre><code>#TODO  Build image  &lt;your-github-user&gt;/gowebapp-mysql:v1\n</code></pre> <p>Build the  image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally</p>"},{"location":"assignment5_dockerbas/#25-test-application-by-running-with-docker-engine","title":"2.5 Test application by running with Docker Engine.","text":"<p>Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly.</p> <p>Step 1 Create Docker user-defined network</p> <p>To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers:</p> <pre><code>docker network create gowebapp \\\n-d bridge \\\n--subnet 172.19.0.0/16\n</code></pre> <p>Note</p> <p>Default bridge only allows connecting container by IP addresses which is not viable solution as IP address of docker change at startup. There for we creating a user defined bridge network <code>gowebapp</code></p> <p>Step 2  Launch <code>backend</code> container</p> <p>Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable:</p> <pre><code>#TODO Launch `backend` container in background\n#TODO Container needs to run on network: `gowebapp`\n#TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` \n#TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd`\n</code></pre> <p>Step 3  Launch <code>frontend</code> container</p> <p>Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine:</p> <pre><code>#TODO Launch `frontend` container in background\n#TODO Container needs to run on network: `gowebapp`\n#TODO Use this settings: `--name gowebapp` `--hostname gowebapp` \n#TODO Map the container port 80  - to port 8080 on the host machine\n</code></pre> <p>Step 4  Test the application locally</p> <p>Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container.</p> <p>Task</p> <p>Take a screenshot of running application. </p> <p>Step 5 Inspect the MySQL database</p> <p>Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly:</p> <pre><code>#TODO docker xxx\n</code></pre> <p>Step 6 Once inside the container, connect to MySQL database:</p> <pre><code>mysql -u root -p\npassword:\n</code></pre> <p>Note</p> <p>Use password that has beed used in <code>MYSQL_ROOT_PASSWORD</code> env variable.</p> <p>Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence:</p> <pre><code>#Simple SQL to navigate\nSHOW DATABASES;\nUSE gowebapp;\nSHOW TABLES;\nSELECT * FROM &lt;table_name&gt;;\nexit;\n</code></pre>"},{"location":"assignment5_dockerbas/#26-cleanup-running-applications","title":"2.6 Cleanup running applications","text":"<pre><code>### TODO docker xxx\n</code></pre>"},{"location":"assignment5_dockerbas_backup/","title":"1 Containerize Applications","text":"<p>Objective:</p> <ul> <li>Use GCP Cloud Source Repositories to commit code</li> <li>Review process of containerizing of applications</li> <li>Review creation of Docker Images</li> <li>Review build image process</li> <li>Review process to launch containers with docker cli</li> </ul>"},{"location":"assignment5_dockerbas_backup/#prepare-lab-environment","title":"Prepare Lab Environment","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p>"},{"location":"assignment5_dockerbas_backup/#1-configure-cloud-source-repository","title":"1 Configure Cloud Source Repository","text":"<p>Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote,  and push the contents of the local repository. You will use the source browser included  in Source Repositories to view your repository files from within the Cloud Console.</p> <p>Note</p> <p>Google Cloud Source Repository is similar tool to GitHub, so if you know Github CLI it is same experience, apart from that UI is different than GITHUB.</p>"},{"location":"assignment5_dockerbas_backup/#11-create-a-personal-repository-within-google-cloud-source-repository","title":"1.1 Create a personal repository within <code>Google Cloud Source Repository</code>","text":"<p>Step 1 Run the following command to create a new Cloud Source Repository named $student_name-notepad, where $student_name - is you mcgill student-ID:</p> <p>Setup Environment Variable: </p> <pre><code>export PROJECT_ID=&lt;project_id&gt;\n</code></pre> <p>Here is how you can find you project_ID:</p> <p></p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <p>Important</p> <p>Replace above with your project_id student_name</p> <pre><code>gcloud config set project $PROJECT_ID\ngcloud source repos create $student_name-notepad\n</code></pre> <p>Press Y</p> <pre><code>API [sourcerepo.googleapis.com] not enabled on project [686694291909]. Would you like to enable and retry (this will take a few minutes)? (y/N)?\n</code></pre> <p>You can safely ignore any billing warnings for creating repositories.</p> <p>Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session:</p> <pre><code>gcloud source repos clone $student_name-notepad\n</code></pre> <p>The <code>gcloud source repos clone</code> command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository.</p> <p>Step 3 Go into the local repository you've created:</p> <pre><code>ls\n</code></pre> <p>Observe that repository has been cloned</p> <p>Result</p> <p>You've created a Personal Repository, this is the locaton where you going to submit you assignments going forward</p>"},{"location":"assignment5_dockerbas_backup/#12-locate-module-5-assignment","title":"1.2 Locate Module 5 Assignment","text":"<p>Step 1 Locate directory where Dockerfile and Readme.md are stored.</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit019_2022\ncd ~/ycit019_2022/Mod5_assignment/\nls\n</code></pre> <p>Result</p> <p>You can see Readme.md where you will document docker commands given in the assignment</p> <pre><code>ls gowebapp-mysql\nls gowebapp\n</code></pre> <p>Result</p> <p>You can see Dockerfiles for gowebapp and  gowebapp-mysql that you will be working with in this Assignment</p> <p>Step 2 Go into your personal Google Cloud Source Repository:</p> <pre><code>cd ~\ncd ~/$student_name-notepad\n</code></pre> <p>Step 3 Copy Mod 5 Assignment <code>Mod5_assignment</code> folder to your personal repo:</p> <pre><code>cp -r ~/ycit019_2022/Mod5_assignment .\n</code></pre> <p>Step 4 Configure Git Parametres</p> <pre><code>git config --global user.email \"you@example.com\"   #You GCP Account User\ngit config --global user.name \"Your Name\"\n</code></pre> <p>Step 5 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"adding Dockerfiles and Readme for Module 5 Assignement\"\n</code></pre> <p>Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"assignment5_dockerbas_backup/#13-review-cloud-source-repositories","title":"1.3 Review Cloud Source Repositories","text":"<p>Use the <code>Google Cloud Source Repositories</code> code browser to view repository files.  You can filter your view to focus on a specific branch, tag, or comment.</p> <p>Step 1 Browse the Mod5_assignment files you pushed to the repository by opening the Navigation menu and selecting Source Repositories:</p> <pre><code>Click Menu -&gt; Source Repositories &gt; Source Code.\n</code></pre> <p>Result</p> <p>The console shows the files in the master branch at the most recent commit.</p> <p>Step 2 View a file in the Google Cloud repository</p> <pre><code>Click $student_name-notepad &gt; gowebapp/Dockerfile to view content of the Dockerfile for gowebapp\n</code></pre> <pre><code>Click $student_name-notepad &gt; gowebapp/Dockerfile to view content of the Dockerfile for gowebapp-mysql\n</code></pre> <pre><code>Click $student_name-notepad &gt; Readme.md to view content of the Readme\n</code></pre>"},{"location":"assignment5_dockerbas_backup/#14-grant-viewing-permissions-for-a-repository-to-instructorsteachers","title":"1.4 Grant viewing permissions for a repository to Instructors/Teachers","text":"<p>Reference document</p> <p>Step 1 This step will grant view access for Instructor to check you assignments</p> <p>In your Cloud Terminal:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID --member='user:ayrat.khayretdinov@gmail.com' --role=roles/viewer\n</code></pre> <p>Result</p> <p>Your instructor will be able to review you code and grade it.</p>"},{"location":"assignment5_dockerbas_backup/#15-browse-and-edit-files-in-cloud-shell-editor","title":"1.5 Browse and edit files in <code>Cloud Shell Editor</code>","text":"<p>Step 1  Browse files in the <code>Google Cloud Source repository</code> </p> <pre><code>edit ~/$student_name-notepad\n</code></pre> <p>Result</p> <p>Editor opens and you can easily modify you code and save it as you go.</p>"},{"location":"assignment5_dockerbas_backup/#2-build-and-deploy-gowebapp-application","title":"2 Build and Deploy gowebapp application","text":""},{"location":"assignment5_dockerbas_backup/#21-overview-of-the-sample-application","title":"2.1 Overview of the Sample Application","text":"<p>This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes:</p> <ul> <li>gowebapp</li> </ul> <p>This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page.</p> <p>Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL.</p> <p>Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises.</p> <p>For more details about the internal design and implementation of the Go web application, see code/README.md.</p> <ul> <li>gowebapp-mysql</li> </ul> <p>This directory contains the schema file used to setup the backing MySQL database for the Go web application.</p>"},{"location":"assignment5_dockerbas_backup/#23-build-dockers-image-for-frontend-application","title":"2.3 Build Dockers image for frontend application","text":"<p>Step 1 Locate and review the go source code:</p> <pre><code>cd ~/$student_name-notepad/Mod5_assignment/\n</code></pre> <p>Result</p> <p>Two folders with go app and mysql config has been reviewed.</p> <p>Step 2 Write <code>Dockerfile</code> for your frontend application</p> <pre><code>cd ~/$student_name-notepad/Mod5_assignment/gowebapp\n</code></pre> <p>Modify a file named <code>Dockerfile</code> in this directory for the frontend Go app. Use Cloud Editor or editor of you choice</p> <pre><code>edit Dockerfile\n</code></pre> <p>The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands:</p> <pre><code>#TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang`\n#https://hub.docker.com/_/golang/\n#https://docs.docker.com/engine/reference/builder/#from\n\n\n#TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels.\n#MAINTAINER should be you student e-mail.\n\n#TODO --- Define a version label for this image\n#https://docs.docker.com/engine/reference/builder/#label\n\nEXPOSE 80\n\nENV GOPATH=/go\n\n#TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp\n#https://docs.docker.com/engine/reference/builder/#copy\n\nWORKDIR $GOPATH/src/gowebapp/\n\nRUN go get &amp;&amp; go install\n\n#TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts\n#https://docs.docker.com/engine/reference/builder/#entrypoint\n</code></pre> <p>Step 4  Build gowebapp Docker image locally</p> <p>Build the  image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message.</p> <pre><code>#TODO  Build image `&lt;your-github-user&gt;/gowebapp:v1\n</code></pre>"},{"location":"assignment5_dockerbas_backup/#24-build-docker-image-for-backend-application","title":"2.4 Build Docker image for backend application","text":"<p>Step 1 Locate folder with mysql config</p> <pre><code>cd ~/$student_name-notepad/Mod5_assignment/gowebapp-mysql\n</code></pre> <p>Step 2 Write Dockerfile for your backend application</p> <p>Create a file named <code>Dockerfile</code> in this directory for the backend MySQL database application. Use Cloud Editor or editor of you choice.</p> <pre><code>edit Dockerfile\n</code></pre> <p>The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands:</p> <pre><code>#TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image\n#https://hub.docker.com/_/mysql/\n#https://docs.docker.com/engine/reference/builder/#from\n\n#TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels.\n#MAINTAINER should be you student e-mail.\n\nLABEL gowebapp-mysql \"v1\"\n\n#TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up\n#https://hub.docker.com/_/mysql/\n#https://docs.docker.com/engine/reference/builder/#copy\n</code></pre> <p>Step 2 Build gowebapp-mysql Docker image locally</p> <pre><code>#TODO  Build image  &lt;your-github-user&gt;/gowebapp-mysql:v1\n</code></pre> <p>Build the  image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally</p>"},{"location":"assignment5_dockerbas_backup/#25-test-application-by-running-with-docker-engine","title":"2.5 Test application by running with Docker Engine.","text":"<p>Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly.</p> <p>Step 1 Create Docker user-defined network</p> <p>To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers:</p> <pre><code>docker network create gowebapp \\\n-d bridge \\\n--subnet 172.19.0.0/16\n</code></pre> <p>Note</p> <p>Default bridge only allows connecting container by IP addresses which is not viable solution as IP address of docker change at startup. There for we creating a user defined bridge network <code>gowebapp</code></p> <p>Step 2  Launch <code>backend</code> container</p> <p>Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable:</p> <pre><code>#TODO Launch `backend` container in background\n#TODO Container needs to run on network: `gowebapp`\n#TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` \n#TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd`\n</code></pre> <p>Step 3  Launch <code>frontend</code> container</p> <p>Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine:</p> <pre><code>#TODO Launch `frontend` container in background\n#TODO Container needs to run on network: `gowebapp`\n#TODO Use this settings: `--name gowebapp` `--hostname gowebapp` \n#TODO Map the container port 80  - to port 8080 on the host machine\n</code></pre> <p>Step 4  Test the application locally</p> <p>Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container.</p> <p>Task</p> <p>Take a screenshot of running application. </p> <p>Step 5 Inspect the MySQL database</p> <p>Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly:</p> <pre><code>#TODO docker xxx\n</code></pre> <p>Step 6 Once inside the container, connect to MySQL database:</p> <pre><code>mysql -u root -p\npassword:\n</code></pre> <p>Note</p> <p>Use password that has beed used in <code>MYSQL_ROOT_PASSWORD</code> env variable.</p> <p>Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence:</p> <pre><code>#Simple SQL to navigate\nSHOW DATABASES;\nUSE gowebapp;\nSHOW TABLES;\nSELECT * FROM &lt;table_name&gt;;\nexit;\n</code></pre>"},{"location":"assignment5_dockerbas_backup/#26-cleanup-running-applications","title":"2.6 Cleanup running applications","text":"<pre><code>### TODO docker xxx\n</code></pre>"},{"location":"assignment5_dockerbas_backup/#3-submit-assignment-to-instructor","title":"3 Submit Assignment to Instructor","text":""},{"location":"assignment5_dockerbas_backup/#31-commit-dockerfiles-and-readmemd-to-repository-and-share-it-with-instructorteacher","title":"3.1 Commit <code>DOCKERFILEs</code> and <code>README.md</code> to repository and share it with Instructor/Teacher","text":"<p>Step 1 Edit Readme.md files with command you've created a working <code>gowebapp</code> application</p> <pre><code>edit  ~/$student_name-notepad/Mod5_assignment/Readme.md\n</code></pre> <p>Step 2 Commit gowebapp and gowebapp-mysql folders and Readme.md using the following Git commands:</p> <pre><code>cd  ~/$student_name-notepad/\n</code></pre> <pre><code>git add .\ngit commit -m \"adding DOCKREFILEs and Readme\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre> <p>Step 3 Submit link to your Cloud Source Repository to LMS, replace with you values</p> <pre><code>https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO\n</code></pre> <p>e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad</p>"},{"location":"assignment6_docker_adv/","title":"1 Containerize Applications","text":"<p>Objective:</p> <ul> <li>Use GCP Cloud Source Repositories</li> <li>Push Images to GCR and DockerHub</li> <li>Automate local Development with Docker-Compose</li> </ul>"},{"location":"assignment6_docker_adv/#prepare-the-cloud-source-repository-environment-with-module-6-assignment","title":"Prepare the Cloud Source Repository Environment with Module 6 Assignment","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p> <p>Cloud Source Repositories: Qwik Start</p> <p>Step 1 Locate directory where <code>docker-compose</code> manifest going to be stored.</p> <pre><code>cd ~/ycit019_2022/\ngit pull       # Pull latest Mod6_assignment\n</code></pre> <p>In case you don't have this folder clone it as following:</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit019_2022\ncd ~/ycit019_2022/Mod6_assignment/\nls\n</code></pre> <p>Step 2 Go into the local repository you've created:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <p>Important</p> <p>Replace above with your project_id student_name</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <p>Step 3 Copy <code>Mod6_assignment</code> folder to your repo:</p> <pre><code>git pull                              # Pull latest code from you repo\ncp -r ~/ycit019_2022/Mod6_assignment/ .\n</code></pre> <p>Step 4 Commit <code>Mod6_assignment</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"adding `Mod6_assignment` with docker-compose manifest\"\n</code></pre> <p>Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre> <p>Step 6 Review Cloud Source Repositories</p> <p>Use the <code>Google Cloud Source Repositories</code> code browser to view repository files.  You can filter your view to focus on a specific branch, tag, or comment.</p> <p>Browse the Mod6_assignment files you pushed to the repository by opening the Navigation menu and selecting Source Repositories:</p> <pre><code>Click Menu -&gt; Source Repositories &gt; Source Code.\n</code></pre> <p>Result</p> <p>The console shows the files in the master branch at the most recent commit.</p>"},{"location":"assignment6_docker_adv/#2-build-and-push-docker-images-to-google-container-registry-gcr","title":"2 Build and push Docker images to Google Container Registry (GCR)","text":""},{"location":"assignment6_docker_adv/#21-build-and-push-gowebapp-mysql-image-to-gcr","title":"2.1 Build and push <code>gowebapp-mysql</code> Image to GCR","text":"<p>Step 1 Locate folder with mysql config</p> <pre><code>cd ~/$student_name-notepad/Mod6_assignment/gowebapp-mysql\n</code></pre> <p>Step 2 Review the existing Dockerfile</p> <pre><code>cat Dockerfile\n</code></pre> <p>output:</p> <pre><code>FROM mysql:8.0\n\nLABEL maintainer \"student@mcgill.ca\"\nLABEL gowebapp-sql \"v1\"\n\nCOPY gowebapp.sql /docker-entrypoint-initdb.d/\n</code></pre> <p>Step 2 Set the Project ID in  Environment Variable: </p> <pre><code>export PROJECT_ID=&lt;project_id&gt;\n</code></pre> <p>Here is how you can find you project_ID:</p> <p></p> <p>Set the project ID as default</p> <pre><code>gcloud config set project $PROJECT_ID\n</code></pre> <p>Step 3 Enable the required APIs:</p> <pre><code>gcloud services enable containerregistry.googleapis.com\n</code></pre> <p>Step 4 Build gowebapp-mysql Docker image with GCR registry address locally</p> <pre><code>docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 .\n</code></pre> <p>Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally</p> <p>Step 5 Push the image to gcr.io:</p> <pre><code>docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1\n</code></pre> <p>Step 6  Login to GCP console -&gt; Container Registry -&gt; Images</p> <p>Result</p> <p>Docker images has been pushed to GCR registry</p>"},{"location":"assignment6_docker_adv/#21-build-and-push-gowebapp-image-to-gcr","title":"2.1 Build and push <code>gowebapp</code> Image to GCR","text":"<p>Step 1 Locate folder with mysql config</p> <pre><code>cd ~/$student_name-notepad/Mod6_assignment/gowebapp\n</code></pre> <p>Step 2 Review the existing Dockerfile</p> <pre><code>cat Dockerfile\n</code></pre> <p>output:</p> <pre><code>FROM golang:1.16.4\n\nLABEL maintainer \"student@mcgill.ca\"\nLABEL gowebapp \"v1\"\n\nEXPOSE 80\n\nENV GO111MODULE=auto\nENV GOPATH=/go\n\nCOPY /code $GOPATH/src/gowebapp/\n\nWORKDIR $GOPATH/src/gowebapp/\n\nRUN go get &amp;&amp; go install\n\nENTRYPOINT $GOPATH/bin/gowebapp\n</code></pre> <p>Note</p> <p>We've updated our application to support golang version 1.16.</p> <p>Step 4 Build <code>gowebapp</code> Docker image with GCR registry address l locally</p> <pre><code>docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 .\n</code></pre> <p>Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally</p> <p>Step 5 Push the image to gcr.io:</p> <pre><code>docker push gcr.io/${PROJECT_ID}/gowebapp:v1\n</code></pre> <p>Step 6  Login to GCP console -&gt; Container Registry -&gt; Images</p> <p>Result</p> <p>Docker images has been pushed to GCR registry</p> <p>Step 7 Delete locally build images, as we want to test how images will be pulled from  gcr registry in the next step:</p> <pre><code>docker rmi gcr.io/$PROJECT_ID/gowebapp:v1\ndocker rmi gcr.io/$PROJECT_ID/gowebapp-mysql:v1\n</code></pre>"},{"location":"assignment6_docker_adv/#23-test-application-by-running-with-docker-engine","title":"2.3 Test application by running with Docker Engine.","text":"<p>Step 1 Create Docker user-defined network</p> <p>To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers:</p> <pre><code>docker network create gowebapp \\\n-d bridge \\\n--subnet 172.19.0.0/16\n</code></pre> <p>Step 2  Launch backend container</p> <p>Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable:</p> <pre><code>docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\\n-d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1\n</code></pre> <p>Step 3 Wait for <code>mysql</code> container to start</p> <pre><code>docker ps\ndocker logs &lt;id&gt;\n</code></pre> <p>You should see following output: <code>[Server] /usr/sbin/mysqld: ready for connections</code></p> <p>Step 3  Launch frontend container</p> <p>Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080  on the host machine:</p> <pre><code>docker run -p 8080:80 --net gowebapp -d --name gowebapp \\\n--hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1\n</code></pre> <p>Step 4  Test the application locally</p> <p>Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p>"},{"location":"assignment6_docker_adv/#24-cleanup-running-applications-and-unused-networks","title":"2.4 Cleanup running applications and unused networks","text":"<pre><code>docker ps\n</code></pre> <pre><code>docker rm -f &lt;container_id_gowebapp&gt; &lt;container_id_gowebapp-mysql&gt;\ndocker network\ndocker network rm gowebapp\n</code></pre>"},{"location":"assignment6_docker_adv/#3-docker-compose","title":"3 Docker Compose","text":""},{"location":"assignment6_docker_adv/#31-test-application-locally-with-docker-compose","title":"3.1 Test application locally with Docker Compose","text":"<p>Task: Automate local testing with <code>Docker Compose</code> by creating <code>docker-compose.yaml</code> file which contains:</p> <ul> <li> <p>User-defined network <code>gowebapp1</code></p> </li> <li> <p>Service <code>gowebapp-mysql</code></p> </li> <li> <p>Service <code>gowebapp</code></p> </li> </ul> <p>Reference</p> <p>Docker Compose v3 documentations</p> <p>Implementation</p> <ol> <li>Ensure that <code>Mysql</code> start first and then <code>webapp</code> services</li> <li>Ensure that <code>Mysql</code> database is fully up prior to start <code>webapp</code> services using healthcheck feature of docker compose.</li> <li>Ensure that <code>webapp-mysql</code>and <code>webapp</code> build with Docker-Compose</li> <li>Ensure that environment variable <code>MYSQL_ROOT_PASSWORD</code> is set inside of the docker compose file.</li> </ol> <p>Step 1 Create compose file</p> <pre><code>cd ~/$student_name-notepad/Mod6_assignment\n</code></pre> <p>Edit existing docker-compose file:</p> <pre><code>edit docker-compose.yaml\n</code></pre> <p>Create structure as following:</p> <pre><code>#TODO Specify docker-compose file '3.4'\n\nservices:\n  gowebapp-mysql:\n    #TODO Make a build in the `gowebapp-mysql` folder\n    #TODO Add Environment variable for MYSQL_DATABASE\n    #TODO Add healthcheck test that you can connect to Database and execute `SHOW DATABASES`\n    #TODO Add healthcheck, timeout: 45s, interval: 10s, retries: 10, start_period 15 sec\n    #TODO Reference doc for healthcheck https://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck\n    #TODO Add Restart  `always`\n    #TODO Attach to created to User-defined network `gowebapp1`\n\n  gowebapp:\n    #TODO Make a build in the appropriate folder\n    #TODO Allocate ports so that contaiener port mapped to local port\n    #TODO Restart container in case failed\n    #TODO Attach to created to User-defined network `gowebapp1`\n    #TODO Add Environment variable for GOPATH\n    #TODO Add Dependency for `gowebapp-mysql` to start first\n\nnetworks:\n#TODO Create User-defined network `gowebapp1` type `driver: bridge`\n</code></pre> <p>Note</p> <p>In this case as we using Compose v3.4, you can also try <code>start_period</code> in the health-check</p> <p>Step 2 Run compose file</p> <pre><code>export CLOUDSDK_PYTHON=python2 # https://github.com/google-github-actions/setup-gcloud/issues/128\n</code></pre> <pre><code>docker-compose up -d\n</code></pre> <p>Step 3  Test the application locally</p> <p>Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Step 4 Tear down environment</p> <pre><code>docker-compose down\n</code></pre> <p>Step 5 Cleanup created networks</p> <pre><code>docker network ls\n</code></pre> <p>Important</p> <p>Make sure <code>gowebapp</code> and <code>gowebapp1</code> networks has been deleted!!!</p>"},{"location":"assignment6_docker_adv/#32-commit-docker-compose-file-to-repository-and-share-it-with-instructorteacher","title":"3.2 Commit docker-compose file to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands:</p> <pre><code>git add .\ngit commit -m \"adding docker-compose.yml\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"assignment6_docker_adv/#33-submit-link-to-your-assignment-to-lms","title":"3.3 Submit link to your assignment to LMS","text":"<p>Submit link to your Cloud Source Repository to LMS, replace with you values</p> <pre><code>https://source.cloud.google.com/${PROJECT_ID}/$student_name-notepad\n</code></pre> <p>e.g: https://source.cloud.google.com/ycit019_2022-project/ayratk-notepad</p>"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/","title":"backup ycit019 Lab 4 Docker Images Docker Hub Quya io","text":"<p>Lab 4 Managing Docker Images</p> <p>Objective:</p> <ul> <li>Learn to build docker images using Dockerfiles.</li> <li>Store images in Docker Hub</li> <li>Learn alternative registry solutions (Quya.io)</li> <li>Automate image build process with Docker Cloud</li> </ul>"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#1-building-docker-images","title":"1 Building Docker Images","text":"<p>In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a registry of Docker images. In this section we will create a simple web application from scratch. We will use Flask (http://flask.pocoo.org/), a microframework for Python. Our application for each request will display a random picture from the defined set.</p> <p>In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay.</p> <p>The code for this application is also available in GitHub:</p> <pre><code>git clone https://github.com/archyufa/k8scanada\n</code></pre>"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#11-create-dockerfile","title":"1.1 Create DOCKERFILE","text":"<p>Step 1 Clone git repo on you laptop:</p> <pre><code>git clone https://github.com/archyufa/k8scanada\ncd k8scanada/Module4/flask-app/\n</code></pre> <p>Step 2 In this directory, we see following files:</p> <pre><code>flask-app/\n    Dockerfile\n    app.py\n    requirements.txt\n    templates/\n        index.html\n</code></pre> <p>Step 3 Let\u2019s review file app.py with the following content:</p> <pre><code>from flask import Flask, render_template\nimport random\n\napp = Flask(__name__)\n\n# list of cat images\nimages = [\n    \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26388-1381844103-11.gif\",\n    \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr01/15/9/anigif_enhanced-buzz-31540-1381844535-8.gif\",\n    \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26390-1381844163-18.gif\",\n    \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-1376-1381846217-0.gif\",\n    \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3391-1381844336-26.gif\",\n    \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-29111-1381845968-0.gif\",\n    \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3409-1381844582-13.gif\",\n    \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr02/15/9/anigif_enhanced-buzz-19667-1381844937-10.gif\",\n    \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26358-1381845043-13.gif\",\n    \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-18774-1381844645-6.gif\",\n    \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-25158-1381844793-0.gif\",\n    \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/10/anigif_enhanced-buzz-11980-1381846269-1.gif\"\n]\n\n@app.route('/')\ndef index():\n    url = random.choice(images)\n    return render_template('index.html', url=url)\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\")\n</code></pre> <p>Step 4 Below is the content of requirements.txt file:</p> <pre><code>Flask==0.10.1\n</code></pre> <p>Step 5 Under directory templates observe index.html with the following content:</p> <pre><code>&lt;html&gt;\n  &lt;head&gt;\n    &lt;style type=\"text/css\"&gt;\n      body {\n        background: black;\n        color: white;\n      }\n      div.container {\n        max-width: 500px;\n        margin: 100px auto;\n        border: 20px solid white;\n        padding: 10px;\n        text-align: center;\n      }\n      h4 {\n        text-transform: uppercase;\n      }\n    &lt;/style&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div class=\"container\"&gt;\n      &lt;h4&gt;Cat Gif of the day&lt;/h4&gt;\n      &lt;img src=\"{{url}}\" /&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Step 6 Let\u2019s review content of the Dockerfile:</p> <pre><code># our base image\nFROM alpine:3.5\n\n# Install python and pip\nRUN apk add --update py2-pip\n\n# upgrade pip\nRUN pip install --upgrade pip\n\n# install Python modules needed by the Python app\nCOPY requirements.txt /usr/src/app/\nRUN pip install --no-cache-dir -r /usr/src/app/requirements.txt\n\n# copy files required for the app to run\nCOPY app.py /usr/src/app/\nCOPY templates/index.html /usr/src/app/templates/\n\n# tell the port number the container should expose\nEXPOSE 5000\n\n# run the application\nCMD [\"python\", \"/usr/src/app/app.py\"]\n</code></pre>"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#12-build-a-docker-image","title":"1.2 Build a Docker image","text":"<p>Step 1 Now let\u2019s build our Docker image. In the command below, replace  with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step. <pre><code>docker build -t &lt;user-name&gt;/myfirstapp .\n</code></pre> <p>Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below:</p> <pre><code>docker images\n</code></pre> <p>Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000:</p> <pre><code>docker run -dp 80:5000 --name myfirstapp &lt;user-name&gt;/myfirstapp\n</code></pre> <p>Step 4 Use your browser to open the address http:// and check that the application works. <p>Step 5 Stop the container and remove it:</p> <pre><code>docker stop myfirstapp\n\ndocker rm myfirstapp\nmyfirstapp\n</code></pre>"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#121-share-docker-images-with-tar-files","title":"1.2.1 Share docker images with tar files","text":"<p>Now ideally you want to share you freshly build docker image with someone or run it in different environment which for some reason don\u2019t have internet access so images can not be pulled from Online Docker registries. In that case Docker images can be shared as we share traditionally regular files by creating tarballs  using docker save  command as following</p> <p>Step 1 Create a tar file using docker save  command:</p> <pre><code>docker save &lt;user-name&gt;/myfirstapp &gt; myfirstapp.tar\n</code></pre> <p>Or</p> <pre><code>docker save --output myfirstapp1.tar archyufa/myfirstapp\nls -trh | grep tar\n</code></pre> <p>Step 2 Transfer images to another environment using scp command.</p> <p>Hint</p> <p>You can also store this images in Object storages, e.g. Swift or Amazon S3 using version control.</p> <p>Step 3 Now you can restore this images using docker load, that will load a tarred repository from a file or the standard input stream. It restores both images and tags.</p> <pre><code>docker load &lt; myfirstapp.tar\n</code></pre> <pre><code>23b9c7b43573: Loading layer [==================================================&gt;]   4.23MB/4.23MB\n\nb3b5c1214f71: Loading layer [==================================================&gt;]  52.87MB/52.87MB\n\nf877d8dd64d3: Loading layer [==================================================&gt;]  8.636MB/8.636MB\n\nbba871f91589: Loading layer [==================================================&gt;]  3.584kB/3.584kB\n\n1c131e92eb5f: Loading layer [==================================================&gt;]  5.053MB/5.053MB\n\n3f6463bcb64c: Loading layer [==================================================&gt;]   5.12kB/5.12kB\n\n47c61110467a: Loading layer [==================================================&gt;]  4.096kB/4.096kB\n\nLoaded image: archyufa/myfirstapp:latest\n</code></pre> <pre><code>docker images\n</code></pre>"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#122-publish-docker-image-to-docker-hub","title":"1.2.2 Publish Docker Image to Docker Hub","text":"<p>However the most popular way to share and work with you images is to push them to the Docker Hub.</p> <p>Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images.</p> <p>Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here. Make a note of your username and password.</p> <p>Step 2 Log in to your local machine.</p> <pre><code>docker login\n</code></pre> <p>Step 3 Now, publish your image to docker Hub.</p> <pre><code>docker push &lt;user-name&gt;/myfirstapp\n</code></pre> <p>Step 4 Login to https://hub.docker.com and verify simage and tags.</p> <p>Result</p> <p>Image been pushed and can be observed in Docker Hub, with the tag latest.</p> <p>Step 5 It is also possible to specify a custom tag for image prior to push it to the registry</p> <pre><code>docker tag 5b45ce063cea &lt;user-name&gt;/myfirstapp:v1\ndocker push &lt;user-name&gt;/myfirstapp:v1\n</code></pre> <p>Result</p> <p>Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1</p> <p>Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands:</p> <pre><code>docker pull &lt;user-name&gt;/myfirstapp:latest\n\ndocker pull &lt;user-name&gt;/myfirstapp:v1\n</code></pre> <p>Result</p> <p>Images stored locally</p> <pre><code>docker images\n</code></pre> <p>Output:</p> <pre><code>myfirstapp       v1       f50f9524513f    1 hour ago   22 MB\n\nmyfirstapp       latest   f50f9524513f    1 hour ago   22 MB\n</code></pre> <p>Finally run images with specific tag:</p> <pre><code>docker run &lt;user-name&gt;/myfirstapp:v1\n</code></pre>"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#123-automated-builds-with-docker-cloud","title":"1.2.3 Automated Builds with Docker Cloud","text":"<p>Live Demo:</p> <ul> <li>Docker Security scanning</li> <li>Setting Up Auto-Build in Docker Cloud and notifications to slac</li> <li>Automated Tests with PR</li> </ul>"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#124-push-docker-images-to-quayio","title":"1.2.4 Push Docker Images to quay.io","text":"<p>Prerequisite: Register to  quay.io with your Github user</p> <p>Step 1 Login to quay.io from CLI</p> <pre><code>docker login quay.io\n</code></pre> <p>Step 2 Build image with quay prefix</p> <pre><code>docker build -t quay.io/archyufa/myfirstapp .\n</code></pre> <p>Step 3 Push image to quay registry</p> <pre><code>docker push quay.io/archyufa/myfirstapp\n</code></pre>"},{"location":"backup_ycit019_Lab_4_Docker_Images_Docker_Hub_Quya_io/#125-demo-quayio","title":"1.2.5 Demo quay.io","text":"<p>Live Demo:</p> <ul> <li>Docker Security scanning</li> <li>Setting Up Auto-Build in Docker Cloud / Quai.io</li> </ul>"},{"location":"glossary_5_docker_basic/","title":"Glossary &amp; Docker Commands","text":""},{"location":"glossary_5_docker_basic/#glossary","title":"Glossary","text":"<ul> <li>Container \u2014 A lightweight, isolated process based on a Docker image.</li> <li>Image \u2014 A template used to create containers, containing everything needed to run an application.</li> <li>Dockerfile \u2014 A text file with instructions to build a Docker image.</li> <li>Volume \u2014 Storage outside of a container to persist data beyond the container lifecycle.</li> <li>Docker Hub \u2014 Cloud service for storing and distributing Docker images.</li> <li>Network \u2014 A virtual network allowing containers to communicate.</li> <li>Docker Daemon \u2014 Background service managing containers and images.</li> </ul>"},{"location":"glossary_5_docker_basic/#common-docker-commands-cheat-sheet","title":"Common Docker Commands Cheat Sheet","text":"Command Description <code>docker run [image]</code> Run a new container from an image <code>docker ps</code> List running containers <code>docker ps -a</code> List all containers (including stopped ones) <code>docker stop [container_id]</code> Stop a running container <code>docker rm [container_id]</code> Remove a container <code>docker images</code> List local Docker images <code>docker rmi [image_id]</code> Remove an image <code>docker build -t [name] .</code> Build an image from a Dockerfile in the current folder <code>docker logs [container_id]</code> View container logs <code>docker exec -it [container_id] bash</code> Open an interactive shell inside a container <code>docker-compose up</code> Run services defined in docker-compose.yml <code>docker network ls</code> List Docker networks"},{"location":"glossary_6_docker_adv/","title":"Glossary &amp; Docker Commands","text":""},{"location":"glossary_6_docker_adv/#glossary","title":"Glossary","text":"<ul> <li>CNM (Container Networking Model) \u2014 Docker's pluggable networking model supporting multiple drivers.</li> <li>Overlay Network \u2014 A network enabling communication between containers across different hosts.</li> <li>Volume \u2014 Persistent storage mechanism for Docker containers.</li> <li>Registry \u2014 A service to store and distribute Docker images (e.g., Docker Hub, GCR).</li> <li>Tag \u2014 Label assigned to an image version, e.g., <code>myapp:1.0.0</code>.</li> <li>Trivy \u2014 Open-source vulnerability scanner for container images.</li> <li>Kaniko \u2014 Tool to build container images without requiring Docker daemon or root privileges.</li> <li>Buildpacks \u2014 Frameworks for building container images directly from application source code.</li> <li>Docker Compose \u2014 Tool for defining and running multi-container Docker applications.</li> <li>CI/CD \u2014 Continuous Integration and Continuous Deployment for automated application delivery.</li> </ul>"},{"location":"glossary_6_docker_adv/#common-docker-commands-cheat-sheet","title":"Common Docker Commands Cheat Sheet","text":"Command Description <code>docker network ls</code> List all Docker networks <code>docker network create mynet</code> Create a custom network <code>docker volume create myvol</code> Create a named volume <code>docker run -v myvol:/data busybox</code> Mount volume inside a container <code>docker tag image user/image:1.0</code> Add a tag to an image <code>docker push user/image:1.0</code> Push image to Docker registry <code>docker-compose up</code> Start all services defined in <code>docker-compose.yml</code> <code>docker-compose down</code> Stop and remove containers, networks, volumes <code>docker build -t myimage:latest .</code> Build an image from Dockerfile <code>trivy image myimage:latest</code> Scan image for vulnerabilities (Trivy)"},{"location":"quiz_5_dockerbas/","title":"Docker Fundamentals Quiz","text":"## Question 1   **What is Docker?**   - [ ] A virtual machine platform   - [x] A platform for developing, shipping, and running applications using containers   - [ ] A cloud storage system   - [ ] A network monitoring tool    ## Question 2   **Which of the following are benefits of Docker? (Select all that apply)**   - [x] Speed   - [x] Simplicity   - [x] Density   - [ ] Closed source    ## Question 3   **What type of architecture does Docker use?**   - [ ] Peer-to-peer   - [x] Client-server   - [ ] Single-tier   - [ ] Monolithic    ## Question 4   **What is a Docker image?**   - [ ] A running container process   - [x] A read-only template used to create containers   - [ ] A container log file   - [ ] A network interface    ## Question 5   **Where are Docker images stored?**   - [ ] Only on local machines   - [x] In registries   - [ ] In GitHub repositories   - [ ] In container logs    ## Question 6   **Which command runs a container from an image?**   - [ ] docker exec   - [x] docker run   - [ ] docker build   - [ ] docker ps    ## Question 7   **Which command shows all containers, including stopped ones?**   - [ ] docker ps   - [x] docker ps -a   - [ ] docker container prune   - [ ] docker image ls    ## Question 8   **What is the difference between CMD and RUN?**   - [ ] CMD executes during build time, RUN during container start   - [x] RUN executes during build time, CMD defines the default command for container start   - [ ] There is no difference   - [ ] CMD is used only on Windows images    Next Retry"},{"location":"quiz_6_docker_adv/","title":"Docker Advanced Quiz","text":"## Question 1   **This Docker tool provides cluster management and lets you orchestrate, scale and load balance Docker Containers across multiple nodes.**   - [ ] Docker Engine   - [ ] Docker Compose   - [ ] Notary   - [x] Docker Swarm   - [ ] Docker Hub   - [ ] Docker Machine   - [ ] Docker Cloud   - [ ] Docker Desktop    ## Question 2   **This Docker tool allows you to store, distribute and even scan your Public and Private Docker images.**   - [ ] Docker Engine   - [ ] Docker Compose   - [ ] Notary   - [ ] Docker Swarm   - [x] Docker Hub   - [ ] Docker Machine   - [ ] Docker Cloud   - [ ] Docker Desktop    ## Question 3   **This Docker tool that has been donated to CNCF lets you implement secure software distribution by ensuring that all pulled docker images are signed, correct and untampered, which is one of the major security concerns for Docker-based deployments.**   - [ ] Docker Engine   - [ ] Docker Compose   - [x] Notary   - [ ] Docker Swarm   - [ ] Docker Hub   - [ ] Docker Machine   - [ ] Docker Cloud   - [ ] Docker Desktop    ## Question 4   **This Docker tool lets you create and start one or more containers for each dependency specified in the manifest with a single command, and thus can be used in automated testing or quick local development environment deployment use cases.**   - [ ] Docker Engine   - [x] Docker Compose   - [ ] Notary   - [ ] Docker Swarm   - [ ] Docker Hub   - [ ] Docker Machine   - [ ] Docker Cloud   - [ ] Docker Desktop    ## Question 5   **This Docker tool let you easily deploy docker engine, compose and kubernetes on your laptop and let you build, share and deploy containerized applications on your local Windows, Mac machines.**   - [ ] Docker Engine   - [ ] Docker Compose   - [ ] Notary   - [ ] Docker Swarm   - [ ] Docker Hub   - [ ] Docker Machine   - [ ] Docker Cloud   - [x] Docker Desktop    ## Question 6   **This Docker tool let you build and deploy Docker Containers on a single host using docker CLI:**   - [x] Docker Engine   - [ ] Docker Compose   - [ ] Notary   - [ ] Docker Swarm   - [ ] Docker Hub   - [ ] Docker Machine   - [ ] Docker Cloud   - [ ] Docker Desktop    ## Question 7   **In what language is Docker written in?**   - [ ] Java   - [ ] Python   - [x] Go   - [ ] Rust   - [ ] .NET Core   - [ ] All above    ## Question 8   **What is the high level container runtime used by Docker Engine?**   - [ ] Docker   - [x] Containerd   - [ ] Runc   - [ ] CRI-O    ## Question 9   **What is the host port on which the web application will be exposed?**   ![Web app port](/images/quiz_6_docker_adv.png)   - [ ] 80   - [x] 8080   - [ ] v1   - [ ] foobar    ## Question 10   **Which of the following statements are true?**   ![Compose info](/images/quiz_6_docker_adv.png)   - [ ] All of the web, redis and db images will be built before deploying containers.   - [ ] The redis image will be built and the web image will be pulled from Dockerhub if it doesn\u2019t already exist on the host.   - [x] The web image will be built and the redis image will be pulled from Dockerhub if it doesn\u2019t already exist on the host.   - [ ] All images will be pulled from Dockerhub.    ## Question 11   **Which is the correct statement referring to the following Compose file?**   ![Compose file](/images/quiz_6_docker_adv.png)   - [ ] The depends_on configuration is not supported in Compose version 3   - [x] db and redis services will be started before web service   - [ ] web service will be started before db and redis services   - [ ] None of the above    ## Question 12   **Which command can be used to create and start containers in background or in detached mode in compose using the existing docker-compose.yml?**   - [ ] docker-compose up   - [ ] docker-compose up --background   - [ ] docker-compose up -t   - [x] docker-compose up -d    ## Question 13   **Which command can be used to stop (only and not delete) the whole stack of containers created by a compose file?**   - [ ] docker-compose down   - [x] docker-compose stop   - [ ] docker-compose destroy   - [ ] docker-compose halt    ## Question 14   **Select the right answer. Which command can be used to delete the application stack created using a compose file?**   - [ ] docker-compose rm   - [ ] docker-compose stop   - [x] docker-compose down   - [ ] docker-compose destroy    ## Question 15   **With the docker-compose up command, we can run containers on multiple docker hosts.**   - [ ] True   - [x] False    ## Question 16   **Using Docker Compose, we can configure containers and the communication between them in a declarative way.**   - [x] True   - [ ] False    ## Question 17   **What is the default public registry for docker?**   - [x] Docker Hub   - [ ] Amazon Container Registry   - [ ] Google Container Registry   - [ ] Docker Trusted Registry    ## Question 18   **If we do not specify a tag when building an image, what is the default tag that will be used?**   - [ ] none   - [ ] default   - [x] Latest   - [ ] v1    ## Question 19   **Which of the following commands we can use to run an ubuntu container with the trusty tag.**   - [ ] docker run ubuntu   - [ ] docker run ubuntu:latest   - [x] docker run ubuntu:trusty   - [ ] docker run ubuntu -t trusty    ## Question 20   **Which of the following commands you would use to download an nginx image from the Google Container Registry.**   - [ ] docker image pull nginx   - [ ] docker image build nginx   - [ ] docker image load nginx   - [x] docker pull gcr.io/google-containers/nginx    ## Question 21   **How did cgroups enable containers in their current shape?**   - [ ] cgroups allow isolating processes in their own namespaces   - [ ] cgroups allow controlling Linux capabilities for the processes   - [x] cgroups allow setting resource limits and constraints for the processes   - [ ] cgroups wrap the PID of a process preventing other processes from accessing it    Next Retry"},{"location":"readingmat_5/","title":"Module 5 Assignment - Docker Setup &amp; Application Deployment","text":""},{"location":"readingmat_5/#1-explore-docker-installation-options","title":"1. Explore Docker Installation Options","text":"<p>Explore various options to install Docker locally on your laptop:  </p> <ul> <li> <p>Docker Desktop  -  Used to be popular but now requires a paid license. </p> </li> <li> <p>Colima - Container runtime for macOS (Intel and M1 Macs) and Linux  </p> <ul> <li>Simple CLI interface, Docker and Containerd support  </li> <li>Port forwarding, volume mounts, Kubernetes  </li> <li>Designed as a replacement for Docker Desktop  </li> </ul> </li> <li> <p>Podman - Container Swiss-Army knife from RedHat  </p> <ul> <li>Supports multiple image formats including OCI and Docker  </li> <li>Full container lifecycle management and image layer management  </li> <li>Podman 3.4+ supports M1 Apple Macs  </li> <li>Alternative to Docker Desktop  </li> </ul> </li> </ul> <p>Task: Set up Docker on your laptop using one of the above options (or another solution you find). Please let us know which one you deployed at the end. To pass this task: provide a screenshot.  </p>"},{"location":"readingmat_5/#2-docker-hub-account","title":"2. Docker Hub Account","text":"<p>Create a free Docker Hub account: https://hub.docker.com/ </p> <p>Task: Provide your Docker ID or a screenshot of your account page.  </p>"},{"location":"readingmat_5/#3-read-on-cmd-and-entrypoint","title":"3. Read on CMD and ENTRYPOINT","text":"<p>Resources: - Demystifying ENTRYPOINT &amp; CMD - Dockerfile CMD reference - Dockerfile ENTRYPOINT reference - ENTRYPOINT Glossary </p> <p>Task: - Explain when to use CMD - Explain when to use ENTRYPOINT - Explain when to use ENTRYPOINT in combination with CMD </p>"},{"location":"readingmat_6/","title":"Docker CMD, ENTRYPOINT &amp; Additional Topics","text":""},{"location":"readingmat_6/#1-read-on-cmd-and-entrypoint-usage","title":"1. Read on CMD and ENTRYPOINT Usage","text":"<p>Resources: - CMD in Dockerfile - ENTRYPOINT in Dockerfile - ENTRYPOINT Glossary </p> <p>Task: - Explain when to use CMD - Explain when to use ENTRYPOINT - Explain when to use ENTRYPOINT in combination with CMD</p>"},{"location":"readingmat_6/#2-vim-101-for-assignment","title":"2. Vim - 101 (For Assignment)","text":"<p>If you plan to use Vim for editing during the assignment, see: - Vim 101 \u2013 Beginner\u2019s Guide</p>"},{"location":"readingmat_6/#3-review-docker-network-drivers","title":"3. Review Docker Network Drivers","text":"<p>Resource: - Docker Network Drivers</p>"},{"location":"readingmat_6/#4-extra-reading-no-tasks","title":"4. Extra Reading (No Tasks)","text":""},{"location":"readingmat_6/#41-docker-hub-rate-limitations","title":"4.1 Docker Hub Rate Limitations","text":"<ul> <li>Docker Hub rate limits (introduced November 20, 2020)</li> </ul>"},{"location":"readingmat_6/#42-docker-logging-drivers","title":"4.2 Docker Logging Drivers","text":"<ul> <li>Configure Logging Drivers</li> </ul>"},{"location":"readingmat_6/#43-overlay-vs-overlay2-storage-drivers","title":"4.3 Overlay vs Overlay2 Storage Drivers","text":"<ul> <li>Overlay Storage Driver </li> <li>Overlay2 Storage Driver </li> <li>How Container Reads/Writes Work</li> </ul>"},{"location":"readingmat_6/#44-docker-volumes","title":"4.4 Docker Volumes","text":"<ul> <li>Docker Storage Overview </li> <li>Docker Volume Command Reference</li> </ul>"},{"location":"ycit019_Lab_10_Networking/","title":"K8s Networking","text":"<p>Objective:</p> <ul> <li>Kubernetes <code>Network Policy</code></li> <li>Run applications and expose them via service via <code>Ingress</code> resource.</li> </ul>"},{"location":"ycit019_Lab_10_Networking/#0-create-gke-cluster","title":"0 Create GKE Cluster","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud container clusters create k8s-networking \\\n--zone us-central1-c \\\n--enable-network-policy \\\n--num-nodes 2\n</code></pre> <p>Note</p> <p>To use GKE Ingress, you must have the HTTP(S) Load Balancing add-on enabled. GKE clusters have HTTP(S) Load Balancing enabled by default;</p> <p>Output:</p> <pre><code>NAME          LOCATION       MASTER_VERSION   MASTER_IP      MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\nk8s-scaling  us-central1-c  1.19.9-gke.1400  34.121.222.83  e2-medium     1.19.9-gke.1400  2          RUNNING\n</code></pre> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials k8s-networking --zone us-central1-c\n</code></pre>"},{"location":"ycit019_Lab_10_Networking/#1-network-policy","title":"1. Network Policy","text":""},{"location":"ycit019_Lab_10_Networking/#11-basic-policy-demo","title":"1.1 Basic Policy Demo","text":"<p>This guide will deploy pods in a Kubernetes Namespaces. Let\u2019s create the Namespace object for this guide.</p> <p>Step 1 Create <code>policy-demo</code> Namespace:</p> <pre><code>kubectl create ns policy-demo\n</code></pre> <p>Step 2 Than create a <code>policy-demo</code> nginx deployment in <code>policy-demo</code> namespace:</p> <pre><code>kubectl create deployment --namespace=policy-demo nginx --replicas=2 --image=nginx\n</code></pre> <p>Then create a <code>policy-demo</code> service:</p> <pre><code>kubectl expose --namespace=policy-demo deployment nginx --port=80\n</code></pre> <p>Step 3 Ensure  nginx service is accessible. In order to do that create <code>busybox</code> pod and try to access the <code>nginx</code> service.</p> <pre><code>kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh\n</code></pre> <p>Output: </p> <pre><code>Waiting for pod `policy-demo/access-472357175-y0m47` to be running, status is\nPending, pod ready: false\n</code></pre> <p>If you don't see a command prompt, try pressing enter.</p> <pre><code>/ # wget -q nginx -O -\n</code></pre> <p>Success</p> <p>You should see a response from <code>nginx</code>. Great! Our service is accessible. You can exit the Pod now.</p> <p>Result</p> <p>Pods in a given namespace can be accessed by anyone.</p> <p>Step 4 Now let\u2019s turn <code>on</code> isolation in our policy-demo Namespace. Calico will then prevent connections to pods in this Namespace. In order for Calico to prevent connection to pods in a namespace, we first need to enable network isolation in the namespace by creating following <code>NetworkPolicy</code>:</p> <pre><code>kubectl create -f - &lt;&lt;EOF\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: default-deny\n  namespace: policy-demo\nspec:\n  podSelector:\n    matchLabels: {}\nEOF\n</code></pre> <p>Note</p> <p>Current Lab is using Calico v3.0. Older versions of Calico (2.1 and prior) used namespace annotation to deny all traffic.</p> <p>Step 5 Verify that all access to the nginx Service is blocked. We can see the effect by trying to access the Service again.</p> <p>In order to do that, run a <code>busybox</code> deployment and try to access the <code>nginx</code> service.</p> <pre><code>kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh\n</code></pre> <pre><code>Waiting for pod policy-demo/access-472357175-y0m47 to be running, status is\nPending, pod ready: false\n</code></pre> <p>If you don't see a command prompt, try pressing enter.</p> <pre><code>/ # wget -q --timeout=5 nginx -O -\n</code></pre> <p>Result</p> <p><code>download timed out</code></p> <p>The request should time out after 5 seconds. By enabling isolation on the Namespace, we\u2019ve prevented access to the Service.</p> <p>Step 6 Allow Access using a NetworkPolicy</p> <p>Now, let\u2019s enable access to the nginx Service using a NetworkPolicy. This will allow incoming connections from our access Pod, but not from anywhere else.</p> <p>Create a network policy <code>access-nginx</code> with the following contents:</p> <pre><code>kubectl create -f - &lt;&lt;EOF\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: access-nginx\n  namespace: policy-demo\nspec:\n  podSelector:\n    matchLabels:\n      app: nginx\n  ingress:\n    - from:\n      - podSelector:\n          matchLabels:\n            run: access\nEOF\n</code></pre> <p>Notice</p> <p>The NetworkPolicy allows traffic from Pods with the label <code>run: access</code> to Pods with the label <code>app: nginx</code>. The labels are automatically added by kubectl and are based on the name of the resource.</p> <p>Step 7: We should now be able to access the Service from the <code>access</code> Pod.</p> <p>In order to check that create <code>busybox</code> deployment and try to access the <code>nginx</code> Service.</p> <pre><code>kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh\n</code></pre> <pre><code>Waiting for pod policy-demo/access-472357175-y0m47 to be running, status is\nPending, pod ready: false\n</code></pre> <p>If you don't see a command prompt, try pressing enter.</p> <pre><code>/ # wget -q --timeout=5 nginx -O -\n</code></pre> <p>Step 8 Without closing a prompt to a container, open a new terminal and run</p> <pre><code>kubectl get pods --show-labels -n policy-demo \n</code></pre> <p>Output: </p> <pre><code>NAME                     READY   STATUS    RESTARTS   AGE   LABELS\naccess                   1/1     Running   0          5s    run=access\nnginx-6799fc88d8-5r64l   1/1     Running   0          17m   app=nginx,pod-template-hash=6799fc88d8\nnginx-6799fc88d8-dbtgk   1/1     Running   0          17m   app=nginx,pod-template-hash=6799fc88d8\n</code></pre> <p>Result</p> <p>We can see the Labels <code>run=access</code> and <code>app=nginx</code>, that has been defined in Network Policy.</p> <p>Step 9 However, we still cannot access the Service from a Pod without the label <code>run: access:</code></p> <p>Once again run <code>busybox</code> deployment and try to access the <code>nginx</code> service.</p> <pre><code>kubectl run --namespace=policy-demo cant-access --rm -ti --image busybox /bin/sh\n</code></pre> <pre><code>Waiting for pod policy-demo/cant-access-472357175-y0m47 to be running, status\nis Pending, pod ready: false\n</code></pre> <p>If you don't see a command prompt, try pressing enter.</p> <pre><code>/ # wget -q --timeout=5 nginx -O -\nwget: download timed out\n/ #\n</code></pre> <p>Step 9 You can clean up the demo by deleting the demo Namespace:</p> <pre><code>kubectl delete ns policy-demo\n</code></pre>"},{"location":"ycit019_Lab_10_Networking/#12-demo-stars-policy","title":"1.2 (Demo) Stars Policy","text":""},{"location":"ycit019_Lab_10_Networking/#121-deploy-3-tier-app","title":"1.2.1 Deploy 3 tier-app","text":"<p>Let's Deploy 3 tier-app: UI, frontend and backend service, as well as a client service.  And configures network policy on each service.</p> <p>Step 1 Deploy <code>Stars</code> Namespace and <code>management-ui</code> apps inside it:</p> <pre><code>kubectl create -f - &lt;&lt;EOF\n---\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: stars\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: management-ui\n  labels:\n    role: management-ui\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: management-ui\n  namespace: management-ui\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 9001\n    targetPort: 9001\n  selector:\n    role: management-ui\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: management-ui\n  labels:\n    role: management-ui\n  namespace: management-ui\nspec:\n  selector:\n    matchLabels:\n      role: management-ui\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        role: management-ui\n    spec:\n      containers:\n      - name: management-ui\n        image: calico/star-collect:v0.1.0\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 9001\nEOF\n</code></pre> <p>Step 2 Deploy <code>backend</code> application inside of the  <code>stars</code> Namespace:</p> <p>Backend:</p> <pre><code>kubectl create -f - &lt;&lt;EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend\n  namespace: stars\nspec:\n  ports:\n  - port: 6379\n    targetPort: 6379\n  selector:\n    role: backend\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\n  labels:\n    role: backend\n  namespace: stars\nspec:\n  selector:\n    matchLabels:\n      role: backend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        role: backend\n    spec:\n      containers:\n      - name: backend\n        image: calico/star-probe:v0.1.0\n        imagePullPolicy: Always\n        command:\n        - probe\n        - --http-port=6379\n        - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status\n        ports:\n        - containerPort: 6379\nEOF\n</code></pre> <p>Step 3 Deploy <code>frontend</code> application inside of the <code>stars</code> Namespace:</p> <pre><code>kubectl create -f - &lt;&lt;EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\n  namespace: stars\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    role: frontend\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    role: frontend\n  namespace: stars\nspec:\n  selector:\n    matchLabels:\n      role: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        role: frontend\n    spec:\n      containers:\n      - name: frontend\n        image: calico/star-probe:v0.1.0\n        imagePullPolicy: Always\n        command:\n        - probe\n        - --http-port=80\n        - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Step 4 Finally deploy <code>client</code> application inside of the <code>client</code> Namespace:</p> <pre><code>kubectl create -f - &lt;&lt;EOF\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: client\n  labels:\n    role: client\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\n  labels:\n    role: client\n  namespace: client\nspec:\n  selector:\n    matchLabels:\n      role: client\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        role: client\n    spec:\n      containers:\n      - name: client\n        image: calico/star-probe:v0.1.0\n        imagePullPolicy: Always\n        command:\n        - probe\n        - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status\n        ports:\n        - containerPort: 9000\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: client\n  namespace: client\nspec:\n  ports:\n  - port: 9000\n    targetPort: 9000\n  selector:\n    role: client\nEOF\n</code></pre> <p>Step 5 Wait for all the pods to enter <code>Running</code> state.</p> <pre><code>kubectl get pods --all-namespaces --watch\n</code></pre> <p>Note</p> <p>It may take several minutes to download the necessary Docker images for this demo.</p> <p>Step 6 Locate LoadBalancer IP:</p> <pre><code>kubectl get svc -n management-ui\n</code></pre> <p>Step 9 Open UI in you browser <code>http://EXTERNAL-IP:9001</code> in a browser.</p> <p>Result</p> <p>Once all the pods are started, they should have full connectivity. You can see this by visiting the UI.  Each service is represented by a single node in the graph.</p> <ul> <li><code>backend</code> -&gt; Node \"B\"</li> <li><code>frontend</code> -&gt; Node \"F\"</li> <li><code>client</code> -&gt; Node \"C\"</li> </ul>"},{"location":"ycit019_Lab_10_Networking/#122-enable-isolation","title":"1.2.2 Enable isolation","text":"<p>Step 1 Enable isolation</p> <p>Running the following commands will prevent all Ingress access to the frontend, backend, and client Services located in namespaces <code>starts</code> and <code>client</code></p> <pre><code>kubectl create -f - &lt;&lt;EOF\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: default-deny\n  namespace: stars\nspec:\n  podSelector:\n    matchLabels: {}\n  ingress: []\n---\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: default-deny\n  namespace: client\nspec:\n  podSelector:\n    matchLabels: {}\n  policyTypes:\n  - Ingress\nEOF\n</code></pre> <p>Result</p> <p>Now that we've enabled isolation, the UI can no longer access the pods, and so they will no longer show up in the UI.</p> <p>Note</p> <p>You need to Refresh web-browser to see result.</p> <p>Step 2 Allow apps from <code>stars</code> and <code>client</code> namespace access <code>UI</code> service via <code>NetworkPolicy</code> rules:</p> <pre><code>kubectl create -f - &lt;&lt;EOF\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  namespace: stars\n  name: allow-ui\nspec:\n  podSelector:\n   matchLabels: {}\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              role: management-ui\n---\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  namespace: client\n  name: allow-ui\nspec:\n  podSelector:\n   matchLabels: {}\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              role: management-ui\nEOF\n</code></pre> <p>Result</p> <p>refresh the UI - it should now show the Services, but they should not be able to access each other any more.</p> <p>Step 4 Create the <code>NetworkPolicy</code> to allow traffic from the frontend to the backend.</p> <pre><code>kubectl create -f - &lt;&lt;EOF\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  namespace: stars\n  name: backend-policy\nspec:\n  podSelector:\n    matchLabels:\n      role: backend\n  ingress:\n    - from:\n        - podSelector:\n            matchLabels:\n              role: frontend\n      ports:\n        - protocol: TCP\n          port: 6379\nEOF\n</code></pre> <p>Result</p> <p>Refresh the UI.  You should see the following:</p> <ul> <li>The frontend can now access the backend (on TCP port 80 only).</li> <li>The backend cannot access the frontend at all.</li> <li>The client cannot access the frontend, nor can it access the backend.</li> </ul> <p>Step 5 Expose the frontend service to the <code>client</code> namespace.</p> <pre><code>kubectl create -f - &lt;&lt;EOF\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  namespace: stars\n  name: frontend-policy\nspec:\n  podSelector:\n    matchLabels:\n      role: frontend\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              role: client\n      ports:\n        - protocol: TCP\n          port: 80\nEOF\n</code></pre> <p>Success</p> <p>We isolated our app using <code>NetworkPolicy</code> ingress rules so that:</p> <ul> <li>The client can now access the frontend, but not the backend.</li> <li>Neither the frontend nor the backend can initiate connections to the client.</li> <li>The frontend can still access the backend.</li> </ul>"},{"location":"ycit019_Lab_10_Networking/#123-cleanup","title":"1.2.3 Cleanup","text":"<p>Step 1 You can clean up the demo by deleting the demo Namespaces:</p> <pre><code>kubectl delete ns client stars management-ui\n</code></pre>"},{"location":"ycit019_Lab_10_Networking/#2-ingress","title":"2. Ingress","text":"<p>In GKE, Ingress is implemented using Cloud Load Balancing. In other words, when yiu create an Ingress resource in your cluster, GKE creates an <code>HTTP(S)</code> LB for you and configure it. In this lab, we will create a fanout Ingress with two backends for two verisons of the <code>hello-app</code> application.</p> <p>Note</p> <p>If you want to experiment with a different type of Ingress, you can install <code>Nginx Controller</code> using Helm. You can also follow these [instructions] (https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md).</p>"},{"location":"ycit019_Lab_10_Networking/#21-deploy-an-application","title":"2.1 Deploy an Application","text":"<p>Step 1 Deploy the web application version 1, and its service</p> <pre><code>cat &lt;&lt;EOF &gt; web-service-v1.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-v1\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      run: web\n      version: v1\n  template:\n    metadata:\n      labels:\n        run: web\n        version: v1\n    spec:\n      containers:\n      - image: gcr.io/google-samples/hello-app:1.0\n        imagePullPolicy: IfNotPresent\n        name: web\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-v1\n  namespace: default\nspec:\n  ports:\n  - port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    run: web\n    version: v1\n  type: NodePort\nEOF\n</code></pre> <p>Step 2 Apply the resources to the cluster</p> <pre><code>kubectl apply -f web-service-v1.yaml\n</code></pre> <p>Step 3 Deploy the web application version 2, and its service</p> <pre><code>cat &lt;&lt;EOF &gt; web-service-v2.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-v2\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      run: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        run: web\n        version: v2\n    spec:\n      containers:\n      - image: gcr.io/google-samples/hello-app:2.0\n        imagePullPolicy: IfNotPresent\n        name: web\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-v2\n  namespace: default\nspec:\n  ports:\n  - port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    run: web\n    version: v2\n  type: NodePort\nEOF\n</code></pre> <p>Step 4 Apply the resources to the cluster</p> <pre><code>kubectl apply -f web-service-v2.yaml\n</code></pre> <p>Step 5 Take a look at the pods created and view their IPs</p> <pre><code>kubectl get pod -o wide\n</code></pre> <p>Step 6 Take a look at the endpoints created and notice how they match the IP for the Pods in the matching service.</p> <pre><code>kubectl get ep\n</code></pre>"},{"location":"ycit019_Lab_10_Networking/#22-creating-single-service-ingress-rule","title":"2.2 Creating Single Service Ingress rule","text":"<p>Let's expose our web-v1 service using Ingress!</p> <p>Step 1 To start, let's create an Ingress resource that directs traffic to v1 of the <code>web</code> Service</p> <pre><code>cat &lt;&lt;EOF &gt; single-svc-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n name: single-svc-ingress\nspec:\n  defaultBackend:\n    service:\n      name: web-v1\n      port:\n        number: 8080\nEOF\n</code></pre> <p>This manifest defines a <code>Single Service Ingress rule</code>, which makes sure all HTTP requests that will hit the external IP for the Cloud LB created for the Ingress, will be proxied to the <code>web-v1</code> service on port <code>8080</code>.</p> <p>Step 2 Create the Ingress resource</p> <pre><code>kubectl apply -f single-svc-ingress.yaml\n</code></pre> <p>Step 3  Verify Ingress Resource</p> <pre><code>kubectl get ing\n</code></pre> <p>Output</p> <pre><code>NAME                 CLASS    HOSTS   ADDRESS          PORTS   AGE\nsingle-svc-ingress   &lt;none&gt;   *       34.117.137.144   80      110s\n</code></pre> <p>Notice that the Ingress controllers and load balancers may take a minute or two to allocate an IP address.</p> <p>Bingo!</p> <p>Step 4 descirbe the Ingress object created and notice the Google Cloud LB resources created.</p> <pre><code>kubectl describe ing single-svc-ingress\n</code></pre> <p>Notice that you might need to give it some time to get a similar output to what I have below. Output:</p> <pre><code>Name:             single-svc-ingress\nNamespace:        default\nAddress:          34.117.137.144\nDefault backend:  web-v1:8080 (10.32.0.10:8080)\nRules:\n  Host        Path  Backends\n  ----        ----  --------\n  *           *     web-v1:8080 (10.32.0.10:8080)\nAnnotations:  ingress.kubernetes.io/backends: {\"k8s-be-30059--a2b52ac0331abe29\":\"HEALTHY\"}\n              ingress.kubernetes.io/forwarding-rule: k8s2-fr-8rwy2qu4-default-single-svc-ingress-74hvjigm\n              ingress.kubernetes.io/target-proxy: k8s2-tp-8rwy2qu4-default-single-svc-ingress-74hvjigm\n              ingress.kubernetes.io/url-map: k8s2-um-8rwy2qu4-default-single-svc-ingress-74hvjigm\nEvents:\n  Type    Reason     Age                  From                     Message\n  ----    ------     ----                 ----                     -------\n  Normal  Sync       10m                  loadbalancer-controller  UrlMap \"k8s2-um-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created\n  Normal  Sync       10m                  loadbalancer-controller  TargetProxy \"k8s2-tp-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created\n  Normal  Sync       10m                  loadbalancer-controller  ForwardingRule \"k8s2-fr-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created\n  Normal  IPChanged  10m                  loadbalancer-controller  IP is now 34.117.137.144\n  Normal  Sync       4m27s (x6 over 11m)  loadbalancer-controller  Scheduled for sync\n</code></pre> <p>Step 5 Access deployed app via Ingress by navigating to the public IP of the ingress, which we got in step 3.</p> <p>Step 6 Finally execute the following command to cleanup the ingress.</p> <pre><code>kubectl delete -f single-svc-ingress.yaml\n</code></pre>"},{"location":"ycit019_Lab_10_Networking/#23-simple-fanout","title":"2.3 Simple fanout","text":"<p>Let's explore more sophisticated Ingress rules. This time we going to deploy <code>Simple fanout</code>type that allows for exposing multiple services on same host, but via different paths. This type is very handy when you running in CloudProvider and want to cut cost on creating LoadBalancers for each of you application. Or when running on prem. and it's required to expose multiple services via same host.</p> <p>Step 1 Let's start by a simple fanout with one service. Notice that we are not using a <code>hostname</code> here as for this lab, we don't want to go through setting up DNS. If you have a DNS setup, you can try with an actual <code>hostname</code>. Just make sure to update you DNS records with the Ingress external IP.</p> <pre><code>cat &lt;&lt;EOF &gt; fanout-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n name: fanout-ingress\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /v1\n        pathType: ImplementationSpecific\n        backend:\n          service:\n            name: web-v1\n            port:\n              number: 8080\nEOF\n</code></pre> <p>Step 2 Create the Ingress resource</p> <pre><code>kubectl apply -f fanout-ingress.yaml\n</code></pre> <p>Step 3  Verify Ingress Resource</p> <pre><code>kubectl get ing fanout-ingress\n</code></pre> <p>Step 4 Verify that you can navigate to http://INGRESS_PUBLIC_IP/v1 and get the following</p> <pre><code>Hello, world!\nVersion: 1.0.0\n</code></pre> <p>Step 5 Verify that ingress is returning <code>default backend - 404</code> page in a browser.</p> <p>Open the browser with public_ip:</p> <pre><code>http://INGRESS_PUBLIC_IP/test\n</code></pre> <p>Step 6 On your own modify the Ingress resource to add a second path so when we navigate to  <code>http://INGRESS_PUBLIC_IP/v2</code> users will get routed to <code>web-v2</code></p> <p>Step 7 Delete the demo Finally execute the following command to cleanup the application and ingress.</p> <pre><code>kubectl delete -f fanout-ingress.yaml\nkubectl apply -f web-service-v1.yaml\nkubectl apply -f web-service-v2.yaml\n</code></pre>"},{"location":"ycit019_Lab_10_Networking/#24-cleaning-up","title":"2.4 Cleaning Up","text":"<p>Step 1 Delete the cluster</p> <pre><code>gcloud container clusters delete k8s-networking\n</code></pre>"},{"location":"ycit019_Lab_11_Storage/","title":"Kubernetes Storage Concepts","text":"<p>Objective:</p> <ul> <li>Create a PersistentVolume (PV) referencing a disk in your environment.</li> <li>Learn how to <code>dynamically provision</code> volumes.</li> <li>Create a Single MySQL <code>Deployment</code> based on the <code>Volume Claim</code></li> <li>Deploy a Replicated MySQL (Master/Slaves) with a <code>StatefulSet</code> controller.</li> </ul>"},{"location":"ycit019_Lab_11_Storage/#0-create-regional-gke-cluster","title":"0 Create Regional GKE Cluster","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud container clusters create k8s-storage \\\n--region us-central1 \\\n--enable-network-policy \\\n--num-nodes 2 \\\n--machine-type \"e2-standard-2\" \\\n --node-locations \"us-central1-b\",\"us-central1-c\"\n</code></pre> <p>Note</p> <p>We created a Regional cluster with Nodes deployed in  \"us-central1-b\" and \"us-central1-c\" zones.</p> <p>Output:</p> <pre><code>NAME          LOCATION       MASTER_VERSION   MASTER_IP      MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\nk8s-storage  us-central1-c  1.19.9-gke.1400  34.121.222.83  e2-medium     1.19.9-gke.1400  2          RUNNING\n</code></pre> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials k8s-storage --region us-central1 --project jfrog2021\n</code></pre>"},{"location":"ycit019_Lab_11_Storage/#1-dynamically-provision-volume","title":"1 Dynamically Provision Volume","text":"<p>Our Lab already has provisioned Default Storageclass created by Cluster Administrator.</p> <p>Step 1 Verify what storage class is used in our lab:</p> <pre><code>kubectl get sc\n</code></pre> <p>Output:</p> <pre><code>NAME                 PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\npremium-rwo          pd.csi.storage.gke.io   Delete          WaitForFirstConsumer   true                   5m2s\nstandard (default)   kubernetes.io/gce-pd    Delete          Immediate              true                   5m2s\nstandard-rwo         pd.csi.storage.gke.io   Delete          WaitForFirstConsumer   true                   5m2s\n</code></pre> <p>Info</p> <p>The PROVISIONER field determines what volume plugin is used for provisioning PVs.</p> <ul> <li><code>standard</code> provisions  <code>standard</code> GCP PDs  (In-tree volume plugin)</li> <li><code>standard-rwo</code>  provisions <code>balanced</code> GCP persistent disk (CSI based)</li> <li><code>premium-rwo</code> provisions GCP SSD PDs (CSI based)</li> </ul> <p>Info</p> <p>The RECLAIMPOLICY field tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted</p> <ul> <li><code>Delete</code> reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk</li> <li><code>Retain</code> reclaim policy allows for manual reclamation of the resource. When the persistent volume is released (this happens when you delete the claim that\u2019s bound to it), Kubernetes retains the volume. The cluster administrator must manually reclaim the volume. This is the default policy for manually created persistent volumes aka Static Provisioners</li> <li><code>Recycle</code> - This option is deprecated and shouldn\u2019t be used as it may not be supported by the underlying volume plugin. This policy typically causes all files on the volume to be deleted and makes the persistent volume available again without the need to delete and recreate it.</li> </ul> <p>If no <code>reclaimPolicy</code> is specified when a <code>StorageClass</code> object is created, it will default to Delete.</p> <p>Info</p> <p>The VOLUMEBINDINGMODE field controls when volume binding and dynamic provisioning should occur. When unset, \"Immediate\" mode is used by default. </p> <ul> <li>Immediate</li> </ul> <p>The <code>Immediate</code> mode indicates that volume binding and dynamic provisioning occurs once the <code>PersistentVolumeClaim</code> is created. For storage backends that are topology-constrained and not globally accessible from all Nodes in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod's scheduling requirements. This may result in unschedulable Pods.</p> <ul> <li><code>WaitForFirstConsumer</code> The volume is provisioned and bound to the claim when the first pod that uses this claim is created. This mode is used for topology-constrained volume types. </li> </ul> <p>The following plugins support <code>WaitForFirstConsumer</code> with dynamic provisioning:</p> <ul> <li>AWSElasticBlockStore</li> <li>GCEPersistentDisk</li> <li>AzureDisk</li> </ul> <pre><code>kubectl  describe sc\n</code></pre> <p>Output:</p> <pre><code>Name:                  premium-rwo\nIsDefaultClass:        No\nAnnotations:           components.gke.io/component-name=pdcsi,components.gke.io/component-version=0.9.6,components.gke.io/layer=addon\nProvisioner:           pd.csi.storage.gke.io\nParameters:            type=pd-ssd\nAllowVolumeExpansion:  True\nMountOptions:          &lt;none&gt;\nReclaimPolicy:         Delete\nVolumeBindingMode:     WaitForFirstConsumer\nEvents:                &lt;none&gt;\n\n\nName:                  standard\nIsDefaultClass:        Yes\nAnnotations:           storageclass.kubernetes.io/is-default-class=true\nProvisioner:           kubernetes.io/gce-pd\nParameters:            type=pd-standard\nAllowVolumeExpansion:  True\nMountOptions:          &lt;none&gt;\nReclaimPolicy:         Delete\nVolumeBindingMode:     Immediate\nEvents:                &lt;none&gt;\n\n\nName:                  standard-rwo\nIsDefaultClass:        No\nAnnotations:           components.gke.io/layer=addon,storageclass.kubernetes.io/is-default-class=false\nProvisioner:           pd.csi.storage.gke.io\nParameters:            type=pd-balanced\nAllowVolumeExpansion:  True\nMountOptions:          &lt;none&gt;\nReclaimPolicy:         Delete\nVolumeBindingMode:     WaitForFirstConsumer\nEvents:                &lt;none&gt;\n</code></pre> <p>Summary</p> <p>The StorageClass resource specifies which provisioner should be used for provisioning the persistent volume when a persistent volume claim requests this storage class. The parameters defined in the storage class definition are passed to the provisioner and are specific to each provisioner plugin.</p> <p>Step 2 Let's create a new <code>StorageClass</code> for Regional PDs:</p> <pre><code>cat &gt; regionalpd-sc.yaml &lt;&lt; EOF\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: regionalpd-storageclass\nprovisioner: pd.csi.storage.gke.io\nparameters:\n  type: pd-standard\n  replication-type: regional-pd\nallowedTopologies:\n  - matchLabelExpressions:\n      - key: failure-domain.beta.kubernetes.io/zone\n        values:\n          - us-central1-b\n          - us-central1-c\nEOF\n</code></pre> <pre><code>kubectl apply  -f regionalpd-sc.yaml\n</code></pre> <pre><code>kubectl  describe sc regionalpd-storageclass\n</code></pre> <p>Result</p> <p>We've created a new <code>StorageClass</code> that uses GCP PD csi provisioner to create Regional Disks in GCP.</p> <p>Step 3 Create a Persistent Volume Claim (PVC) <code>pvc-demo-ssd.yaml</code> file that will Dynamically creates 30G GCP PD Persistent Volume (PV),  using SSD persistent disk Provisioner.</p> <pre><code>cat &gt; pvc-demo-ssd.yaml &lt;&lt; EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: hello-web-disk\nspec:\n  storageClassName: premium-rwo\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 30G\nEOF\n</code></pre> <p>Step 3 Create a PVC:</p> <pre><code>kubectl create -f pvc-demo-ssd.yaml\n</code></pre> <p>Step 4 Verify  <code>STATUS</code> of PVC</p> <pre><code>kubectl get pvc\n</code></pre> <p>Output:</p> <pre><code>NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nmongodb-pvc   Pending                                      standard-rwo   5s\n</code></pre> <p>List PVs:</p> <pre><code>kubectl get pv\n</code></pre> <p>List GCP Disks:</p> <pre><code>gcloud compute disks list\n</code></pre> <p>Result</p> <p>What we see is that:</p> <ul> <li><code>PVC</code> is in <code>Pending</code> </li> <li><code>PV</code> is not created</li> <li>GCP <code>PD</code> is not created</li> </ul> <p><code>Question</code>: why PVC is in Pending State ?</p> <p>Step 5 Let's review <code>VolumeBindingMode</code> of <code>premium-rwo</code> Storage Class:</p> <pre><code>kubectl  describe sc premium-rwo | grep VolumeBindingMode\n</code></pre> <p>Info</p> <p>This StorageClass using VolumeBindingMode -  <code>WaitForFirstConsumer</code> that creates PV only, when the first pod that uses this claim is created. </p> <p>Result</p> <p>Ok so if we want PV created we actually need to create a <code>Pod</code> first. This mode is especially important in the Cloud, as <code>Pods</code> can be created in different zones and so <code>PV</code> needs to be created in the correct zone as well.</p> <p>Step 6 Create a <code>pod-volume-demo.yaml</code> manifest that will create a <code>Pod</code> and mount <code>Persistent Volume</code> from <code>hello-web-disk</code> <code>PVC</code>.</p> <pre><code>cat &gt; pod-volume-demo.yaml &lt;&lt; EOF\nkind: Pod\napiVersion: v1\nmetadata:\n  name: pvc-demo-pod\nspec:\n  containers:\n    - name: frontend\n      image: nginx\n      volumeMounts:\n      - mountPath: \"/var/www/html\"\n        name: pvc-demo-volume\n  volumes:\n    - name: pvc-demo-volume\n      persistentVolumeClaim:\n        claimName: hello-web-disk\nEOF\n</code></pre> <p>Step 3 Create a Pod</p> <pre><code>kubectl create -f pod-volume-demo.yaml\n</code></pre> <p>Step 4 Verify  <code>STATUS</code> of PVC now</p> <pre><code>kubectl get pvc\n</code></pre> <p>Output:</p> <pre><code>NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nhello-web-disk   Bound    pvc-4c006173-284e-4786-b752-028bdae768e9   28Gi       RWO            premium-rwo    15m\n</code></pre> <p>Result</p> <p>PVC <code>STATUS</code> shows as Claim <code>hello-web-disk</code> as <code>Bound</code>, and that Claim has been attached to   VOLUME <code>pvc-4c006173-284e-4786-b752-028bdae768e9</code> with <code>CAPACITY</code> 28Gi and <code>ACCESS MODES</code> RWO via <code>STORAGECLASS</code> premium-rwo using SSD.</p> <p>List PVs:</p> <pre><code>kubectl get pv\n</code></pre> <p>Result</p> <p>PV <code>STATUS</code> shows as <code>Bound</code> to the  <code>CLAIM</code> default/hello-web-disk, with <code>RECLAIM POLICY</code> Delete,  meaning that SSD Disk will be deleted after PVC is deleted from Kubernetes.</p> <p>Output:</p> <pre><code>NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE\npersistentvolume/pvc-4c006173-284e-4786-b752-028bdae768e9   28Gi       RWO            Delete           Bound    default/hello-web-disk   premium-rwo             14m\n</code></pre> <p>List GCP Disks:</p> <pre><code>gcloud compute disks list\n</code></pre> <p>Output:</p> <pre><code>pvc-4c006173-284e-4786-b752-028bdae768e9    us-central1-c  zone            28       pd-ssd       READ\n</code></pre> <p>Result</p> <p>We can see that CSI Provisioner created SSD disk on GCP infrastructure.</p> <p>Step 5 Verify  <code>STATUS</code> of Pod</p> <pre><code>kubectl get pod\n</code></pre> <p>Output:</p> <pre><code>NAME           READY   STATUS    RESTARTS   AGE\npvc-demo-pod   1/1     Running   0          21m\n</code></pre> <p>Step 6 Delete PVC</p> <pre><code>kubectl delete pod pvc-demo-pod\nkubeclt delete pvc hello-web-disk\n</code></pre> <p>Step 6 Verify resources has been released:</p> <pre><code>kubectl get pv,pvc,pods\n</code></pre>"},{"location":"ycit019_Lab_11_Storage/#2-deploy-single-mysql-database-with-volume","title":"2 Deploy Single MySQL Database with Volume","text":"<p>You can run a stateful application by creating a Kubernetes Deployment and connecting it to an existing PersistentVolume using a PersistentVolumeClaim.</p> <p>Step 1 Below Manifest file going to creates 3 Kubernetes resources:</p> <ul> <li><code>PersistentVolumeClaim</code> that looks for a 2G volume. This claim will be   satisfied by dynamic provisioner <code>general</code> and appropriate PV going to be created</li> <li><code>Deployment</code> that runs MySQL and references the PersistentVolumeClaim that is   mounted in /var/lib/mysql.</li> <li><code>Service</code> that depoyed as <code>ClusterIP:None</code> that lets the Service DNS name   resolve directly to the Pod\u2019s IP</li> </ul> <pre><code>kubectl create -f - &lt;&lt;EOF\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  ports:\n  - port: 3306\n  selector:\n    app: mysql\n  clusterIP: None\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pv-claim\nspec:\n  storageClassName: standard\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 2Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - image: mysql:5.6.37\n        name: mysql\n        env:\n          # Use secret in real use case\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pv-claim\nEOF\n</code></pre> <p>Note</p> <p>The password is defined inside of the Manifest as environment, which is not insecure. See Kubernetes Secrets for a secure solution.</p> <p>Result</p> <p>Single node MySQL database has been deployed with a Volume</p> <p>Step 3 Display information about the Deployment:</p> <pre><code>kubectl describe deployment mysql\n</code></pre> <p>Output:</p> <pre><code> Name:                 mysql\n Namespace:            default\n CreationTimestamp:    Tue, 01 Nov 2016 11:18:45 -0700\n Labels:               app=mysql\n Annotations:          deployment.kubernetes.io/revision=1\n Selector:             app=mysql\n Replicas:             1 desired | 1 updated | 1 total | 0 available | 1 unavailable\n StrategyType:         Recreate\n MinReadySeconds:      0\n Pod Template:\n   Labels:       app=mysql\n   Containers:\n    mysql:\n     Image:      mysql:5.6\n     Port:       3306/TCP\n     Environment:\n       MYSQL_ROOT_PASSWORD:      password\n     Mounts:\n       /var/lib/mysql from mysql-persistent-storage (rw)\n   Volumes:\n    mysql-persistent-storage:\n     Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n     ClaimName:  mysql-pv-claim\n     ReadOnly:   false\n Conditions:\n   Type          Status  Reason\n   ----          ------  ------\n   Available     False   MinimumReplicasUnavailable\n   Progressing   True    ReplicaSetUpdated\n OldReplicaSets:       &lt;none&gt;\n NewReplicaSet:        mysql-63082529 (1/1 replicas created)\n Events:\n   FirstSeen    LastSeen    Count    From                SubobjectPath    Type        Reason            Message\n   ---------    --------    -----    ----                -------------    --------    ------            -------\n   33s          33s         1        {deployment-controller }             Normal      ScalingReplicaSet Scaled up replica set mysql-63082529 to 1\n</code></pre> <p>Step 4 List the pods created by the Deployment:</p> <pre><code>kubectl get pods -l app=mysql\n</code></pre> <p>Output:</p> <pre><code>NAME                   READY     STATUS    RESTARTS   AGE\nmysql-63082529-2z3ki   1/1       Running   0          3m\n</code></pre> <p>Step 5 Inspect the PersistentVolumeClaim:</p> <pre><code>kubectl describe pvc mysql-pv-claim\n</code></pre> <p>Output:</p> <pre><code> Name:         mysql-pv-claim\n Namespace:    default\n StorageClass:\n Status:       Bound\n Volume:       mysql-pv\n Labels:       &lt;none&gt;\n Annotations:    pv.kubernetes.io/bind-completed=yes\n                 pv.kubernetes.io/bound-by-controller=yes\n Capacity:     20Gi\n Access Modes: RWO\n Events:       &lt;none&gt;\n</code></pre> <p>Step 5 Inspect created PersistentVolume:</p> <pre><code>kubectl get pv\nkubectl describe pv\ngcloud compute disks list\n</code></pre> <p>Step 6  Access the MySQL instance</p> <p>The Service option <code>clusterIP: None</code> lets the Service DNS name resolve directly to the Pod's IP address. This is optimal when you have only one Pod behind a Service and you don't intend to increase the number of Pods.</p> <p>Run a MySQL client to connect to the server:</p> <pre><code>kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h mysql -ppassword\n</code></pre> <p>This command creates a new Pod in the cluster running a MySQL client and connects it to the server through the Service. If it connects, you know your stateful MySQL database is up and running.</p> <pre><code>Waiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false\nIf you don't see a command prompt, try pressing enter.\n\nmysql&gt; show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n+--------------------+\n3 rows in set (0.00 sec)\n\nmysql&gt; exit\n</code></pre> <p>Step 7  Update the MySQL instance</p> <p>The image or any other part of the Deployment can be updated as usual with the <code>kubectl apply</code> command.</p> <p>Important</p> <ul> <li>Don't scale the app. This setup is for single-instance apps   only. The underlying PersistentVolume can only be mounted to one   Pod. For clustered stateful apps, see the   StatefulSet documentation.</li> <li>Use <code>strategy:</code> <code>type: Recreate</code> in the Deployment configuration   YAML file. This instructs Kubernetes to not use rolling   updates. Rolling updates will not work, as you cannot have more than   one Pod running at a time. The <code>Recreate</code> strategy will stop the   first pod before creating a new one with the updated configuration.</li> </ul> <p>Step 8 Delete the MySQL instance</p> <p>Delete the deployed objects by name:</p> <pre><code>kubectl delete deployment,svc mysql\nkubectl delete pvc mysql-pv-claim\n</code></pre> <p>Since we used a dynamic provisioner, it automatically deletes the PersistentVolume when it sees that you deleted the PersistentVolumeClaim.</p> <p>Step 9 Check that GCP Volume has been deleted:</p> <pre><code>gcloud compute disks list\n</code></pre> <p>Note</p> <p>If PersistentVolume was manually provisioned, it is requrire to manually delete it, as well as release the underlying resource.</p>"},{"location":"ycit019_Lab_11_Storage/#3-deploying-highly-available-postgresql-with-gke","title":"3 Deploying highly available PostgreSQL with GKE","text":""},{"location":"ycit019_Lab_11_Storage/#31-deploying-postgresql","title":"3.1 Deploying PostgreSQL","text":"<p>Step 1  Create <code>regional persistent disk</code> StorageClass</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: regionalpd-storageclass\nprovisioner: kubernetes.io/gce-pd\nparameters:\n  type: pd-standard\n  replication-type: regional-pd\nallowedTopologies:\n  - matchLabelExpressions:\n      - key: failure-domain.beta.kubernetes.io/zone\n        values:\n          - us-central1-b\n          - us-central1-c\nEOF\n</code></pre> <p>Step 2  Create PersistentVolumeClaim based on a <code>regional persistent disk</code> StorageClass</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: postgresql-pv\nspec:\n  storageClassName: regionalpd-storageclass\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 300Gi\nEOF\n</code></pre> <p>Step 3  Create a PostgreSQL deployment:</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n name: postgres\nspec:\n strategy:\n   rollingUpdate:\n     maxSurge: 1\n     maxUnavailable: 1\n   type: RollingUpdate\n replicas: 1\n selector:\n   matchLabels:\n     app: postgres\n template:\n   metadata:\n     labels:\n       app: postgres\n   spec:\n     containers:\n       - name: postgres\n         image: postgres:10\n         resources:\n           limits:\n             cpu: \"1\"\n             memory: \"3Gi\"\n           requests:\n             cpu: \"1\"\n             memory: \"2Gi\"\n         ports:\n           - containerPort: 5432\n         env:\n           - name: POSTGRES_PASSWORD\n             value: password\n           - name: PGDATA\n             value: /var/lib/postgresql/data/pgdata\n         volumeMounts:\n           - mountPath: /var/lib/postgresql/data\n             name: postgredb\n     volumes:\n       - name: postgredb\n         persistentVolumeClaim:\n           claimName: postgresql-pv\nEOF\n</code></pre> <p>Step 4 Create PostgreSQL service:</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\nspec:\n  ports:\n    - port: 5432\n  selector:\n    app: postgres\n  clusterIP: None\nEOF\n</code></pre> <p>Step 5 Check Regional PVs has been Provisioned based on PVC request</p> <pre><code>kubectl get pvc,pv\n</code></pre> <p>Step 6 Check that Postgres is up and <code>Running</code></p> <pre><code>kubectl get deploy,pods\n</code></pre>"},{"location":"ycit019_Lab_11_Storage/#32-creating-a-test-dataset","title":"3.2 Creating a test dataset","text":"<p>Step 1  Connect to your PostgreSQL instance:</p> <pre><code>POD=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $1}'`\n\nkubectl exec -it $POD -- psql -U postgres\n</code></pre> <p>Step 2  Create a database and a table, and then insert some test rows:</p> <pre><code>create database gke_test_regional;\n\n\\c gke_test_regional;\n\nCREATE TABLE test(\n   data VARCHAR (255) NULL\n);\n\ninsert into test values\n  ('Learning GKE is fun'),\n  ('Databases on GKE are easy');\n</code></pre> <p>Step 3  Verify that the test rows were inserted, select all rows:</p> <pre><code>select * from test;\n</code></pre> <p>Step 4 Exit the PostgreSQL shell:</p> <pre><code>\\q\n</code></pre>"},{"location":"ycit019_Lab_11_Storage/#33-simulating-database-instance-failover","title":"3.3 Simulating database instance failover","text":"<p>Step 0  Identify the node that is currently hosting PostgreSQL</p> <pre><code>kubectl get pods -l app=postgres -o wide\n</code></pre> <p>Note</p> <p>Take a note on which of the nodes Pod is <code>Running</code></p> <p>Step 1 Prepare that node to be CORDONED in other words Disabled for  scheduling:</p> <pre><code>CORDONED_NODE=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $7}'`\n\necho ${CORDONED_NODE}\n\ngcloud compute instances list --filter=\"name=${CORDONED_NODE}\"\n</code></pre> <p>Step 2  Disable scheduling of any new pods on this node:</p> <pre><code>kubectl cordon ${CORDONED_NODE}\n\nkubectl get nodes\n</code></pre> <p>Result</p> <p>The node is cordoned, so scheduling is disabled on the node that the database instance resides on.</p> <p>Step 3  Delete the existing PostgreSQL pod</p> <pre><code>POD=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $1}'`\n\nkubectl delete pod ${POD}\n</code></pre> <p>Step 4  Verify that a new pod is created on the other node.</p> <pre><code>kubectl get pods -l app=postgres -o wide\n</code></pre> <p>Important</p> <p>It might take a while for the new pod to be ready (usually around 30 seconds).</p> <p>Step 5  Verify the node's zone</p> <pre><code>NODE=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $7}'`\n\necho ${NODE}\n\ngcloud compute instances list --filter=\"name=${NODE}\"\n</code></pre> <p>Result</p> <p>Notice that the pod is deployed in a different zone from where the node was created at the beginning of this procedure.</p> <p>Step 6 Connect to the database instance</p> <pre><code>POD=`kubectl get pods -l app=postgres -o wide | grep -v NAME | awk '{print $1}'`\n\nkubectl exec -it $POD -- psql -U postgres\n</code></pre> <p>Step 7 Verify that the test dataset exists </p> <pre><code>\\c gke_test_regional;\n\nselect * from test;\n\n\\q\n</code></pre> <p>Step 8 Re-enable scheduling for the node for which scheduling was disabled:</p> <pre><code>kubectl uncordon $CORDONED_NODE\n</code></pre> <p>Step 9 Check that the node is ready again:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Step 10 Cleanup Postgres <code>Deployment</code> and <code>PVC</code></p> <pre><code>kubectl delete pvc postgresql-pv\nkubectl delete deploy postgres\n</code></pre>"},{"location":"ycit019_Lab_11_Storage/#4-deploy-statefulset","title":"4 Deploy StatefulSet","text":"<p>Scale up our GKE cluster to <code>4</code> nodes:</p> <pre><code>gcloud container clusters resize  k8s-storage --node-pool=default-pool --num-nodes=2 --region us-central1\n</code></pre>"},{"location":"ycit019_Lab_11_Storage/#41-deploy-replicated-mysql-masterslaves-cluster-using-statefulset","title":"4.1 Deploy Replicated MySQL (Master/Slaves) Cluster using StatefulSet.","text":"<p>Our Replicated MySQL deployment going to consists of:</p> <ul> <li>1 ConfigMap</li> <li>2 Services</li> <li>1 StatefulSet</li> </ul> <p>Step 1 Create the ConfigMap (just copy paste below):</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\ndata:\n  primary.cnf: |\n    # Apply this config only on the primary.\n    [mysqld]\n    log-bin\n  replica.cnf: |\n    # Apply this config only on replicas.\n    [mysqld]\n    super-read-only\nEOF\n</code></pre> <p>Result</p> <p>This ConfigMap provides overrides that let you independently control configuration on the MySQL master and slaves. In this case:</p> <ul> <li>master going to be able to serve replication logs to slaves</li> <li>slaves to reject any writes that don't come via replication.</li> </ul> <p>There's nothing special about the ConfigMap itself that causes different portions to apply to different Pods. Each Pod decides which portion to look at as it's initializing, based on information provided by the StatefulSet controller.</p> <p>Step 2 Create 2 Services (just copy paste below):</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\n# Headless service for stable DNS entries of StatefulSet members.\n# Headless service for stable DNS entries of StatefulSet members.\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\nspec:\n  ports:\n  - name: mysql\n    port: 3306\n  clusterIP: None\n  selector:\n    app: mysql\n---\n# Client service for connecting to any MySQL instance for reads.\n# For writes, you must instead connect to the primary: mysql-0.mysql.\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-read\n  labels:\n    app: mysql\nspec:\n  ports:\n  - name: mysql\n    port: 3306\n  selector:\n    app: mysql\nEOF\n</code></pre> <p>Info</p> <p>The Headless Service provides a home for the DNS entries that the StatefulSet controller creates for each Pod that's part of the set. Because the Headless Service is named mysql, the Pods are accessible by resolving .mysql from within any other Pod in the same Kubernetes cluster and namespace. <p>The Client Service, called mysql-read, is a normal Service with its own cluster IP that distributes connections across all MySQL Pods that report being Ready. The set of potential endpoints includes the MySQL master and all slaves.</p> <p>Note</p> <p>Only read queries can use the load-balanced Client Service. Because there is only one MySQL master, clients should connect directly to the MySQL master Pod (through its DNS entry within the Headless Service) to execute writes.</p> <p>Step 3 Create StatefulSet  <code>mysql-statefulset.yaml</code>  manifest:</p> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  serviceName: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - \"-c\"\n        - |\n          set -ex\n          # Generate mysql server-id from pod ordinal index.\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n          ordinal=${BASH_REMATCH[1]}\n          echo [mysqld] &gt; /mnt/conf.d/server-id.cnf\n          # Add an offset to avoid reserved server-id=0 value.\n          echo server-id=$((100 + $ordinal)) &gt;&gt; /mnt/conf.d/server-id.cnf\n          # Copy appropriate conf.d files from config-map to emptyDir.\n          if [[ $ordinal -eq 0 ]]; then\n            cp /mnt/config-map/primary.cnf /mnt/conf.d/\n          else\n            cp /mnt/config-map/replica.cnf /mnt/conf.d/\n          fi\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - \"-c\"\n        - |\n          set -ex\n          # Skip the clone if data already exists.\n          [[ -d /var/lib/mysql/mysql ]] &amp;&amp; exit 0\n          # Skip the clone on primary (ordinal index 0).\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n          ordinal=${BASH_REMATCH[1]}\n          [[ $ordinal -eq 0 ]] &amp;&amp; exit 0\n          # Clone data from previous peer.\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n          # Prepare the backup.\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: \"1\"\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command: [\"mysqladmin\", \"ping\"]\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            # Check we can execute queries over TCP (skip-networking is off).\n            command: [\"mysql\", \"-h\", \"127.0.0.1\", \"-e\", \"SELECT 1\"]\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - \"-c\"\n        - |\n          set -ex\n          cd /var/lib/mysql\n\n          # Determine binlog position of cloned data, if any.\n          if [[ -f xtrabackup_slave_info &amp;&amp; \"x$(&lt;xtrabackup_slave_info)\" != \"x\" ]]; then\n            # XtraBackup already generated a partial \"CHANGE MASTER TO\" query\n            # because we're cloning from an existing replica. (Need to remove the tailing semicolon!)\n            cat xtrabackup_slave_info | sed -E 's/;$//g' &gt; change_master_to.sql.in\n            # Ignore xtrabackup_binlog_info in this case (it's useless).\n            rm -f xtrabackup_slave_info xtrabackup_binlog_info\n          elif [[ -f xtrabackup_binlog_info ]]; then\n            # We're cloning directly from primary. Parse binlog position.\n            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\n            rm -f xtrabackup_binlog_info xtrabackup_slave_info\n            echo \"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\n                  MASTER_LOG_POS=${BASH_REMATCH[2]}\" &gt; change_master_to.sql.in\n          fi\n\n          # Check if we need to complete a clone by starting replication.\n          if [[ -f change_master_to.sql.in ]]; then\n            echo \"Waiting for mysqld to be ready (accepting connections)\"\n            until mysql -h 127.0.0.1 -e \"SELECT 1\"; do sleep 1; done\n\n            echo \"Initializing replication from clone position\"\n            mysql -h 127.0.0.1 \\\n                  -e \"$(&lt;change_master_to.sql.in), \\\n                          MASTER_HOST='mysql-0.mysql', \\\n                          MASTER_USER='root', \\\n                          MASTER_PASSWORD='', \\\n                          MASTER_CONNECT_RETRY=10; \\\n                        START SLAVE;\" || exit 1\n            # In case of container restart, attempt this at-most-once.\n            mv change_master_to.sql.in change_master_to.sql.orig\n          fi\n\n          # Start a server to send backups when requested by peers.\n          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \\\n            \"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      storageClassName: \"standard-rwo\"\n      resources:\n        requests:\n          storage: 10Gi\n</code></pre> <p>Deploy <code>StatefulSet</code>:</p> <pre><code>kubectl apply -f https://k8s.io/examples/application/mysql/mysql-statefulset.yaml\n</code></pre> <p>Step 4 Monitor Deployment Process/Sequence</p> <pre><code>watch kubectl get statefulset,pvc,pv,pods -l app=mysql\n</code></pre> <p>Press <code>Ctrl+C</code> to cancel the watch when all pods, pvc and statefulset provisioned.</p> <pre><code>kubectl get  pods\n</code></pre> <p>Output:</p> <pre><code>NAME      READY   STATUS     RESTARTS   AGE\nmysql-0   2/2     Running    0          2m6s\nmysql-1   2/2     Running    0          77s\n</code></pre> <p>Result</p> <p>The StatefulSet controller started Pods one at a time, in order by their ordinal index. It waits until each Pod reports being Ready before starting the next one.</p> <p>In addition, the controller assigned each Pod a unique, stable name of the form <code>&lt;statefulset-name&gt;-&lt;ordinal-index&gt;</code>. In this case, that results in Pods named <code>mysql-0</code>, <code>mysql-1</code>, and <code>mysql-2</code>.</p> <p>The Pod template in the above StatefulSet manifest takes advantage of these properties to perform orderly startup of MySQL replication.</p> <p>Generating configuration Before starting any of the containers in the Pod spec, the Pod first runs any [Init Containers] in the order defined.</p> <p>The first Init Container, named <code>init-mysql</code>, generates special MySQL config files based on the ordinal index.</p> <p>The script determines its own ordinal index by extracting it from the end of the Pod name, which is returned by the <code>hostname</code> command. Then it saves the ordinal (with a numeric offset to avoid reserved values) into a file called <code>server-id.cnf</code> in the MySQL <code>conf.d</code> directory. This translates the unique, stable identity provided by the StatefulSet controller into the domain of MySQL server IDs, which require the same properties.</p> <p>The script in the <code>init-mysql</code> container also applies either <code>master.cnf</code> or <code>slave.cnf</code> from the ConfigMap by copying the contents into <code>conf.d</code>. Because the example topology consists of a single MySQL master and any number of slaves, the script simply assigns ordinal <code>0</code> to be the master, and everyone else to be slaves. Combined with the StatefulSet controller's <code>deployment order guarantee</code> ensures the MySQL master is Ready before creating slaves, so they can begin replicating.</p> <p>Cloning existing data In general, when a new Pod joins the set as a slave, it must assume the MySQL master might already have data on it. It also must assume that the replication logs might not go all the way back to the beginning of time. These conservative assumptions are the key to allow a running StatefulSet to scale up and down over time, rather than being fixed at its initial size.</p> <p>The second Init Container, named <code>clone-mysql</code>, performs a clone operation on a slave Pod the first time it starts up on an empty PersistentVolume. That means it copies all existing data from another running Pod, so its local state is consistent enough to begin replicating from the master.</p> <p>MySQL itself does not provide a mechanism to do this, so the example uses a popular open-source tool called Percona XtraBackup. During the clone, the source MySQL server might suffer reduced performance. To minimize impact on the MySQL master, the script instructs each Pod to clone from the Pod whose ordinal index is one lower. This works because the StatefulSet controller always ensures Pod <code>N</code> is Ready before starting Pod <code>N+1</code>.</p> <p>Starting replication After the Init Containers complete successfully, the regular containers run. The MySQL Pods consist of a <code>mysql</code> container that runs the actual <code>mysqld</code> server, and an <code>xtrabackup</code> container that acts as a sidecar.</p> <p>The <code>xtrabackup</code> sidecar looks at the cloned data files and determines if it's necessary to initialize MySQL replication on the slave. If so, it waits for <code>mysqld</code> to be ready and then executes the <code>CHANGE MASTER TO</code> and <code>START SLAVE</code> commands with replication parameters extracted from the XtraBackup clone files.</p> <p>Once a slave begins replication, it remembers its MySQL master and reconnects automatically if the server restarts or the connection dies. Also, because slaves look for the master at its stable DNS name (<code>mysql-0.mysql</code>), they automatically find the master even if it gets a new Pod IP due to being rescheduled.</p> <p>Lastly, after starting replication, the <code>xtrabackup</code> container listens for connections from other Pods requesting a data clone. This server remains up indefinitely in case the StatefulSet scales up, or in case the next Pod loses its PersistentVolumeClaim and needs to redo the clone.</p>"},{"location":"ycit019_Lab_11_Storage/#42-test-the-mysql-cluster-app-and-running","title":"4.2 Test the MySQL cluster app and running","text":"<p>Step 1 Create Database, Table and message on Master MySQL database</p> <p>Send test queries to the MySQL master (hostname <code>mysql-0.mysql</code>) by running a temporary container with the <code>mysql:5.7</code> image and running the <code>mysql</code> client binary.</p> <pre><code>kubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\\\n  mysql -h mysql-0.mysql &lt;&lt;EOF\nCREATE DATABASE test;\nCREATE TABLE test.messages (message VARCHAR(250));\nINSERT INTO test.messages VALUES ('hello');\nEOF\n</code></pre> <p>Step 2 Verify that recorded data has been replicated to the slaves:</p> <p>Use the hostname <code>mysql-read</code> to send test queries to any server that reports being Ready:</p> <pre><code>kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\\\n  mysql -h mysql-read -e \"SELECT * FROM test.messages\"\n</code></pre> <p>You should get output like this:</p> <pre><code>Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false\n+---------+\n| message |\n+---------+\n| hello   |\n+---------+\npod \"mysql-client\" deleted\n</code></pre> <p>Step 3 Demonstrate that the <code>mysql-read</code> Service distributes connections across servers, you can run <code>SELECT @@hostname</code> in a loop:</p> <pre><code>kubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\\\n  bash -ic \"while sleep 1; do mysql -h mysql-read -e 'SELECT @@hostname,NOW()'; done\"\n</code></pre> <p>You should see the reported <code>@@hostname</code> change randomly, because a different endpoint might be selected upon each connection attempt:</p> <pre><code>+-------------+---------------------+\n| @@hostname  | NOW()               |\n+-------------+---------------------+\n|         100 | 2006-01-02 15:04:05 |\n+-------------+---------------------+\n+-------------+---------------------+\n| @@hostname  | NOW()               |\n+-------------+---------------------+\n|         102 | 2006-01-02 15:04:06 |\n+-------------+---------------------+\n+-------------+---------------------+\n| @@hostname  | NOW()               |\n+-------------+---------------------+\n|         101 | 2006-01-02 15:04:07 |\n+-------------+---------------------+\n</code></pre> <p>You can press Ctrl+C when you want to stop the loop, but it's useful to keep it running in another window so you can see the effects of the following steps.</p>"},{"location":"ycit019_Lab_11_Storage/#43-delete-pods","title":"4.3 Delete Pods","text":"<p>The StatefulSet recreates Pods if they're deleted, similar to what a ReplicaSet does for stateless Pods.</p> <p>Step 1 Try to fail Mysql cluster by deleting <code>mysql-1</code> pod:</p> <pre><code>kubectl delete pod mysql-1\n</code></pre> <p>The StatefulSet controller notices that no <code>mysql-1</code> Pod exists anymore, and creates a new one with the same name and linked to the same PersistentVolumeClaim.</p> <p>Step 5 Monitor Deployment Process/Sequence</p> <pre><code>watch kubectl get statefulset,pvc,pv,pods -l app=mysql\n</code></pre> <p>Result</p> <p>You should see server ID <code>102</code> disappear from the loop output for a while and then return on its own.</p>"},{"location":"ycit019_Lab_11_Storage/#44-scaling-the-number-of-slaves","title":"4.4 Scaling the number of slaves","text":"<p>With MySQL replication, you can scale your read query capacity by adding slaves. With StatefulSet, you can do this with a single command:</p> <p>Step 1 Scale up statefulset:</p> <pre><code>kubectl scale statefulset mysql  --replicas=5\n</code></pre> <p>Step 2 Watch the new Pods come up by running:</p> <pre><code>kubectl get pods -l app=mysql --watch\n</code></pre> <p>Step 3 Watch the new Pods come up by running:</p> <pre><code>kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\\\n  mysql -h mysql-3.mysql -e \"SELECT * FROM test.messages\"\n</code></pre> <pre><code>Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false\n+---------+\n| message |\n+---------+\n| hello   |\n+---------+\npod \"mysql-client\" deleted\n</code></pre> <p>Step 4 Scaling back down is also seamless:</p> <pre><code>kubectl scale statefulset mysql --replicas=3\n</code></pre> <p>Note, however, that while scaling up creates new PersistentVolumeClaims automatically, scaling down does not automatically delete these PVCs. This gives you the choice to keep those initialized PVCs around to make scaling back up quicker, or to extract data before deleting them.</p> <p>You can see this by running:</p> <pre><code>kubectl get pvc -l app=mysql\n</code></pre> <p>Which shows that all 3 PVCs still exist, despite having scaled the StatefulSet down to 1:</p> <pre><code>NAME           STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\ndata-mysql-0   Bound     pvc-8acbf5dc-b103-11e6-93fa-42010a800002   10Gi       RWO           20m\ndata-mysql-1   Bound     pvc-8ad39820-b103-11e6-93fa-42010a800002   10Gi       RWO           20m\ndata-mysql-2   Bound     pvc-8ad69a6d-b103-11e6-93fa-42010a800002   10Gi       RWO           20m\n</code></pre> <p>If you don't intend to reuse the extra PVCs, you can delete them:</p> <pre><code>kubectl delete pvc data-mysql-0\nkubectl delete pvc data-mysql-1\nkubectl delete pvc data-mysql-2\nkubectl delete pvc data-mysql-3\nkubectl delete pvc data-mysql-4\n</code></pre> <p>Step 5 Cancel the <code>SELECT @@server_id</code> loop by pressing Ctrl+C in its terminal,    or running the following from another terminal:</p> <pre><code>kubectl delete pod mysql-client-loop --now\n</code></pre> <p>Step 6 Delete the StatefulSet. This also begins terminating the Pods.</p> <pre><code>kubectl delete statefulset mysql\n</code></pre>"},{"location":"ycit019_Lab_11_Storage/#5-cleaning-up","title":"5 Cleaning Up","text":"<p>Step 1 Delete the cluster</p> <pre><code>gcloud container clusters delete k8s-storage\n</code></pre>"},{"location":"ycit019_Lab_2_Docker_basics/","title":"ycit019 Lab 2 Docker basics","text":"<p>Lab 2 Docker basics</p> <p>Objective:</p> <ul> <li>Practice to run Docker containers</li> </ul>"},{"location":"ycit019_Lab_2_Docker_basics/#prepare-lab-environment","title":"Prepare Lab Environment","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p>"},{"location":"ycit019_Lab_2_Docker_basics/#1-docker-basics","title":"1 Docker basics","text":""},{"location":"ycit019_Lab_2_Docker_basics/#11-show-running-containers","title":"1.1 Show running containers","text":"<p>Step 1 Run docker ps to show running containers:</p> <pre><code>docker ps\n</code></pre> <p>Step 2 The output shows that there are no running containers at the moment. Use the command docker <code>ps -a</code> to list all containers including the ones has been stopped:</p> <pre><code>docker ps -a\n</code></pre> <p>Output:</p> <pre><code>CONTAINER ID IMAGE       COMMAND  CREATED        STATUS             PORTS  NAMES\n6e6db2a24a8e hello-world \"/hello\" 15 minutes ago Exited (0) 15 min  dreamy_nobel\n</code></pre> <p>Review the collumns <code>CONTAINER ID</code>, <code>STATUS</code>, <code>COMMAND</code>, <code>PORTS</code>, <code>NAMES</code>.</p> <p>In the previous section we started one container and the command docker <code>ps -a</code> shows it as <code>Exited</code>.</p> <p>Note</p> <p>You can name your own containers with --name when you use docker run. If you do not provide a name, Docker will generate a random one like the one you have.</p> <p>Question</p> <p>Why Docker names are random? How docker containers named?</p> <p>Step 3 Let\u2019s run the command <code>docker images</code> to show all the images on your local system:</p> <pre><code>docker images\n</code></pre> <p>As you see, there is only one image that was downloaded from the Docker Hub.</p>"},{"location":"ycit019_Lab_2_Docker_basics/#12-specify-a-container-main-process","title":"1.2 Specify a container main process","text":"<p>Step 1 Let\u2019s run our own \"hello world\" container. For that we will use the official Ubuntu image:</p> <pre><code>docker run ubuntu /bin/echo 'Hello world'\n</code></pre> <p>Output:</p> <pre><code>Unable to find image 'ubuntu:latest' locally\nlatest: Pulling from library/ubuntu\n...\nStatus: Downloaded newer image for ubuntu:latest\nHello world\n</code></pre> <p>As you see, Docker downloaded the image ubuntu because it was not on the local machine.</p> <p>Step 2 Let\u2019s run the command <code>docker images</code> again:</p> <pre><code>docker images\n</code></pre> <p>Output:</p> <pre><code>REPOSITORY          TAG                 IMAGE ID            CREATED        SIZE\nubuntu              latest              42118e3df429        11 days ago  124.8 MB\nhello-world         latest              c54a2cc56cbb        4 weeks ago  1.848 kB\n</code></pre> <p>Step 3 If you run the same \"hello world\" container again, Docker will use a local copy of the image:</p> <pre><code>docker run ubuntu /bin/echo 'Hello world'\n</code></pre> <p>Output:</p> <pre><code>Hello world\n</code></pre> <p>Question</p> <p>Compare Ubuntu Docker image with ISO image or with Cloud VM image.</p> <ul> <li>Why the size is so different ?</li> </ul> <p>Summary</p> <p>Pulling docker images from Docker Hub takes sometime. This time depends on:</p> <ul> <li>How large is the image?</li> <li>How fast is the network to Internet ?</li> </ul> <p>However, it is still much faster than booting traditional OS with Ubuntu on VM.</p> <p>If image already pulled on local host it takes fraction of a second to start a container.</p> <p>Running application in docker containers considered as a best practice for running CI/CD pipelines as it considerably faster than using VMs and reduce time for deploying a test environments.</p>"},{"location":"ycit019_Lab_2_Docker_basics/#13-specify-an-image-version","title":"1.3 Specify an image version","text":"<p>Step 1 As you see, Docker has downloaded the ubuntu:latest image. You can see Ubuntu version by running the following command:</p> <pre><code>docker run ubuntu /bin/cat /etc/issue.net\n</code></pre> <p>Output:</p> <pre><code>Ubuntu 16.04 LTS\n</code></pre> <p>Let\u2019s say you need a previous Ubuntu LTS release. In this case, you can specify the version you need:</p> <pre><code>docker run ubuntu:14.04 /bin/cat /etc/issue.net\n</code></pre> <p>Output:</p> <pre><code>Unable to find image 'ubuntu:14.04' locally\n14.04: Pulling from library/ubuntu\n...\nStatus: Downloaded newer image for ubuntu:14.04\nUbuntu 14.04.4 LTS\n</code></pre> <p>Step 2 The <code>docker images</code> command should show that we have 3 Ubuntu images downloaded locally:</p> <pre><code>docker images\n</code></pre> <p>Output:</p> <pre><code>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nubuntu              latest              42118e3df429        11 days ago         124.8 MB\nubuntu              14.04               0ccb13bf1954        11 days ago         188 MB\nhello-world         latest              c54a2cc56cbb        4 weeks ago         1.848 kB\n</code></pre> <p>Tip</p> <p>Running CI/CD pipeline with Docker using <code>latest</code> tag considered as a Bad Practice.</p> <p>Instead consider using:</p> <ul> <li>Versioning</li> <li><code>SHA</code> tagging.</li> </ul>"},{"location":"ycit019_Lab_2_Docker_basics/#14-run-an-interactive-container","title":"1.4 Run an interactive container","text":"<p>Step 1 Let\u2019s use the <code>ubuntu</code> image to run an interactive bash session and inspect what is running inside our docker image.</p> <p>To achive that we going to use  <code>-i</code> and <code>-t</code> flags.</p> <p>The  <code>-i</code> is shorthand for <code>--interactive</code>, which instructs Docker to keep <code>stdin</code> open so that we can send commands to the sprocess.</p> <p>The <code>-t</code> flag is short for <code>--tty</code> and allocates a <code>pseudo-TTY</code> or terminal inside of the session.</p> <pre><code>docker run -it ubuntu /bin/bash\nroot@17d8bdeda98e:/#\n</code></pre> <p>Result</p> <p>We get a  bash shell prompt inside of the container.</p> <p>Note</p> <p>Bash prompt is not availabe for all docker images.</p> <p>Step 2 Let's print the system information of the latest <code>Ubuntu</code> image:</p> <pre><code>root@17d8bdeda98e:/#  uname -a\nLinux 17d8bdeda98e 3.19.0-31-generic ...\n</code></pre> <p>Step 3 Let's verify what Ubuntu version is run by <code>latest</code> image of ubuntu:</p> <pre><code>root@17d8bdeda98e:/#  lsb_release -a\nbash: lsb_release: command not found\n</code></pre> <p>Failure</p> <p>Why the standard Ubuntu command that checks version of OS is not working as expeced ?</p> <p>Step 4 Let's verify Ubuntu version using alternative way by checking <code>/etc/lsb-release</code> file.</p> <pre><code>root@8cbcbd0fe8d2:/# cat /etc/lsb-release\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=16.04\nDISTRIB_CODENAME=xenial\nDISTRIB_DESCRIPTION=\"Ubuntu 16.04.3 LTS\"\n</code></pre> <p>Step 5 Let's compare the number of executable binaries availabe inside of the docker image versus Cloud VM that we running our class environment. First, run <code>ls</code> command on  <code>/bin</code> and <code>/usr/bin</code> directories inside of the running ubuntu container as well as <code>dpkg --list</code> command that shows total number of installed packages:</p> <pre><code>root@8cbcbd0fe8d2:/# ls /bin | wc -l\n86\nroot@8cbcbd0fe8d2:/# ls /usr/bin | wc -l\n233\nroot@eb11cd0b4106:/# dpkg --list | wc -l\n101\n</code></pre> <p>Step 6 Use the <code>exit</code> command or press <code>Ctrl-D</code> to exit the interactive bash session back to Cloud VM.</p> <pre><code>root@eb11cd0b4106:/# exit\n</code></pre> <p>Step 7 Now run <code>ls</code> command on <code>/bin</code> and <code>/usr/bin</code> directories on Cloud VM that we using as our class environment:</p> <pre><code>cca-user@userx-docker-vm:~$ ls /bin | wc -l\n171\ncca-user@userx-docker-vm:~$ ls /usr/bin | wc -l\n660\ncca-user@userx-docker-vm:~$ dpkg --list | wc -l\n463\n</code></pre> <p>Result</p> <p>Official Docker container has much less binaries and packages installed vs Ubuntu Cloud Image.</p> <p>Summary</p> <p>Some of the use cases running docker containers in <code>interactive mode</code> are:</p> <ul> <li>Troubleshooting containerized applications</li> <li>Deploying and running containerized application on the existing production systems without affecting it.</li> </ul> <p>We've also learned that an official Docker \"minimal\" ubuntu image, does not include <code>lsb_release</code> command, as well as many other commands and packages that can be found in Official Ubuntu ISO image. The docker images are ment to contain only required core system commands and functions to make Images as light as possible. That say you can still install required packages using <code>apt-get install</code>, however this may increase size of docker image considerably.</p> <p>Hint</p> <p>While Docker <code>Ubuntu</code> image we used so far or Docker <code>Centos</code> image are very familiar to users and can be good starting point for learning docker containers. Using them in production or development considered as a Bad Practice.</p> <p>This is due those images still considered as <code>heavy</code> and potentially contain a lot more valnurabilities compare to specialized images.</p> <p>To reduce image pull time from docker hub and follow the best secuirity practices consider using specialized images that works well with you underlining code (Node image for NodeJS applications and etc.). Examples of specialized images are:</p> <ul> <li>Alpine Linux</li> <li>Node</li> <li>Atomic</li> </ul> <p>In fact, not so long ago all the official Docker Images in Docker-Hub has been moved to use Alpine Image.</p> <p>Step 8 Finally let\u2019s check that when the shell process has finished, the container stops:</p> <pre><code>docker ps\n</code></pre>"},{"location":"ycit019_Lab_2_Docker_basics/#15-run-a-container-in-a-background","title":"1.5 Run a container in a background","text":"<p>Now we know how to connect to running container and execute commands in it. However in most cases you just want run a container in a background so it can do a specific action.</p> <p>Step 1 Run a container in a background using the <code>-d</code> command line argument:</p> <pre><code>docker run -d ubuntu /bin/sh -c \"while true; do date; echo hello world; sleep 1; done\"\n</code></pre> <p>Result</p> <p>Command should return the container ID.</p> <p>Step 2 Let\u2019s use the <code>docker ps</code> command to see running containers:</p> <pre><code>docker ps\n</code></pre> <pre><code> CONTAINER ID IMAGE  COMMAND                  CREATED        STATUS  PORTS NAMES\nac231579e57f ubuntu \"/bin/sh -c 'while tr\"   1 minute ago   Up 11 minute  evil_golick\n</code></pre> <p>Note</p> <p>Container id is going to  be different in your case</p> <p>Hint</p> <p>Instead of using full <code>container-id</code> when building commands, it is possible simply type first few characters of container-id, to make things nice and easy.</p> <p>Step 3 Let\u2019s use <code>container-id</code> to show the container standard output:</p> <pre><code>docker logs &lt;container-id&gt;\n</code></pre> <pre><code>Thu Jan 26 00:23:45 UTC 2017\nhello world\nThu Jan 26 00:23:46 UTC 2017\nhello world\nThu Jan 26 00:23:47 UTC 2017\nhello world\n...\n</code></pre> <p>As you can see, in the <code>docker ps</code> command output, the auto generated container name is <code>evil_golick</code> (your container can have a different name).</p> <p>Step 4 Now, instead of using docker <code>contaier-id</code> use container name to show the container standard output:</p> <pre><code>docker logs &lt;name&gt;\n</code></pre> <pre><code>Thu Jan 26 00:23:51 UTC 2017\nhello world\nThu Jan 26 00:23:52 UTC 2017\nhello world\nThu Jan 26 00:23:53 UTC 2017\nhello world\n...\n</code></pre> <p>Step 5 Finally, let\u2019s stop our container:</p> <pre><code>docker stop &lt;name&gt;\n</code></pre> <p>Step 6 Check, that there are no running containers:</p> <pre><code>docker ps\n</code></pre> <p>Summary</p> <p><code>docker logs</code> is a very usefull command to troubleshoot containers, and going to be used very often both for Docker and Kubernertes troubleshooting.</p>"},{"location":"ycit019_Lab_2_Docker_basics/#16-accessing-containers-from-the-internet","title":"1.6 Accessing Containers from the Internet","text":"<p>Step 1 Let\u2019s run a simple web application. We will use the existing image training/webapp, which contains a Python Flask application:</p> <pre><code>docker run -d -P training/webapp python app.py\n</code></pre> <pre><code>...\nStatus: Downloaded newer image for training/webapp:latest\n6e88f42d3d853762edcbfe1fe73fdc5c48865275bc6df759b83b0939d5bd2456\n</code></pre> <p>In the command above we specified the main process (python app.py), the <code>-d</code> command line argument, which tells Docker to run the container in the background. The <code>-P</code> command line argument tells Docker to map any required network ports inside our container to our host. This allows us to access the web application in the container.</p> <p>Step 2 Use the <code>docker ps</code> command to list running containers:</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID IMAGE           COMMAND         CREATED       STATUS       PORTS                   NAMES\n6e88f42d3d85 training/webapp \"python app.py\" 3 minutes ago Up 3 minutes 0.0.0.0:32768-&gt;5000/tcp determined_torvalds\n</code></pre> <p>The PORTS column contains the mapped ports. In our case, Docker has exposed port 5000 (the default Python Flask port) on port 32768 (can be different in your case).</p> <p>Step 3 The <code>docker port</code> command shows the exposed port. We will use the container name (determined_torvalds in the example above, it can be different in your case):</p> <pre><code>docker port &lt;name&gt; 5000\n0.0.0.0:32768\n</code></pre> <p>Step 4 Let\u2019s check that we can access the web application exposed port:</p> <pre><code>curl http://localhost:&lt;port&gt;/\n</code></pre> <p>Result</p> <p><code>Hello world!</code></p> <p>Step 5 Let\u2019s stop our web application for now:</p> <pre><code>docker stop &lt;name&gt;\n</code></pre> <p>Step 6 We want to manually specify the local port to expose (-p argument). Let\u2019s use the standard HTTP port 80. We also want to specify the container name (--name argument):</p> <pre><code>docker run -d -p 80:5000 --name webapp training/webapp python app.py\n</code></pre> <p>Step 7 Let\u2019s check that the port 80 is exposed:</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID IMAGE           COMMAND         CREATED       STATUS       PORTS                NAMES\n249476631f7d training/webapp \"python app.py\" 1 minute  ago Up 1 minute  0.0.0.0:80-&gt;5000/tcp webapp\n</code></pre> <pre><code>curl http://localhost/\n</code></pre> <p>Result</p> <p>`Hello world!``</p> <p>Step 8 You can also observe <code>Hello world!</code> webapp from you laptop, for that you need to use you public VM IP that can be gather from VMs list:</p> <pre><code>Your_VM_Public_IP\n</code></pre> <p>Than paste VM Public IP address in you browser.</p> <p>Result</p> <p>Our web-app can be accessed from Internet!</p>"},{"location":"ycit019_Lab_2_Docker_basics/#17-restart-a-container","title":"1.7 Restart a container","text":"<p>Step 1 Let\u2019s stop the container with web application:</p> <pre><code>docker stop webapp\n</code></pre> <p>The main process inside of the container will receive SIGTERM, and after a grace period, SIGKILL.</p> <p>Step 2 You can start the container later using the <code>docker start</code> command:</p> <pre><code>docker start webapp\n</code></pre> <p>Step 3 Check that the web application works:</p> <pre><code>curl http://localhost/\nHello world!\n</code></pre> <p>Step 4 You also can restart the running container using the docker restart command.</p> <pre><code>docker restart webapp\n</code></pre> <p>Step 4  Run <code>docker ps</code> command and check  <code>STATUS</code> field:</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID    IMAGE             COMMAND           CREATED           STATUS              \n6e400179070f    training/webapp   \"python app.py\"   25 minutes ago    Up 3 seconds\n</code></pre>"},{"location":"ycit019_Lab_2_Docker_basics/#18-ensuring-container-uptime","title":"1.8 Ensuring Container Uptime","text":"<p>Docker considers any containers to exit with a non-zero exit code to have crashed. By default a crashed container will remain stopped.</p> <p>Step 1 Start the container that outputs a message and then exits with code 1 to simulate a crash.</p> <pre><code>docker run -d --name restart-default scrapbook/docker-restart-example\n</code></pre> <pre><code>docker ps -a | grep restart-default\n</code></pre> <pre><code>CONTAINER ID  IMAGE                             CREATED       STATUS               NAMES\nc854289d2f39  scrapbook/docker-restart-example  5 seconds ago Exited 3 sec ago    restart-default\n$ docker logs restart-default\nSun Sep 17 20:34:55 UTC 2017 Booting up...\n</code></pre> <p>Result</p> <p>Container crushed and exited.</p> <p>However, there are several ways to ensure that you container up and running even if it\u2019s restarts.</p> <p>Step 2 The option <code>--restart=on-failure</code>: allows you to say how many times Docker should try again:</p> <pre><code>docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example\n</code></pre> <pre><code>docker logs restart-3\n</code></pre> <pre><code>Thu Apr 20 14:01:27 UTC 2017 Booting up...\nThu Apr 20 14:01:28 UTC 2017 Booting up...\nThu Apr 20 14:01:29 UTC 2017 Booting up...\nThu Apr 20 14:01:31 UTC 2017 Booting up...\n</code></pre> <p>Step 3 Finally, Docker can always restart a failed container. In this case, Docker will keep trying until the container is explicitly told to stop.</p> <pre><code>docker run -d --name restart-always --restart=always scrapbook/docker-restart-example\ndocker logs restart-always\n</code></pre> <p>Step 4  After sometime stop running docker container, as it will be keep failing and starting again:</p> <pre><code>docker stop restart-always\n</code></pre>"},{"location":"ycit019_Lab_2_Docker_basics/#19-inspect-a-container","title":"1.9 Inspect a container","text":"<p>Step 1 You can use the <code>docker inspect</code> command to see the configuration and status information for the specified container:</p> <pre><code>docker inspect webapp\n</code></pre> <pre><code>[\n    {\n        \"Id\": \"249476631f7d...\",\n        \"Created\": \"2016-08-02T23:42:56.932135327Z\",\n        \"Path\": \"python\",\n        \"Args\": [\n            \"app.py\"\n        ],\n        \"State\": {\n            \"Status\": \"running\",\n            \"Running\": true,\n            \"Paused\": false,\n            \"Restarting\": false,\n            \"OOMKilled\": false,\n            \"Dead\": false,\n            \"Pid\": 16055,\n            \"ExitCode\": 0,\n            \"Error\": \"\",\n            ...\n</code></pre> <p>Step 2 You can specify a filter (-f command line argument) to show only specific elements. For example:</p> <pre><code>docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp\n</code></pre> <pre><code>172.17.0.2\n</code></pre> <p>The command returns the IP address of the container.</p>"},{"location":"ycit019_Lab_2_Docker_basics/#110-interacting-with-containers","title":"1.10 Interacting with containers","text":"<p>In some cases using <code>docker log</code> is not enough to undertand issues and you want to login inside of running VM. Also sometimes you package you applicaiton and in order to run it you need to login inside of container and execute and leave it running in background. Below provded few ways to interacting with containers that can help to achive descrined use cases.</p>"},{"location":"ycit019_Lab_2_Docker_basics/#1101-detach-from-interactive-container","title":"1.10.1 Detach from Interactive container","text":"<p>In Module, <code>1.4 Run an interactive container</code> we run an <code>Ubuntu</code> container with <code>-it</code> flag and able directly login inside of the container to interact with it, however after we exited contianer using <code>Ctrl-D</code> or <code>exit</code> command container stopped. However you can exit from <code>Interactive mode</code> without stoping a container. Let's demonstrate how this works:</p> <p>Step 1 Start Ubunu container in interactive  mode:</p> <pre><code>docker run -it ubuntu /bin/bash\n</code></pre> <p>Step 2 Run <code>watch date</code> command inside running container in order to exit <code>date</code> command every 2 seconds.</p> <pre><code>root@1d688a9f4ed4:/# watch date\n</code></pre> <p>Step 3 Detach from a container and leave it running using the <code>CTRL-p</code> <code>CTRL-q</code> key sequence.</p> <p>Step 4 Verify that Ubuntu container is still running:</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID  IMAGE   COMMAND      CREATED        STATUS        NAMES\n1d688a9f4ed4  ubuntu  \"/bin/bash\"  1 minutes ago  Up 1 minutes  admiring_lovelace\n</code></pre> <p>Result</p> <p>Great you were able to detach from Docker container without stopping it, while it is executing a process in it. What about attaching back to container ?</p> <p>Important</p> <p><code>CTRL-p</code> <code>CTRL-q</code>  sequence key only works if docker contaienr started with <code>-it</code> command!</p>"},{"location":"ycit019_Lab_2_Docker_basics/#1112-attach-to-a-container","title":"1.11.2 Attach to a container","text":"<p>Now let's get back and <code>attach</code> to our running Ubuntu image. For that docker provides <code>docker attach</code> command.</p> <pre><code>docker attach &lt;container name&gt;\n</code></pre> <pre><code>Every 2.0s: date                                                                                                                    Mon Sep 18 00:08:57 2017\n</code></pre> <p>Summary</p> <p><code>docker attach</code> attaches your contairs terminal\u2019s standard input, output, and error (or any combination of the 3) to a running container. This allows you to view its ongoing output or to control it interactively, as though the commands were running directly in your terminal.</p>"},{"location":"ycit019_Lab_2_Docker_basics/#1113-execute-a-process-in-a-container","title":"1.11.3 Execute a process in a container","text":"<p>Step 1 Let verify if webapp container is still running</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID IMAGE           COMMAND         CREATED       STATUS       PORTS                NAMES\n249476631f7d training/webapp \"python app.py\" 1 minute  ago Up 1 minute  0.0.0.0:80-&gt;5000/tcp webapp\n</code></pre> <p>If not running start it with following command: <code>$ docker run -d -p 80:5000 --name webapp training/webapp python app.py</code> other wise skip to next step.</p> <p>Step 2 Use the docker exec command to execute a command in the running container. For example:</p> <pre><code>docker exec webapp ps aux\n</code></pre> <pre><code>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.2  0.0  52320 17384 ?        Ss   00:11   0:00 python app.py\nroot        26  0.0  0.0  15572  2104 ?        Rs   00:12   0:00 ps aux\n</code></pre> <p>The same command with the <code>-it</code> command line argument can be used to run an interactive session in the container:</p> <pre><code>docker exec -it webapp bash\nroot@249476631f7d:/opt/webapp# ps auxw\nps auxw\n</code></pre> <pre><code>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.0  0.0  52320 17384 ?        Ss   00:11   0:00 python app.py\nroot        32  0.0  0.0  18144  3064 ?        Ss   00:14   0:00 bash\nroot        47  0.0  0.0  15572  2076 ?        R+   00:16   0:00 ps auxw\n</code></pre> <p>Step 2 Use the <code>exit</code> command or press <code>Ctrl-D</code> to exit the interactive bash session:</p> <pre><code>root@249476631f7d:/opt/webapp# exit\n</code></pre> <p>Summary</p> <p><code>docker exec</code> is one of the most usefull docker commands used for troubleshooting containers.</p>"},{"location":"ycit019_Lab_2_Docker_basics/#112-copy-files-tofrom-container","title":"1.12 Copy files to/from container","text":"<p>The <code>docker cp</code> command allows you to copy files from the container to the local machine or from the local file system to the container. This command works for a running or stopped container.</p> <p>Step 1 Let\u2019s copy the container\u2019s app.py file to the local machine:</p> <pre><code>docker cp webapp:/opt/webapp/app.py .\n</code></pre> <p>Step 2 Edit the local app.py file. For example, change the line return 'Hello '+provider+'!' to return 'Hello '+provider+'!!!'. Copy the modified file back and restart the container:</p> <pre><code>docker cp app.py webapp:/opt/webapp/\n\ndocker restart webapp\n</code></pre> <p>Step 3 Check that the modified web application works::</p> <pre><code>curl http://localhost/\n</code></pre> <p>Result</p> <p>`Hello world!!!``</p>"},{"location":"ycit019_Lab_2_Docker_basics/#112-remove-containers","title":"1.12 Remove containers","text":"<p>Now let's clean up the environment and at the same time learn how delete containers.</p> <p>Step 1 First list running containers:</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID IMAGE           COMMAND         CREATED       STATUS       PORTS                NAMES\n81c4c66baaf9 training/webapp \"python app.py\" 1 minute  ago Up 1 minute  0.0.0.0:80-&gt;5000/tcp webapp\n</code></pre> <p>Step 2 Than try to delete running container using <code>docker rm &lt;container_id&gt;</code></p> <pre><code>docker rm $container_id\n</code></pre> <pre><code>Error response from daemon: You cannot remove a running container 81c4c66baaf9. Stop the container before attempting removal or force remove.\n</code></pre> <p>Failure</p> <p>Docker containers needs to be first stopped or deleted using <code>--force</code> flag.</p> <pre><code>docker rm $container_id -f\n</code></pre> <p>Alternatively, you can run <code>stop</code> and <code>rm</code> in sequence:</p> <pre><code>docker stop 81c4c66baaf9\ndocker rm 81c4c66baaf9\n</code></pre> <p>Summary</p> <p>We've learned a lot of docker commands which are very handy to know both when using Docker and Kubernetes. We've also learned how to create Docker Images from DOCKERFILE.</p>"},{"location":"ycit019_Lab_3_Advanced_Docker/","title":"ycit019 Lab 3 Advanced Docker","text":"<p>Lab 3 Docker Networking, Persistence, Monitoring and Logging</p> <p>Objective:</p> <ul> <li>Networks<ul> <li>Docker basics</li> <li>User-defined private Networks</li> </ul> </li> <li>Persistence<ul> <li>Data Volumes</li> </ul> </li> </ul>"},{"location":"ycit019_Lab_3_Advanced_Docker/#1-docker-networking","title":"1 Docker Networking","text":""},{"location":"ycit019_Lab_3_Advanced_Docker/#11-docker-networking-basics","title":"1.1 Docker Networking Basics","text":"<p>Step 1: The Docker Network Command The <code>docker network</code> command is the main command for configuring and managing container networks. Run the <code>docker network</code> command from the first terminal.</p> <pre><code>docker network\n</code></pre> <pre><code>Usage:  docker network COMMAND\n\nManage networks\n\nOptions:\n      --help   Print usage\n\nCommands:\n  connect     Connect a container to a network\n  create      Create a network\n  disconnect  Disconnect a container from a network\n  inspect     Display detailed information on one or more networks\n  ls          List networks\n  prune       Remove all unused networks\n  rm          Remove one or more networks\n\nRun 'docker network COMMAND --help' for more information on a command.\n</code></pre> <p>The command output shows how to use the command as well as all of the <code>docker network</code> sub-commands. As you can see from the output, the <code>docker network</code> command allows you to create new networks, list existing networks, inspect networks, and remove networks. It also allows you to connect and disconnect containers from networks.</p> <p>Step 2 Run a <code>docker network ls</code> command to view existing container networks on the current Docker host.</p> <pre><code>docker network ls\n</code></pre> <pre><code>NETWORK ID          NAME                DRIVER              SCOPE\n3430ad6f20bf        bridge              bridge              local\na7449465c379        host                host                local\n06c349b9cc77        none                null                local\n</code></pre> <p>The output above shows the container networks that are created as part of a standard installation of Docker.</p> <p>New networks that you create will also show up in the output of the <code>docker network ls</code> command.</p> <p>You can see that each network gets a unique <code>ID</code> and <code>NAME</code>. Each network is also associated with a single driver. Notice that the \"bridge\" network and the \"host\" network have the same name as their respective drivers.</p> <p>Step 3: The <code>docker network inspect</code> command is used to view network configuration details. These details include; name, ID, driver, IPAM driver, subnet info, connected containers, and more.</p> <p>Use <code>docker network inspect &lt;network&gt;</code> to view configuration details of the container networks on your Docker host. The command below shows the details of the network called <code>bridge</code>.</p> <pre><code>docker network inspect bridge\n</code></pre> <pre><code>[\n    {\n        \"Name\": \"bridge\",\n        \"Id\": \"3430ad6f20bf1486df2e5f64ddc93cc4ff95d81f59b6baea8a510ad500df2e57\",\n        \"Created\": \"2017-04-03T16:49:58.6536278Z\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.17.0.0/16\",\n                    \"Gateway\": \"172.17.0.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Containers\": {},\n        \"Options\": {\n            \"com.docker.network.bridge.default_bridge\": \"true\",\n            \"com.docker.network.bridge.enable_icc\": \"true\",\n            \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n            \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n            \"com.docker.network.bridge.name\": \"docker0\",\n            \"com.docker.network.driver.mtu\": \"1500\"\n        },\n        \"Labels\": {}\n    }\n]\n</code></pre> <p>Note</p> <p>The syntax of the <code>docker network inspect</code> command is <code>docker network inspect &lt;network&gt;</code>, where <code>&lt;network&gt;</code> can be either network name or network ID. In the example above we are showing the configuration details for the network called \"bridge\". Do not confuse this with the \"bridge\" driver.</p> <p>Step 4 Now, list Docker supported network driver plugins. For that run <code>docker info</code> command, that  shows a lot of interesting information about a Docker installation.</p> <p>Run the <code>docker info</code> command and locate the list of network plugins.</p> <pre><code>docker info\n</code></pre> <pre><code>Containers: 0\n Running: 0\n Paused: 0\n Stopped: 0\nImages: 0\nServer Version: 17.03.1-ee-3\nStorage Driver: aufs\n&lt;Snip&gt;\nPlugins:\n Volume: local\n Network: bridge host macvlan null overlay\nSwarm: inactive\nRuntimes: runc\n&lt;Snip&gt;\n</code></pre> <p>The output above shows the bridge, host,macvlan, null, and overlay drivers.</p> <p>Summary</p> <p>We've quickly reviewed available docker networking commands as well as found what drivers current docker setup supports.</p>"},{"location":"ycit019_Lab_3_Advanced_Docker/#12-default-bridge-network","title":"1.2 Default bridge network","text":"<p>Every clean installation of Docker comes with a pre-built network called Default bridge network. Let's explore in more details how it works.</p> <p>Step 1 Verify this with the <code>docker network ls</code>.</p> <pre><code>docker network ls\n</code></pre> <pre><code>NETWORK ID          NAME                DRIVER              SCOPE\n3430ad6f20bf        bridge              bridge              local\na7449465c379        host                host                local\n06c349b9cc77        none                null                local\n</code></pre> <p>Result</p> <p>The output above shows that the bridge network is associated with the bridge driver. It's important to note that the network and the driver are connected, but they are not the same. In this example the network and the driver have the same name - but they are not the same thing!</p> <p>The output above also shows that the bridge network is scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the bridge driver - the bridge driver provides single-host networking.</p> <p>All networks created with the bridge driver are based on a Linux bridge (a.k.a. a virtual switch).</p> <p>Step 5 Start webapp in Default bridge network</p> <pre><code>docker run -d -p 80:5000 --name webapp training/webapp python app.py\n</code></pre> <p>Step 6 Check that the webapp and db containers are running:</p> <p>Command:</p> <pre><code>docker ps\n</code></pre>"},{"location":"ycit019_Lab_3_Advanced_Docker/#13-user-defined-private-networks","title":"1.3 User-defined Private Networks","text":"<p>So far we\u2019ve learned how Docker networking works with Docker default bridge network. With the introduction of user-defined networking in Docker 1.9, it is now possible to create multiple Docker bridges to allow network segregation within the same host or multi-host networking to allow communicate Docker containers between hosts.</p> <p>The commands are available through the Docker Engine CLI are:</p> <pre><code>docker network create\ndocker network connect\ndocker network ls\ndocker network rm\ndocker network disconnect\ndocker network inspect\n</code></pre> <p>Let's demonstrate how to create a custom bridge network.</p> <p>Step 1 By default, Docker runs containers in the bridge network. You may want to isolate one or more containers in a separate network. Let\u2019s create a new network:</p> <pre><code>docker network create my-network \\\n-d bridge \\\n--subnet 172.19.0.0/16\n</code></pre> <p>The <code>-d</code> bridge command line argument specifies the bridge network driver and the <code>--subnet</code> command line argument specifies the network segment in CIDR format. If you do not specify a subnet when creating a network, then Docker assigns a subnet automatically, so it is a good idea to specify a subnet to avoid potential conflicts with the existing networks.</p> <p>Below are some other options that are available with the bridge Driver:</p> <ul> <li> <p>com.docker.network.bridge.enable_ip_masquerade: This instructs the Docker   host to hide or masquerade all containers in this network behind the Docker   host's interfaces if the container attempts to route off the local host .</p> </li> <li> <p>com.docker.network.bridge.name: This is the name you wish to give to the   bridge.</p> </li> <li> <p>com.docker.network.bridge.enable_icc: This turns on or off Inter-Container   Connectivity (ICC) mode for the bridge.</p> </li> <li> <p>com.docker.network.bridge.host_binding_ipv4: This defines the host interface   that should be used for port binding.</p> </li> <li> <p>com.docker.network.driver.mtu: This sets MTU for containers attached to this   bridge.</p> </li> </ul> <p>Step 2 To check that the new network is created, execute docker network ls:</p> <pre><code>docker network ls\n</code></pre> <pre><code>NETWORK ID          NAME                DRIVER              SCOPE\nd428e49e4869        bridge              bridge              local\n0d1f78528cc5        host                host                local\n56ef0481820d        my-network          bridge              local\n4a07cef84617        none                null                local\n</code></pre> <p>Step 3 Let\u2019s inspect the new network:</p> <pre><code>docker network inspect my-network\n</code></pre> <pre><code>[\n    {\n        \"Name\": \"my-network\",\n        \"Id\": \"56ef0481820d...\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": {},\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.19.0.0/16\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Containers\": {},\n        \"Options\": {},\n        \"Labels\": {}\n    }\n]\n</code></pre> <p>Step 4 As expected, there are no containers connected to the my-network. Let\u2019s recreate the db container in the my-network:</p> <pre><code>docker rm -f db\n</code></pre> <pre><code>docker run -d --network=my-network --name db training/postgres\n</code></pre> <p>Step 5 Inspect the <code>my-network</code>again:</p> <pre><code>docker network inspect my-network\n</code></pre> <p>Output:</p> <pre><code>    \"Containers\": {\n        \"93af62cdab64...\": {\n            \"Name\": \"db\",\n            \"EndpointID\": \"b1e8e314cff0...\",\n            \"MacAddress\": \"02:42:ac:12:00:02\",\n            \"IPv4Address\": \"172.19.0.2/16\",\n            \"IPv6Address\": \"\"\n        }\n    },\n...\n</code></pre> <p>As you see, the <code>db</code> container is connected to the my-network and has 172.19.0.2 address.</p> <p>Step 6 Let\u2019s start an interactive session in the db container and ping the IP address of the webapp again:</p> <p>Note</p> <p>Quick reminder how to locate webapp ip:</p> <p><code>docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp</code></p> <pre><code>docker exec -it db bash\n</code></pre> <p>Once inside of container run:</p> <pre><code>root@c3afff20019a:/# ping -c 1 172.17.0.3\nPING 172.17.0.3 (172.17.0.3) 56(84) bytes of data.\n\n--- 172.17.0.3 ping statistics ---\n1 packets transmitted, 0 received, 100% packet loss, time 0ms\n</code></pre> <p>As expected, the webapp container is no longer accessible from the db container, because they are connected to different networks.</p> <p>Summary</p> <p>Using <code>Multi-host networking</code> provides network isolation within a Docker host via network namepsaces. This is can be used if you want to deploy different applications on same host for isolation or resource duplicate prevention.</p> <p>Step 7 Let\u2019s connect the webapp container to the my-network:</p> <pre><code>docker network connect my-network webapp\n</code></pre> <p>Step 8 Check that the webapp container now is connected to the my-network:</p> <pre><code>docker network inspect my-network\n</code></pre> <p>Output:</p> <pre><code>...\n    \"Containers\": {\n        \"62ed4a627356...\": {\n            \"Name\": \"webapp\",\n            \"EndpointID\": \"ae95b0103bbc...\",\n            \"MacAddress\": \"02:42:ac:12:00:03\",\n            \"IPv4Address\": \"172.19.0.3/16\",\n            \"IPv6Address\": \"\"\n        },\n        \"93af62cdab64...\": {\n            \"Name\": \"db\",\n            \"EndpointID\": \"b1e8e314cff0...\",\n            \"MacAddress\": \"02:42:ac:12:00:02\",\n            \"IPv4Address\": \"172.19.0.2/16\",\n            \"IPv6Address\": \"\"\n        }\n    },\n...\n</code></pre> <p>The output shows that two containers are connected to the my-network and the webapp container has 172.19.0.3 address in that network.</p> <p>Step 9 Check that the webapp container is accessible from the db container using its new IP address:</p> <pre><code>docker exec -it db bash\n</code></pre> <pre><code>root@c3afff20019a:/# ping -c 1 172.19.0.3\nPING 172.19.0.3 (172.19.0.3) 56(84) bytes of data.\n64 bytes from 172.19.0.3: icmp_seq=1 ttl=64 time=0.136 ms\n\n--- 172.19.0.3 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms\n</code></pre> <p>Success</p> <p>As expected containers can communicate with each other.</p> <p>Step 10 You can now remove the existing container. You should stop the container before removing it. Alternatively you can use the -f command line argument:</p> <pre><code>docker rm -f webapp\ndocker rm -f db\ndocker network rm  my-network\n</code></pre> <p>Hint</p> <p>Use below command to delete running containers in bulk:</p> <p><code>docker rm -f $(docker ps -q)</code></p> <p>Summary</p> <p>It is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses</p>"},{"location":"ycit019_Lab_3_Advanced_Docker/#14-access-containers-from-outside","title":"1.4 Access containers from outside","text":"<p>External Access to the Containers can be configured via publishing mechanism.</p> <p>Docker provides 2 options to publish ports:</p> <ul> <li><code>-P</code> flag publishes all exposed ports</li> <li><code>-p</code> flag allows you to specify specific ports and interfaces to use when     mapping ports.</li> </ul> <p>The <code>-p</code> flag can take several different forms with the syntax looking like this:</p> <ul> <li> <p>Specify the host port and container port:</p> <p><code>\u2013p &lt;host port&gt;:&lt;container port&gt;</code></p> </li> <li> <p>Specify the host interface, host port, and container port:</p> <p><code>\u2013p &lt;host IP interface&gt;:&lt;host port&gt;:&lt;container port&gt;</code></p> </li> <li> <p>Specify the host interface, have Docker choose a random host port, and specify the container port:</p> <p><code>\u2013p &lt;host IP interface&gt;::&lt;container port&gt;</code></p> </li> <li> <p>Specify only a container port and have Docker use a random host port:</p> <p><code>\u2013p &lt;container port&gt;</code></p> </li> </ul> <p>Let's test exposing containers. For that let's start a new NGINX container and map port 8080 on the Docker host to port 80 inside of the container. This means that traffic that hits the Docker host on port 8080 will be passed on to port 80 inside the container.</p> <p>Note</p> <p>If you start a new container from the official NGINX image without specifying a command to run, the container will run a basic web server on port 80.</p> <p>Step 1 Start a new container based off the official NGINX image by running <code>docker run --name web1 -d -p 8080:80 nginx</code>.</p> <pre><code>docker run --name web1 -d -p 8080:80 nginx\n</code></pre> <pre><code>Unable to find image 'nginx:latest' locally\nlatest: Pulling from library/nginx\n6d827a3ef358: Pull complete\nb556b18c7952: Pull complete\n03558b976e24: Pull complete\n9abee7e1ef9d: Pull complete\nDigest: sha256:52f84ace6ea43f2f58937e5f9fc562e99ad6876e82b99d171916c1ece587c188\nStatus: Downloaded newer image for nginx:latest\n4e0da45b0f169f18b0e1ee9bf779500cb0f756402c0a0821d55565f162741b3e\n</code></pre> <p>Step 2 Review the container status and port mappings by running <code>docker ps</code>.</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID   IMAGE   COMMAND                  PORTS                           NAMES\n4e0da45b0f16   nginx   \"nginx -g 'daemon ...\"   443/tcp, 0.0.0.0:8080-&gt;80/tcp   web1\n</code></pre> <p>Result</p> <p>The top line shows the new web1 container running NGINX. Take note of the command the container is running as well as the port mapping - <code>0.0.0.0:8080-&gt;80/tcp</code> maps port 8080 on all host interfaces to port 80 inside the web1 container. This port mapping is what effectively makes the containers web service accessible from external sources (via the Docker hosts IP address on port 8080).</p> <p>Step 3 Test connectivity to the NGINX web server, by pasting <code>&lt;Public_IP:8080&gt;</code> of VM to the browser.</p> <p>Note</p> <p>In order to locate Public IP see the list of VMs.</p> <p>Alternatively from inside of VM run <code>curl 127.0.0.1:8080</code> command.</p> <pre><code>curl 127.0.0.1:8080\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;Snip&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n    &lt;Snip&gt;\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Success</p> <p>Both CLI and UI method works!</p> <p>If you try and curl the IP address on a different port number it will fail.</p> <p>Summary</p> <p>Docker provides easy way to expose containers outside of the Docker Node. This can ber used for connecting containers between each other:</p> <ul> <li>Between networks on the same host</li> <li>Between networks on different host</li> <li>Accessing containers from outside (e.g web site)</li> </ul> <p>However, port mapping is implemented via port address translation (PAT) unlike in Kubernetes which we learn soon, exposes applications via service IPs and communicates via POD IPs using (NAT)</p> <p>Step 4 Cleanup environment</p> <pre><code>docker rm -f $(docker ps -q)\n</code></pre>"},{"location":"ycit019_Lab_3_Advanced_Docker/#2-persistant-volumes","title":"2 Persistant Volumes","text":""},{"location":"ycit019_Lab_3_Advanced_Docker/#21-storage-driver","title":"2.1 Storage driver","text":"<p>We've discussed several Storage drivers (graphdrivers) during the class. Let's find out what graphdriver is running in our Lab environment.</p> <pre><code>docker info | grep  Storage\n</code></pre> <pre><code>WARNING: No swap limit support\nStorage Driver: aufs\n</code></pre> <p>Result</p> <p>Our Classroom is running <code>aufs</code> storage driver. Not a suprise as we running our Lab on Ubuntu VM.</p> <p>Summary</p> <p>Systems runnng Ubuntu or Debian ,going to run <code>aufs</code> graphdriver by default and will most likely meet the majority of your needs. In future <code>overlay2</code> may replace <code>aufs</code> stay tunned!</p>"},{"location":"ycit019_Lab_3_Advanced_Docker/#22-persisting-data-using-volumes","title":"2.2 Persisting Data Using Volumes","text":"<p>Docker Volumes are created and assigned when containers are started. Data Volumes allow you to map a host directory to a container for sharing data.</p> <p>This mapping is bi-directional. It allows data stored on the host to be accessed from within the container. It also means data saved by the process inside the container is persisted on the host.</p>"},{"location":"ycit019_Lab_3_Advanced_Docker/#221-create-and-manage-volumes","title":"2.2.1  Create and manage volumes","text":"<p>Step 1 Create a volume:</p> <pre><code>docker volume create --name my-vol\n</code></pre> <p>Step 2 List volumes:</p> <pre><code>docker volume ls\n</code></pre> <pre><code>Output:\nlocal               my-vol\n</code></pre> <p>Step 3 Inspect a volume:</p> <pre><code>docker volume inspect my-vol\n</code></pre> <pre><code>[\n    {\n        \"Driver\": \"local\",\n        \"Labels\": {},\n        \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\",\n        \"Name\": \"my-vol\",\n        \"Options\": {},\n        \"Scope\": \"local\"\n    }\n]\n</code></pre> <p>Step 3 Add some data to the <code>Mountpoint</code> of the volume:</p> <pre><code>sudo touch  /var/lib/docker/volumes/my-vol/_data/test_vol\nsudo ls  /var/lib/docker/volumes/my-vol/_data/\n</code></pre> <p>Step 4 Create a container <code>busybox</code> alpine image and attach created <code>my-vol</code> volume in to it:</p> <pre><code>docker run -it -v my-vol:/world busybox\n</code></pre> <pre><code>/ # ls /world\ntest_vol\n/ #\n</code></pre> <p>Result</p> <p>Volume is mounted and <code>test_vol</code> file is under <code>/world</code> folder as expected</p> <p>Step 5  Try to delete the volume:</p> <pre><code>docker volume rm  my-vol\n</code></pre> <pre><code>Error response from daemon: unable to remove volume: remove my-vol: volume is in use - [6ef3055b516b306847150af8fcea796c02cd90578967802ac29c39d3a2c90102]\n</code></pre> <p>Failure</p> <p>Deleting container that is attached is not permited. However you can delete with <code>-f</code> option</p> <p>Step 5  Busybox container stopped, howerver it is not deleted. Let's locate stopped <code>busybox</code> container and delete it:</p> <pre><code>docker ps -a | grep busybox\n</code></pre> <pre><code>docker rm $docker_id\n</code></pre> <p>Step 6 You can now delete <code>my-vol</code></p> <p>Note</p> <p>Volume is still avaiable if needed to be reattached any time</p> <pre><code>docker volume ls\ndocker volume rm my-vol\ndocker volume ls\n</code></pre> <p>Summary</p> <p>Volumes can be craeted and managed separately from containers.</p>"},{"location":"ycit019_Lab_3_Advanced_Docker/#222-start-a-container-with-a-volume","title":"2.2.2 Start a container with a volume","text":"<p>If you start a container with a volume that does not yet exist, Docker creates the volume for you.</p> <p>Step 1 Add a data volume to a container:</p> <pre><code>docker run -d -P --name webapp -v /webapp training/webapp python app.py\n</code></pre> <p>Result</p> <p>Command started a new container and created a new volume inside the container at /webapp.</p> <p>Step 2 Locate the volume on the host using the docker inspect command:</p> <pre><code> docker inspect webapp | grep -A9 Mounts\n ```\n **Output:**\n ```       \"Mounts\": [\n            {\n                \"Type\": \"volume\",\n                \"Name\": \"39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\",\n                \"Source\": \"/var/lib/docker/volumes/39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d/_data\",\n                \"Destination\": \"/webapp\",\n                \"Driver\": \"local\",\n                \"Mode\": \"\",\n                \"RW\": true,\n                \"Propagation\": \"\"\n</code></pre> <p>Step 3 List container</p> <pre><code>docker volume ls\n</code></pre> <p>Output:</p> <pre><code>DRIVER              VOLUME NAME\nlocal               39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\n</code></pre> <p>Step 5 Alternatively, you can specify a host directory you want to use as a data volume:</p> <pre><code>mkdir db\n\ndocker run -d --name db -v ~/db:/db training/postgres\n</code></pre> <p>Step 2 Start an interactive session in the db container and create a new file in the /db directory:</p> <pre><code>docker exec -it db bash\n</code></pre> <p>Type inside docker containers console:</p> <pre><code>root@9a7a4fbcc929:/# cd /db\n\nroot@9a7a4fbcc929:/db# touch hello_from_db_container\n\nroot@9a7a4fbcc929:/db# exit\n</code></pre> <p>Step 4 Check that the local db directory contains the new file:</p> <pre><code>ls db\nhello_from_db_container\n</code></pre> <p>Step 5 Check that the data volume is persistent. Remove the db container:</p> <pre><code>docker rm -f db\n</code></pre> <p>Step 6 Create the db container again:</p> <pre><code>docker run -d --name db -v ~/db:/db training/postgres\n</code></pre> <p>Step 7 Check that its /db directory contains the hello_from_db_container file:</p> <pre><code>docker exec -it db bash\n</code></pre> <p>Run commands inside container:</p> <pre><code>root@47a60c01590e:/# ls /db\nhello_from_db_container\nroot@47a60c01590e:/# exit\n</code></pre>"},{"location":"ycit019_Lab_3_Advanced_Docker/#223-use-a-read-only-volume","title":"2.2.3 Use a read-only volume","text":"<p>Step 1 Mounting Volumes gives the container full read and write access to the directory. You can specify read-only permissions on the directory by adding the permissions :ro to the mount. If the container attempts to modify data within the directory it will error.</p> <pre><code>docker run -d --name db1 -v ~/db:/db:ro training/postgres\ndocker exec -it db1 bash\ncd db\ntouch test\n</code></pre> <p>Result</p> <pre><code>touch: cannot touch 'test': Read-only file system\n$ exit\n</code></pre> <p>** Step 2** Clean up containers and volumes:</p> <pre><code>docker rm -f $(docker ps -q)\ndocker volume ls\n</code></pre> <p>Output</p> <pre><code>DRIVER              VOLUME NAME\nlocal               39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\n</code></pre> <pre><code>docker volume rm 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\n</code></pre> <p>Summary</p> <p>We've learned how to manage volumes with containers</p> <p>Hint</p> <p>If you Docker host has several Storage plugins configured (e.g. ceph, gluster) you can specify via <code>--opt type=btrfs, nfs</code> or <code>--driver=glusterfs</code> during docker volume creation.</p>"},{"location":"ycit019_Lab_5_Docker_Compose/","title":"ycit019 Lab 5 Docker Compose","text":"<p>Lab 5 Docker Compose and Docker Security</p> <p>Objective:</p> <ul> <li>Practice to use Docker Compose, </li> </ul>"},{"location":"ycit019_Lab_5_Docker_Compose/#1-docker-security","title":"1 Docker Security","text":""},{"location":"ycit019_Lab_5_Docker_Compose/#11-scan-images-with-trivy","title":"1.1 Scan images with Trivy","text":"<p>Trivy (tri pronounced like trigger, vy pronounced like envy) is a simple and comprehensive vulnerability scanner for containers and other artifacts. A software vulnerability is a glitch, flaw, or weakness present in the software or in an Operating System. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and application dependencies (Bundler, Composer, npm, yarn, etc.). Trivy is easy to use. Just install the binary and you're ready to scan. All you need to do for scanning is to specify a target such as an image name of the container.</p> <p>Step 1  Install Trivy</p> <pre><code>sudo apt-get install wget apt-transport-https gnupg lsb-release\nwget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -\necho deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list\nsudo apt-get update\nsudo apt-get install trivy\n</code></pre> <p>Step 2 Specify an image name (and a tag).</p> <pre><code>$ trivy image [YOUR_IMAGE_NAME]\n</code></pre> <p>For example:</p> <pre><code>$ trivy image python:3.4-alpine\n</code></pre> <pre><code>2019-05-16T01:20:43.180+0900    INFO    Updating vulnerability database...\n2019-05-16T01:20:53.029+0900    INFO    Detecting Alpine vulnerabilities...\n\npython:3.4-alpine3.9 (alpine 3.9.2)\n===================================\nTotal: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)\n\n+---------+------------------+----------+-------------------+---------------+--------------------------------+\n| LIBRARY | VULNERABILITY ID | SEVERITY | INSTALLED VERSION | FIXED VERSION |             TITLE              |\n+---------+------------------+----------+-------------------+---------------+--------------------------------+\n| openssl | CVE-2019-1543    | MEDIUM   | 1.1.1a-r1         | 1.1.1b-r1     | openssl: ChaCha20-Poly1305     |\n|         |                  |          |                   |               | with long nonces               |\n+---------+------------------+----------+-------------------+---------------+--------------------------------+\n</code></pre> <p>Step 3  Explore local images in your environment. </p>"},{"location":"ycit019_Lab_5_Docker_Compose/#2-docker-compose","title":"2 Docker Compose","text":"<p>In this module, will guide you through the process of building a multi-container application using docker compose. The application code is available at GitHub: https://github.com/Cloud-Architects-Program/ycit019</p>"},{"location":"ycit019_Lab_5_Docker_Compose/#21-deploy-guestbook-app-with-compose","title":"2.1 Deploy Guestbook app with Compose","text":"<p>Let\u2019s build another application. This time we going to create famous Guestbook application.</p> <p>Guestbook consists of three services. A redis-master node, a set of redis-slave that can be scaled and find the redis-master via its DNS name. And a PHP frontend that exposes itself on port 80. The resulting application allows you to leave short messages which are stored in the redis cluster.</p> <p>Step 1 Change directory to the guestbook</p> <pre><code>cd ~/ycit019/Module5/guestbook/\nls\n</code></pre> <p>Step 2 Let\u2019s review the docker-guestbook.yml file</p> <pre><code>version: \"2\"\n\nservices:\n redis-master:\n   image: gcr.io/google_containers/redis:e2e\n   ports:\n     - \"6379\"\n redis-slave:\n   image: gcr.io/google_samples/gb-redisslave:v1\n   ports:\n     - \"6379\"\n   environment:\n     - GET_HOSTS_FROM=dns\n frontend:\n   image: gcr.io/google-samples/gb-frontend:v4\n   ports:\n     - \"80:80\"\n   environment:\n     - GET_HOSTS_FROM=dns\n</code></pre> <p>Step 3 Let\u2019s run docker-guestbook.yml with compose</p> <pre><code>export LD_LIBRARY_PATH=/usr/local/lib\ndocker-compose -f docker-guestbook.yml up -d\n</code></pre> <pre><code>Creating network \"examples_default\" with the default driver\nCreating examples_redis-slave_1\nCreating examples_frontend_1\nCreating examples_redis-master_1\n</code></pre> <p>Note</p> <p><code>-d</code> -  Detached mode: Run containers in the background, print new container names.</p> <p><code>-f</code> -  Specify an alternate compose file (default: docker-compose.yml)</p> <p>Step 4 Check that all containers are running:</p> <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID        IMAGE                                    COMMAND\nd1006d1beee5        gcr.io/google-samples/gb-frontend:v4     \"apache2-foreground\"\nfb3a15fde23f        gcr.io/google_containers/redis:e2e       \"redis-server /etc...\"\n326b94d4cdd7        gcr.io/google_samples/gb-redisslave:v1   \"/entrypoint.sh /b...\"\n</code></pre> <p>Step 5  Test the application locally</p> <p>Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Success</p> <p>Nice you now have compose stuck up and running!</p> <p>Step 6 Cleanup environment:</p> <pre><code>docker-compose -f docker-guestbook.yml down\n</code></pre> <pre><code>Stopping guestbook_frontend_1 ... done\nStopping guestbook_redis-master_1 ... done\nStopping guestbook_redis-slave_1 ... done\nRemoving guestbook_frontend_1 ... done\nRemoving guestbook_redis-master_1 ... done\nRemoving guestbook_redis-slave_1 ... done\nRemoving network guestbook_default\n</code></pre>"},{"location":"ycit019_Lab_5_Docker_Compose/#22-deploy-voting-app-using-compose","title":"2.2 Deploy Voting App using Compose","text":"<p>Step 1 Switch to <code>Module5/example-voting-app</code> folder :</p> <pre><code>cd ~/ycit019/Module5/example-voting-app/\n</code></pre> <p>Step 2 The existing file docker-compose.yml defines several images:</p> <ul> <li> <p>A voting-app container based on a Python image</p> </li> <li> <p>A result-app container based on a Node.js image</p> </li> <li> <p>A Redis container based on a redis image, to temporarily store the data.</p> </li> <li> <p>A worker app based on a dotnet image</p> </li> <li> <p>A Postgres container based on a postgres image</p> </li> </ul> <p>App Architecture: </p> <p>Note that three of the containers are built from Dockerfiles, while the other two are images on Docker Hub. Let's review them closely:</p> <p>Step 3 Review files that going to be deployed with <code>tree</code> command. Alternatively view the files in gitrepo page here</p> <pre><code>sudo apt install tree\ntree\n</code></pre> <p>Step 5 Let\u2019s change the default port to expose. Edit the docker-compose.yml file and find the following lines:</p> <pre><code>ports:\n  - \"5000:80\"\n</code></pre> <p>Change 5000 to 8080:</p> <pre><code>ports:\n  - \"8080:80\"\n</code></pre> <p>Step 4 Verify Docker Compose version:</p> <pre><code>docker-compose version\n</code></pre> <p>Step 5 Use the docker-compose tool to launch your application:</p> <pre><code>docker-compose up -d\n</code></pre> <p>Step 6 Check that all containers are running, volumes created. Check compose state and logs :</p> <pre><code>#Docker state\ndocker ps\ndocker volumes\n#Docker compose state\ndocker-compose ps\ndocker-compose logs\n</code></pre> <p>Step 7 Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Step 8 Cleanup up.</p> <pre><code>docker-compose down\n</code></pre> <pre><code>Stopping examplevotingapp_worker_1 ... done\nStopping examplevotingapp_redis_1 ... done\nStopping examplevotingapp_result_1 ... done\nStopping examplevotingapp_db_1 ... done\nStopping examplevotingapp_vote_1 ... done\nRemoving examplevotingapp_worker_1 ... done\nRemoving examplevotingapp_redis_1 ... done\nRemoving examplevotingapp_result_1 ... done\nRemoving examplevotingapp_db_1 ... done\nRemoving examplevotingapp_vote_1 ... done\nRemoving network examplevotingapp_default\n</code></pre> <p>Step 9 You Boss told you that the application has a bug. Update the the app by editing the vote/app.py file and change the following lines near the top of the file:</p> <pre><code>vim vote/app.py\n</code></pre> <p>Press 'i'</p> <pre><code>option_a = os.getenv('OPTION_A', \"Cats\")\noption_b = os.getenv('OPTION_B', \"Dogs\")\n</code></pre> <p>Step 10 Replace \u201cCats\u201d and \u201cDogs\u201d with two options of your choice. For example:</p> <pre><code>option_a = os.getenv('OPTION_A', \"Java\")\noption_b = os.getenv('OPTION_B', \"Python\")\n</code></pre> <p>Press 'wq!'</p> <p>Step 11 Use docker-compose tool to launch your Update application:</p> <pre><code>docker-compose up -d\n</code></pre> <p>Check the UI</p> <p>Bingo</p> <p>Let's see who wins the battle of Orchestrations!</p> <p>Step 8 Cleanup up</p> <pre><code>docker-compose down\n</code></pre> <p>Congratulations</p> <p>You are now docker expert! We were able to start 2 microservices application with docker compose. First microservice had 3 services. Second microservice had 5 servics written in 3 different languages and able to talk to each other.</p> <p>Summary</p> <p>So far we've learned docker-compose v2. docker-compose v3 is out of scope for this Lab. However you got the idea!</p> <p>Read the Docker-Compose documentation on new syntax.</p> <p>Also example of v3 version of <code>voting-app</code> is here for you reference.</p>"},{"location":"ycit019_Module_10_Kubernetes_Scaling/","title":"1 Kubernetes Autoscaling","text":"<p>Objective</p> <ul> <li>Resource &amp; Limits</li> <li>Scheduling</li> <li>HPA</li> <li>VPA</li> <li>Cluster Autoscaling</li> <li>Node Auto provisioning (NAP)</li> </ul>"},{"location":"ycit019_Module_10_Kubernetes_Scaling/#0-create-gke-cluster","title":"0 Create GKE Cluster","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud container clusters create k8s-scaling \\\n--zone us-central1-c \\\n--enable-vertical-pod-autoscaling \\\n--num-nodes 2\n</code></pre> <p>Output:</p> <pre><code>NAME          LOCATION       MASTER_VERSION   MASTER_IP      MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\nk8s-scaling  us-central1-c  1.19.9-gke.1400  34.121.222.83  e2-medium     1.19.9-gke.1400  2          RUNNING\n</code></pre> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials k8s-scaling --zone us-central1-c\n</code></pre>"},{"location":"ycit019_Module_10_Kubernetes_Scaling/#11-resource-and-limits","title":"1.1 Resource and Limits","text":"<p>Step 1: Inspecting a node\u2019s capacity</p> <pre><code>kubectl describe nodes | grep -A15  Capacity:\n</code></pre> <p>The output shows two sets of amounts related to the available resources on the node: the node\u2019s capacity and allocatable resources. The capacity represents the total resources of a node, which may not all be available to pods. Certain resources may be reserved for Kubernetes and/or system components. The Scheduler bases its decisions only on the allocatable resource amounts.</p> <p>Step 2: Show metrics for a given node</p> <pre><code>kubectl top nodes\nkubectl top pods -n kube-system\n</code></pre> <p>Result</p> <p>CPU and Memory information is available for pods and node through the metrics API.</p> <p>Step 3 Create a <code>deployment</code> <code>best_effort.yaml</code> as showed below. This is regular deployment with  <code>resources</code> configured</p> <pre><code>cat &lt;&lt;EOF &gt; best_effort.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia\nspec:\n  selector:\n    matchLabels:\n      app: kubia\n  replicas: 3\n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - image: luksa/kubia:v1\n        name: nodejs\nEOF\n</code></pre> <p>Step 4 Deploy application</p> <pre><code>kubectl create -f best_effort.yaml\n</code></pre> <p>Step 5 Verify what is the QOS for this pod:</p> <pre><code>kubectl describe pods  | grep QoS\n</code></pre> <p>Result</p> <p>If you don't specify request/limits K8s provides <code>Best Effort</code> QOS</p> <p>Step 6 Cleanup</p> <pre><code>kubectl delete -f best_effort.yaml\n</code></pre> <p>Step 7 Create a <code>deployment</code> <code>guaranteed.yaml</code> as showed below. This is regular deployment with  <code>resources</code> configured</p> <pre><code>cat &lt;&lt;EOF &gt; guaranteed.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia\nspec:\n  selector:\n    matchLabels:\n      app: kubia\n  replicas: 3\n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - image: luksa/kubia:v1\n        name: nodejs\n        resources:\n          requests:\n            cpu: 100m\n            memory: 200Mi\n          limits:\n            cpu: 100m\n            memory: 200Mi\nEOF\n</code></pre> <p>Step 8 Deploy application</p> <pre><code>kubectl create -f guaranteed.yaml\n</code></pre> <p>Step 9 Verify what is the QOS for this pod:</p> <pre><code>kubectl describe pods  | grep QoS\n</code></pre> <p>Result</p> <p>If you request = limits K8s provides <code>guaranteed</code> QOS</p> <p>Step 10 Cleanup</p> <pre><code>kubectl delete -f guaranteed.yaml\n</code></pre> <p>Step 11 Create a <code>deployment</code> <code>burstable.yaml</code> as showed below. This is regular deployment with  <code>resources</code> configured</p> <pre><code>cat &lt;&lt;EOF &gt; burstable.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia\nspec:\n  selector:\n    matchLabels:\n      app: kubia\n  replicas: 3\n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - image: luksa/kubia:v1\n        name: nodejs\n        resources:\n          requests:\n            cpu: 3000\nEOF\n</code></pre> <p>Step 12 Deploy application</p> <pre><code>kubectl create -f burstable.yaml\n</code></pre> <p>Step 13 Verify what is the QOS for this pod:</p> <pre><code>kubectl describe pods  | grep QoS\n</code></pre> <p>Result</p> <p>If you specify <code>request &gt; or &lt; limits</code>  K8s provides <code>Burstable</code> QOS</p> <p>Step 14  Check status of the Pods </p> <pre><code>kubectl get pods\n</code></pre> <p>Pending</p> <p>Why the deployment failed ???</p> <p>Step 15 Cleanup</p> <pre><code>kubectl delete -f burstable.yaml\n</code></pre>"},{"location":"ycit019_Module_10_Kubernetes_Scaling/#12-creating-a-horizontal-pod-autoscaler-based-on-cpu-usage","title":"1.2 Creating a Horizontal Pod Autoscaler based on CPU usage","text":"<p>Prerequisites: Ensure metrics api is running in your cluster.</p> <pre><code>kubectl get pod -n kube-system\n</code></pre> <p>Check the status of <code>metrics-server-*****</code> pod status. It should be <code>Running</code></p> <pre><code>kubectl top nodes\nkubectl top pods -n kube-system\n</code></pre> <p>Result</p> <p>CPU and Memory information is available for pods and node through the metrics API.</p> <p>Let\u2019s create a horizontal pod autoscaler now and configure it to scale pods based on their CPU utilization.</p> <p>Step 1 Create a <code>deployment.yaml</code> as showed below. This is regular deployment with  <code>resources</code> configured</p> <pre><code>cat &lt;&lt;EOF &gt; deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia\nspec:\n  selector:\n    matchLabels:\n      app: kubia\n  replicas: 3\n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - image: luksa/kubia:v1\n        name: nodejs\n        resources:\n          requests:\n            cpu: 100m\nEOF\n</code></pre> <p>Step 2 Deploy application</p> <pre><code>kubectl create -f deployment.yaml\n</code></pre> <p>Step 3 After creating the deployment, to enable horizontal autoscaling of its pods, you need to create a HorizontalPodAutoscaler (HPA) object and point it to the deployment.</p> <pre><code>kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5\n</code></pre> <p>Note</p> <p>This creates the HPA object for us and sets the deployment called <code>kubia</code> as the scaling target. We\u2019re setting the target CPU utilization of the pods to 30% and specifying the minimum and maximum number of replicas. The autoscaler will thus constantly keep adjusting the number of replicas to keep their CPU utilization around 30%, but it will never scale down to less than 1 or scale up to more than 5 replicas.</p> <p>Step 4 Verify definition of the Horizontal Pod Autoscaler resource to gain a better understanding of it:</p> <pre><code>kubectl get hpa kubia -o yaml\n</code></pre> <p>Result:</p> <pre><code>apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\n...\nspec:\n  maxReplicas: 5\n  minReplicas: 1\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: kubia\n  targetCPUUtilizationPercentage: 30\nstatus:\n  currentReplicas: 0\n  desiredReplicas: 0\n</code></pre> <p>Step 5 Take a closer look at the HPA and notice that it is still not ready to do the autoscaling.</p> <pre><code>kubectl describe hpa kubia\n</code></pre> <p>Results</p> <pre><code>Events:\n  Type     Reason                        Age                   From                       Message\n  ----     ------                        ----                  ----                       -------\n  Warning  FailedGetResourceMetric       2m29s                 horizontal-pod-autoscaler  unable to get metrics for resource cpu: no metrics returned from resource metrics API\n  Warning  FailedComputeMetricsReplicas  2m29s                 horizontal-pod-autoscaler  failed to compute desired number of replicas based on listed metrics for Deployment/default/kubia: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API\n  Warning  FailedGetResourceMetric       118s (x3 over 2m14s)  horizontal-pod-autoscaler  did not receive metrics for any ready pods\n  Warning  FailedComputeMetricsReplicas  118s (x3 over 2m14s)  horizontal-pod-autoscaler  failed to compute desired number of replicas based on listed metrics for Deployment/default/kubia: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: did not receive metrics for any ready pods\n</code></pre> <p>Given that historical data is not available yet, you will see the above in the events section.</p> <p>Give it a minute or so and try again. Eventually, you will see the following in the <code>Events</code> section.</p> <pre><code>  Normal   SuccessfulRescale             41s                    horizontal-pod-autoscaler  New size: 1; reason: All metrics below target\n</code></pre> <p>If you take a look at the <code>kubia</code> deployment, you will see it was scaled down from 3 pods to 1 pod.</p> <p>Step 6 Create a service</p> <pre><code>kubectl expose deployment kubia --port=80 --target-port=8080\n</code></pre> <p>Step 7 Start another terminal session and run:</p> <pre><code>watch -n 1 kubectl get hpa,deployment\n</code></pre> <p>Step 8 Generate load to the Application</p> <pre><code>kubectl run -it --rm --restart=Never loadgenerator --image=busybox \\\n-- sh -c \"while true; do wget -O - -q http://kubia.default; done\"\n</code></pre> <p>Step 9 Observe autoscaling In the other terminal you will start noticing that the deployment is being scaled up.</p> <p>Step 10 Terminate both sessions by pressing <code>Ctrl+c</code></p>"},{"location":"ycit019_Module_10_Kubernetes_Scaling/#13-scale-size-of-pods-with-vertical-pod-autoscaling","title":"1.3 Scale size of pods with Vertical Pod Autoscaling","text":"<p>Step 1 Verify that Vertical Pod Autoscaling has already been enabled on the cluster. We enabled VPA when we created the cluster, by using <code>--enable-vertical-pod-autoscaling</code>. This command can be handy if you want to check VPA on an existing cluster.</p> <pre><code>gcloud container clusters describe k8s-scaling --zone us-central1-c | grep ^verticalPodAutoscaling -A 1\n</code></pre> <p>Step 2 Apply the hello-server deployment to your cluster</p> <pre><code>kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:2.0\n</code></pre> <p>Step 3 Ensure the deployment was successfully created</p> <pre><code>kubectl get deployment hello-server\n</code></pre> <p>Step 4 Assign a CPU resource request of 100m to the deployment</p> <pre><code>kubectl set resources deployment hello-server --requests=cpu=100m\n</code></pre> <p>Step 5 Inspect the container specifics of the <code>hello-server</code> pods, find <code>Requests</code> section, and notice that this pod is currently requesting the 450m CPU we assigned.</p> <pre><code>kubectl describe pod hello-server | sed -n \"/Containers:$/,/Conditions:/p\"\n</code></pre> <p>Output</p> <pre><code>Containers:\n  hello-app:\n    Image:      gcr.io/google-samples/hello-app:2.0\n    Port:       &lt;none&gt;\n    Host Port:  &lt;none&gt;\n    Requests:\n      cpu:        100m\n    Environment:  &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rw2gr (ro)\nConditions:\nContainers:\n  hello-app:\n    Container ID:   containerd://e9bb428186f5d6a6572e81a5c0a9c37118fd2855f22173aa791d8429f35169a6\n    Image:          gcr.io/google-samples/hello-app:2.0\n    Image ID:       gcr.io/google-samples/hello-app@sha256:37e5287945774f27b418ce567cd77f4bbc9ef44a1bcd1a2312369f31f9cce567\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Wed, 09 Jun 2021 11:34:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rw2gr (ro)\nConditions:\n</code></pre> <p>Step 6 Create a manifest for you Vertical Pod Autoscale</p> <pre><code>cat &lt;&lt; EOF &gt; hello-vpa.yaml\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: hello-server-vpa\nspec:\n  targetRef:\n    apiVersion: \"apps/v1\"\n    kind:       Deployment\n    name:       hello-server\n  updatePolicy:\n    updateMode: \"Off\"\nEOF\n</code></pre> <p>Step 7 Apply the manifest for <code>hello-vpa</code></p> <pre><code>kubectl apply -f hello-vpa.yaml\n</code></pre> <p>Step 8 Wait a minute, and then view the VerticalPodAutoscaler</p> <pre><code>kubectl describe vpa hello-server-vpa\n</code></pre> <p>Step 9 Locate the \"Container Recommendations\" at the end of the output from the <code>describe</code> command. If you don't see it, wait a little longer and try the previous command again. When it appears, you'll see several different recommendation types, each with values for CPU and memory:</p> <ul> <li>Lower Bound: this is the lower bound number VPA looks at for triggering a resize. If your pod utilization goes below this, VPA will delete the pod and scale it down.</li> <li>Target: this is the value VPA will use when resizing the pod.</li> <li>Uncapped Target: if no minimum or maximum capacity is assigned to the VPA, this will be the target utilization for VPA.</li> <li>Upper Bound: this is the upper bound number VPA looks at for triggering a resize. If your pod utilization goes above this, VPA will delete the pod and scale it up.</li> </ul> <p>Notice that the VPA is recommending new values for CPU instead of what we set, and also giving you a suggested number for how much memory should be requested. We can at this point manually apply these suggestions, or allow VPA to apply them.</p> <p>Step 10 Update the manifest to set the policy to Auto and apply the configuration</p> <pre><code>sed -i 's/Off/Auto/g' hello-vpa.yaml\nkubectl apply -f hello-vpa.yaml\n</code></pre> <p>In order to resize a pod, Vertical Pod Autoscaler will need to delete that pod and recreate it with the new size. By default, to avoid downtime, VPA will not delete and resize the last active pod. Because of this, you will need at least 2 replicas to see VPA make any changes.</p> <p>Step 11 Scale hello-server deployment to 2 replicas:</p> <pre><code>kubectl scale deployment hello-server --replicas=2\n</code></pre> <p>Step 12 Watch your pods</p> <pre><code>kubectl get pods -w\n</code></pre> <p>Step 13 The VPA should have resized your pods in the hello-server deployment. Inspect your pods:</p> <pre><code>kubectl describe pod hello-server | sed -n \"/Containers:$/,/Conditions:/p\"\n</code></pre>"},{"location":"ycit019_Module_10_Kubernetes_Scaling/#17-cleaning-up","title":"1.7 Cleaning Up","text":"<p>Step 1 Delete the cluster</p> <pre><code>gcloud container clusters delete k8s-scaling\n</code></pre>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/","title":"Deploy Kubernetes","text":"<p>In this Lab, we are going to:</p> <ul> <li>Deploy Single Node Kubernetes cluster using kubeadm on a Google Compute Engine node</li> <li>Deploy an application to  Kubernetes</li> </ul>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#1-deploy-kubernetes-and-calico-with-kubeadm","title":"1. Deploy Kubernetes and Calico with Kubeadm","text":"<p>In general to deploy Kubernetes with <code>kubeadm</code> it is required to use following official Kubernetes documentation.</p> <p>Step 1 Set the Project ID in  Environment Variable: </p> <pre><code>export PROJECT_ID=&lt;project_id&gt;\n</code></pre> <p>Here is how you can find you project_ID:</p> <p></p> <p>Set the project ID as default</p> <pre><code>gcloud config set project $PROJECT_ID\n</code></pre>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#11-create-a-vm-and-ssh-into-it","title":"1.1 Create a VM, and ssh into it","text":"<p>Step 1 Create the Compute instance</p> <p>The compute instances in this lab will be provisioned using Ubuntu Server 20.04, which has good support for the containerd container runtime. </p> <pre><code>gcloud compute instances create k8s-cluster \\\n--zone us-central1-c \\\n--machine-type=e2-standard-4 \\\n--can-ip-forward \\\n--image-family ubuntu-2004-lts \\\n--image-project ubuntu-os-cloud\n</code></pre> <p>Step 2 SSH in to VM where we going to install Kubernetes:</p> <pre><code>gcloud compute ssh --zone \"us-central1-c\" \"k8s-cluster\"\n</code></pre>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#12-install-containerd","title":"1.2 Install Containerd","text":"<p>Docker has been deprecated from Kubernetes starting K8s 1.24, so we will need to intall containerd instaed</p> <p>Step 1 Update the apt package index and install containerd:</p> <pre><code>sudo su\napt update\n</code></pre> <pre><code>apt install containerd\n</code></pre> <p>Pres Y, to install packages.</p> <p>Step 2 Enable systemd to start on reboot:</p> <pre><code>systemctl enable containerd\nsystemctl start containerd\nsystemctl status containerd\n</code></pre> <p>Output:</p> <pre><code>\u25cf containerd.service - containerd container runtime\n     Loaded: loaded (/lib/systemd/system/containerd.service; enabled; vendor preset: enabled)\n     Active: active (running) since Wed 2022-05-18 21:33:39 UTC; 8s ago\n       Docs: https://containerd.io\n   Main PID: 2106 (containerd)\n      Tasks: 15\n     Memory: 23.3M\n     CGroup: /system.slice/containerd.service\n             \u2514\u25002106 /usr/bin/containerd\n</code></pre> <p>Step 3 To interact with containerd Install nerdctl:</p> <pre><code>wget https://github.com/containerd/nerdctl/releases/download/v0.18.0/nerdctl-0.18.0-linux-amd64.tar.gz\ntar zxvf nerdctl-0.18.0-linux-amd64.tar.gz nerdctl\nmv nerdctl /usr/local/bin\n</code></pre> <p>Step 4 Install CNI Plugins to test containerd can start containers:</p> <pre><code>wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz\nmkdir -p /opt/cni/bin/\ntar zxvf cni-plugins-linux-amd64-v1.1.1.tgz -C /opt/cni/bin/\n</code></pre> <p>Step 5 Run a Docker like command with Nerdctl to start test <code>ubuntu</code> container:</p> <p>See Nerdctl CLI reference here</p> <pre><code>nerdctl run -d ubuntu bash\n</code></pre> <pre><code>nerdctl ps -a\n</code></pre> <p>Sucess</p> <p>We can see our docker container been started few seconds ago.</p>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#13-let-iptables-see-bridged-traffic","title":"1.3 Let iptables see bridged traffic","text":"<pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n\n# sysctl params required by setup, params persist across reboots\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\nEOF\n\n# Apply sysctl params without reboot\nsudo sysctl --system\n</code></pre>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#14-install-kubeadm-and-prerequisite-packages-on-each-node","title":"1.4 Install kubeadm and prerequisite packages on each node","text":"<p>The next step is to install <code>kubeadm</code> and prerequisite packages as showed here.</p> <p>Step 1 Deploy <code>kubeadm</code> and prerequisite packages</p> <p>Update the apt package index and install packages needed to use the Kubernetes apt repository:</p> <pre><code>apt-get update &amp;&amp; apt-get install -y apt-transport-https ca-certificates curl\n</code></pre> <p>Download the Google Cloud public signing key:</p> <pre><code>sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg\n</code></pre> <p>Add the Kubernetes apt repository:</p> <pre><code>echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\n</code></pre> <p>Update apt package index, install kubelet, kubeadm and kubectl, and pin their version:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n</code></pre> <p>Step 2 Verify <code>kubeadm</code> version</p> <pre><code>kubeadm version\n</code></pre> <p>Output:</p> <pre><code>kubeadm version: &amp;version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.0\", GitCommit:\"4ce5a8954017644c5420bae81d72b09b735c21f0\", GitTreeState:\"clean\", BuildDate:\"2022-05-03T13:44:24Z\", GoVersion:\"go1.18.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n</code></pre> <p>Result</p> <pre><code>Latest available version of Kubernetes/kubeadm has been installed from [GitHub Kubernetes repo release page.](https://github.com/kubernetes/kubernetes/releases)\n</code></pre>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#15-kubeadm-init-the-master","title":"1.5 'Kubeadm init' the Master","text":"<p>Run On the Master node only:</p> <p>Step 1: Build kubeadm Custom Config</p> <pre><code>cat &lt;&lt;EOF &gt; kubeadm-config.yaml\nkind: ClusterConfiguration\napiVersion: kubeadm.k8s.io/v1beta3\nkubernetesVersion: v1.24.0\n---\nkind: KubeletConfiguration\napiVersion: kubelet.config.k8s.io/v1beta1\ncgroupDriver: systemd\nEOF\n</code></pre> <p>Make sure the IP address was updated:</p> <pre><code>cat  kubeadm-config.yaml\n</code></pre> <p>Note</p> <p>To expose custom Config you can create a  kubeadm.conf and specify     during <code>kubecadm init</code> execution. For instance:</p> <pre><code>    * ControllerManager configs\n    * Custom Subnet\n    * Custom version\n    * Apiserver configs such as authentication, authorization and etc.\n</code></pre> <p>Step 2: Create a cluster</p> <pre><code>kubeadm init --config=kubeadm-config.yaml\n</code></pre> <p>Result</p> <pre><code>Once the command completes, configure the KUBECONFIG env variable with the path to admin.conf (recommend adding it to your .bashrc):\n</code></pre> <pre><code>  mkdir -p $HOME/.kube\n  cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  chown $(id -u):$(id -g) $HOME/.kube/config\n  export KUBECONFIG=$HOME/.kube/config\n</code></pre> <p>Let's validate that the installation was successful. You should now be able to run kubectl commands and see that all cluster Pods are running (except DNS one):</p> <pre><code>watch kubectl get pods --all-namespaces -o wide\n</code></pre> <p>To exit back to the terminal, press ctrl+c</p>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#16-deploy-cilium-cni","title":"1.6 Deploy Cilium CNI","text":"<p>Step 1 Download Helm Package manager:</p> <pre><code>wget https://get.helm.sh/helm-v3.8.2-linux-amd64.tar.gz\n</code></pre> <pre><code>tar -zxvf helm-v3.8.2-linux-amd64.tar.gz\n</code></pre> <pre><code>mv linux-amd64/helm /usr/local/bin/helm\n</code></pre> <p>Step 1  Setup Helm Cilium repository:</p> <pre><code>helm repo add cilium https://helm.cilium.io/\n</code></pre> <p>Step 3  Now lets deploy Cilium Networking</p> <pre><code>helm install cilium cilium/cilium --version 1.9.16 --namespace kube-system\n</code></pre> <p>Watch the Cilium/node pod for the master get created (hopefully successfully)</p> <pre><code>watch kubectl get pods --all-namespaces -o wide\n</code></pre>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#17-join-worker-node","title":"1.7 Join worker node","text":"<p>If you have other nodes around you can run the 'kubeadm join ...' command from the output of kubeadm init on each worker node (incl token). Watch the calico/node pods get created for each worker node automatically.</p> <pre><code>e.g. kubeadm join --token ****\n</code></pre> <p>For this lab, we are creating a one node kubernetes clusters, so in order to be able to deploy applications on the same node as the control plane, we need to remove the taint that prevent such deployment.</p> <pre><code>kubectl taint nodes --all node-role.kubernetes.io/master:NoSchedule-\n</code></pre>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#18-now-lets-create-a-test-deployment-with-2-replicas","title":"1.8 Now lets create a test deployment with 2 replicas","text":"<pre><code>kubectl create deployment nginx --replicas=2 --image=nginx --port=8080\n</code></pre> <p>Lets get some more detail about the deployment:</p> <pre><code>kubectl describe deployment nginx\nkubectl get deployment nginx\n</code></pre> <p>And pods that has been created by <code>nginx</code> deployment:</p> <pre><code>kubectl get pods\n</code></pre> <p>Congrats. Now you have a working Kubernetes+Calico cluster.</p>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#21-verify-kubernetes-components-deployed-by-kubeadm","title":"2.1 Verify Kubernetes components deployed by kubeadm","text":""},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#211-check-kubernetes-version","title":"2.1.1 Check Kubernetes version","text":"<p>Step 1 Verify that Kubernetes is deployed and working.</p> <pre><code>kubectl get nodes\n</code></pre> <p>Result</p> <ul> <li>Kubernetes has single node for workload scheduling.</li> <li>Kubernetes running version 1.24.0</li> </ul> <p>Note</p> <pre><code>At Kubernetes community, we define 3 types of Kubernetes releases:\n\n* Major (x.0.0)\n* Minor (x.x.0)\n* Patch (x.x.x)\n</code></pre> <p>Note</p> <pre><code>At a single point of time, we develop the new \"Major\"/\"Minor\" version of Kubernetes (today - Kubernetes 1.21), and we support three existing releases as the \"Patch\" releases (today - 1.19.x, 1.20.x and 1.21.x).\n</code></pre>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#212-verify-cluster-default-namespaces","title":"2.1.2 Verify Cluster default namespaces.","text":"<p>Step 1 Verify namespaces created in K8s systems</p> <pre><code>$ kubectl get ns\nNAME              STATUS   AGE\ndefault           Active   5h50m\nkube-node-lease   Active   5h50m\nkube-public       Active   5h50m\nkube-system       Active   5h50m\n</code></pre> <p>Info</p> <p>Namespaces are intendent to isolate groups/teams and give them access to a set of resources. They avoid name collisions between resources. Namespaces provides with a soft Multitenancy, meaning they not provide full isolation.</p> <p>Result</p> <p>By default Kubernetes deployed by <code>kubeadm</code> starts with 4 namespaces:</p> <ul> <li><code>default</code> The default namespace for objects with no other namespace. When listing resources with the kubectl get command, we\u2019ve never specified the namespace explicitly, so kubectl always defaulted to the default namespace, showing us just the objects inside that namespace.</li> <li><code>kube-system</code> The namespace for objects created by the Kubernetes system</li> <li><code>kube-public</code> Readable by all users, and mostly reserved for cluster usage.</li> <li><code>kube-node-lease</code> This namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales.</li> </ul>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#213-verify-kubelet","title":"2.1.3 Verify kubelet","text":"<p>Step 1 Verify that <code>kubelet</code> installed in K8s Cluster:</p> <pre><code>systemctl -l | grep kubelet\nsystemctl status kubelet\n</code></pre> <p>Note</p> <p>Service and its config file can be found in <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code></p> <p>Step 2 Find manifests file for other master Node components:</p> <p>Once <code>kubelet</code> is deployed, all the rest master node components are deployed as a <code>static pods</code> on Kubernetes Master node. Setting <code>--pod-manifest-path=</code> specifies from where to read Static Pod manifests used for spinning up the control plane.</p> <p>Step 3 List K8s components manifest files that is going to be used for cluster deployment and run as <code>Static Pods</code> by <code>kubelet</code>:</p> <pre><code>sudo ls /etc/kubernetes/manifests\n</code></pre> <pre><code>etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml\n</code></pre> <p>Result</p> <p>We see etcd, api-server, controller-manager and scheduler that has been used to deploy on this cluster and managed by <code>kubelet</code>.</p> <p>Step 4 Verify K8s Components deployed as containers on K8s:</p> <pre><code>kubectl get pods -n kube-system\n</code></pre> <pre><code>NAME                                       READY   STATUS    RESTARTS   AGE\ncalico-kube-controllers-6d7b4db76c-h242g   1/1     Running   0          27m\ncalico-node-gwnng                          1/1     Running   0          27m\ncoredns-74ff55c5b-5s7rp                    1/1     Running   0          5h59m\ncoredns-74ff55c5b-l6hd4                    1/1     Running   0          5h59m\netcd-k8s-cluster                           1/1     Running   0          5h59m\nkube-apiserver-k8s-cluster                 1/1     Running   0          5h59m\nkube-controller-manager-k8s-cluster        1/1     Running   0          5h59m\nkube-proxy-f8647                           1/1     Running   0          5h59m\nkube-scheduler-k8s-cluster                 1/1     Running   0          5h59m\n</code></pre> <p>Result</p> <ul> <li>We can see that Kubernetes components: etcd, api-server, controller-manager  and scheduler deployed on K8s cluster via kubelet.</li> <li>Calico Networking including calico-etcd, calico-node, calico-policy-controller has been deployed as a last step of kubeadm installation</li> <li>Or Cilium Networking containers</li> </ul>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#214-verify-etcd-database-deployment","title":"2.1.4 Verify etcd database deployment.","text":"<p>Step 1 Verify <code>etcd</code> config file</p> <pre><code>sudo cat /etc/kubernetes/manifests/etcd.yaml\n</code></pre> <p>Step 2  Overview <code>etcd</code> pod deployed on K8s cluster:</p> <pre><code>kubectl get pods -n kube-system | grep etcd\nkubectl describe pods/etcd-k8s-cluste -n kube-system\n</code></pre> <p>Result</p> <ul> <li><code>etcd</code> has been deployed as a static pod.</li> <li>Annotation <code>Priority Class Name:  system-node-critical</code> tells to K8s that this Pod is  critical and will have highest <code>QOS</code>.</li> </ul> <p>Step 3 Check the location of etcd db and snapshot dumps.</p> <pre><code>sudo ls /var/lib/etcd/member\n</code></pre> <p>Result</p> <p>The data directory has two sub-directories in it:</p> <ul> <li>wal: write ahead log files are stored here.</li> <li>snap: log snapshots are stored here.</li> </ul> <p>When first started, etcd stores its configuration into a data directory specified by the data-dir configuration parameter. Configuration is stored in the write ahead log and includes: the local member ID, cluster ID, and initial cluster configuration. The write ahead log and snapshot files are used during member operation and to recover after a restart.</p>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#215-verify-api-server-deployment-on-the-k8s-cluster","title":"2.1.5 Verify <code>api-server</code> deployment on the K8s cluster.","text":"<p>Step 1 Review configuration file:</p> <pre><code>sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml\n</code></pre> <p>Step 2 Overview <code>api-server</code> pod and its parameters.</p> <pre><code>kubectl describe pods/kube-apiserver-k8s-cluster -n kube-system\n</code></pre>"},{"location":"ycit019_Module_7_Lab_Deploy_Kubernetes_kubeadm/#216-verify-controller-manager-and-scheduler-deployment","title":"2.1.6 Verify <code>Controller-manager</code> and <code>scheduler</code> deployment.","text":"<p>Step 1 <code>Controller-manager</code> and <code>scheduler</code> deployed on K8s cluster via kubelet the same way <code>api-server</code>. Verify both configuration files and pods running on K8s Cluster.</p> <p>Summary</p> <ul> <li>K8s is an orchestration system for containers. Since most of the k8s components are the <code>go</code> binaries that can be containerized, K8s has been designed to run itself. This makes system  itself HA, easily deployable, scaleable and upgradable.</li> </ul>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/","title":"Kubernetes Concepts","text":"<p>Objective:</p> <p>Learn basic Kubernetes concepts:</p> <ul> <li>Create a GKE Cluster</li> <li>Pods</li> <li>Labels, Selectors and Annotations</li> <li>Create Deployments</li> <li>Create Services</li> <li>namespaces</li> </ul>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#0-create-gke-cluster","title":"0 Create GKE Cluster","text":"<p>Step 1 Enbale the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with two nodes:</p> <pre><code>gcloud container clusters create k8s-concepts \\\n--zone us-central1-c \\\n--num-nodes 2\n</code></pre> <p>Output:</p> <pre><code>NAME          LOCATION       MASTER_VERSION   MASTER_IP      MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\nk8s-concepts  us-central1-c  1.19.9-gke.1400  34.121.222.83  e2-medium     1.19.9-gke.1400  2          RUNNING\n</code></pre> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials k8s-concepts --zone us-central1-c\n</code></pre>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#1-pods","title":"1 Pods","text":""},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#11-create-a-pod-with-manifest","title":"1.1 Create a Pod with manifest","text":"<p>Reference: Pod Overview</p> <p>Step 1 Printout explanation of the object and lists of attributes:</p> <pre><code>kubectl explain pods\n</code></pre> <p>See all possible fields available for the pods:</p> <pre><code>kubectl explain pods.spec --recursive\n</code></pre> <p>Note</p> <p>It's not require to provide all possible fields for the Pods or any other resources. Most of the fields will be added by default if not specified. For the Pods at minimum it is required to specify <code>image</code>, <code>name</code>, <code>ports</code> inside of spec.containers.</p> <p>Step 2 Define a new pod in the file <code>echoserver-pod.yaml</code> :</p> <pre><code>cat &lt;&lt;EOF &gt; echoserver-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: echoserver\n  labels:\n    app: echoserver\nspec:\n  containers:\n  - name: echoserver\n    image: gcr.io/google_containers/echoserver:1.10\n    ports:\n    - containerPort: 8080\nEOF\n</code></pre> <p>Here, we use the existing image <code>echoserver</code>. This is a simple server that responds with the http headers it received. It runs on nginx server and implemented using lua in the nginx configuration: https://github.com/kubernetes/contrib/tree/master/ingress/echoheaders</p> <p>Step 3 Create the <code>echoserver</code> pod:</p> <pre><code>kubectl apply -f echoserver-pod.yaml\n</code></pre> <p>Step 4 Use <code>kubectl get pods</code> to watch the pod get created:</p> <pre><code>kubectl get pods\n</code></pre> <p>Result:</p> <pre><code>NAME         READY     STATUS    RESTARTS   AGE\nechoserver   1/1       Running   0          5s\n</code></pre> <p>Step 5 Use <code>kubectl describe pods/podname</code> to watch the details about scheduled pod:</p> <pre><code>kubectl describe pods/echoserver\n</code></pre> <p>Note</p> <p>Review and discuss the following fields:</p> <ul> <li> <p>Namespace</p> </li> <li> <p>Status</p> </li> <li> <p>Containers</p> </li> <li> <p>QoS Class</p> </li> <li> <p>Events</p> </li> </ul> <p>Step 6 Now let\u2019s get the pod definition back from Kubernetes:</p> <pre><code>kubectl get pods echoserver -o yaml &gt; echoserver-pod-created.yaml\ncat echoserver-pod-created.yaml\n</code></pre> <p>Compare echoserver-pod.yaml and echoserver-pod-created.yaml to see additional properties that have been added to the original pod definition.</p>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#2-labels-selectors","title":"2 Labels &amp; Selectors","text":"<p>Organizing pods and other resources with labels.</p>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#21-label-and-select-pods","title":"2.1 Label and Select Pods","text":"<p>Reference: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/</p> <p>Step 1 Label Pod <code>hello-world</code> with label <code>app=hello</code> and <code>env=test</code></p> <pre><code>kubectl label pods echoserver dep=sales\nkubectl label pods echoserver env=test\n</code></pre> <p>Step 2 See all Pods and all their Labels.</p> <pre><code>kubectl get pods --show-labels\n</code></pre> <p>Step 3 Select all Pods with labels <code>env=test</code></p> <pre><code>kubectl get pods -l env=test\n</code></pre>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#22-label-nodes","title":"2.2 Label Nodes","text":"<p>Reference: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</p> <p>Step 1 List available nodes</p> <pre><code>kubectl get nodes\n</code></pre> <p>Step 2 List a detailed view of nodes</p> <pre><code>kubectl get nodes -o wide\n</code></pre> <p>Step 3 List Nodes and their labels</p> <pre><code>kubectl get nodes --show-labels\n</code></pre> <p>Step 4  Label the node as <code>size: small</code>. Make sure to replace YOUR_NODE_NAME with one of the nodes you have.</p> <pre><code>kubectl label node YOUR_NODE_NAME size=small\n</code></pre> <p>Step 5 Check the labels for this node</p> <pre><code>kubectl get node YOUR_NODE_NAME --show-labels | grep size\n</code></pre> <p>Note</p> <p>In the upcoming classes we will use node labels to make sure our applications run on eligible nodes only.</p>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#3-services","title":"3 Services","text":""},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#31-create-a-service","title":"3.1 Create a Service","text":"<p>We have three running <code>echoserver</code> pods, but we cannot access them yet, because the container ports are not accessible. Let\u2019s define a new service that will expose echoserver ports and make them accessible.</p> <p>Step 1 Create a new file <code>echoserver-service.yaml</code> with the following content:</p> <pre><code>cat &lt;&lt;EOF &gt; echoserver-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: echoserver\nspec:\n  selector:\n    app: echoserver\n  type: \"NodePort\"\n  ports:\n    - port: 8080\n      protocol: TCP\n      targetPort: 8080\n  selector:\n    app: echoserver\nEOF\n</code></pre> <p>Step 2 Create a new service:</p> <pre><code>kubectl create -f echoserver-service.yaml\n</code></pre> <p>Step 3 Check the service details:</p> <pre><code>kubectl describe services/echoserver\n</code></pre> <p>Output:</p> <pre><code>Name:               echoserver\nNamespace:          default\nLabels:             &lt;none&gt;\nSelector:           app=echoserver\nType:               NodePort\nIP:                 ...\nPort:               &lt;unset&gt; 8080/TCP\nNodePort:           &lt;unset&gt; 30366/TCP\nEndpoints:          ...:8080,...:8080,..:8080\nSession Affinity:   None\nNo events.\n</code></pre> <p>Note</p> <p>The above output contains one endpoint and a node port, 30366, but it can be different in your case. Remember this port to use it in the next step.</p> <p>Step 4 We need to open the node port on one of the cluster nodes to be able to access the service externally. Let's first find the exteran IP address of one of the nodes.</p> <pre><code>kubectl get nodes -o wide\n</code></pre> <p>Output:</p> <pre><code>NAME                                          STATUS   ROLES    AGE   VERSION            INTERNAL-IP   EXTERNAL-IP    OS-IMAGE                             KERNEL-VERSION   CONTAINER-RUNTIME\ngke-k8s-concepts-default-pool-ad96fd50-1rf1   Ready    &lt;none&gt;   20m   v1.19.9-gke.1400   10.128.0.32   34.136.1.22\ngke-k8s-concepts-default-pool-ad96fd50-jpd2   Ready    &lt;none&gt;   20m   v1.19.9-gke.1400   10.128.0.31   34.69.114.67\n</code></pre> <p>Step 5 Create a firewall rule to allow TCP traffic on your node port. Make sure to replace YOUR_NODE_PORT.</p> <pre><code>gcloud compute firewall-rules create echoserver-node-port --allow tcp:YOUR_NODE_PORT\n</code></pre> <p>Step 6 To access a service exposed via a node port, specify the node port from the previous step and use one of the IP addresses of the cluster nodes. Make sure to replace both NODE_IP and YOUR_NODE_PORT</p> <pre><code>curl http://NODE_IP:YOUR_NODE_PORT\n</code></pre>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#32-cleanup-services-and-pods","title":"3.2 Cleanup Services and Pods","text":"<p>Step 1 Before diving into Kubernetes deployment, let\u2019s delete our service and pods. To delete the service execute the following command:</p> <pre><code>kubectl delete service echoserver\n</code></pre> <p>Step 2 delete the pod</p> <pre><code>kubectl delete pod echoserver\n</code></pre> <p>Step 3 Check that there are no running pods:</p> <pre><code>kubectl get pods\n</code></pre>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#4-deployments","title":"4 Deployments","text":""},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#41-deploy-hello-app-on-kubernetes-using-deployments","title":"4.1 Deploy hello-app on Kubernetes using Deployments","text":""},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#411-create-a-deployment","title":"4.1.1 Create a Deployment","text":"<p>Step 1 The simplest way to create a new deployment for a single-container pod is to use <code>kubectl run</code>:</p> <pre><code>kubectl create deployment hello-app \\\n--image=gcr.io/google-samples/hello-app:1.0 \\\n--port=8080 \\\n--replicas=2\n</code></pre> <p>Note</p> <p><code>--port</code> Deployment opens port 8080 for use by the Pods.</p> <p><code>--replicas</code> number of replicas.</p> <p>Step 2 Check pods:</p> <pre><code>kubectl get pods\n</code></pre> <p>Step 3 To access the <code>hello-app</code> deployment, create a new service of type LoadBalancer this time using <code>kubectl expose deployment</code>:</p> <pre><code>kubectl expose deployment hello-app --type=LoadBalancer\n</code></pre> <p>To get the external IP for the loadbalancer that got created:</p> <pre><code>kubectl get services/hello-app\n</code></pre> <p>The Loadbalancer might take few minutes to get created, and it'll show pending status.</p> <p>Step 4 Check that the hello-app is accessible: Make sure to replace the LB_IP.</p> <pre><code>curl http://LB_IP:8080\n</code></pre> <p>Output:</p> <pre><code>Hello, world!\nVersion: 1.0.0\nHostname: hello-app-76f778987d-rdhr7\n</code></pre> <p>Step 5 You can open the app in the browser by navigating to LB_IP:8080</p> <p>Summary</p> <p>We learned how to create a deployment and expose our container.</p>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#412-scale-a-deployment","title":"4.1.2 Scale a Deployment","text":"<p>Now, let's scale our application as our website get popular.</p> <p>Step 1 Deployments using replica set (RS) to scale the containers. Let's check how replica set (RS) looks like:</p> <pre><code>kubectl get rs,deploy\n</code></pre> <pre><code>NAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/hello-app-76f778987d   2         2         2       5m12s\n\nNAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/hello-app   2/2     2            2           5m12s\n</code></pre> <p>Step 2 Let\u2019s scale number of pods in replica for the deployment. Use going to use <code>kubectl scale</code> to change the number of replicas to 5:</p> <pre><code>kubectl scale  deployment hello-app --replicas=5\n</code></pre> <p>Step 3 View the deployment details:</p> <pre><code>kubectl describe deployment hello-app\n</code></pre> <p>Step 4 Check that there are 5 running pods:</p> <pre><code>kubectl get pods\n</code></pre>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#413-rolling-update-of-containers","title":"4.1.3 Rolling Update of Containers","text":"<p>To perform rolling upgrade we need a new version of our application and then  perform Rolling Upgrade using <code>deployments</code></p> <p>Step 4 Use <code>kubectl rollout history deployment</code> to see revisions of the deployment:</p> <pre><code>kubectl rollout history deployment hello-app\n</code></pre> <p>Output:</p> <pre><code>deployment.apps/hello-app \nREVISION    CHANGE-CAUSE\n1           &lt;none&gt;\n</code></pre> <p>Result</p> <p>Since we've just deployed there is only 1 revision that currenly running.</p> <p>Step 5 Now we want to replace our <code>hello-app</code> with a new implementation. We want to use a new version of <code>hello-app</code> image. We are going to use <code>kubectl set</code> command this time around.</p> <p>Hint</p> <p><code>kubectl set</code> used only to change image name/version. You can use this for command for CI/CD pipeline.</p> <p>Suppose that we want to update the webk8sbirthday Pods to use the hello-app:2.0 image instead of the hello-app:1.0 image.</p> <pre><code>kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record\nkubectl get pods\n</code></pre> <p>Note</p> <p>It is a good practice to paste <code>--record</code> at the end of the rolling upgrade command as it will record the action in the <code>rollout history</code></p> <p>Result: We can see that the Rolling Upgraded was recorded:</p> <pre><code>kubectl rollout history deployment hello-app\n</code></pre> <p>Output:</p> <pre><code>deployment.apps/hello-app \nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record=true\n</code></pre> <p>Step 6 Refresh browser and see new version of app deployed</p> <pre><code>http://LB_IP:8080\n</code></pre> <p>Step 7 Let's assume there was something wrong with this new version and we need to rollback with <code>kubectl rollout undo</code> our deployment:</p> <pre><code>kubectl rollout undo deployment/hello-app\n</code></pre> <p>Refresh the browser again to see how we rolledback to version 1.0.0</p> <p>We have successfully rolled back the deployment and our pods are based on the hello-app:1.0.0 image again.</p> <p>Step 8 Let's delete the deployment and the service:</p> <pre><code>kubectl delete deployment hello-app\nkubectl delete services/hello-app\n</code></pre> <p>Success</p> <p>You are now up to speed with Kubernetes Concepts such as Pods, Services and Deployments.  Let's move on to Kubernetes Features to learn what else Kubernetes is capable of!</p>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#5-namespace","title":"5 NameSpace","text":"<p>Namespace can be used for:</p> <ul> <li>Splitting complex systems with several components into smaller groups</li> <li>Separating resources in a multi-tenant env: production, development and QA environments</li> <li>Separating resources per production</li> <li>Separating per-user or per-department or any other logical group</li> </ul> <p>Some other rules and regulations:</p> <ul> <li>Resource names only need to be unique within a namespace.</li> <li>Two different namespaces can contain resources of the same name.</li> <li>Most of the Kubernetes resources (e.g. pods, svc, rcs, and others) are   namespaced.</li> <li>However, some resource can be cluster-wide e.g nodes, persistentVolumes and   PodSecurityPolicy.</li> </ul>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#51-viewing-namespaces","title":"5.1 Viewing namespaces","text":"<p>Step 1 List the current namespaces in a cluster using:</p> <pre><code>kubectl get ns\n</code></pre> <p>Output:</p> <pre><code>NAME              STATUS   AGE\ndefault           Active   71m\nkube-node-lease   Active   71m\nkube-public       Active   71m\nkube-system       Active   71m\n</code></pre> <p>Step 2 You can also get the summary of a specific namespace using:</p> <pre><code>kubectl get namespaces &lt;name&gt;\n</code></pre> <p>Or you can get detailed information with:</p> <pre><code>kubectl describe namespaces &lt;name&gt;\n</code></pre> <p>A namespace can be in one of two phases:</p> <ul> <li><code>Active</code> the namespace is in use</li> <li><code>Terminating</code> the namespace is being deleted, and can not be used for new     objects</li> </ul> <p>Note</p> <p>These details show both <code>resource quota</code> (if present) as well as <code>resource and limit</code> ranges.</p> <p><code>Resource quota</code> tracks aggregate usage of resources in the Namespace and allows cluster operators to define Hard resource usage limits that a Namespace may consume.</p> <p>A <code>limit range</code> defines min/max constraints on the amount of resources a  single entity can consume in a Namespace.</p> <p>Step 3 Let\u2019s have a look at the pods that belong to the <code>kube-system</code> namespace, by telling kubectl to list pods in that namespace:</p> <pre><code>kubectl get po --namespace kube-system\n</code></pre> <p>Output:</p> <pre><code>NAME                                                        READY   STATUS    RESTARTS   AGE\nevent-exporter-gke-67986489c8-5fsdv                         2/2     Running   0          71m\nfluentbit-gke-fqcsx                                         2/2     Running   0          71m\nfluentbit-gke-ppb9j                                         2/2     Running   0          71m\ngke-metrics-agent-5vl7t                                     1/1     Running   0          71m\ngke-metrics-agent-bxt2r                                     1/1     Running   0          71m\nkube-dns-5d54b45645-9srx6                                   4/4     Running   0          71m\nkube-dns-5d54b45645-b7njm                                   4/4     Running   0          71m\nkube-dns-autoscaler-58cbd4f75c-2scrv                        1/1     Running   0          71m\nkube-proxy-gke-k8s-concepts-default-pool-ad96fd50-1rf1      1/1     Running   0          71m\nkube-proxy-gke-k8s-concepts-default-pool-ad96fd50-jpd2      1/1     Running   0          71m\nl7-default-backend-66579f5d7-dsbdt                          1/1     Running   0          71m\nmetrics-server-v0.3.6-6c47ffd7d7-mtls4                      2/2     Running   0          71m\npdcsi-node-knlqp                                            2/2     Running   0          71m\npdcsi-node-vh4tx                                            2/2     Running   0          71m\nstackdriver-metadata-agent-cluster-level-6f7d66dc98-zcd25   2/2     Running   0          71m\n</code></pre> <p>Tip</p> <p>You can also use -n instead of --namespace</p> <p>Yot may already know some of the pods, the rest we will cover later. It\u2019s clear from the name of the namespace, that resources inside <code>kube-system</code> related to the Kubernetes system itself. By having them in this separate namespace, it keeps everything nicely organized. If they were all in the default namespace, mixed in with the resources we create ourselves, we\u2019d have a hard time seeing what belongs where and we might inadvertently delete some system resources.</p> <p>Step 4 Now you know how to view resources in specific namespaces. Additionally, it is also possible to view list all resources in all namespaces. For example below is example to list all pods in all namespaces:</p> <pre><code>kubectl get pods --all-namespaces\n</code></pre>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#52-creating-namespaces","title":"5.2 Creating Namespaces","text":"<p>A namespace is a Kubernetes resource, therefore it is possible to create it by posting a YAML file to the Kubernetes API server or using <code>kubectl create ns</code>.</p> <p>Step 1 First, create a custom-namespace.yaml file with the following content:</p> <pre><code>cat &lt;&lt;EOF &gt; custom-namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: custom-namespace\nEOF\n</code></pre> <p>Step 2 Than, use kubectl to post the file to the Kubernetes API server:</p> <pre><code>kubectl create -f custom-namespace.yaml\n</code></pre> <p>Step 3 A much easier and faster way to create a namespaces using <code>kubectl create ns</code> command, as shown below:</p> <pre><code>kubectl create namespace custom-namespace2\n</code></pre>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#53-setting-the-namespace-preference","title":"5.3 Setting the namespace preference.","text":"<p>By default, a Kubernetes cluster will instantiate a <code>default namespace</code> when provisioning the cluster to hold the default set of Pods, Services, and Deployments used by the cluster. So by default all <code>kubectl</code> calls such as list or create resources will end up in <code>default namespace</code>.</p> <p>However sometimes you want to list or create resources in other namespaces than <code>default namespace</code>. As we discussed in previous exercise this can be done by specifying <code>-n</code>  or <code>--namespace</code> to point in which namespaces action has to be done. However it is not convenient to do this action every time. Below example will show how to create 2 namespaces <code>dev</code> and <code>prod</code> and switch between each other.</p> <p>Step 1 <code>kubectl</code> API uses so called <code>kubeconfig context</code> where you can controls which namespace, user or cluster needs to be accessed.</p> <p>In order to display which context is currently in use run:</p> <pre><code>KUBECONFIG=~/.kube/config\nkubectl config current-context\n</code></pre> <p>Result</p> <p>We running in <code>kubernetes-admin@kubernetes</code> context, which is default for lab environment.</p> <p>Step 3 To see full view of the <code>kubeconfig context</code> run:</p> <pre><code>kubectl config view\n</code></pre> <p>Result</p> <p>Our context named as <code>kubernetes-admin@kubernetes</code> uses cluster <code>cluster</code>, and user <code>kubernetes-admin</code></p> <p>Step 4 The result of the above command comes from kubeconfig file, in our case we defined it under <code>~/.kube/config</code>.</p> <pre><code>echo $KUBECONFIG\n</code></pre> <p>Result</p> <p>KUBECONFIG is configured to use following ~/.kube/config file</p> <pre><code>cat ~/.kube/config\n</code></pre> <p>Note</p> <p>The KUBECONFIG environment variable is a list of paths to configuration files. The list is colon-delimited for Linux and Mac, and semicolon-delimited for Windows. We already set KUBECONFIG environment variable in a first step of exercise to be <code>~/.kube/config</code></p> <p>Tip</p> <p>You can use use multiple <code>kubeconfig</code> files at the same time and view merged config:</p> <p><code>$ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view</code></p> <p>Step 5 Create two new namespaces <code>dev</code>:</p> <pre><code>kubectl create namespace dev\n</code></pre> <p>And  <code>prod</code> namespace:</p> <pre><code>kubectl create namespace prod\n</code></pre> <p>Step 7 Let\u2019s switch to operate in the <code>development</code> namespace:</p> <pre><code>kubectl config set-context --current --namespace=dev\n</code></pre> <p>We can see now that our current context is switched to dev:</p> <pre><code>kubectl config view | grep namespace\n</code></pre> <p>Output:</p> <pre><code>    namespace: dev\n</code></pre> <p>Result</p> <p>At this point, all requests we make to the Kubernetes cluster from the command line are scoped to the development namespace.</p> <p>Step 8 Let's test that all resources going to be created in <code>dev</code> namespace.</p> <pre><code>kubectl run devsnowflake --image=nginx\n</code></pre> <p>Step 9 Verify result of creation:</p> <pre><code>kubectl get pods\n</code></pre> <p>Success</p> <p>Developers are able to do what they want, and they do not have to worry about affecting content in the production namespace.</p> <p>Step 10 Now switch to the <code>production</code> namespace and show how resources in one namespace are hidden from the other.</p> <pre><code>kubectl config set-context --current --namespace=prod\n</code></pre> <p>The production namespace should be empty, and the following commands should return nothing.</p> <pre><code>kubectl get pods\n</code></pre> <p>Step 11 Let's create some <code>production</code> workloads:</p> <pre><code>kubectl run prodapp --image=nginx\n</code></pre> <pre><code>kubectl get pods -n prod\n</code></pre> <p>Summary</p> <p>At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace.</p>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#64-deleting-namespaces","title":"6.4 Deleting Namespaces","text":"<p>Step 1 Delete a namespace with</p> <pre><code>kubectl delete namespaces custom-namespace\nkubectl delete namespaces dev\nkubectl delete namespaces prod\n</code></pre> <p>Warning</p> <p>Unlike with OpenStack where when you delete a project/tenant, underlining resources will still exist as zombies and not deleted. In Kubernetes when you delete namespace it deletes everything under it (pods, svc, rc, and etc.)! This is called <code>resource garbage collection</code> in Kubernetes.</p> <p>Delete process is asynchronous, so you may see <code>Terminating</code> state for some time.</p>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#65-create-a-pod-in-a-different-namespace","title":"6.5 Create a pod in a different namespace","text":"<p>Create <code>test</code> namespace:</p> <pre><code>kubectl create ns test\n</code></pre> <p>Create a pod in this namespaces:</p> <pre><code>cat &lt;&lt;EOF &gt; echoserver-pod_ns.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: echoserverns\nspec:\n  containers:\n  - name: echoserver\n    image: gcr.io/google_containers/echoserver:1.4\n    ports:\n    - containerPort: 8080\nEOF\n</code></pre> <p>Create Pod in namespaces:</p> <pre><code>kubectl create -f echoserver-pod_ns.yaml -n test\n</code></pre> <p>Verify Pods created in specified namespaces:</p> <pre><code>kubectl get pods -n test\n</code></pre>"},{"location":"ycit019_Module_8_Lab_Kubernetes_Concepts/#6-cleaning-up","title":"6 Cleaning Up","text":"<p>Step 1 Delete the cluster</p> <pre><code>gcloud container clusters delete k8s-concepts\n</code></pre> <p>Step 2 Delete the firewall rule</p> <pre><code>gcloud compute firewall-rules delete echoserver-node-port\n</code></pre>"},{"location":"ycit019_Module_9_Kubernetes_Features/","title":"ycit019 Module 9 Kubernetes Features","text":"<p>Lab 8 Kubernetes Features</p> <p>Objective</p> <ul> <li>Use Liveness Probes to healthcheck you application while it is running</li> <li>Learn about secrets and configmaps</li> <li>Deploy a Daemonset and jobs</li> </ul>"},{"location":"ycit019_Module_9_Kubernetes_Features/#0-create-gke-cluster","title":"0 Create GKE Cluster","text":"<p>Step 1 Enbale the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud container clusters create k8s-features \\\n--zone us-central1-c \\\n--num-nodes 2\n</code></pre> <p>Output:</p> <pre><code>NAME          LOCATION       MASTER_VERSION   MASTER_IP      MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\nk8s-features  us-central1-c  1.19.9-gke.1400  34.121.222.83  e2-medium     1.19.9-gke.1400  2          RUNNING\n</code></pre> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials k8s-features --zone us-central1-c\n</code></pre>"},{"location":"ycit019_Module_9_Kubernetes_Features/#1-kubernetes-features","title":"1 Kubernetes Features","text":""},{"location":"ycit019_Module_9_Kubernetes_Features/#11-using-liveness-probes","title":"1.1 Using Liveness Probes","text":"<p>Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes p rovides liveness probes to detect and remedy such situations.</p> <p>As we already discussed Kubernetes provides 3 types of <code>Probes</code> to perform Liveness checks:</p> <ul> <li>HTTP GET</li> <li>EXEC</li> <li>tcpSocket</li> </ul> <p>In below example we are going to use HTTP GET probe for a Pod that runs a container based on the <code>gcr.io/google_containers/liveness</code> image.</p> <p>** Step 1** Create <code>http-liveness.yaml</code> manifest with below content:</p> <pre><code>cat &lt;&lt;EOF &gt; http-liveness.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-http\nspec:\n  containers:\n  - name: liveness\n    image: gcr.io/google_containers/liveness\n    args:\n    - /server\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: X-Custom-Header\n          value: Awesome\n      initialDelaySeconds: 3\n      periodSeconds: 3\nEOF\n</code></pre> <p>Based on the manifest, we can see that the Pod has a single Container. The <code>periodSeconds</code> field specifies that the kubelet should perform a liveness probe every 3 seconds. The <code>initialDelaySeconds</code> field tells the kubelet that it should wait 3 seconds before performing the first probe. To perform a probe, the kubelet sends an HTTP GET request to the server that is running in the Container and listening on port 8080. If the handler for the server's <code>/healthz</code> path returns a success code, the kubelet considers the Container to be alive and healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it.</p> <p>Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure.</p> <p>Full code for for a reference in  server.go.</p> <p>For the first 10 seconds that the Container is alive, the <code>/healthz</code> handler returns a status of 200. After that, the handler returns a status of 500.</p> <pre><code>http.HandleFunc(\"/healthz\", func(w http.ResponseWriter, r *http.Request) {\n    duration := time.Now().Sub(started)\n    if duration.Seconds() &gt; 10 {\n        w.WriteHeader(500)\n        w.Write([]byte(fmt.Sprintf(\"error: %v\", duration.Seconds())))\n    } else {\n        w.WriteHeader(200)\n        w.Write([]byte(\"ok\"))\n    }\n})\n</code></pre> <p>The kubelet starts performing health checks 3 seconds after the Container starts. So the first couple of health checks will succeed. But after 10 seconds, the health checks will fail, and the kubelet will kill and restart the Container.</p> <p>Step 2 Let's create a Pod and see how HTTP liveness check works:</p> <pre><code>kubectl create -f http-liveness.yaml\n</code></pre> <p>Step 3 Monitor the Pod</p> <pre><code>watch kubectl get pod\n</code></pre> <p>Result</p> <p>After 10 seconds Pods has beed restarted.</p> <p>Exit the shell session by using ctrl+c</p> <p>Step 4 Verify status of the Pod and review the Events happened after restart</p> <pre><code>kubectl describe pod liveness-http\n</code></pre> <p>Result</p> <p>Pod events shows that liveness probes have failed and the Container has been restarted.</p> <p>Step 5 Clean up</p> <pre><code>kubectl delete -f http-liveness.yaml\n</code></pre>"},{"location":"ycit019_Module_9_Kubernetes_Features/#12-using-configmaps","title":"1.2 Using ConfigMaps","text":"<p>In Kubernetes ConfigMaps could be use in several cases:</p> <ul> <li>Storing configuration values as key-values in ConfigMap and referencing them   in a Pod as environment variables</li> <li>Storing configurations as a file inside of ConfigMap and referencing it in   a Pod as a Volume</li> </ul> <p>Let's try second option and deploy nginx pod while storing its config in a ConfigMap.</p> <p>Step 1 Create nginx <code>my-nginx-config.conf</code> config file as below:</p> <pre><code>cat &lt;&lt;EOF &gt; my-nginx-config.conf\nserver {\n  listen              80;\n  server_name         www.cloudnative.tech;\n\n  gzip on;\n  gzip_types text/plain application/xml;\n\n  location / {\n    root   /usr/share/nginx/html;\n    index  index.html index.htm;\n  }\n}\nEOF\n</code></pre> <p>Step 2 Create ConfigMap from this file</p> <pre><code>kubectl create configmap nginxconfig --from-file=my-nginx-config.conf\n</code></pre> <p>Step 3 Review the ConfigMap</p> <pre><code>kubectl describe cm nginxconfig\n</code></pre> <p>Result:</p> <pre><code>  ```\n  Name:     nginxconfig\n  Namespace:    default\n  Labels:       &lt;none&gt;\n  Annotations:  &lt;none&gt;\n\n  Data\n  ====\n  my-nginx-config.conf:\n  ----\n  server {\n    listen              80;\n    server_name         _;\n\n    gzip off;\n    gzip_types text/plain application/xml;\n\n    location / {\n      root   /usr/share/nginx/html;\n      index  index.html index.htm;\n    }\n  }\n\n  Events:   &lt;none&gt;\n  ```\n</code></pre> <p>Step 4 Create Nginx Pod <code>website.yaml</code> file, where ConfigMap referenced as a Volume</p> <pre><code>cat &lt;&lt;EOF &gt; website.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: website\nspec:\n  containers:\n  - image: nginx:alpine\n    name: website\n    volumeMounts:\n    - name: html\n      mountPath: /usr/share/nginx/html\n      readOnly: false\n    - name: config\n      mountPath: /etc/nginx/conf.d\n      readOnly: true\n  volumes:\n  - name: html\n    emptyDir: {}\n  - name: config\n    configMap:\n      name: nginxconfig\nEOF\n</code></pre> <p>Step 5 Deploy Nginx Pod <code>website.yaml</code></p> <pre><code>kubectl apply -f website.yaml\n</code></pre> <p>You can expose a service as we usually do with NodePort or LoadBalancer or by using the port-forward technicque in the next step</p> <p>Step 6 Open second <code>SSH</code> terminal by pressing \"+\" icon in the cloud shell and run following command</p> <pre><code>kubectl port-forward website 8080:80\n</code></pre> <p>Result</p> <p>This opens a tunnel and expose our application on port '80' to localhost:8080.</p> <p>Step 7 Test that website is running Navigate back to the first terminal tab and run the following</p> <pre><code>curl localhost:8080\n</code></pre> <p>Result:</p> <pre><code>  ```\n  &lt;html&gt;\n  &lt;head&gt;&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;\n  &lt;body&gt;\n  &lt;center&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;/center&gt;\n  &lt;hr&gt;&lt;center&gt;nginx/1.21.0&lt;/center&gt;\n  &lt;/body&gt;\n  &lt;/html&gt;\n  ```\n</code></pre> <p>Success</p> <p>You've just learned how to use ConfigMaps.</p> <p>You can also use the <code>preview</code> feature with the console</p> <p>Step 8 start a shell in the pod by using <code>exec</code></p> <pre><code>kubectl exec website -it -- sh\n</code></pre> <p>Your terminal now should have <code>/#</code></p> <p>Step 9 view the content of the folder where we added the volume</p> <pre><code>ls /etc/nginx/conf.d\n</code></pre> <p>Step 10 check the content of the file in that folder and notice that it's the same as the config file for nginx we mounted.</p> <pre><code>cat /etc/nginx/conf.d/my-nginx-config.conf\n</code></pre> <p>Step 11 Exit the shell</p> <pre><code>exit\n</code></pre> <p>Make sure you're back to the cloud shell terminal.</p>"},{"location":"ycit019_Module_9_Kubernetes_Features/#13-using-secrets","title":"1.3 Using Secrets","text":""},{"location":"ycit019_Module_9_Kubernetes_Features/#131-kubernetes-secrets","title":"1.3.1 Kubernetes Secrets","text":"<p>Kubernetes secrets allow users to define sensitive information outside of containers and expose that information to containers through environment variables as well as files within Pods. In this section we will declare and create secrets to hold our database connection information that will be used by Wordpress to connect to its backend database.</p> <p>Step 1 Open up two terminal windows. We will use one window to generate encoded strings that will contain our sensitive data. The other window will be used to create the secrets YAML declaration.</p> <p>Step 2 In the first terminal window, execute the following commands to encode our strings:</p> <pre><code>echo -n \"admin\" | base64\necho -n \"t0p-Secret\" | base64\n</code></pre> <p>Step 3 create the secret. For this lab, we added the encoded values for you. Feel free to change the username and password, and replace the values in the file below:</p> <pre><code>cat &lt;&lt;EOF &gt; app-secrets.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secret\ntype: Opaque\ndata:\n  username: YWRtaW4=\n  password: dDBwLVNlY3JldA==\nEOF\n</code></pre> <p>Step 4 Create the secret:</p> <pre><code>kubectl create -f app-secrets.yaml\n</code></pre> <p>Step 5 Verify secret creation and get details:</p> <pre><code>kubectl get secrets\n</code></pre> <p>Step 6 Get details of this secret:</p> <pre><code>kubectl describe secrets/app-secrets\n</code></pre> <p>Result:</p> <pre><code>Name:         app-secret\nNamespace:    default\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\n\nType:  Opaque\n\nData\n====\npassword:  10 bytes\nusername:  5 bytes\n</code></pre> <p>Step 7 Create a pod that will reference the secret as a volume. Use either nano or vim to create secret-pod.yaml, and copy the following to it</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: alpine\n      command: [\"/bin/sh\", \"-ec\", \"export LOGIN=$(cat /etc/secret-volume/username);export PWD=$(cat /etc/secret-volume/password);while true; do echo hello $LOGIN your password is $PWD; sleep 10; done\"]\n      volumeMounts:\n          - name: secret-volume\n            mountPath: /etc/secret-volume\n  volumes:\n    - name: secret-volume\n      secret:\n        secretName: app-secret\n  restartPolicy: Never\n</code></pre> <p>Step 8 View the logs of the pod.</p> <pre><code>kubectl logs secret-test-pod\n</code></pre> <p>Notice how we were able to pull the content of the secret using a command. Not very secure, right?</p> <p>Step 9 Delete both the pod and the secret</p> <pre><code>kubectl delete pod secret-test-pod\nkubectl delete secret app-secret\n</code></pre> <p>Summary</p> <p>Secrets has been created and they not visiable when you view them via <code>kubectl describe</code> This does not make Kubernetes secrets secure as we have experienced. Additonal measures need to be in place to protect secrets.</p>"},{"location":"ycit019_Module_9_Kubernetes_Features/#14-jobs","title":"1.4 Jobs","text":"<p>A Job creates one or more pods and ensures that a specified number of them successfully complete. A job keeps track of successful completion of a pod. When the specified number of pods have successfully completed, the job itself is complete. The job will start a new pod if the pod fails or is deleted due to hardware failure. A successful completion of the specified number of pods means the job is complete.</p> <p>This is different from a replica set or a deployment which ensures that a certain number of pods are always running. So if a pod in a replica set or deployment terminates, then it is restarted again. This makes replica set or deployment as long-running processes. This is well suited for a web server, such as NGINX. But a job is completed if the specified number of pods successfully completes. This is well suited for tasks that need to run only once. For example, a job may convert an image format from one to another. Restarting this pod in replication controller would not only cause redundant work but may be harmful in certain cases.</p> <p>Jobs are complementary to Replica Set. A Replica Set manages pods which are not expected to terminate (e.g. web servers), and a Job manages pods that are expected to terminate (e.g. batch jobs).</p>"},{"location":"ycit019_Module_9_Kubernetes_Features/#141-non-parallel-job","title":"1.4.1 Non-parallel Job","text":"<p>Only one pod per job is started, unless the pod fails. Job is complete as soon as the pod terminates successfully.</p> <p>Use the image \"busybox\" and have it sleep for 10 seconds and then complete. Run your job to be sure it works.</p> <p>Step 1 Create a Job Manifest</p> <pre><code>cat &lt;&lt;EOF &gt; busybox.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: busybox\nspec:\n  template:\n    spec:\n      containers:\n      - name: busybox\n        image: busybox\n        command: [\"sleep\", \"20\"]\n      restartPolicy: Never\nEOF\n</code></pre> <p>Step 2 Run a Job</p> <pre><code>kubectl create -f busybox.yaml\n</code></pre> <p>Step 3 Look at the job:</p> <pre><code>kubectl get jobs\n</code></pre> <p>Output:</p> <pre><code>NAME      DESIRED   SUCCESSFUL   AGE\nbusybox    1         0            0s\n</code></pre> <p>Result</p> <p>The output shows that the job is not successful yet.</p> <p>Step 4 Watch the pod status</p> <pre><code>kubectl get -w pods\n</code></pre> <p>Output:</p> <pre><code>NAME         READY     STATUS    RESTARTS   AGE\nbusybox-lk49x   1/1       Running   0          7s\nbusybox-lk49x   0/1       Completed   0         24s\n</code></pre> <p>Result</p> <p>It starts with pod for the job is <code>Running</code>. Then pod successfully exits after a few seconds and shows the <code>Completed</code> status.</p> <p>Step 5 Watch the job status again:</p> <pre><code>kubectl get jobs\n</code></pre> <p>Output:</p> <pre><code>NAME      COMPLETIONS   DURATION   AGE\nbusybox   1/1           21s        1m\n</code></pre> <p>Step 6 Delete a Job</p> <pre><code>kubectl delete -f busybox.yaml\n</code></pre>"},{"location":"ycit019_Module_9_Kubernetes_Features/#142-parallel-job","title":"1.4.2 Parallel Job","text":"<p>Non-parallel jobs run only one pod per job. This API is used to run multiple pods in parallel for the job. The number of pods to complete is defined by <code>.spec.completions</code> attribute in the configuration file. The number of pods to run in parallel is defined by <code>.spec.parallelism</code> attribute in the configuration file. The default value for both of these attributes is 1.</p> <p>The job is complete when there is one successful pod for each value in the range in 1 to <code>.spec.completions</code>. For that reason, it is also called as fixed completion count job.</p> <p>Step 1 Create a Job Manifest</p> <pre><code>cat &lt;&lt;EOF &gt; job-parallel.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: wait\nspec:\n  completions: 6\n  parallelism: 2\n  template:\n    metadata:\n      name: wait\n    spec:\n      containers:\n      - name: wait\n        image: ubuntu\n        command: [\"sleep\",  \"10\"]\n      restartPolicy: Never\nEOF\n</code></pre> <p>Note</p> <p>This job specification is similar to the non-parallel job specification above. However it has two new attributes added: <code>.spec.completions</code> and <code>.spec.parallelism</code>. This means the job will be complete when six pods have successfully completed. A maximum of two pods will run in parallel at a given time.</p> <p>Step 2 Create a parallel job using the command:</p> <pre><code>kubectl apply -f job-parallel.yaml\n</code></pre> <p>Step 3 Watch the status of the job as shown:</p> <pre><code>kubectl get -w jobs\n</code></pre> <p>Output:</p> <pre><code>NAME   COMPLETIONS   DURATION   AGE\nwait   0/6           6s         6s\nwait   1/6           12s        12s\nwait   2/6           12s        12s\nwait   3/6           24s        24s\nwait   4/6           24s        24s\nwait   5/6           36s        36s\nwait   6/6           36s        36s\n</code></pre> <p>Results</p> <p>The output shows that 2 pods are created about every 12 seconds.</p> <p>Step 4  In another terminal window, watch the status of pods created:</p> <pre><code>kubectl get -w pods -l job-name=wait\n</code></pre> <p>Output:</p> <pre><code>NAME         READY   STATUS      RESTARTS   AGE\nwait-5blwm   0/1     Completed   0          17s\nwait-stmk4   0/1     Completed   0          17s\nwait-ts6xt   1/1     Running     0          5s\nwait-xlhl6   1/1     Running     0          5s\nwait-xlhl6   0/1     Completed   0          12s\nwait-rq6z5   0/1     Pending     0          0s\nwait-ts6xt   0/1     Completed   0          12s\nwait-rq6z5   0/1     Pending     0          0s\nwait-rq6z5   0/1     ContainerCreating   0          0s\nwait-f85bj   0/1     Pending             0          0s\nwait-f85bj   0/1     Pending             0          0s\nwait-f85bj   0/1     ContainerCreating   0          0s\nwait-rq6z5   1/1     Running             0          2s\nwait-f85bj   1/1     Running             0          2s\nwait-f85bj   0/1     Completed           0          12s\nwait-rq6z5   0/1     Completed           0          12s\n</code></pre> <p>Step 6 Once the job is completed, you can get the list of completed pods</p> <pre><code>kubectl get pods -l job-name=wait\n</code></pre> <p>Result:</p> <pre><code>NAME         READY   STATUS      RESTARTS   AGE\nwait-5blwm   0/1     Completed   0          2m55s\nwait-f85bj   0/1     Completed   0          2m31s\nwait-rq6z5   0/1     Completed   0          2m31s\nwait-stmk4   0/1     Completed   0          2m55s\nwait-ts6xt   0/1     Completed   0          2m43s\nwait-xlhl6   0/1     Completed   0          2m43s\n</code></pre> <p>Step 5 Similarly, <code>kubectl get jobs</code> shows the status of the job after it has completed:</p> <pre><code>kubectl get jobs\n</code></pre> <p>Result:</p> <pre><code>NAME   COMPLETIONS   DURATION   AGE\nwait   6/6           36s        3m54s\n</code></pre> <p>Step 6 Deleting a job deletes all the pods as well. Delete the job as:</p> <pre><code>kubectl delete -f job-parallel.yaml\n</code></pre>"},{"location":"ycit019_Module_9_Kubernetes_Features/#15-cron-jobs","title":"1.5 Cron Jobs","text":"<p>A Cron Job is a job that runs on a given schedule, written in Cron format. There are two primary use cases:</p> <ul> <li>Run jobs once at a specified point in time</li> <li>Repeatedly at a specified point in time</li> </ul>"},{"location":"ycit019_Module_9_Kubernetes_Features/#151-create-cron-job","title":"1.5.1 Create Cron Job","text":"<p>Step 1 Create <code>CronJob</code> manifest that prints the current timestamp and the message \"<code>Hello World</code>\" every minute.</p> <pre><code>cat &lt;&lt;EOF &gt; cronjob.yaml\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"*/1 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            app: hello-cronpod\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            args:\n            - /bin/sh\n            - -c\n            - date; echo Hello World!\n          restartPolicy: OnFailure\nEOF\n</code></pre> <p>Step 2 Create the Cron Job as shown in the command:</p> <pre><code>kubectl create -f cronjob.yaml\n</code></pre> <p>Step 3 Watch the status of the job as shown:</p> <pre><code>kubectl get -w cronjobs\n</code></pre> <p>Output:</p> <pre><code>NAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE\nhello   */1 * * * *   False     0        &lt;none&gt;          6s\nhello   */1 * * * *   False     1        5s              39s\nhello   */1 * * * *   False     0        15s             49s\nhello   */1 * * * *   False     1        5s              99s\nhello   */1 * * * *   False     0        15s             109s\nhello   */1 * * * *   False     1        6s              2m40s\nhello   */1 * * * *   False     0        16s             2m50s\n</code></pre> <p>Step 4 In another terminal window, watch the status of pods created:</p> <pre><code>kubectl get -w pods -l app=hello-cronpod\n</code></pre> <p>Output:</p> <pre><code>NAME                     READY   STATUS      RESTARTS   AGE\nhello-1622584020-kc46c   0/1     Completed   0          118s\nhello-1622584080-c2pcq   0/1     Completed   0          58s\nhello-1622584140-hxnv2   0/1     Pending     0          0s\nhello-1622584140-hxnv2   0/1     Pending     0          0s\nhello-1622584140-hxnv2   0/1     ContainerCreating   0          0s\nhello-1622584140-hxnv2   1/1     Running             0          1s\nhello-1622584140-hxnv2   0/1     Completed           0          2s\n</code></pre> <p>Step 5 Get logs from one of the pods:</p> <pre><code>kubectl logs hello-1622584140-hxnv2\n</code></pre> <p>Output:</p> <pre><code>Tue Jun  1 21:49:07 UTC 2021\nHello World!\n</code></pre> <p>Step 6 Delete Cron Job</p> <pre><code>kubectl delete -f cronjob.yaml\n</code></pre>"},{"location":"ycit019_Module_9_Kubernetes_Features/#16-daemon-set","title":"1.6 Daemon Set","text":"<p>Daemon Set ensures that a copy of the pod runs on a selected set of nodes. By default, all nodes in the cluster are selected. A selection critieria may be specified to select a limited number of nodes.</p> <p>As new nodes are added to the cluster, pods are started on them. As nodes are removed, pods are removed through garbage collection.</p> <p>The following is an example DaemonSet that runs a Prometheus exporter container that used for collecting machine metrics from each node.</p> <p>Step 1 Create a DaemonSet manifest</p> <pre><code>cat &lt;&lt;EOF &gt; daemonset.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: prometheus-daemonset\nspec:\n  selector:\n    matchLabels:\n      tier: monitoring\n      name: prometheus-exporter\n  template:\n    metadata:\n      labels:\n        tier: monitoring\n        name: prometheus-exporter\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/node-exporter\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Step 2  Run the following command to create the ReplicaSet and pods:</p> <pre><code>kubectl create -f daemonset.yaml --record\n</code></pre> <p>Note</p> <p>The <code>--record</code> flag will track changes made through each revision.</p> <p>Step 3 Get basic details about the DaemonSet:</p> <pre><code>kubectl get daemonsets/prometheus-daemonset\n</code></pre> <p>Output:</p> <pre><code>NAME                   DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE-SELECTOR   AGE\nprometheus-daemonset   1         1         1         1            1           &lt;none&gt;          5s\n</code></pre> <p>Step 4 Get more details about the DaemonSet:</p> <pre><code>kubectl describe daemonset/prometheus-daemonset\n</code></pre> <p>Output:</p> <pre><code>Name:           prometheus-daemonset\nSelector:       name=prometheus-exporter,tier=monitoring\nNode-Selector:  &lt;none&gt;\nLabels:         &lt;none&gt;\nAnnotations:    deprecated.daemonset.template.generation: 1\n                kubernetes.io/change-cause: kubectl create --filename=daemonset.yaml --record=true\nDesired Number of Nodes Scheduled: 2\nCurrent Number of Nodes Scheduled: 2\nNumber of Nodes Scheduled with Up-to-date Pods: 2\nNumber of Nodes Scheduled with Available Pods: 2\nNumber of Nodes Misscheduled: 0\nPods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  name=prometheus-exporter\n           tier=monitoring\n  Containers:\n   prometheus:\n    Image:        prom/node-exporter\n    Port:         80/TCP\n    Host Port:    0/TCP\n    Environment:  &lt;none&gt;\n    Mounts:       &lt;none&gt;\n  Volumes:        &lt;none&gt;\nEvents:\n  Type    Reason            Age    From                  Message\n  ----    ------            ----   ----                  -------\n  Normal  SuccessfulCreate  9m     daemonset-controller  Created pod: prometheus-daemonset-f4xl2\n  Normal  SuccessfulCreate  2m18s  daemonset-controller  Created pod: prometheus-daemonset-w596z\n</code></pre> <p>Step 5 Get pods in the DaemonSet:</p> <pre><code>kubectl get pods -l name=prometheus-exporter\n</code></pre> <p>Output:</p> <pre><code>NAME                         READY   STATUS    RESTARTS   AGE\nprometheus-daemonset-f4xl2   1/1     Running   0          8m27s\nprometheus-daemonset-w596z   1/1     Running   0          105s\n</code></pre> <p>Step 6 Verify that the Prometheus pod was successfully deployed to the cluster nodes:</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>Output:</p> <pre><code>NAME                         READY   STATUS    RESTARTS   AGE     IP          NODE                                          NOMINATED NODE   READINESS GATES\nprometheus-daemonset-f4xl2   1/1     Running   0          6m51s   10.0.0.19   gke-k8s-features-default-pool-73e09df7-m2lf   &lt;none&gt;           &lt;none&gt;\nprometheus-daemonset-w596z   1/1     Running   0          9s      10.0.1.2    gke-k8s-features-default-pool-73e09df7-k7hj   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>Notes</p> <p>It is possible to Limit DaemonSets to specific nodes by changing the <code>spec.template.spec</code> to include a <code>nodeSelector</code> to matche <code>node</code> label.</p> <p>Step 7  Delete a DaemonSet</p> <pre><code>kubectl delete -f daemonset.yaml\n</code></pre>"},{"location":"ycit019_Module_9_Kubernetes_Features/#17-cleaning-up","title":"1.7 Cleaning Up","text":"<p>Step 1 Delete the cluster</p> <pre><code>gcloud container clusters delete k8s-concepts\n</code></pre>"},{"location":"ycit019_ass1_sol/","title":"1 Containerize Applications","text":"<p>Objective:</p> <ul> <li>Review process of containerizing of applications</li> <li>Review creation of Docker Images</li> <li>Review build image process</li> </ul>"},{"location":"ycit019_ass1_sol/#prepare-lab-environment","title":"Prepare Lab Environment","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p>"},{"location":"ycit019_ass1_sol/#11-overview-of-the-sample-application","title":"1.1 Overview of the Sample Application","text":"<p>This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes:</p> <ul> <li>gowebapp</li> </ul> <p>This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page.</p> <p>Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL.</p> <p>Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises.</p> <p>For more details about the internal design and implementation of the Go web application, see code/README.md.</p> <ul> <li>gowebapp-mysql</li> </ul> <p>This directory contains the schema file used to setup the backing MySQL database for the Go web application.</p>"},{"location":"ycit019_ass1_sol/#11-build-dockers-image-for-frontend-application","title":"1.1 Build Dockers image for frontend application","text":"<p>Result</p> <p>Two folders with go app and mysql config has been reviewed.</p> <p>Step 2 Write Dockerfile for your frontend application</p> <p>Create a file named <code>Dockerfile</code> in this directory for the frontend Go application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands:</p> <pre><code>cd ~/ycit019_2022/Mod5_assignment/gowebapp\n</code></pre> <pre><code>vim Dockerfile\n</code></pre> <pre><code>FROM golang:1.15.11\n\nLABEL maintainer \"student@mcgill.ca\"\nLABEL gowebapp \"v1\"\n\nEXPOSE 80\n\nENV GOPATH=/go\n\nCOPY /code $GOPATH/src/gowebapp/\n\nWORKDIR $GOPATH/src/gowebapp/\n\nRUN go get &amp;&amp; go install\n\nENTRYPOINT $GOPATH/bin/gowebapp\n</code></pre> <p>Step 3  Build gowebapp Docker image locally</p> <p>Build the  image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message.</p> <pre><code>docker build -t &lt;user-name&gt;/gowebapp:v1 .\n</code></pre>"},{"location":"ycit019_ass1_sol/#12-build-docker-image-for-backend-application","title":"1.2 Build Docker image for backend application","text":"<p>Step 1 Locate folder with mysql config</p> <pre><code>cd ~/ycit019_2022/Mod5_assignment/gowebapp-mysql\n</code></pre> <p>Step 2 Write Dockerfile for your backend application</p> <p>Create a file named <code>Dockerfile</code> in this directory for the backend MySQL database application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands:</p> <pre><code>FROM mysql:8.0\n\nLABEL maintainer \"student@mcgill.ca\"\nLABEL gowebapp-sql \"v1\"\n\nCOPY gowebapp.sql /docker-entrypoint-initdb.d/\n</code></pre> <p>Step 2 Build gowebapp-mysql Docker image locally</p> <pre><code>docker build -t &lt;user-name&gt;/gowebapp-mysql:v1 .\n</code></pre> <p>Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally</p>"},{"location":"ycit019_ass1_sol/#14-test-application-by-running-with-docker-engine","title":"1.4 Test application by running with Docker Engine.","text":"<p>Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly.</p> <p>Step 1 Create Docker user-defined network</p> <p>To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers:</p> <pre><code>docker network create gowebapp \\\n-d bridge \\\n--subnet 172.19.0.0/16\n</code></pre> <p>Step 2  Launch backend container</p> <p>Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable:</p> <pre><code>docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\\n-d -e MYSQL_ROOT_PASSWORD=rootpasswd &lt;user-name&gt;/gowebapp-mysql:v1\n</code></pre> <p>docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd archyufa/gowebapp-mysql:v1</p> <p>Step 3  Launch frontend container</p> <p>Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080  on the host machine:</p> <pre><code>docker run -p 8080:80 --net gowebapp -d --name gowebapp \\\n--hostname gowebapp &lt;user-name&gt;/gowebapp:v1\n</code></pre> <p>docker run -p 8080:80 -d --net gowebapp --name gowebapp \\ --hostname gowebapp archyufa/gowebapp:v1</p> <p>Step 4  Test the application locally</p> <p>Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container.</p> <p>Task</p> <p>Take a screenshot of running application. </p> <p>Step 5 Inspect the MySQL database</p> <p>Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly:</p> <pre><code>docker exec -it gowebapp-mysql  bash\n</code></pre> <p>Step 6 Once inside the container, connect to MySQL database:</p> <pre><code>mysql -u root -p\npassword:\n</code></pre> <p>Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence:</p> <pre><code>#Simple SQL to navigate\nSHOW DATABASES;\nUSE gowebapp;\nSHOW TABLES;\nSELECT * FROM &lt;table_name&gt;;\nexit;\n</code></pre>"},{"location":"ycit019_ass1_sol/#15-cleanup-running-applications-and-unused-networks","title":"1.5 Cleanup running applications and unused networks","text":"<pre><code>docker rm -f $(docker ps -q)\n</code></pre>"},{"location":"ycit019_ass2_solution/","title":"1 Containerize Applications","text":"<p>Objective:</p> <ul> <li>Use GCP Cloud Source Repositories</li> <li>Push Images to GCR and DockerHub</li> <li>Automate local Development with Docker-Compose</li> </ul>"},{"location":"ycit019_ass2_solution/#prepare-the-cloud-source-repository-environment-with-module-6-assignment","title":"Prepare the Cloud Source Repository Environment with Module 6 Assignment","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p> <p>Cloud Source Repositories: Qwik Start</p> <p>Step 1 Locate directory where <code>docker-compose</code> manifest going to be stored.</p> <pre><code>cd ~/ycit019_2022/\ngit pull       # Pull latest Mod6_assignment\n</code></pre> <p>In case you don't have this folder clone it as following:</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit019_2022\ncd ~/ycit019_2022/Mod6_assignment/\nls\n</code></pre> <p>Step 2 Go into the local repository you've created:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <p>Important</p> <p>Replace above with your project_id student_name</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <p>Step 3 Copy <code>Mod6_assignment</code> folder to your repo:</p> <pre><code>git pull                              # Pull latest code from you repo\ncp -r ~/ycit019_2022/Mod6_assignment/ .\n</code></pre> <p>Step 4 Commit <code>Mod6_assignment</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"adding `Mod6_assignment` with docker-compose manifest\"\n</code></pre> <p>Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre> <p>Step 6 Review Cloud Source Repositories</p> <p>Use the <code>Google Cloud Source Repositories</code> code browser to view repository files.  You can filter your view to focus on a specific branch, tag, or comment.</p> <p>Browse the Mod6_assignment files you pushed to the repository by opening the Navigation menu and selecting Source Repositories:</p> <pre><code>Click Menu -&gt; Source Repositories &gt; Source Code.\n</code></pre> <p>Result</p> <p>The console shows the files in the master branch at the most recent commit.</p>"},{"location":"ycit019_ass2_solution/#2-build-and-push-docker-images-to-google-container-registry-gcr","title":"2 Build and push Docker images to Google Container Registry (GCR)","text":""},{"location":"ycit019_ass2_solution/#21-build-and-push-gowebapp-mysql-image-to-gcr","title":"2.1 Build and push <code>gowebapp-mysql</code> Image to GCR","text":"<p>Step 1 Locate folder with mysql config</p> <pre><code>cd ~/$student_name-notepad/Mod6_assignment/gowebapp-mysql\n</code></pre> <p>Step 2 Review the existing Dockerfile</p> <pre><code>cat Dockerfile\n</code></pre> <p>output:</p> <pre><code>FROM mysql:8.0\n\nLABEL maintainer \"student@mcgill.ca\"\nLABEL gowebapp-sql \"v1\"\n\nCOPY gowebapp.sql /docker-entrypoint-initdb.d/\n</code></pre> <p>Step 2 Set the Project ID in  Environment Variable: </p> <pre><code>export PROJECT_ID=&lt;project_id&gt;\n</code></pre> <p>Here is how you can find you project_ID:</p> <p></p> <p>Set the project ID as default</p> <pre><code>gcloud config set project $PROJECT_ID\n</code></pre> <p>Step 3 Enable the required APIs:</p> <pre><code>gcloud services enable containerregistry.googleapis.com\n</code></pre> <p>Step 4 Build gowebapp-mysql Docker image with GCR registry address locally</p> <pre><code>docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 .\n</code></pre> <p>Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally</p> <p>Step 5 Push the image to gcr.io:</p> <pre><code>docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1\n</code></pre> <p>Step 6  Login to GCP console -&gt; Container Registry -&gt; Images</p> <p>Result</p> <p>Docker images has been pushed to GCR registry</p>"},{"location":"ycit019_ass2_solution/#21-build-and-push-gowebapp-image-to-gcr","title":"2.1 Build and push <code>gowebapp</code> Image to GCR","text":"<p>Step 1 Locate folder with mysql config</p> <pre><code>cd ~/$student_name-notepad/Mod6_assignment/gowebapp\n</code></pre> <p>Step 2 Review the existing Dockerfile</p> <pre><code>cat Dockerfile\n</code></pre> <p>output:</p> <pre><code>FROM golang:1.16.4\n\nLABEL maintainer \"student@mcgill.ca\"\nLABEL gowebapp \"v1\"\n\nEXPOSE 80\n\nENV GO111MODULE=auto\nENV GOPATH=/go\n\nCOPY /code $GOPATH/src/gowebapp/\n\nWORKDIR $GOPATH/src/gowebapp/\n\nRUN go get &amp;&amp; go install\n\nENTRYPOINT $GOPATH/bin/gowebapp\n</code></pre> <p>Note</p> <p>We've updated our application to support golang version 1.16.</p> <p>Step 4 Build <code>gowebapp</code> Docker image with GCR registry address l locally</p> <pre><code>docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 .\n</code></pre> <p>Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally</p> <p>Step 5 Push the image to gcr.io:</p> <pre><code>docker push gcr.io/${PROJECT_ID}/gowebapp:v1\n</code></pre> <p>Step 6  Login to GCP console -&gt; Container Registry -&gt; Images</p> <p>Result</p> <p>Docker images has been pushed to GCR registry</p> <p>Step 7 Delete locally build images, as we want to test how images will be pulled from  gcr registry in the next step:</p> <pre><code>docker rmi gcr.io/$PROJECT_ID/gowebapp:v1\ndocker rmi gcr.io/$PROJECT_ID/gowebapp-mysql:v1\n</code></pre>"},{"location":"ycit019_ass2_solution/#23-test-application-by-running-with-docker-engine","title":"2.3 Test application by running with Docker Engine.","text":"<p>Step 1 Create Docker user-defined network</p> <p>To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers:</p> <pre><code>docker network create gowebapp \\\n-d bridge \\\n--subnet 172.19.0.0/16\n</code></pre> <p>Step 2  Launch backend container</p> <p>Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable:</p> <pre><code>docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\\n-d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1\n</code></pre> <p>Step 3 Wait for <code>mysql</code> container to start</p> <pre><code>docker ps\ndocker logs &lt;id&gt;\n</code></pre> <p>You should see following output: <code>[Server] /usr/sbin/mysqld: ready for connections</code></p> <p>Step 3  Launch frontend container</p> <p>Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080  on the host machine:</p> <pre><code>docker run -p 8080:80 --net gowebapp -d --name gowebapp \\\n--hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1\n</code></pre> <p>Step 4  Test the application locally</p> <p>Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p>"},{"location":"ycit019_ass2_solution/#24-cleanup-running-applications-and-unused-networks","title":"2.4 Cleanup running applications and unused networks","text":"<pre><code>docker ps\n</code></pre> <pre><code>docker rm -f &lt;container_id_gowebapp&gt; &lt;container_id_gowebapp-mysql&gt;\ndocker network\ndocker network rm gowebapp\n</code></pre>"},{"location":"ycit019_ass2_solution/#3-docker-compose","title":"3 Docker Compose","text":""},{"location":"ycit019_ass2_solution/#31-test-application-locally-with-docker-compose","title":"3.1 Test application locally with Docker Compose","text":"<p>Task: Automate local testing with <code>Docker Compose</code> by creating <code>docker-compose.yaml</code> file which contains:</p> <ul> <li> <p>User-defined network <code>gowebapp1</code></p> </li> <li> <p>Service <code>gowebapp-mysql</code></p> </li> <li> <p>Service <code>gowebapp</code></p> </li> </ul> <p>Reference</p> <p>Docker Compose v3 documentations</p> <p>Implementation</p> <ol> <li>Ensure that <code>Mysql</code> start first and then <code>webapp</code> services</li> <li>Ensure that <code>Mysql</code> database is fully up prior to start <code>webapp</code> services using healthcheck feature of docker compose.</li> <li>Ensure that <code>webapp-mysql</code>and <code>webapp</code> build with Docker-Compose</li> <li>Ensure that environment variable <code>MYSQL_ROOT_PASSWORD</code> is set inside of the docker compose file.</li> </ol> <p>Step 1 Create compose file</p> <pre><code>cd ~/$student_name-notepad/Mod6_assignment\n</code></pre> <p>Edit existing docker-compose file:</p> <pre><code>edit docker-compose.yaml\n</code></pre> <p>Create structure as following:</p> <pre><code>version: '2.4'\nservices:\n  gowebapp-mysql:\n    build: ./gowebapp-mysql\n    environment:\n      MYSQL_ROOT_PASSWORD: rootpasswd\n    container_name: gowebapp-mysql\n    healthcheck:\n      test: \"/usr/bin/mysql --user=root --password=rootpasswd --execute \\\"SHOW DATABASES;\\\"\"\n      interval: 2s\n      timeout: 5s\n      retries: 30\n    networks:\n      - gowebapp1\n  gowebapp:\n    build: ./gowebapp\n    container_name: gowebapp\n    ports:\n      - \"8080:80\"\n    depends_on:\n      gowebapp-mysql:\n        condition: service_healthy\n    networks:\n      - gowebapp1\nnetworks:\n  gowebapp1:\n    driver: bridge\n</code></pre> <p>Step 2 Run compose file</p> <pre><code>export CLOUDSDK_PYTHON=python2  # https://github.com/google-github-actions/setup-gcloud/issues/128\n</code></pre> <pre><code>docker-compose up -d\n</code></pre> <p>Step 3  Test the application locally</p> <p>Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Step 4 Tear down environment</p> <pre><code>docker-compose down\n</code></pre> <p>Step 5 Cleanup created networks</p> <pre><code>docker network ls\n</code></pre> <p>Important</p> <p>Make sure <code>gowebapp</code> and <code>gowebapp1</code> networks has been deleted!!!</p>"},{"location":"ycit019_ass2_solution/#32-commit-docker-compose-file-to-repository-and-share-it-with-instructorteacher","title":"3.2 Commit docker-compose file to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands:</p> <pre><code>git add .\ngit commit -m \"adding docker-compose.yml\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"ycit019_ass2_solution/#33-grant-viewing-permissions-for-a-repository-to-instructorsteachers","title":"3.3 Grant viewing permissions for a repository to Instructors/Teachers","text":"<p>Submit link to your Cloud Source Repository to LMS, replace with you values</p> <pre><code>https://source.cloud.google.com/${PROJECT_ID}/$student_name-notepad\n</code></pre> <p>e.g: https://source.cloud.google.com/ycit019_2022-project/ayratk-notepad</p>"},{"location":"ycit019_ass3/","title":"1 Deploy Applications on Kubernetes","text":"<p>Objective:</p> <ul> <li>Review process of creating NameSpaces</li> <li>Review process of changing Context</li> <li>Review process of creating K8s:<ul> <li>Services</li> <li>Labels, Selectors</li> <li>Deployments</li> <li>Rolling Updates</li> </ul> </li> </ul>"},{"location":"ycit019_ass3/#prepare-the-cloud-source-repository-environment-with-module-6-assignment","title":"Prepare the Cloud Source Repository Environment with Module 6 Assignment","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p> <p>Cloud Source Repositories: Qwik Start</p> <p>Step 1 Locate directory where kubernetes <code>YAML</code> manifest going to be stored.</p> <pre><code>cd ~/ycit019_2022/\ngit pull       # Pull latest Mod8_assignment\n</code></pre> <p>In case you don't have this folder clone it as following:</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit019_2022\ncd ~/ycit019_2022/Mod8_assignment/\nls\n</code></pre> <p>Step 2 Go into the local repository you've created:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <p>Important</p> <p>Replace above with your project_id student_name</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <p>Step 3 Copy <code>Mod8_assignment</code> folder to your repo:</p> <pre><code>git pull                              # Pull latest code from you repo\ncp -r ~/ycit019_2022/Mod8_assignment/ .\n</code></pre> <p>Step 4 Commit <code>Mod8_assignment</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"adding `Mod8_assignment` with kubernetes YAML manifest\"\n</code></pre> <p>Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre> <p>Step 6 Review Cloud Source Repositories</p> <p>Use the <code>Google Cloud Source Repositories</code> code browser to view repository files.  You can filter your view to focus on a specific branch, tag, or comment.</p> <p>Browse the Mod8_assignment files you pushed to the repository by opening the Navigation menu and selecting Source Repositories:</p> <pre><code>Click Menu -&gt; Source Repositories &gt; Source Code.\n</code></pre> <p>Result</p> <p>The console shows the files in the master branch at the most recent commit.</p>"},{"location":"ycit019_ass3/#11-development-tools","title":"1.1 Development Tools","text":"<p>Note</p> <p>This steps is optional</p> <p>Step 1 Choose one of the following option to develop the YAML manifests:</p> <p>Option 1: You can develop in Google Cloud Shell Editor</p> <p>Option 2: You can also develop locally on your laptop using VCScode. We recommend to use it in conjunction with VSC YAML extension from Redhat</p> <p>Option 4: Use your preferred text editor on Linux VM (vim, nano).</p> <p>Option 3: Use your preferred text editor on Linux VM (vim, nano).</p>"},{"location":"ycit019_ass3/#21-create-gke-cluster","title":"2.1 Create GKE Cluster","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with two nodes:</p> <pre><code>gcloud container clusters create k8s-concepts \\\n--zone us-central1-c \\\n--num-nodes 2\n</code></pre> <p>Output:</p> <pre><code>NAME          LOCATION       MASTER_VERSION   MASTER_IP      MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\nk8s-concepts  us-central1-c  1.19.9-gke.1400  34.121.222.83  e2-medium     1.19.9-gke.1400  2          RUNNING\n</code></pre> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials k8s-concepts --zone us-central1-c\n</code></pre>"},{"location":"ycit019_ass3/#22-setup-kubectl-autocomplete","title":"2.2 Setup KUBECTL AUTOCOMPLETE","text":"<p>Since we going to use a lot of kubectl cli let's setup autocomplete.</p> <pre><code>source &lt;(kubectl completion bash)\necho \"source &lt;(kubectl completion bash)\" &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"ycit019_ass3/#23-create-dev-namespace-and-make-it-default","title":"2.3 Create 'dev' namespace and make it default.","text":"<p>Step 1 Create 'dev' namespace that's going to be used to develop and deploy <code>Notestaker</code> application on Kubernetes using <code>kubetl</code> CLI.</p> <pre><code>kubectl create ns dev\n</code></pre> <p>Step 2 Use <code>dev</code> context to create K8s resources inside this namespace.</p> <pre><code>kubectl config set-context --current --namespace=dev\n</code></pre> <p>Step 3 Verify current context:</p> <pre><code>kubectl config view | grep namespace\n</code></pre> <p>Result</p> <p><code>dev</code></p>"},{"location":"ycit019_ass3/#24-create-service-object-for-mysql","title":"2.4 Create Service Object for MySQL","text":"<p>Step 1 Locate folder with Kubernetes Manifests:</p> <pre><code>cd ~/$student_name-notepad/Mod8_assignment/deploy\nls\n</code></pre> <p>Output: <code>gowebapp-deployment.yaml  gowebapp-mysql-deployment.yaml  gowebapp-mysql-service.yaml  gowebapp-service.yaml</code></p> <p>Result</p> <p>You can see 4 manifest with <code>Deployment</code> and <code>Service</code> manifests for <code>gowebapp</code> and  <code>gowebapp-mysql</code></p> <p>Step 6 Define a Kubernetes Service object for the backend MySQL database.</p> <p>Follow instructions below to populate  <code>gowebapp-mysql-service.yaml</code></p> <p>Reference K8s Docs: </p> <ol> <li>Service</li> </ol> <p>Additionally, you can use <code>kubectl</code> built-in docs for any type of resources:</p> <pre><code>kubectl explain service\n</code></pre> <pre><code>edit gowebapp-mysql-service.yaml\n</code></pre> <p>Note</p> <p>You can also use VCS or Cloud Code to work with <code>yaml</code> manifest.</p> <pre><code>#TODO: Specify Kubernetes API apiVersion\n#TODO: Identify the kind of Object\nmetadata:\n #TODO: Give the service a name: \"gowebapp-mysql\"\n  labels:\n    #TODO: Add a label KV \"run: gowebapp-mysql\"\nspec:\n  #TODO: leave the clusterIP to None. We allow k8s to assign clusterIP.\n  ports:\n    #TODO: Define a \"port\" as 3306\n    #TODO: Define a \"targetPort\" as 3306\n  #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp-mysql\"\n</code></pre> <p>Step 3 Create a Service object for MySQL</p> <pre><code>kubectl apply -f gowebapp-mysql-service.yaml --record\n</code></pre> <p>Step 4 Check to make sure it worked</p> <pre><code>kubectl get service -l \"run=gowebapp-mysql\"\n</code></pre>"},{"location":"ycit019_ass3/#25-create-deployment-object-for-the-backend-mysql-database","title":"2.5 Create Deployment object for the backend MySQL database","text":"<p>Step 1 Follow instructions below to populate <code>gowebapp-mysql-deployment.yaml</code></p> <p>For reference, please see <code>Deployment</code> doc:</p> <p>Reference K8s Docs: </p> <ol> <li>Deployments</li> <li>Updating Env variables</li> </ol> <pre><code>edit gowebapp-mysql-deployment.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\n#TODO: Identify the type of Object\nmetadata:\n  #TODO: Give the Deployment a name \"gowebapp-mysql\"\n  labels:\n  #TODO: Add a label KV \"run: gowebapp-mysql\"\n  #TODO: give the Deployment a label: tier: backend\nspec:\n  #TODO: Define number of replicas, set it to 1\n  #TODO: Starting from Deplloyment v1 selectors are mandatory\n    #add selector KV \"run: gowebapp-mysql\"\n  strategy:\n    type: # Set strategy type as `Recreate`\n  template:\n    metadata:\n      #TODO: Add a label called \"run\" with the name of the service: \"gowebapp-mysql\"\n    spec:\n      containers:\n      - env:\n         - #TODO: define name as MYSQL_ROOT_PASSWORD \n           #TODO: define value as rootpasswd\n         image: #TODO define mysql image created in previous assignment, located in gcr registry\n         name: gowebapp-mysql\n         ports:\n         #TODO: define containerPort: 3306\n</code></pre> <p>Step 2 Create a Deployment object for MySQL</p> <pre><code>kubectl apply -f gowebapp-mysql-deployment.yaml --record\n</code></pre> <p>Step 3 Check to make sure it worked</p> <pre><code>kubectl get deployment -l \"run=gowebapp-mysql\"\n</code></pre> <p>Step 3 Check <code>mysql</code> pod logs:</p> <p>List mysql Pods and note the name the <code>pod</code>:</p> <pre><code>kubectl get pods -l \"run=gowebapp-mysql\"\n</code></pre> <p>Ensure <code>Mysql</code> is up by looking at <code>pod</code> logs:</p> <pre><code>kubectl logs &lt;Pod_name&gt;\n</code></pre> <p>Result</p> <p>We have created <code>Service</code> and <code>Deployment</code> for backend application.</p>"},{"location":"ycit019_ass3/#25-create-a-k8s-service-for-the-frontend-gowebapp","title":"2.5 Create a K8s Service for the frontend gowebapp.","text":"<p>Step 1 Follow instructions below to populate <code>gowebapp-service.yaml</code></p> <pre><code>edit gowebapp-service.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: gowebapp\n  labels:\n    #TODO: give the Service a label: run: gowebapp\n    #TODO: give the Service a label: tier: frontend\nspec:\n  #TODO: Define a \"ports\" array with the \"port\" attribute: 9000 and \"targetPort\" attributes: 80\n  #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp\"\n  #TODO: Add a Service Type of LoadBalancer\n  #If you need help, see reference: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types\n</code></pre> <p>Step 2 Create a Service object for gowebapp</p> <pre><code>kubectl apply -f gowebapp-service.yaml --record\n</code></pre> <p>Step 3 Check to make sure it worked</p> <pre><code>kubectl get service -l \"run=gowebapp\"\n</code></pre>"},{"location":"ycit019_ass3/#26-create-a-k8s-deployment-object-for-the-frontend-gowebapp","title":"2.6 Create a K8s Deployment object for the frontend gowebapp","text":"<p>Step 1 Follow instructions below to populate <code>gowebapp-deployment.yaml</code></p> <p>Reference K8s Docs: </p> <ol> <li>Deployments</li> <li>Updating Env variables</li> </ol> <pre><code>edit gowebapp-deployment.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\n#TODO: define the kind of object as Deployment\nmetadata:\n  #TODO: Add a name attribute for the service as \"gowebapp\"\n  labels:\n    #TODO: give the Deployment a label: run: gowebapp\n    #TODO: give the Deployment a label: tier: frontend\nspec:\n  #TODO: Define number of replicas, set it to 2\n  #TODO: add selector KV \"run: gowebapp\"\n  template:\n    metadata:\n      labels:\n        run: gowebapp\n        tier: frontend\n    spec:\n      containers:\n      - env:\n        - #TODO: define name as MYSQL_ROOT_PASSWORD\n          #TODO: define value as rootpasswd\n        image: #TODO define gowebapp image created in previous assignment, located in gcr registry\n        name: gowebapp\n        ports:\n        - #TODO: define the container port as 80\n</code></pre> <p>Step 2 Create a Deployment object for gowebapp</p> <pre><code>kubectl apply -f gowebapp-deployment.yaml --record\n</code></pre> <p>Step 3 Check to make sure it worked</p> <pre><code>kubectl get deployment -l \"run=gowebapp\"\n</code></pre> <p>Step 4 Access your application on Public IP via automatically created Loadbalancer  created for <code>gowebapp</code> service.</p> <p>To get the value of <code>Loadbalancer</code> run following command:</p> <pre><code>kubectl get svc gowebapp -o wide\n</code></pre> <p>Expected output:</p> <pre><code>NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\ngowebapp         Loadbalancer 10.107.15.39  XXXXXX        9000:32634/TCP   30m\ngowebapp-mysql   ClusterIP   None           &lt;none&gt;        3306/TCP         1h\n</code></pre> <p>Step 5 Access <code>Loadbalancer</code> IP via browser:</p> <p>Result</p> <p>Congrats!!! You've deployed you code to Kubernetes</p>"},{"location":"ycit019_ass3/#27-fix-gowebapp-code-bugs-and-build-a-new-image","title":"2.7 Fix gowebapp code bugs and build a new image.","text":"<p>Task: As you've noticed <code>gowebapp</code> frontend app has <code>YCIT019</code> logo in it. Since you may want to use application for you personal needs, let's change <code>YCIT019</code> logo to you <code>Name</code>.</p> <p>Step 1 Modify gowebapp frontend so that it has name of you company and link to company web page e.g.</p> <pre><code>edit ~/$student_name-notepad/Mod8_assignment/gowebapp/code/template/partial/footer.tmpl\n</code></pre> <p>Step 2 Build a new version of Image</p> <pre><code>cd ~/$student_name-notepad/Mod8_assignment/gowebapp\ndocker build -t gcr.io/${PROJECT_ID}/gowebapp:v2 .\ndocker push gcr.io/${PROJECT_ID}/gowebapp:v2 .\n</code></pre>"},{"location":"ycit019_ass3/#27-rolling-upgrade","title":"2.7 Rolling Upgrade","text":"<p>For <code>gowebapp</code> frontend deployment manifest we've not specified any upgrade strategy type. It means application will use default Upgrade strategy called <code>RollingUpdate</code>.</p> <p><code>RollingUpdate</code> strategy - updates Pods in a rolling update fashion.</p> <p><code>maxUnavailable</code> - is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. By default, it ensures that at least 25% less than the desired number of Pods are up (25% max unavailable).</p> <p><code>Max Surge</code> - is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. By default, it ensures that at most 25% more than the desired number of Pods are up (25% max surge).</p> <p>Step 1 Locate directory with manifest</p> <pre><code>cd  ~/$student_name-notepad/Mod8_assignment/deploy\n</code></pre> <p>Step 2 Trigger rolling upgrade using <code>kubectl set</code> command</p> <pre><code>#TO DO\n</code></pre> <p>Step 3 Verify rollout history</p> <pre><code>#TO DO\n</code></pre> <p>Step 4 Perform Rollback to v1</p> <pre><code>#TO DO\n</code></pre>"},{"location":"ycit019_ass3/#28-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","title":"2.8 Commit K8s manifests to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>cd ~/$student_name-notepad/\n</code></pre> <pre><code>git add .\ngit commit -m \"k8s manifests\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"ycit019_ass3/#29-cleaning-up","title":"2.9 Cleaning Up","text":"<p>Step 1 Delete the cluster</p> <pre><code>gcloud container clusters delete k8s-concepts\n</code></pre>"},{"location":"ycit019_ass3_solution/","title":"1 Deploy Applications on Kubernetes","text":"<p>Objective:</p> <ul> <li>Review process of creating NameSpaces</li> <li>Review process of changing Context</li> <li>Review process of creating K8s:<ul> <li>Services</li> <li>Labels, Selectors</li> <li>Deployments</li> <li>Rolling Updates</li> </ul> </li> </ul>"},{"location":"ycit019_ass3_solution/#11-development-tools","title":"1.1 Development Tools","text":"<p>Note</p> <p>This steps is optional</p> <p>Step 1 Choose one of the following option to develop the YAML manifests:</p> <p>Option 1: You can develop in Google Cloud Shell Editor</p> <p>Option 2: You can also develop locally on your laptop using VCScode. We recommend to use it in conjunction with VSC YAML extension from Redhat</p> <p>Option 3: Use your preferred text editor on Linux VM (vim, nano).</p>"},{"location":"ycit019_ass3_solution/#21-create-gke-cluster","title":"2.1 Create GKE Cluster","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with two nodes:</p> <pre><code>gcloud container clusters create k8s-concepts \\\n--zone us-central1-c \\\n--num-nodes 2\n</code></pre> <p>Output:</p> <pre><code>NAME          LOCATION       MASTER_VERSION   MASTER_IP      MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\nk8s-concepts  us-central1-c  1.19.9-gke.1400  34.121.222.83  e2-medium     1.19.9-gke.1400  2          RUNNING\n</code></pre> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials k8s-concepts --zone us-central1-c\n</code></pre>"},{"location":"ycit019_ass3_solution/#22-setup-kubectl-autocomplete","title":"2.2 Setup KUBECTL AUTOCOMPLETE","text":"<p>Since we going to use a lot of kubectl cli let's setup autocomplete.</p> <pre><code>source &lt;(kubectl completion bash)\necho \"source &lt;(kubectl completion bash)\" &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"ycit019_ass3_solution/#23-create-dev-namespace-and-make-it-default","title":"2.3 Create 'dev' namespace and make it default.","text":"<p>Step 1 Create 'dev' namespace that's going to be used to develop and deploy <code>Notestaker</code> application on Kubernetes using <code>kubetl</code> CLI.</p> <pre><code>kubectl create ns dev\n</code></pre> <p>Step 2 Use <code>dev</code> context to create K8s resources inside this namespace.</p> <pre><code>kubectl config set-context --current --namespace=dev\n</code></pre> <p>Step 3 Verify current context:</p> <pre><code>kubectl config view | grep namespace\n</code></pre> <p>Result</p> <p><code>dev</code></p>"},{"location":"ycit019_ass3_solution/#24-create-service-object-for-mysql","title":"2.4 Create Service Object for MySQL","text":"<p>Step 1 Locate directory where kubernetes manifests going to be stored.</p> <pre><code>cd ~/ycit019/\ngit pull       # Pull latest assignement3\n</code></pre> <p>In case you don't have this folder clone it as following:</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit019\ncd ~/ycit019/Assignment3/\nls\n</code></pre> <p>Step 2 Go into the local repository you've created:</p> <pre><code>cd ~/$MY_REPO\n</code></pre> <p>Step 3 Copy Assignment 3 <code>deploy</code> folder to your repo:</p> <pre><code>git pull                              # Pull latest code from you repo\ncp -r ~/ycit019/Assignment3/deploy .\n</code></pre> <p>Step 4 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"adding K8s manifests in deploy folder\"\n</code></pre> <p>Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre> <p>Step 6 Define a Kubernetes Service object for the backend MySQL database.</p> <pre><code>cd ~/$MY_REPO/deploy\n</code></pre> <p>Follow instructions below to populate  <code>gowebapp-mysql-service.yaml</code></p> <p>For reference, please see <code>Service</code> docs:</p> <pre><code>https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service\n</code></pre> <p>Additionally, you can use <code>kubectl</code> built-in docs for any type of resources:</p> <pre><code>kubectl explain service\n</code></pre> <pre><code>vim gowebapp-mysql-service.yaml\n</code></pre> <p>Note</p> <p>You can also use VCS or Cloud Code to work with <code>yaml</code> manifest.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: gowebapp-mysql\n  labels:\n    run: gowebapp-mysql\nspec:\n  clusterIP: None\n  ports:\n  - port: 3306\n    targetPort: 3306\n  selector:\n    run: gowebapp-mysql\n</code></pre> <p>Step 3 Create a Service object for MySQL</p> <pre><code>kubectl apply -f gowebapp-mysql-service.yaml --record\n</code></pre> <p>Step 4 Check to make sure it worked</p> <pre><code>kubectl get service -l \"run=gowebapp-mysql\"\n</code></pre>"},{"location":"ycit019_ass3_solution/#25-create-deployment-object-for-the-backend-mysql-database","title":"2.5 Create Deployment object for the backend MySQL database","text":"<p>Step 1 Follow instructions below to populate <code>gowebapp-mysql-deployment.yaml</code></p> <p>For reference, please see <code>Deployment</code> doc:</p> <pre><code>https://kubernetes.io/docs/concepts/workloads/controllers/deployment\n</code></pre> <pre><code>vim gowebapp-mysql-deployment.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gowebapp-mysql\n  labels:\n    run: gowebapp-mysql\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: gowebapp-mysql\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        run: gowebapp-mysql\n    spec:\n      containers:\n      - env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: rootpasswd\n        image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1\n        name: gowebapp-mysql\n        ports:\n        - containerPort: 3306\n</code></pre> <p>Step 2 Create a Deployment object for MySQL</p> <pre><code>kubectl apply -f gowebapp-mysql-deployment.yaml --record\n</code></pre> <p>Step 3 Check to make sure it worked</p> <pre><code>kubectl get deployment -l \"run=gowebapp-mysql\"\n</code></pre> <p>Step 3 Check <code>mysql</code> pod logs:</p> <p>List mysql Pods and note the name the <code>pod</code>:</p> <pre><code>kubectl get pods -l \"run=gowebapp-mysql\"\n</code></pre> <p>Ensure <code>Mysql</code> is up by looking at <code>pod</code> logs:</p> <pre><code>kubectl logs &lt;Pod_name&gt;\n</code></pre> <p>Result</p> <p>We have created <code>Service</code> and <code>Deployment</code> for backend application.</p>"},{"location":"ycit019_ass3_solution/#25-create-a-k8s-service-for-the-frontend-gowebapp","title":"2.5 Create a K8s Service for the frontend gowebapp.","text":"<p>Step 1 Follow instructions below to populate <code>gowebapp-service.yaml</code></p> <pre><code>vim gowebapp-service.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: gowebapp\n  labels:\n    run: gowebapp\nspec:\n  ports:\n  - port: 9000\n    targetPort: 80\n  selector:\n    run: gowebapp\n  type: LoadBalancer\n</code></pre> <p>Step 2 Create a Service object for gowebapp</p> <pre><code>kubectl apply -f gowebapp-service.yaml --record\n</code></pre> <p>Step 3 Check to make sure it worked</p> <pre><code>kubectl get service -l \"run=gowebapp\"\n</code></pre>"},{"location":"ycit019_ass3_solution/#26-create-a-k8s-deployment-object-for-the-frontend-gowebapp","title":"2.6 Create a K8s Deployment object for the frontend gowebapp","text":"<p>Step 1 Follow instructions below to populate <code>gowebapp-deployment.yaml</code></p> <pre><code>vim gowebapp-deployment.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gowebapp\n  labels:\n    run: gowebapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: gowebapp\n  template:\n    metadata:\n      labels:\n        run: gowebapp\n    spec:\n      containers:\n      - env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: rootpasswd\n        image: gcr.io/${PROJECT_ID}/gowebapp:v1\n        name: gowebapp\n        ports:\n        - containerPort: 80\n</code></pre> <p>Step 2 Create a Deployment object for gowebapp</p> <pre><code>kubectl apply -f gowebapp-deployment.yaml --record\n</code></pre> <p>Step 3 Check to make sure it worked</p> <pre><code>kubectl get deployment -l \"run=gowebapp\"\n</code></pre> <p>Step 4 Access your application on Public IP via automatically created Loadbalancer  created for <code>gowebapp</code> service.</p> <p>To get the value of <code>Loadbalancer</code> run following command:</p> <pre><code>kubectl get svc gowebapp -o wide\n</code></pre> <p>Expected output:</p> <pre><code>NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\ngowebapp         Loadbalancer 10.107.15.39  XXXXXX        9000:32634/TCP   30m\ngowebapp-mysql   ClusterIP   None           &lt;none&gt;        3306/TCP         1h\n</code></pre> <p>Step 5 Access <code>Loadbalancer</code> IP via browser:</p> <p>Result</p> <p>Congrats!!! You've deployed you code to Kubernetes</p>"},{"location":"ycit019_ass3_solution/#27-fix-gowebapp-code-bugs-and-build-a-new-image","title":"2.7 Fix gowebapp code bugs and build a new image.","text":"<p>Task: As you've noticed <code>gowebapp</code> frontend app has <code>YCIT019</code> logo in it. Since you may want to use application for you personal needs, let's change <code>YCIT019</code> logo to you <code>Name</code>.</p> <p>Step 1 Modify gowebapp frontend so that it has name of you company and link to company web page e.g.</p> <pre><code>vim ~/$MY_REPO/gowebapp/code/template/partial/footer.tmpl\n</code></pre> <p>Step 2 Build a new version of Image</p> <pre><code>cd ~/$MY_REPO/gowebapp\ndocker build -t gcr.io/${PROJECT_ID}/gowebapp:v2 .\ndocker push gcr.io/${PROJECT_ID}/gowebapp:v2 .\n</code></pre>"},{"location":"ycit019_ass3_solution/#27-rolling-upgrade","title":"2.7 Rolling Upgrade","text":"<p>For <code>gowebapp</code> frontend deployment manifest we've not specified any upgrade strategy type. It means application will use default Upgrade strategy called <code>RollingUpdate</code>.</p> <p><code>RollingUpdate</code> strategy - updates Pods in a rolling update fashion.</p> <p><code>maxUnavailable</code> - is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. By default, it ensures that at least 25% less than the desired number of Pods are up (25% max unavailable).</p> <p><code>Max Surge</code> - is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. By default, it ensures that at most 25% more than the desired number of Pods are up (25% max surge).</p> <p>Step 1 Locate directory with manifest</p> <pre><code>cd  ~/$MY_REPO/deploy\n</code></pre> <p>Step 2 Trigger rolling upgrade using <code>kubectl set</code> command</p> <pre><code>kubectl set image deployments/gowebapp gowebapp=gcr.io/${PROJECT_ID}/gowebapp:v2\n</code></pre> <p>Step 3 Verify rollout history</p> <pre><code>kubectl rollout history deployment/gowebapp\n</code></pre> <p>Step 4 Perform Rollback to v1</p> <pre><code>kubectl rollout undo deployment/gowebapp --to-revision=1\n</code></pre>"},{"location":"ycit019_ass3_solution/#28-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","title":"2.8 Commit K8s manifests to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git add .\ngit commit -m \"k8s manifests\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"ycit019_ass3_solution/#29-cleaning-up","title":"2.9 Cleaning Up","text":"<p>Step 1 Delete the cluster</p> <pre><code>gcloud container clusters delete k8s-concepts\n</code></pre>"},{"location":"ycit019_ass4_2022/","title":"Deploy Applications on Kubernetes","text":"<p>Objective:</p> <ul> <li>Review process of creating K8s:<ul> <li>Liveness Probes</li> <li>Readiness Probes</li> <li>Secrets</li> <li>ConfigMaps</li> <li>Requests</li> <li>Limits</li> <li>HPA</li> <li>VPA</li> </ul> </li> </ul>"},{"location":"ycit019_ass4_2022/#prepare-the-cloud-source-repository-environment-with-module-6-assignment","title":"Prepare the Cloud Source Repository Environment with Module 6 Assignment","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p> <p>Cloud Source Repositories: Qwick Start</p> <p>Step 1 Locate directory where kubernetes <code>YAML</code> manifest going to be stored.</p> <pre><code>cd ~/ycit019_2022/\ngit pull       # Pull latest Mod9_assignment\n</code></pre> <p>In case you don't have this folder clone it as following:</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit019_2022\ncd ~/ycit019_2022/Mod9_assignment/\nls\n</code></pre> <p>Step 2 Go into the local repository you've created:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <p>Important</p> <p>Replace above with your project_id student_name</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <p>Step 3 Copy <code>Mod9_assignment</code> folder to your repo:</p> <pre><code>git pull                              # Pull latest code from you repo\ncp -r ~/ycit019_2022/Mod9_assignment/ .\n</code></pre> <p>Step 4 Commit <code>Mod9_assignment</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"adding `Mod9_assignment` with kubernetes YAML manifest\"\n</code></pre> <p>Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre> <p>Step 6 Review Cloud Source Repositories</p> <p>Use the <code>Google Cloud Source Repositories</code> code browser to view repository files.  You can filter your view to focus on a specific branch, tag, or comment.</p> <p>Browse the Mod9_assignment files you pushed to the repository by opening the Navigation menu and selecting Source Repositories:</p> <pre><code>Click Menu -&gt; Source Repositories &gt; Source Code.\n</code></pre> <p>Result</p> <p>The console shows the files in the master branch at the most recent commit.</p>"},{"location":"ycit019_ass4_2022/#11-create-gke-cluster-with-cluster-and-vertical-autoscaling-support","title":"1.1 Create GKE Cluster with Cluster and Vertical Autoscaling Support","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with two nodes:</p> <pre><code>gcloud container clusters create k8s-features \\\n--zone us-central1-c \\\n--enable-vertical-pod-autoscaling \\\n--num-nodes 2 \\\n--enable-autoscaling --min-nodes 1 --max-nodes 3\n</code></pre> <p>Output:</p> <pre><code>NAME          LOCATION       MASTER_VERSION   MASTER_IP      MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\nk8s-features  us-central1-c  1.19.9-gke.1400  34.121.222.83  e2-medium     1.19.9-gke.1400  2          RUNNING\n</code></pre> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials k8s-features --zone us-central1-c\n</code></pre>"},{"location":"ycit019_ass4_2022/#21-externalize-web-application-configuration","title":"2.1 Externalize Web Application Configuration","text":"<p>Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time.</p> <p>Step 1: Move config file outside compiled application</p> <p>First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path</p> <pre><code>cd ~\nmkdir ~/$student_name-notepad/Mod9_assignment/gowebapp/config\n</code></pre> <pre><code>cp ~/$student_name-notepad/Mod9_assignment/gowebapp/code/config/config.json \\\n~/$student_name-notepad/Mod9_assignment/gowebapp/config\n</code></pre> <p>Remove <code>config</code> folder that is located in <code>code</code> directory</p> <pre><code>rm -rf ~/$student_name-notepad/Mod9_assignment/gowebapp/code/config\n</code></pre> <pre><code>ls ~/$student_name-notepad/Mod9_assignment/gowebapp\n</code></pre> <p>Result</p> <p>Your <code>gowebapp</code> folder should look like following:</p> <pre><code>code  config  Dockerfile\n</code></pre> <p>Step 2: Modify app to support setting DB password through environment variable Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time.</p> <p>Use a text editor of your choice (edit, VS code) to modify:</p> <pre><code>edit ~/$student_name-notepad/Mod9_assignment/gowebapp/code/vendor/app/shared/database/database.go\n</code></pre> <ul> <li>Add an import for the <code>\"os\"</code> package at line 8. After making this change, your imports list will look like the following:</li> </ul> <pre><code>import (\n  \"encoding/json\"\n  \"fmt\"\n  \"log\"\n  \"time\"\n  \"os\"\n\n  \"github.com/boltdb/bolt\"\n  _ \"github.com/go-sql-driver/mysql\" // MySQL driver\n  \"github.com/jmoiron/sqlx\"\n  \"gopkg.in/mgo.v2\"\n)\n</code></pre> <ul> <li>Add the following code at line 89 after <code>var err error</code> :</li> </ul> <pre><code>// Check for MySQL Password environment variable and update configuration if present\nif os.Getenv(\"DB_PASSWORD\") != \"\" {\n    d.MySQL.Password = os.Getenv(\"DB_PASSWORD\")\n}\n</code></pre>"},{"location":"ycit019_ass4_2022/#22-build-new-docker-image-for-your-frontend-application","title":"2.2 Build new Docker image for your frontend application","text":"<p>Step 1 Set the Project ID in  Environment Variable: </p> <pre><code>export PROJECT_ID=&lt;project_id&gt;\n</code></pre> <p>Step 2: Update <code>Dockerfile</code> for your <code>gowebapp</code> frontend application and  define environment variable declaration for a default DB_PASSWORD as well as add a volume declaration for the container configuration path </p> <pre><code>cd ~/$student_name-notepad/Mod9_assignment/gowebapp\nedit Dockerfile\n</code></pre> <pre><code>FROM golang:1.16.4\n\nLABEL maintainer \"student@mcgill.ca\"\nLABEL gowebapp \"v1\"\n\nEXPOSE 80\n\nENV GO111MODULE=auto\nENV GOPATH=/go\nENV PASSWORD=rootpasswd\n\nCOPY /code $GOPATH/src/gowebapp/\n\nWORKDIR $GOPATH/src/gowebapp/\n\nRUN go get &amp;&amp; go install\n\nVOLUME $GOPATH/src/gowebapp/config\n\nENTRYPOINT $GOPATH/bin/gowebapp\n</code></pre> <p>Step 2: Build updated <code>gowebapp</code> Docker image locally</p> <pre><code>cd ~/$student_name-notepad/Mod9_assignment/gowebapp\n</code></pre> <p>Build and push the <code>gowebapp</code> image to GCR. Make sure to include \u201c.\u201c at the end of build command. </p> <pre><code>docker build -t gcr.io/${PROJECT_ID}/gowebapp:v3 .\ndocker push gcr.io/${PROJECT_ID}/gowebapp:v3\n</code></pre>"},{"location":"ycit019_ass4_2022/#23-run-and-test-new-docker-image-locally","title":"2.3 Run and test new Docker image locally","text":"<p>Before deploying to Kubernetes, let\u2019s test the updated <code>gowebapp</code> Docker image locally, to ensure that the frontend and backend containers run and integrate properly.</p> <p>Step 1: Launch frontend and backend containers</p> <p>First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it.</p> <p>Note</p> <p>Update user-name with command below with you docker-hub id</p> <pre><code>docker network create gowebapp -d bridge\n</code></pre> <pre><code>docker run --net gowebapp --name gowebapp-mysql --hostname \\\ngowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1\n</code></pre> <p>Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password:</p> <p>Note</p> <p>Update user-name with command below with you docker-hub id</p> <pre><code>docker run -p 8080:80 \\\n-v ~/$student_name-notepad/Mod9_assignment/gowebapp/config:/go/src/gowebapp/config \\\n--net gowebapp -d --name gowebapp \\\n--hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v3\n</code></pre> <p>Step 3  Test the application locally</p> <p>Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Step 4 Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container.</p> <p>Result</p> <p>By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes</p> <p>Step 5 Cleanup environment</p> <pre><code>docker rm -f $(docker ps -q)\ndocker network rm gowebapp\n</code></pre>"},{"location":"ycit019_ass4_2022/#31-create-a-secret","title":"3.1 Create a Secret","text":"<p>Step 1 <code>Base64</code> Encode MySQL password <code>rootpasswd</code>. See Lab 8 for more details.</p> <pre><code>echo -n \"rootpasswd\" | base64\n</code></pre> <p>Result</p> <p>MySQL password has been <code>Base64</code> Encoded</p> <p>Step 2 Edit a secret for the MySQL password</p> <pre><code>cd ~/$student_name-notepad/Mod9_assignment/deploy\nedit secret-mysql.yaml\n</code></pre> <pre><code>kind: Secret\napiVersion: v1\n# TODO1 Create secret name: mysql\n# TODO2 Secret should be using arbitrary user defined type stringData: https://kubernetes.io/docs/concepts/configuration/secret/#secret-types\n# TODO3 Define Mysql Password in base64 encoded format\n</code></pre> <pre><code>kubectl apply -f secret-mysql.yaml\nkubectl describe secret mysql\n</code></pre> <p>Step 2  Update <code>gowebapp-mysql-deployment.yaml</code> under <code>~/$student_name-notepad/Mod9_assignment/deploy</code></p> <pre><code>edit ~/$student_name-notepad/Mod9_assignment/deploy/gowebapp-mysql-deployment.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gowebapp-mysql\n  labels:\n    run: gowebapp-mysql\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: gowebapp-mysql\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        run: gowebapp-mysql\n    spec:\n      containers:\n      - env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n          #TODO: replace value: rootpasswd with secretKeyRef\n          #TODO: name is mysql\n          #TODO: key is password\n        image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1\n        name: gowebapp-mysql\n        ports:\n        - containerPort: 3306\n        #TODO add a livenessProbe which performs tcpSocket probe\n        #aginst port 3306 with an initial\n        # deay of 30 seconds, and a timeout of 2 seconds\n        #TODO add a readinessProbe for tcpSocket port 3306 with a 25 second \n        #initial delay, and a timeout of 2 seconds     \n</code></pre> <p>Step 2  Start the rolling upgrade and record the command used in the rollout history:</p> <pre><code>kubectl apply -f gowebapp-mysql-deployment.yaml --record\n</code></pre> <p>Step 3 Verify that rollout was successful</p> <pre><code>kubectl rollout status deploy gowebapp-mysql\n</code></pre> <p>Step 4 Check if pods are running</p> <pre><code>kubectl get pods\n</code></pre> <p>Step 5 Create a Service object for MySQL</p> <pre><code>kubectl apply -f gowebapp-mysql-service.yaml --record\n</code></pre> <p>Step 6 Check to make sure it worked</p> <pre><code>kubectl get service -l \"run=gowebapp-mysql\"\n</code></pre>"},{"location":"ycit019_ass4_2022/#32-create-configmap-and-probes-for-gowebapp","title":"3.2 Create ConfigMap and Probes for <code>gowebapp</code>","text":"<p>Step 1: Create ConfigMap for gowebapp's config.json file</p> <pre><code>cd ~/$student_name-notepad/Mod9_assignment/gowebapp/config/\nkubectl create configmap gowebapp --from-file=webapp-config-json=config.json\nkubectl describe configmap gowebapp\n</code></pre> <p>Note</p> <p>The entire file contents from config.json are stored under the key <code>webapp-config-json</code></p>"},{"location":"ycit019_ass4_2022/#34-deploy-webapp-by-referencing-secret-configmap-and-define-probes","title":"3.4 Deploy <code>webapp</code> by Referencing Secret, ConfigMap and define Probes","text":"<p>Step 1: Update <code>gowebapp-deployment.yaml</code> under <code>~/$student_name-notepad/Mod9_assignment/deploy/</code></p> <pre><code>cd ~/$student_name-notepad/Mod9_assignment/deploy/\nedit gowebapp-deployment.yaml\n</code></pre> <p>In this exercise, we will add liveness/readiness probes to our deployments. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gowebapp\n  labels:\n    run: gowebapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: gowebapp\n  template:\n    metadata:\n      labels:\n        run: gowebapp\n    spec:\n      containers:\n      - env:\n      name: DB_PASSWORD\n          #TODO: replace value: `rootpasswd` with valueFrom:\n            secretKeyRef:\n              #TODO: name is mysql\n              #TODO: key is password\n        image: gcr.io/${PROJECT_ID}/gowebapp:v3\n        name: gowebapp\n        ports:\n          - containerPort: 80\n        livenessProbe:\n        #TODO add a livenessProbe which performs httpGet\n        #aginst  the /register endpoint on port 80 with an initial\n        # deay of 15 seconds, and a timeout of 5 seconds\n        #TODO add a livenessProbe which performs httpGet \n        # aginst the /register endpoint on port 80 with an initial\n        # deay of 25 seconds, and a timeout of 5 seconds\n        volumeMounts:\n          - #TODO: give the volume a name:config-volume\n            #TODO: specify the mountPath: /go/src/gowebapp/config\n      volumes: \n      - #TODO: define volume name: config-volume\n        configMap:\n          #TODO: identify your ConfigMap name: gowebapp\n          items:\n          - key: webapp-config-json\n            path: config.json\n</code></pre> <pre><code>kubectl apply -f gowebapp-deployment.yaml --record\n</code></pre> <p>Result</p> <p>This will start the rolling upgrade and record the command used in the rollout history</p> <p>Step 3: Verify that rollout was successful</p> <pre><code>kubectl rollout status deploy gowebapp\n</code></pre> <p>Step 4: Get rollout history</p> <pre><code>kubectl rollout history deploy gowebapp\n</code></pre> <p>Step 5: Get rollout history details for specific revision (use number show in output to previous command)</p> <pre><code>kubectl rollout history deploy gowebapp --revision=&lt;latest_version_number\n</code></pre> <p>Step 6 Check if pods are running</p> <pre><code>kubectl get pods\n</code></pre> <p>Step 7 Create a Service object for gowebapp</p> <pre><code>kubectl apply -f gowebapp-service.yaml --record\n</code></pre> <p>Step 8 Access your application on Public IP via automatically created Loadbalancer  created for <code>gowebapp</code> service.</p> <p>To get the value of <code>Loadbalancer</code> run following command:</p> <pre><code>kubectl get svc gowebapp -o wide\n</code></pre> <p>Expected output:</p> <pre><code>NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\ngowebapp         Loadbalancer 10.107.15.39  XXXXXX        9000:32634/TCP   30m\ngowebapp-mysql   ClusterIP   None           &lt;none&gt;        3306/TCP         1h\n</code></pre> <p>**Step ** Access <code>Loadbalancer</code> IP via browser:</p> <p>Result</p> <p>Congrats!!! You've deployed you app to Kubernetes with Secrets, Configmaps and Probes.</p>"},{"location":"ycit019_ass4_2022/#41-configure-vpa-for-gowebapp-mysql-to-find-optimal-resource-request-and-limits-values","title":"4.1 Configure VPA for <code>gowebapp-mysql</code> to find optimal Resource Request and Limits values","text":"<p><code>requests</code> and <code>limits</code> is the way Kubernetes set's QoS for Pods, as well as enable's features like HPA, CA, Resource Quota's and more.</p> <p>However setting best values for resource <code>requests</code> and <code>limits</code> is hard, VPA is here to help. Set VPA for <code>gowebapp</code> and observe usage recommendation for <code>requests</code> and <code>limits</code></p> <p>Step 1 Check our <code>gowebapp-mysql</code> app: </p> <pre><code>kubectl get deploy\n</code></pre> <p>Result</p> <p>Our Deployment is up, however without <code>request</code> and <code>limits</code> it will be treated as Best Effort QoS resource on the Cluster.</p> <p>Step 2 Edit a  manifest for <code>gowebapp-mysql</code> Vertical Pod Autoscaler resource:</p> <pre><code>cd ~/$student_name-notepad/Mod9_assignment/deploy\nedit gowebapp-mysql-vpa.yaml\n</code></pre> <pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: gowebapp-mysql\nspec:\n  targetRef:\n    apiVersion: \"apps/v1\"\n    kind: Deployment\n    name: gowebapp-mysql\n  updatePolicy:\n    updateMode: \"Off\"\n</code></pre> <p>Step 3 Apply the manifest for <code>gowebapp-mysql-vpa</code></p> <pre><code>kubectl apply -f gowebapp-mysql-vpa.yaml\n</code></pre> <p>Step 4 Wait a minute, and then view the VerticalPodAutoscaler</p> <pre><code>kubectl describe vpa gowebapp-mysql\n</code></pre> <p>Note</p> <p>If you don't see it, wait a little longer and try the previous command again. </p> <p>Step 5 Locate the \"Container Recommendations\" at the end of the output from the <code>describe</code> command.</p> <p>Result</p> <p>We will be using <code>Lower Bound</code> values to set our <code>request</code> value and <code>Upper Bound</code> as our <code>limits</code> value.</p>"},{"location":"ycit019_ass4_2022/#42-set-recommended-request-and-limits-values-to-gowebapp-mysql","title":"4.2 Set Recommended Request and Limits values to <code>gowebapp-mysql</code>","text":"<p>Step 1 Edit a manifest for <code>gowebapp</code> deployment resource:</p> <pre><code>cd ~/$student_name-notepad/Mod9_assignment/deploy\nedit gowebapp-mysql-deployment.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gowebapp-mysql\n...\n        #resources:\n        #TODO define a resource request and limits based on VPA Recommender\n</code></pre> <p>Step 2 Redeploy our application with defined resource <code>request</code> and <code>limits</code></p> <pre><code>cd ~/$student_name-notepad/Mod9_assignment/deploy/\nkubectl delete -f gowebapp-mysql-deployment.yaml\nkubectl apply -f gowebapp-mysql-deployment.yaml\n</code></pre>"},{"location":"ycit019_ass4_2022/#43-configure-vpa-for-gowebapp-to-find-optimal-resource-request-and-limits-values","title":"4.3 Configure VPA for <code>gowebapp</code> to find optimal Resource Request and Limits values","text":"<p>Step 1: Create ConfigMap for gowebapp's config.json file</p> <p>Step 2 Deploy <code>gowebapp</code> app under <code>~/$student_name-notepad/Mod9_assignment/deploy/</code></p> <pre><code>cd ~/$student_name-notepad/Mod9_assignment/deploy/\nkubectl apply -f gowebapp-service.yaml    #Create Service\nkubectl apply -f gowebapp-deployment.yaml #Create Deployment\n</code></pre> <pre><code>kubectl get deploy\n</code></pre> <p>Result</p> <p>Our Deployment is up, however without <code>request</code> and <code>limits</code> it will be treated as Best Effort QoS resource on the Cluster.</p> <p>Step 3 Edit a  manifest for <code>gowebapp</code> Vertical Pod Autoscaler resource:</p> <pre><code>cd ~/$student_name-notepad/Mod9_assignment/deploy\nedit gowebapp-vpa.yaml\n</code></pre> <pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: gowebapp\n#TODO: Ref: https://cloud.google.com/kubernetes-engine/docs/how-to/vertical-pod-autoscaling\n#TODO: Configure VPA with updateMode:OFF\n</code></pre> <p>Step 4 Apply the manifest for <code>gowebapp-vpa</code></p> <pre><code>kubectl apply -f gowebapp-vpa.yaml\n</code></pre> <p>Step 5 Wait a minute, and then view the VerticalPodAutoscaler</p> <pre><code>kubectl describe vpa gowebapp\n</code></pre> <p>Note</p> <p>If you don't see it, wait a little longer and try the previous command again. </p> <p>Step 6 Locate the \"Container Recommendations\" at the end of the output from the <code>describe</code> command.</p> <p>Result</p> <p>We will be using <code>Lower Bound</code> values to set our <code>request</code> value and <code>Upper Bound</code> as our <code>limits</code> value.</p>"},{"location":"ycit019_ass4_2022/#44-set-recommended-request-and-limits-values-to-gowebapp","title":"4.4 Set Recommended Request and Limits values to <code>gowebapp</code>","text":"<p>Step 1 Edit a manifest for <code>gowebapp</code> deployment resource:</p> <pre><code>cd ~/$student_name-notepad/Mod9_assignment/deploy\nedit gowebapp-deployment.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gowebapp\n  labels:\n    run: gowebapp\nspec:\n.....\n        resources:\n        #TODO define a resource request and limits based on VPA Recommender\n</code></pre> <p>Step 2 Redeploy our application with defined resource <code>request</code> and <code>limits</code></p> <pre><code>cd ~/$student_name-notepad/Mod9_assignment/deploy/\nkubectl delete -f gowebapp-deployment.yaml\nkubectl apply -f gowebapp-deployment.yaml\n</code></pre>"},{"location":"ycit019_ass4_2022/#45-configure-hpa-for-gowebapp","title":"4.5 Configure HPA for <code>gowebapp</code>","text":"<p>Our NotePad Application is going to Production soon. To make sure our application can scale based on requests we will set HPA for our deployment resource using Horizontal Pod Autoscaler.</p> <p>Step 1 Create HPA for <code>gowebapp</code> based on CPU with minReplicas <code>1</code> and maxReplicas <code>5</code> with target 50.</p> <pre><code>cd ~/$student_name-notepad/Mod9_assignment/deploy\ncat gowebapp-hpa.yaml\n</code></pre> <p>Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/</p> <pre><code>apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: gowebapp-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: gowebapp\n  minReplicas: 1\n  maxReplicas: 5\n  targetCPUUtilizationPercentage: 50\n</code></pre> <p>Step 2 Apply the manifest for <code>gowebapp-hpa</code></p> <pre><code>kubectl apply -f gowebapp-hpa.yaml\n</code></pre> <p>Step 3 Take a closer look at the HPA and observe autoscaling or downscaling if any.</p> <pre><code>kubectl describe hpa gowebapp\n</code></pre> <p>Note</p> <p>It will take some time to collect metrics information about current cpu usage and since our does't have real load it might not trigger any scaling</p>"},{"location":"ycit019_ass4_2022/#51-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","title":"5.1 Commit K8s manifests to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git add .\ngit commit -m \"k8s manifests for Hands-on Assignment 4\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"ycit019_ass4_2022/#52-cleaning-up","title":"5.2 Cleaning Up","text":"<p>Step 1 Delete the cluster</p> <pre><code>gcloud container clusters delete k8s-features\n</code></pre>"},{"location":"ycit019_ass4_sol/","title":"Deploy Applications on Kubernetes","text":"<p>Objective:</p> <ul> <li>Review process of creating K8s:<ul> <li>Startup Probes</li> <li>Liveness Probes</li> <li>Readiness Probes</li> <li>Secrets</li> <li>ConfigMaps</li> </ul> </li> </ul> <p>Externalize Web Application Configuration</p>"},{"location":"ycit019_ass4_sol/#11-create-gke-cluster","title":"1.1 Create GKE Cluster","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with two nodes:</p> <pre><code>gcloud container clusters create k8s-concepts \\\n--zone us-central1-c \\\n--num-nodes 2\n</code></pre> <p>Output:</p> <pre><code>NAME          LOCATION       MASTER_VERSION   MASTER_IP      MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\nk8s-concepts  us-central1-c  1.19.9-gke.1400  34.121.222.83  e2-medium     1.19.9-gke.1400  2          RUNNING\n</code></pre> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials k8s-concepts --zone us-central1-c\n</code></pre>"},{"location":"ycit019_ass4_sol/#12-locate-assignment-4","title":"1.2 Locate Assignment 4","text":"<p>Step 1 Locate directory where Kubernetes manifests going to be stored.</p> <pre><code>cd ~/ycit019/\ngit pull       # Pull latest assignement3\n</code></pre> <p>In case you don't have this folder clone it as following:</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit019\ncd ~/ycit019/Assignment4/\nls\n</code></pre> <p>Result</p> <p>You can see 4 Kubernetes manifests with Assignment tasks.</p> <p>Step 2 Go into your personal Google Cloud Source Repository:</p> <pre><code>MY_REPO=your_student_id-notepad\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$MY_REPO\n</code></pre> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 3 Copy Assignment 4 <code>deploy_a4</code> folder to your repo:</p> <pre><code>cp -r ~/ycit019/Assignment4/deploy_a4 .\n</code></pre> <p>Step 4 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"adding K8s manifests for assignment 4\"\n</code></pre> <p>Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"ycit019_ass4_sol/#21-externalize-web-application-configuration","title":"2.1 Externalize Web Application Configuration","text":"<p>Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time.</p> <p>Step 1: Move config file outside compiled application</p> <p>First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path</p> <pre><code>cd ~\nmkdir $MY_REPO/gowebapp/config\ncp $MY_REPO/gowebapp/code/config/config.json \\\n$MY_REPO/gowebapp/config\n</code></pre> <p>Remove <code>config</code> folder that is located in <code>code</code> directory</p> <pre><code>rm -rf $MY_REPO/gowebapp/code/config\n</code></pre> <p>Result</p> <p>Your <code>gowebapp</code> folder should look like following:</p> <p><code>$ ls gowebapp code  config  Dockerfile</code></p> <p>Step 2: Modify app to support setting DB password through environment variable Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time.</p> <p>Use a text editor of your choice (vim, VS code) to modify:</p> <pre><code>vim ~/$MY_REPO/gowebapp/code/vendor/app/shared/database/database.go\n</code></pre> <p>Note</p> <p>To see the line number in vim you can enable them by running <code>:set number</code></p> <ul> <li>Add an import for the <code>\"os\"</code> package at line 8. After making this change, your imports list will look like the following:</li> </ul> <pre><code>import (\n  \"encoding/json\"\n  \"fmt\"\n  \"log\"\n  \"time\"\n  \"os\"\n\n  \"github.com/boltdb/bolt\"\n  _ \"github.com/go-sql-driver/mysql\" // MySQL driver\n  \"github.com/jmoiron/sqlx\"\n  \"gopkg.in/mgo.v2\"\n)\n</code></pre> <ul> <li>Add the following code at line 89 after <code>var err error</code> :</li> </ul> <pre><code>// Check for MySQL Password environment variable and update configuration if present\nif os.Getenv(\"DB_PASSWORD\") != \"\" {\n    d.MySQL.Password = os.Getenv(\"DB_PASSWORD\")\n}\n</code></pre>"},{"location":"ycit019_ass4_sol/#22-build-new-docker-image-for-your-frontend-application","title":"2.2 Build new Docker image for your frontend application","text":"<p>Step 1: Update <code>Dockerfile</code> for your <code>gowebapp</code> frontend application:</p> <pre><code>cd ~/$MY_REPO/gowebapp\n</code></pre> <pre><code>FROM golang:1.16.4\n\nLABEL maintainer \"student@mcgill.ca\"\nLABEL gowebapp \"v1\"\n\nEXPOSE 80\n\nENV GO111MODULE=auto\nENV GOPATH=/go\nENV PASSWORD=rootpasswd\n\nCOPY /code $GOPATH/src/gowebapp/\n\nWORKDIR $GOPATH/src/gowebapp/\n\nRUN go get &amp;&amp; go install\n\nVOLUME $GOPATH/src/gowebapp/config\n\nENTRYPOINT $GOPATH/bin/gowebapp\n</code></pre> <p>Step 2: Build updated <code>gowebapp</code> Docker image locally</p> <pre><code>cd ~/$MY_REPO/gowebapp\n</code></pre> <p>Build and push the <code>gowebapp</code> image to GCR. Make sure to include \u201c.\u201c at the end of build command. </p> <pre><code>docker build -t gcr.io/${PROJECT_ID}/gowebapp:v3 .\ndocker push gcr.io/${PROJECT_ID}/gowebapp:v3\n</code></pre>"},{"location":"ycit019_ass4_sol/#23-run-and-test-new-docker-image-locally","title":"2.3 Run and test new Docker image locally","text":"<p>Before deploying to Kubernetes, let\u2019s test the updated <code>gowebapp</code> Docker image locally, to ensure that the frontend and backend containers run and integrate properly.</p> <p>Step 1: Launch frontend and backend containers</p> <p>First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it.</p> <p>Note</p> <p>Update user-name with command below with you docker-hub id</p> <pre><code>docker network create gowebapp -d bridge\n</code></pre> <pre><code>docker run --net gowebapp --name gowebapp-mysql --hostname \\\ngowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1\n</code></pre> <p>Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password:</p> <p>Note</p> <p>Update user-name with command below with you docker-hub id</p> <pre><code>docker run -p 8080:80 \\\n-v ~/$MY_REPO/gowebapp/config:/go/src/gowebapp/config \\\n--net gowebapp -d --name gowebapp \\\n--hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v3\n</code></pre> <p>Step 3  Test the application locally</p> <p>Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Step 4 Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container.</p> <p>Result</p> <p>By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes</p> <p>Step 5 Cleanup environment</p> <pre><code>docker rm -f $(docker ps -q)\ndocker network rm gowebapp\n</code></pre>"},{"location":"ycit019_ass4_sol/#24-update-dockercompose-file-to-support-changes","title":"2.4 Update DockerCompose file to support changes","text":"<p>Step 1 Edit docker-compose file</p> <pre><code>cd ~/$MY_REPO/\n</code></pre> <pre><code>vim docker-compose.yml\n\nversion: '2.4'\n\nservices:\n  gowebapp-mysql:\n    container_name: gowebapp-mysql\n    build: ./gowebapp-mysql\n    environment:\n      - MYSQL_ROOT_PASSWORD=rootpasswd\n    healthcheck:\n      test: [ \"CMD\", \"mysqladmin\" ,\"ping\", \"-h\", \"localhost\" ]\n      interval: 30s\n      timeout: 5s\n      retries: 3\n    networks:\n      - gowebapp    \n  gowebapp:\n    container_name: gowebapp\n    build: ./gowebapp\n    ports:\n      - 8080:80\n    depends_on:\n      gowebapp-mysql:\n        condition: service_healthy\n    volumes:\n      - ./gowebapp/config:/go/src/gowebapp/config\n    networks:\n      - gowebapp\n\nnetworks:\n  gowebapp1:\n    driver: bridge\n</code></pre> <p>Step 3 Test application</p> <pre><code>export CLOUDSDK_PYTHON=/usr/bin/python \ndocker-compose up -d\n</code></pre> <p>Step 4  Test the application locally</p> <p>Now that we've launched the application containers, let's try to test the web application locally.</p> <p>You should be able to access the application at Google Cloud <code>Web Preview</code> Console:</p> <p></p> <p>Note</p> <p>Web Preview using port <code>8080</code> by default. If you application using other port, you can edit this as needed.</p> <p>Step 5 Tear down environment</p> <pre><code>docker-compose down\n</code></pre>"},{"location":"ycit019_ass4_sol/#31-create-a-namespace-dev","title":"3.1 Create a Namespace <code>dev</code>","text":"<p>Step 1 Create 'dev' namespace that's going to be used to develop and deploy <code>Notestaker</code> application on Kubernetes using <code>kubetl</code> CLI.</p> <pre><code>kubectl create ns dev\n</code></pre> <p>Step 3 Use <code>dev</code> context to create K8s resources inside this namespace.</p> <pre><code>kubectl config set-context --current --namespace=dev\n</code></pre> <p>Step 4 Verify current context:</p> <pre><code>kubectl config view | grep namespace\n</code></pre> <p>Result</p> <p><code>dev</code></p>"},{"location":"ycit019_ass4_sol/#32-create-a-secret","title":"3.2 Create a Secret","text":"<p>Step 1 <code>Base64</code> Encode MySQL password <code>rootpasswd</code>. See Lab 8 for more details.</p> <p>Step 2 Edit a secret for the MySQL password in <code>dev</code> namespaces.</p> <pre><code>cd ~/$MY_REPO/deploy_a4\nvim secret-mysql.yaml\n</code></pre> <pre><code>kind: Secret\napiVersion: v1\nmetadata:\n  name: mysql\ntype: Opaque\ndata:\n  password: cm9vdHBhc3N3ZA==\n</code></pre> <pre><code>kubectl apply -f secret-mysql.yaml\nkubectl describe secret mysql\n</code></pre> <p>Step 2  Update <code>gowebapp-mysql-deployment.yaml</code> under <code>~/$MY_REPO/deploy_a4</code></p> <pre><code>vim ~/$MY_REPO/deploy_a4/gowebapp-mysql-deployment.yaml\n</code></pre> <pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: gowebapp-mysql\n  labels:\n    run: gowebapp-mysql\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        run: gowebapp-mysql\n    spec:\n      containers:\n      - env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: password\n        image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1\n        name: gowebapp-mysql\n        ports:\n        - containerPort: 3306\n        livenessProbe:\n          tcpSocket:\n            port: 3306\n          initialDelaySeconds: 30\n          timeoutSeconds: 2\n        readinessProbe:\n          tcpSocket:\n            port: 3306\n          initialDelaySeconds: 25\n          timeoutSeconds: 2     \n</code></pre> <p>Step 2  Start the rolling upgrade and record the command used in the rollout history:</p> <pre><code>kubectl apply -f gowebapp-mysql-deployment.yaml --record\n</code></pre> <p>Step 3 Verify that rollout was successful</p> <pre><code>kubectl rollout status deploy gowebapp-mysql\n</code></pre> <p>Step 4 Check if pods are running</p> <pre><code>kubectl get pods\n</code></pre> <p>Step 5 Create a Service object for MySQL</p> <pre><code>kubectl apply -f gowebapp-mysql-service.yaml --record\n</code></pre> <p>Step 6 Check to make sure it worked</p> <pre><code>kubectl get service -l \"run=gowebapp-mysql\"\n</code></pre>"},{"location":"ycit019_ass4_sol/#33-create-configmap-and-probes-for-gowebapp","title":"3.3 Create ConfigMap and Probes for <code>gowebapp</code>","text":"<p>Step 1: Create ConfigMap for gowebapp's config.json file</p> <pre><code>cd ~/$MY_REPO/gowebapp/config/\nkubectl create configmap gowebapp --from-file=webapp-config-json=config.json\nkubectl describe configmap gowebapp\n</code></pre> <p>Note</p> <p>The entire file contents from config.json are stored under the key <code>webapp-config-json</code></p>"},{"location":"ycit019_ass4_sol/#34-deploy-webapp-by-referencing-secret-configmap-and-define-probes","title":"3.4 Deploy <code>webapp</code> by Referencing Secret, ConfigMap and define Probes","text":"<p>Step 1: Update <code>gowebapp-deployment.yaml</code> under <code>~/$MY_REPO/deploy_a4/</code></p> <pre><code>cd ~/$MY_REPO/deploy_a4/\nvim gowebapp-deployment.yaml\n</code></pre> <p>In this exercise, we will add liveness/readiness probes to our deployments. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gowebapp\n  labels:\n    run: gowebapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: gowebapp\n  template:\n    metadata:\n      labels:\n        run: gowebapp\n    spec:\n      containers:\n      - env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n             key: password\n        image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1\n        name: gowebapp\n        ports:\n          - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /register\n            port: 80\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /register\n            port: 80\n          initialDelaySeconds: 25\n          timeoutSeconds: 5\n        volumeMounts:\n          - name: config-volume\n            mountPath: /go/src/gowebapp/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: gowebapp\n          items:\n          - key: webapp-config-json\n            path: config.json\n</code></pre> <pre><code>kubectl apply -f gowebapp-deployment.yaml --record\n</code></pre> <p>Result</p> <p>This will start the rolling upgrade and record the command used in the rollout history</p> <p>Step 3: Verify that rollout was successful</p> <pre><code>kubectl rollout status deploy gowebapp\n</code></pre> <p>Step 4: Get rollout history</p> <pre><code>kubectl rollout history deploy gowebapp\n</code></pre> <p>Step 5: Get rollout history details for specific revision (use number show in output to previous command)</p> <pre><code>kubectl rollout history deploy gowebapp --revision=&lt;latest_version_number\n</code></pre> <p>Step 6 Check if pods are running</p> <pre><code>kubectl get pods\n</code></pre> <p>Step 7 Create a Service object for gowebapp</p> <pre><code>kubectl apply -f gowebapp-service.yaml --record\n</code></pre> <p>Step 8 Access your application on Public IP via automatically created Loadbalancer  created for <code>gowebapp</code> service.</p> <p>To get the value of <code>Loadbalancer</code> run following command:</p> <pre><code>kubectl get svc gowebapp -o wide\n</code></pre> <p>Expected output:</p> <pre><code>NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\ngowebapp         Loadbalancer 10.107.15.39  XXXXXX        9000:32634/TCP   30m\ngowebapp-mysql   ClusterIP   None           &lt;none&gt;        3306/TCP         1h\n</code></pre> <p>**Step ** Access <code>Loadbalancer</code> IP via browser:</p> <p>Result</p> <p>Congrats!!! You've deployed you code to Kubernetes</p>"},{"location":"ycit019_ass4_sol/#35-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","title":"3.5 Commit K8s manifests to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git add .\ngit commit -m \"k8s manifests for Hands-on Assignment 4\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"ycit019_ass4_sol/#36-cleaning-up","title":"3.6 Cleaning Up","text":"<p>Step 1 Delete the cluster</p> <pre><code>gcloud container clusters delete k8s-concepts\n</code></pre>"},{"location":"ycit019_ass5/","title":"Deploy Applications on Kubernetes","text":"<p>Objective:</p> <ul> <li>Review process of creating K8s:<ul> <li>Resource</li> <li>Limits</li> <li>HPA</li> <li>VPA</li> <li>Limit Range</li> </ul> </li> </ul>"},{"location":"ycit019_ass5/#0-create-gke-cluster-with-cluster-and-vertical-autoscaling-support","title":"0 Create GKE Cluster with Cluster and Vertical Autoscaling Support","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud container clusters create k8s-scaling \\\n--zone us-central1-c \\\n--enable-vertical-pod-autoscaling \\\n--num-nodes 2 \\\n--enable-autoscaling --min-nodes 1 --max-nodes 3\n</code></pre> <p>Output:</p> <pre><code>NAME          LOCATION       MASTER_VERSION   MASTER_IP      MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\nk8s-scaling  us-central1-c  1.19.9-gke.1400  34.121.222.83  e2-medium     1.19.9-gke.1400  2          RUNNING\n</code></pre> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials k8s-scaling --zone us-central1-c\n</code></pre>"},{"location":"ycit019_ass5/#12-locate-assignment-5","title":"1.2 Locate Assignment 5","text":"<p>Step 1 Locate directory where Kubernetes manifests going to be stored.</p> <pre><code>cd ~/ycit019/\ngit pull       # Pull latest assignement5\n</code></pre> <p>In case you don't have this folder clone it as following:</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit019\ncd ~/ycit019/Assignment5/\nls\n</code></pre> <p>Result</p> <p>You can see Kubernetes manifests with Assignment tasks.</p> <p>Step 2 Go into your personal Google Cloud Source Repository:</p> <pre><code>MY_REPO=your_student_id-notepad\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$MY_REPO\n</code></pre> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 3 Copy Assignment 5 <code>deploy_a5</code> folder to your repo:</p> <pre><code>cp -r ~/ycit019/Assignment5/deploy_a5 .\n</code></pre> <p>Step 4 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"adding K8s manifests for assignment 5\"\n</code></pre> <p>Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"ycit019_ass5/#13-create-a-namespace-dev","title":"1.3 Create a Namespace <code>dev</code>","text":"<p>Step 1 Create 'dev' namespace that's going to be used to develop and deploy <code>Notestaker</code> application on Kubernetes using <code>kubetl</code> CLI.</p> <pre><code>kubectl create ns dev\n</code></pre> <p>Step 3 Use <code>dev</code> context to create K8s resources inside this namespace.</p> <pre><code>kubectl config set-context --current --namespace=dev\n</code></pre> <p>Step 4 Verify current context:</p> <pre><code>kubectl config view | grep namespace\n</code></pre> <p>Result</p> <p><code>dev</code></p>"},{"location":"ycit019_ass5/#21-configure-vpa-for-gowebapp-mysql-to-find-optimal-resource-request-and-limits-values","title":"2.1 Configure VPA for <code>gowebapp-mysql</code> to find optimal Resource Request and Limits values","text":"<p><code>requests</code> and <code>limits</code> is the way Kubernetes set's QoS for Pods, as well as enable's features like HPA, CA, Resource Quota's and more.</p> <p>However setting best values for resource <code>requests</code> and <code>limits</code> is hard, VPA is here to help. Set VPA for <code>gowebapp</code> and observe usage recommendation for <code>requests</code> and <code>limits</code></p> <p>Step 1 Deploy <code>gowebapp-mysql</code> app under <code>~/$MY_REPO/deploy_a5/</code></p> <pre><code>cd ~/$MY_REPO/deploy_a5/\nkubectl apply -f secret-mysql.yaml              #Create Secret\nkubectl apply -f gowebapp-mysql-service.yaml    #Create Service\nkubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment\n</code></pre> <pre><code>kubectl get deploy\n</code></pre> <p>Result</p> <p>Our Deployment is up, however without <code>request</code> and <code>limits</code> it will be treated as Best Effort QoS resource on the Cluster.</p> <p>Step 2 Edit a  manifest for <code>gowebapp-mysql</code> Vertical Pod Autoscaler resource:</p> <pre><code>cd ~/$MY_REPO/deploy_a5\nvim gowebapp-mysql-vpa.yaml\n</code></pre> <pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: gowebapp-mysql\n#TODO: Configure VPA with updateMode:OFF\n</code></pre> <p>Step 3 Apply the manifest for <code>gowebapp-mysql-vpa</code></p> <pre><code>kubectl apply -f gowebapp-mysql-vpa.yaml\n</code></pre> <p>Step 4 Wait a minute, and then view the VerticalPodAutoscaler</p> <pre><code>kubectl describe vpa gowebapp-mysql\n</code></pre> <p>Note</p> <p>If you don't see it, wait a little longer and try the previous command again. </p> <p>Step 5 Locate the \"Container Recommendations\" at the end of the output from the <code>describe</code> command.</p> <p>Result</p> <p>We will be using <code>Lower Bound</code> values to set our <code>request</code> value and <code>Upper Bound</code> as our <code>limits</code> value.</p>"},{"location":"ycit019_ass5/#22-set-recommended-request-and-limits-values-to-gowebapp-mysql","title":"2.2 Set Recommended Request and Limits values to <code>gowebapp-mysql</code>","text":"<p>Step 1 Edit a manifest for <code>gowebapp</code> deployment resource:</p> <pre><code>cd ~/$MY_REPO/deploy_a5\nvim gowebapp-mysql-deployment.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gowebapp-mysql\n  labels:\n    run: gowebapp-mysql\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: gowebapp-mysql\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        run: gowebapp-mysql\n    spec:\n      containers:\n      - env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: password\n        image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1\n        name: gowebapp-mysql\n        ports:\n        - containerPort: 3306\n        livenessProbe:\n          tcpSocket: \n            port: 3306\n          initialDelaySeconds: 30\n          timeoutSeconds: 2\n        readinessProbe: \n          tcpSocket:\n            port: 3306\n          initialDelaySeconds: 25\n          timeoutSeconds: 2\n#TODO define a resource request and limits based on VPA Recommender\n</code></pre> <p>Step 2 Redeploy our application with defined resource <code>request</code> and <code>limits</code></p> <pre><code>cd ~/$MY_REPO/deploy_a5/\nkubectl delete -f gowebapp-mysql-deployment.yaml\nkubectl apply -f gowebapp-mysql-deployment.yaml\n</code></pre>"},{"location":"ycit019_ass5/#23-configure-vpa-for-gowebapp-to-find-optimal-resource-request-and-limits-values","title":"2.3 Configure VPA for <code>gowebapp</code> to find optimal Resource Request and Limits values","text":"<p>Step 1: Create ConfigMap for gowebapp's config.json file</p> <pre><code>cd ~/$MY_REPO/gowebapp/config/\nkubectl create configmap gowebapp --from-file=webapp-config-json=config.json\nkubectl describe configmap gowebapp\n</code></pre> <p>Step 2 Deploy <code>gowebapp</code> app under <code>~/$MY_REPO/deploy_a5/</code></p> <pre><code>cd ~/$MY_REPO/deploy_a5/\nkubectl apply -f gowebapp-service.yaml    #Create Service\nkubectl apply -f gowebapp-deployment.yaml #Create Deployment\n</code></pre> <pre><code>kubectl get deploy\n</code></pre> <p>Result</p> <p>Our Deployment is up, however without <code>request</code> and <code>limits</code> it will be treated as Best Effort QoS resource on the Cluster.</p> <p>Step 3 Edit a  manifest for <code>gowebapp</code> Vertical Pod Autoscaler resource:</p> <pre><code>cd ~/$MY_REPO/deploy_a5\nvim gowebapp-vpa.yaml\n</code></pre> <pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: gowebapp\n#TODO: Ref: https://cloud.google.com/kubernetes-engine/docs/how-to/vertical-pod-autoscaling\n#TODO: Configure VPA with updateMode:OFF\n</code></pre> <p>Step 4 Apply the manifest for <code>gowebapp-vpa</code></p> <pre><code>kubectl apply -f gowebapp-vpa.yaml\n</code></pre> <p>Step 5 Wait a minute, and then view the VerticalPodAutoscaler</p> <pre><code>kubectl describe vpa gowebapp\n</code></pre> <p>Note</p> <p>If you don't see it, wait a little longer and try the previous command again. </p> <p>Step 6 Locate the \"Container Recommendations\" at the end of the output from the <code>describe</code> command.</p> <p>Result</p> <p>We will be using <code>Lower Bound</code> values to set our <code>request</code> value and <code>Upper Bound</code> as our <code>limits</code> value.</p>"},{"location":"ycit019_ass5/#24-set-recommended-request-and-limits-values-to-gowebapp","title":"2.4 Set Recommended Request and Limits values to <code>gowebapp</code>","text":"<p>Step 1 Edit a manifest for <code>gowebapp</code> deployment resource:</p> <pre><code>cd ~/$MY_REPO/deploy_a5\nvim gowebapp-deployment.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gowebapp\n  labels:\n    run: gowebapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: gowebapp\n  template:\n    metadata:\n      labels:\n        run: gowebapp\n    spec:\n      containers:\n      - env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: password\n        image: gcr.io/${PROJECT_ID}/gowebapp:v3\n        name: gowebapp\n        ports:\n          - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /register\n            port: 80\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet: \n            path: /register\n            port: 80\n          initialDelaySeconds: 25\n          timeoutSeconds: 5\n        resources:\n        #TODO define a resource request and limits based on VPA Recommender\n        volumeMounts:\n          - name: config-volume\n            mountPath: /go/src/gowebapp/config\n      volumes: \n      - name: config-volume\n        configMap:\n          name: gowebapp\n          items:\n          - key: webapp-config-json\n            path: config.json\n</code></pre> <p>Step 2 Redeploy our application with defined resource <code>request</code> and <code>limits</code></p> <pre><code>cd ~/$MY_REPO/deploy_a5/\nkubectl delete -f gowebapp-deployment.yaml\nkubectl apply -f gowebapp-deployment.yaml\n</code></pre>"},{"location":"ycit019_ass5/#25-configure-hpa-for-gowebapp","title":"2.5 Configure HPA for <code>gowebapp</code>","text":"<p>Our NotePad Application is going to Production soon. To make sure our application can scale based on requests we will set HPA for our deployment resource using Horizontal Pod Autoscaler.</p> <p>Step 1 Create HPA for <code>gowebapp</code> based on CPU with minReplicas <code>1</code> and maxReplicas <code>5</code> with target 50.</p> <pre><code>cd ~/$MY_REPO/deploy_a5\nvim gowebapp-hpa.yaml\n</code></pre> <pre><code>apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: gowebapp-hpa\nspec:\n  scaleTargetRef:\n  #TODO: Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/\n  #TODO Create HPA based on CPU with minReplicas `1` and maxReplicas `5` with target 50.\n</code></pre> <p>Step 2 Apply the manifest for <code>gowebapp-hpa</code></p> <pre><code>kubectl apply -f gowebapp-hpa.yaml\n</code></pre> <p>Step 3 Take a closer look at the HPA and observe autoscaling or downscaling if any.</p> <pre><code>kubectl describe hpa kubia\n</code></pre> <p>Note</p> <p>It will take some time to collect metrics information about current cpu usage and since our does't have real load it might not trigger any scaling</p>"},{"location":"ycit019_ass5/#26-configure-limitranges-for-namespace-dev","title":"2.6 Configure LimitRanges for Namespace <code>dev</code>","text":"<p>In order to prevent developers accidentally forget to set values for  <code>request</code> and <code>limits</code>. Ops team decide to create a Configuration <code>LimitRange</code>, that will enforce some default values for  <code>request</code> and <code>limits</code> if they have not been set, as well as <code>Minimum</code> and <code>maximum</code> requests/limits a container can have to prevent resources abuse.</p> <p>Step 1 Edit a  manifest for <code>limit-range</code> that can be used for <code>dev</code> namespace with following requirements:</p> <pre><code>cd ~/$MY_REPO/deploy_a5/\nvim limit-range.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: gowebapp-system\n  #TODO Specify Namespace\nspec:\n#TODO Ref: https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/\n#TODO If not specified the Container's `memory` limit is set to 256Mi, which is the default memory limit for the namespace.\n#TODO If not specified default limit `cpu` is 250m per container\n#TODO If not specified set the Container's Default memory requests to 256Mi\n#TODO If not specified set the Container's Default cpu requests to 250m\n#TODO Configure Minimum `20m` and Maximum `800m` CPU Constraints for a Namespace\n</code></pre>"},{"location":"ycit019_ass5/#27-viewing-cluster-autoscaler-events","title":"2.7 Viewing cluster autoscaler events","text":"<p>Our cluster was configured to use Cluster Autoscaler, verify if during you assignment, cluster did went through autoscaling process?</p> <p>To view the logs, perform the following:</p> <p>Step 1: In the Cloud Console, go to the Logs Viewer page.</p> <p>Step 2: Search for the logs using the basic or advanced query interface.</p> <p>To search for logs using the basic query interface, perform the following:</p> <ul> <li>a. From the resources drop-down list, select Kubernetes Cluster, then select the location of your cluster, and the name of your cluster.</li> <li>b. From the logs type drop-down list, select container.googleapis.com/cluster-autoscaler-visibility.</li> <li>c. From the time-range drop-down list, select the desired time range.</li> </ul> <p>OR search for logs using the advanced query interface, apply the following advanced filter:</p> <pre><code>resource.type=\"k8s_cluster\"\nresource.labels.location=\"cluster-location\"\nresource.labels.cluster_name=\"cluster-name\"\nlogName=\"projects/project-id/logs/container.googleapis.com%2Fcluster-autoscaler-visibility\"\n</code></pre> <p>Reference link</p>"},{"location":"ycit019_ass5/#27-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","title":"2.7 Commit K8s manifests to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git add .\ngit commit -m \"k8s manifests for Hands-on Assignment 5\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"ycit019_ass5/#36-cleaning-up","title":"3.6 Cleaning Up","text":"<p>Step 1 Delete the cluster</p> <pre><code>gcloud container clusters delete k8s-scaling\n</code></pre>"},{"location":"ycit019_ass6/","title":"Deploy Applications on Kubernetes","text":"<p>Objective:</p> <ul> <li>Review process of creating K8s:<ul> <li>Network Policy</li> <li>Ingress</li> <li>PVC, PV</li> </ul> </li> </ul>"},{"location":"ycit019_ass6/#1-prepare-environment","title":"1. Prepare Environment","text":""},{"location":"ycit019_ass6/#11-create-gke-cluster-with-cluster-network-policy-support","title":"1.1 Create GKE Cluster with Cluster Network Policy Support","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud container clusters create k8s-networking \\\n--zone us-central1-c \\\n--enable-ip-alias \\\n--create-subnetwork=\"\" \\\n--network=default \\\n--enable-network-policy \\\n--num-nodes 2\n</code></pre> <p>Output:</p> <pre><code>NAME          LOCATION       MASTER_VERSION   MASTER_IP      MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\nk8s-scaling  us-central1-c  1.19.9-gke.1400  34.121.222.83  e2-medium     1.19.9-gke.1400  2          RUNNING\n</code></pre> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials k8s-networking --zone us-central1-c\n</code></pre>"},{"location":"ycit019_ass6/#12-locate-assignment-6","title":"1.2 Locate Assignment 6","text":"<p>Step 1 Locate directory where Kubernetes manifests going to be stored.</p> <pre><code>cd ~/ycit019/\ngit pull       # Pull latest assignement6\n</code></pre> <p>In case you don't have this folder clone it as following:</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit019\ncd ~/ycit019/Assignment6/\nls\n</code></pre> <p>Result</p> <p>You can see Kubernetes manifests with Assignment tasks.</p> <p>Step 2 Go into your personal Google Cloud Source Repository:</p> <pre><code>MY_REPO=your_student_id-notepad\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$MY_REPO\n</code></pre> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 3 Copy Assignment 6 <code>deploy_a6</code> folder to your repo:</p> <pre><code>cp -r ~/ycit019/Assignment6/deploy_a6 .\n</code></pre> <p>Step 4 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"adding K8s manifests for assignment 6\"\n</code></pre> <p>Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"ycit019_ass6/#21-create-dev-namespace-and-make-it-default","title":"2.1 Create 'dev' namespace and make it default.","text":"<p>Step 1 Create 'dev' namespace that's going to be used to develop and deploy <code>Notestaker</code> application on Kubernetes using <code>kubetl</code> CLI.</p> <pre><code>kubectl apply -f dev-namespace.yaml\n</code></pre> <p>Step 2 Use <code>dev</code> context to create K8s resources inside this namespace.</p> <pre><code>kubens dev\n</code></pre>"},{"location":"ycit019_ass6/#24-create-gowebapp-mysql-deployment","title":"2.4 Create GoWebApp Mysql deployment","text":"<p>Step 2 Deploy <code>gowebapp-mysql</code> app under <code>~/$MY_REPO/deploy_a6/</code></p> <pre><code>cd ~/$MY_REPO/deploy_a6/\nkubectl apply -f secret-mysql.yaml              #Create Secret\nkubectl apply -f gowebapp-mysql-service.yaml    #Create Service\nkubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment\n</code></pre> <p>Check Status of the <code>mysql</code> Pod:</p> <pre><code>kubectl get pods\n</code></pre>"},{"location":"ycit019_ass6/#24-create-gowebapp-deployment","title":"2.4 Create GoWebApp deployment","text":"<p>Step 1: Create ConfigMap for gowebapp's config.json file</p> <pre><code>cd ~/$MY_REPO/gowebapp/config/\nkubectl create configmap gowebapp --from-file=webapp-config-json=config.json\nkubectl describe configmap gowebapp\n</code></pre> <p>Step 2 Deploy <code>gowebapp</code> app under <code>~/$MY_REPO/deploy_a6/</code></p> <pre><code>cd ~/$MY_REPO/deploy_a6/\nkubectl apply -f gowebapp-service.yaml    #Create Service\nkubectl apply -f gowebapp-deployment.yaml #Create Deployment\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p>Step 3 Access your application on Public IP via automatically created Loadbalancer  created for <code>gowebapp</code> service.</p> <p>To get the value of <code>Loadbalancer</code> run following command:</p> <pre><code>kubectl get svc gowebapp -o wide\n</code></pre> <p>Expected output:</p> <pre><code>NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\ngowebapp         Loadbalancer 10.107.15.39  XXXXXX        9000:32634/TCP   30m\ngowebapp-mysql   ClusterIP   None           &lt;none&gt;        3306/TCP         1h\n</code></pre> <p>Step 4 Access <code>Loadbalancer</code> IP via browser:</p> <pre><code>EXTERNAL-IP:9000\n</code></pre> <p>Result</p> <p>Congrats!!! You've deployed you code to Kubernetes</p>"},{"location":"ycit019_ass6/#31-ingress","title":"3.1 Ingress","text":""},{"location":"ycit019_ass6/#31-expose-gowebapp-with-ingress-resource","title":"3.1 Expose <code>gowebapp</code> with Ingress resource","text":"<p>Step 1 Delete the LoadBalancer gowebapp service.</p> <pre><code>#TODO delete the gowebapp service\n</code></pre> <p>Step 2 Modify gowebapp service manifest <code>gowebapp-service.yaml</code> to change the service to a NodePort service instead of LoadBalancer service.</p> <pre><code>cd ~/$MY_REPO/deploy_a6/\nedit gowebapp-service.yaml\n</code></pre> <p>Result</p> <p>Cloud Shell Editor opens. Make sure to update the service and save it!</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: gowebapp\n  labels:\n    run: gowebapp\nspec:\n  ports:\n  - port: 9000\n    targetPort: 80\n  selector:\n    run: gowebapp\n  #TODO add the appropriate type\n</code></pre> <p>Go back to  Cloud Shell Terminal.</p> <pre><code>kubectl apply -f gowebapp-service.yaml   #Re-Create the service\n</code></pre> <p>Step 3 Create an Ingress resource for the gowebapp service to expose it externally at the path /*</p> <pre><code>cat &gt; gowebapp-ingress.yaml &lt;&lt; EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: gowebapp-ingress\nspec:\n  rules:\n  - http:\n      paths:\n      - path: #TODO add the correct path\n        pathType: #TODO add the pathType appropriate for a GKE Ingress\n        backend:\n          service:\n            name: #TODO add the appropriate service name\n            port:\n              number: #TODO add the appropriate service port\nEOF\n</code></pre> <p>Step 4 Deploy <code>gowebapp-ingress</code> app under ~/$MY_REPO/deploy_a6/</p> <pre><code>kubectl apply -f gowebapp-ingress.yaml   #Create Ingress\n</code></pre> <p>Step 5 Verify the Ingress</p> <p>After you have deployed a workload, grouped its Pods into a Service, and created an Ingress for the Service, you should verify that the Ingress has provisioned the container-native load balancer successfully.</p> <pre><code>kubectl describe ingress gowebapp-ingress\n</code></pre> <p>Step 6  Test that application is reachable via <code>Ingress</code></p> <p>Note</p> <p>Wait several minutes for the HTTP(S) load balancer to be configured.</p> <p>Get the Ingress IP address, run the following command:</p> <pre><code>kubectl get ing gowebapp-ingress\n</code></pre> <p>In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser</p> <pre><code>$ADDRESS/*\n</code></pre>"},{"location":"ycit019_ass6/#4-kubernetes-network-policy","title":"4 Kubernetes Network Policy","text":"<p>Let's secure our 2 tier application using Kubernetes Network Policy!</p> <p>Task: We need to ensure the following is configured</p> <ul> <li><code>dev</code> namespace deny by default all <code>Ingress</code> traffic including from outside the cluster (Internet), With-in the namespace, and from inside the Cluster</li> <li><code>mysql</code> pod can be accessed from the <code>gowebapp</code> pod</li> <li><code>gowebapp</code> pod can be accessed from the Internet, in our case GKE Ingress (HTTPs Loadbalancer) and it must be configure to allow the appropriate HTTP(S) load balancer health check IP ranges FROM CIDR: <code>35.191.0.0/16</code> and <code>130.211.0.0/22</code> or   to make it less secure From any Endpoint CIDR: <code>0.0.0.0/0</code></li> </ul>"},{"location":"ycit019_ass6/#prerequisite","title":"Prerequisite","text":"<p>In order to ensure that Kubernetes can configure Network Policy we need to make sure our CNI supports this feature.  As we've learned from class Calico, WeaveNet and Cilium are CNIs that support network Policy.</p> <p>Our GKE cluster is already deployed using Calico CNI. To check that calico is installed in your cluster, run:</p> <pre><code>kubectl get pods -n kube-system | grep calico\n</code></pre> <p>Output </p> <pre><code>calico-node-2n65c                                           1/1     Running   0          20h\ncalico-node-prhgk                                           1/1     Running   0          20h\ncalico-node-vertical-autoscaler-57bfddcfd8-85ltc            1/1     Running   0          20h\ncalico-typha-67c885c7b7-7vf6d                               1/1     Running   0          20h\ncalico-typha-67c885c7b7-zwkll                               1/1     Running   0          20h\ncalico-typha-horizontal-autoscaler-58b8486cd4-pnt7c         1/1     Running   0          20h\ncalico-typha-vertical-autoscaler-7595df8859-zxzmp           1/1     Running   0          20h\n</code></pre>"},{"location":"ycit019_ass6/#41-configure-deny-all-network-policy-for-dev-namespace","title":"4.1 Configure <code>deny-all</code> Network Policy for <code>dev</code> Namespace","text":"<pre><code>cd ~/$MY_REPO/deploy_a6/\n</code></pre> <p>Step 1 Create a policy so that <code>dev</code> namespace deny by default all <code>Ingress</code> traffic including from the Internet, with-in the namespace, and from inside the Cluster</p> <pre><code>cat &gt; default-deny-dev.yaml &lt;&lt; EOF\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: default-deny\nspec:\n  podSelector:\n    matchLabels: #TODO add the appropriate match labels to block all traffic\n  policyTypes:\n    #TODO Default deny all ingress traffic https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-traffic\nEOF\n</code></pre> <p>Edit the required fields and save:</p> <pre><code>edit default-deny-dev.yaml \n</code></pre> <p>Step 2 Deploy <code>default-deny-dev</code> Policy app under ~/$MY_REPO/deploy_a6/</p> <pre><code>kubectl apply -f default-deny-dev.yaml   # deny-all Ingress Traffic to dev Namespace\n</code></pre> <p>Verify Policy:</p> <pre><code>kubectl describe netpol default-deny\n</code></pre> <p>Result</p> <p> - blocking the specific traffic to all pods in this namespace <p>Step 3 Verify that you can't access the application through Ingress anymore:</p> <p>Visit the IP address in a web browser</p> <pre><code>$Ingress IP\n</code></pre> <p>Result</p> <p>can't access the app through ingress loadbalancer anymore.</p>"},{"location":"ycit019_ass6/#42-configure-network-policy-for-gowebapp-mysql","title":"4.2 Configure Network Policy for <code>gowebapp-mysql</code>","text":"<p>Step 1 Create a <code>backend-policy</code> network policy to allow access to <code>gowebapp-mysql</code> pod from the <code>gowebapp</code> pods</p> <pre><code>cd ~/$MY_REPO/deploy_a6/\n</code></pre> <pre><code>cat &gt; gowebapp-mysql-netpol.yaml &lt;&lt; EOF\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: backend-policy\nspec:\n  podSelector:\n    matchLabels:\n      #TODO add the appropriate match labels\n  ingress:\n    - from:\n      - podSelector:\n          matchLabels:\n            #TODO add the appropriate match labels\nEOF\n</code></pre> <p>Edit the required fields and save:</p> <pre><code>edit gowebapp-mysql-netpol.yaml\n</code></pre> <p>Step 2 Using Cilium Editor test you Policy for Ingress traffic only</p> <p>Open Browser, Upload created Policy YAML. </p> <p>Result</p> <p><code>mysql</code> pod can only communicate with <code>gowebapp</code> pod</p> <p>Step 3 Deploy <code>gowebapp-mysql-netpol</code> Policy app under ~/$MY_REPO/deploy_a6/</p> <pre><code>kubectl apply -f gowebapp-mysql-netpol.yaml   # `mysql` pod can be accessed from the `gowebapp` pod\n</code></pre> <p>Verify Policy:</p> <pre><code>kubectl describe netpol backend-policy\n</code></pre> <p>Result</p> <p><code>run=gowebapp-mysql</code> Allowing ingress traffic only From PodSelector: <code>run=gowebapp</code> pod</p>"},{"location":"ycit019_ass6/#43-configure-network-policy-for-gowebapp","title":"4.3 Configure Network Policy for <code>gowebapp</code>","text":"<p>Step 1 Configure a Network Policy to allow access for the Healthcheck IP ranges needed for the Ingress Loadbalancer, and hence allow access through the Ingress Loadbalancer. The IP rangers you need to enable access from are <code>35.191.0.0/16</code> and <code>130.211.0.0/22</code></p> <pre><code>cd ~/$MY_REPO/deploy_a6/\n</code></pre> <pre><code>cat &gt; gowebapp-netpol.yaml &lt;&lt; EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: frontend-policy\nspec:\n  podSelector: \n    matchLabels:\n      run: gowebapp\n  ingress:\n  #TODO add the appropriate rules to enable access from IP ranges\n  policyTypes:\n  - Ingress\nEOF\n</code></pre> <p>Edit the required fields and save:</p> <pre><code>edit gowebapp-netpol.yaml\n</code></pre> <p>Step 2 Using Cilium Editor test you Policy for Ingress traffic only</p> <p>Open Browser, Upload created Policy YAML. </p> <p>Result</p> <p><code>gowebapp</code> pod can only get ingress traffic from CIDRs: <code>35.191.0.0/16</code> and <code>130.211.0.0/22</code></p> <p>Step 3 Deploy <code>gowebapp-netpol</code> Policy app under ~/$MY_REPO/deploy_a6/</p> <pre><code>kubectl apply -f gowebapp-netpol.yaml  # `gowebapp` pod from Internet on CIDR: `35.191.0.0/16` and `130.211.0.0/22`\n</code></pre> <p>Verify Policy:</p> <pre><code>kubectl describe netpol frontend-policy\n</code></pre> <p>Result</p> <p><code>run=gowebapp</code> Allowing ingress traffic from CIDR: 35.191.0.0/16 and 130.211.0.0/22</p> <p>Step 4  Test that application is reachable via <code>Ingress</code> after all Network Policy has been applied.</p> <p>Get the Ingress IP address, run the following command:</p> <pre><code>kubectl get ing gowebapp-ingress\n</code></pre> <p>In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser</p> <pre><code>$ADDRESS/*\n</code></pre> <p>Step 5 Verify that <code>NotePad</code> application is functional (e.g can login and create new entries)</p>"},{"location":"ycit019_ass6/#5-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","title":"5 Commit K8s manifests to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git add .\ngit commit -m \"k8s manifests for Hands-on Assignment 6\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"ycit019_ass6/#6-cleaning-up","title":"6 Cleaning Up","text":"<p>Step 1 Delete the cluster</p> <pre><code>gcloud container clusters delete k8s-scaling\n</code></pre>"},{"location":"ycit019_ass6_solution/","title":"Deploy Applications on Kubernetes","text":"<p>Objective:</p> <ul> <li>Review process of creating K8s:<ul> <li>Network Policy</li> <li>Ingress</li> <li>PVC, PV</li> </ul> </li> </ul>"},{"location":"ycit019_ass6_solution/#1-prepare-environment","title":"1. Prepare Environment","text":""},{"location":"ycit019_ass6_solution/#11-create-gke-cluster-with-cluster-network-policy-support","title":"1.1 Create GKE Cluster with Cluster Network Policy Support","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud container clusters create k8s-networking \\\n--zone us-central1-c \\\n--enable-ip-alias \\\n--create-subnetwork=\"\" \\\n--network=default \\\n--enable-network-policy \\\n--num-nodes 2\n</code></pre> <p>Output:</p> <pre><code>NAME          LOCATION       MASTER_VERSION   MASTER_IP      MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\nk8s-scaling  us-central1-c  1.19.9-gke.1400  34.121.222.83  e2-medium     1.19.9-gke.1400  2          RUNNING\n</code></pre> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials k8s-networking --zone us-central1-c\n</code></pre>"},{"location":"ycit019_ass6_solution/#12-locate-assignment-6","title":"1.2 Locate Assignment 6","text":"<p>Step 1 Locate directory where Kubernetes manifests going to be stored.</p> <pre><code>cd ~/ycit019/\ngit pull       # Pull latest assignement6\n</code></pre> <p>In case you don't have this folder clone it as following:</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit019\ncd ~/ycit019/Assignment6/\nls\n</code></pre> <p>Result</p> <p>You can see Kubernetes manifests with Assignment tasks.</p> <p>Step 2 Go into your personal Google Cloud Source Repository:</p> <pre><code>MY_REPO=your_student_id-notepad\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$MY_REPO\n</code></pre> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 3 Copy Assignment 6 <code>deploy_a6</code> folder to your repo:</p> <pre><code>cp -r ~/ycit019/Assignment6/deploy_a6 .\n</code></pre> <p>Step 4 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"adding K8s manifests for assignment 6\"\n</code></pre> <p>Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"ycit019_ass6_solution/#13-create-a-namespace-dev","title":"1.3 Create a Namespace <code>dev</code>","text":"<p>Step 1 Create 'dev' namespace that's going to be used to develop and deploy <code>Notestaker</code> application on Kubernetes using <code>kubetl</code> CLI.</p> <pre><code>kubectl create ns dev\n</code></pre> <p>Step 3 Use <code>dev</code> context to create K8s resources inside this namespace.</p> <pre><code>kubectl config set-context --current --namespace=dev\n</code></pre> <p>Step 4 Verify current context:</p> <pre><code>kubectl config view | grep namespace\n</code></pre> <p>Result</p> <p><code>dev</code></p>"},{"location":"ycit019_ass6_solution/#2-configure-volumes","title":"2 Configure Volumes","text":""},{"location":"ycit019_ass6_solution/#21-create-persistent-volume-for-gowebapp-mysql","title":"2.1 Create Persistent Volume for <code>gowebapp-mysql</code>","text":"<p>Step 1 Edit a Persistent Volume Claim (PVC) <code>gowebapp-mysql-pvc.yaml</code> manifest so that it will Dynamically creates 5G GCP PD Persistent Volume (PV), using  <code>balanced</code> persistent disk Provisioner with Access Mode <code>ReadWriteOnce</code>.</p> <pre><code>cd ~/$MY_REPO/deploy_a6\nedit gowebapp-mysql-pvc.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: gowebapp-mysql-pvc\n  labels:\n    run: gowebapp-mysql\nspec:\n#TODO Access Mode `ReadWriteOnce`\n#TODO Request 5G GCP PD Persistent Storage\n#TODO Balanced PD CSI Storage Class \n#TODO GKE CSI Reference: https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/gce-pd-csi-driver\n  storageClassName: standard-rwo\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <p>Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims</p> <p>Save the file!</p> <p>Step 2 Apply the manifest for <code>gowebapp-mysql-pvc</code> to Dynamically provision Persistent Volume:</p> <pre><code>kubectl apply -f gowebapp-mysql-pvc.yaml\n</code></pre> <p>Step 3 Verify  <code>STATUS</code> of PVC</p> <pre><code>kubectl get pvc\n</code></pre> <p>Output:</p> <pre><code>NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nmongodb-pvc   Pending                                      standard-rwo   5s\n</code></pre> <p>List PVs:</p> <pre><code>kubectl get pv\n</code></pre> <p>List GCP Disks:</p> <pre><code>gcloud compute disks list\n</code></pre>"},{"location":"ycit019_ass6_solution/#22-create-mysql-deployment","title":"2.2 Create Mysql deployment","text":"<p>Create Mysql deployment using created Persistent Volume (PV)</p> <p>Step 1  Update <code>gowebapp-mysql-deployment.yaml</code> and add Volume</p> <pre><code>cd ~/$MY_REPO/deploy_a6\nedit gowebapp-mysql-deployment.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gowebapp-mysql\n  labels:\n    run: gowebapp-mysql\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: gowebapp-mysql\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        run: gowebapp-mysql\n    spec:\n      containers:\n      - env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: password\n        image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1\n        name: gowebapp-mysql\n        ports:\n        - containerPort: 3306\n        livenessProbe:\n          tcpSocket: \n            port: 3306\n          initialDelaySeconds: 30\n          timeoutSeconds: 2\n        readinessProbe: \n          tcpSocket:\n            port: 3306\n          initialDelaySeconds: 25\n          timeoutSeconds: 2\n        resources:\n          requests:\n            cpu: 20m\n            memory: 252M\n          limits:\n            cpu: 1000m\n            memory: 2G\n        #TODO add the definition for volumeMounts:\n        volumeMounts:\n        - #TODO: add mountPath as /var/lib/mysql\n        - mountPath: /var/lib/mysql\n          name: mysql\n      #TODO Configure Pods access to storage by using the claim as a volume\n      #TODO: define persistentVolumeClaim\n      #TODO: claimName is the name defined in gowebapp-mysql-pvc.yaml\n      #TODO Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes\n      volumes:\n        - name: mysql\n          persistentVolumeClaim:\n            claimName: gowebapp-mysql-pvc\n</code></pre> <p>Save the file!</p> <p>Step 2 Deploy <code>gowebapp-mysql</code> app under <code>~/$MY_REPO/deploy_a6/</code></p> <pre><code>cd ~/$MY_REPO/deploy_a6/\nkubectl apply -f secret-mysql.yaml              #Create Secret\nkubectl apply -f gowebapp-mysql-service.yaml    #Create Service\nkubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment\n</code></pre> <p>Check Status of the <code>mysql</code> Pod:</p> <pre><code>kubectl get pods\n</code></pre> <p>Check Events from the <code>mysql</code> Pod:</p> <pre><code>kubectl get describe $POD\n</code></pre> <p>Note</p> <p>Notice the sequence of events:</p> <ul> <li>Scheduler selects node</li> <li>Volume mounted to Pod</li> <li><code>kubelet</code> pulls image</li> <li><code>kubelet</code> creates and starts container</li> </ul> <p>Result</p> <p>Our Mysql Deployment is up and running with Volume mounted</p>"},{"location":"ycit019_ass6_solution/#24-create-gowebapp-deployment","title":"2.4 Create GoWebApp deployment","text":"<p>Step 1: Create ConfigMap for gowebapp's config.json file</p> <pre><code>cd ~/$MY_REPO/gowebapp/config/\nkubectl create configmap gowebapp --from-file=webapp-config-json=config.json\nkubectl describe configmap gowebapp\n</code></pre> <p>Step 2 Deploy <code>gowebapp</code> app under <code>~/$MY_REPO/deploy_a6/</code></p> <pre><code>cd ~/$MY_REPO/deploy_a6/\nkubectl apply -f gowebapp-service.yaml    #Create Service\nkubectl apply -f gowebapp-deployment.yaml #Create Deployment\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p>Step 3 Access your application on Public IP via automatically created Loadbalancer  created for <code>gowebapp</code> service.</p> <p>To get the value of <code>Loadbalancer</code> run following command:</p> <pre><code>kubectl get svc gowebapp -o wide\n</code></pre> <p>Expected output:</p> <pre><code>NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\ngowebapp         Loadbalancer 10.107.15.39  XXXXXX        9000:32634/TCP   30m\ngowebapp-mysql   ClusterIP   None           &lt;none&gt;        3306/TCP         1h\n</code></pre> <p>Step 5 Access <code>Loadbalancer</code> IP via browser:</p> <pre><code>EXTERNAL-IP:9000\n</code></pre> <p>Result</p> <p>Congrats!!! You've deployed you code to Kubernetes</p>"},{"location":"ycit019_ass6_solution/#3-ingress","title":"3 Ingress","text":""},{"location":"ycit019_ass6_solution/#31-expose-gowebapp-with-ingress-resource","title":"3.1 Expose <code>gowebapp</code> with Ingress resource","text":"<p>Step 1 Delete the LoadBalancer gowebapp service.</p> <pre><code>kubectl delete svc gowebapp\n</code></pre> <p>Step 2 Modify gowebapp service manifest <code>gowebapp-service.yaml</code> to change the service to a ClusterIP service instead of LoadBalancer service.</p> <pre><code>cd ~/$MY_REPO/deploy_a6/\nedit gowebapp-service.yaml\n</code></pre> <p>Result</p> <p>Cloud Shell Editor opens. Make sure update service and save it!</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: gowebapp\n  labels:\n    run: gowebapp\nspec:\n  ports:\n  - port: 9000\n    targetPort: 80\n  selector:\n    run: gowebapp\n  type: NodePort\n</code></pre> <p>Go back to  Cloud Shell Terminal.</p> <pre><code>kubectl apply -f gowebapp-service.yaml  #Re-Create Service with ClusterIP\n</code></pre> <p>Step 3 Create an Ingress resource for the gowebapp service to expose it externally at the path <code>/*</code></p> <pre><code>cat &gt; gowebapp-ingress.yaml &lt;&lt; EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: gowebapp-ingress\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /*\n        pathType: ImplementationSpecific\n        backend:\n          service:\n            name: gowebapp\n            port:\n              number: 9000\nEOF\n</code></pre> <p>Step 4 Deploy <code>gowebapp-ingress</code> app under ~/$MY_REPO/deploy_a6/</p> <pre><code>kubectl apply -f gowebapp-ingress.yaml   #Create Ingress\n</code></pre> <p>Note</p> <p>When you create Ingress first time it takes several minutes, as Ingress Resource will create backing Loadbalancer for it.</p> <p>Step 5 Verify the Ingress</p> <p>After you have deployed a workload, grouped its Pods into a Service, and created an Ingress for the Service, you should verify that the Ingress has provisioned the container-native load balancer successfully.</p> <pre><code>kubectl describe ingress gowebapp-ingress\n</code></pre> <p>Step 6  Test load balancer functionality</p> <p>Note</p> <p>Wait several minutes for the HTTP(S) load balancer to be configured.</p> <p>Get the Ingress IP address, run the following command:</p> <pre><code>kubectl get ing gowebapp-ingress\n</code></pre> <p>In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser</p> <pre><code>$ADDRESS/gowebapp\n</code></pre>"},{"location":"ycit019_ass6_solution/#4-kubernetes-network-policy","title":"4 Kubernetes Network Policy","text":"<p>Let's secure our 2 tier application using Kubernetes Network Policy!</p> <p>Task: We need to ensure following is configured</p> <ul> <li><code>dev</code> namespace Denys by default All <code>Ingress</code> traffic including from Outside, With In Namespace, and in Cluster</li> <li><code>mysql</code> pod can be accessed from the <code>gowebapp</code> pod</li> <li><code>gowebapp</code> pod can be accessed from the Internet, in our case GKE Ingress (HTTPs Loadbalancer) and it must be configure to allow the appropriate HTTP(S) load balancer health check IP ranges FROM CIDR: <code>35.191.0.0/16</code> and <code>130.211.0.0/22</code> or   to make it less secure From any Endpoint CIDR: <code>0.0.0.0/0</code></li> </ul>"},{"location":"ycit019_ass6_solution/#prerequisite","title":"Prerequisite","text":"<p>In order to ensure that Kubernetes can configure Network Policy we need to make sure our CNI supports this feature.  As we've learned from class Calico, WeaveNet and Cilium are CNIs that support network Policy.</p> <p>Our GKE cluster already deployed using Calico CNI. To check that you calico installed you cluster run:</p> <pre><code>kubectl get pods -n kube-system | grep calico\n</code></pre> <p>Output </p> <pre><code>calico-node-2n65c                                           1/1     Running   0          20h\ncalico-node-prhgk                                           1/1     Running   0          20h\ncalico-node-vertical-autoscaler-57bfddcfd8-85ltc            1/1     Running   0          20h\ncalico-typha-67c885c7b7-7vf6d                               1/1     Running   0          20h\ncalico-typha-67c885c7b7-zwkll                               1/1     Running   0          20h\ncalico-typha-horizontal-autoscaler-58b8486cd4-pnt7c         1/1     Running   0          20h\ncalico-typha-vertical-autoscaler-7595df8859-zxzmp           1/1     Running   0          20h\n</code></pre>"},{"location":"ycit019_ass6_solution/#41-configure-deny-all-network-policy-for-dev-namespace","title":"4.1 Configure <code>deny-all</code> Network Policy for dev Namespace","text":"<p>Step 1 Locate Directory</p> <pre><code>cd ~/$MY_REPO/deploy_a6/\n</code></pre> <p>Step 1 Create Policy so that  <code>dev</code> namespace deny by default all <code>Ingress</code> traffic including from Outside Internet, With In Namespace, and inside of Cluster</p> <pre><code>cat &gt; default-deny-dev.yaml &lt;&lt; EOF\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: default-deny\nspec:\n  podSelector:\n    matchLabels: {}\n  policyTypes:\n    - Ingress\n  ingress: []\nEOF\n</code></pre> <p>Edit the required fields and save:</p> <pre><code>edit default-deny-dev.yaml \n</code></pre> <p>Step 2 Deploy <code>default-deny-dev</code> Policy app under ~/$MY_REPO/deploy_a6/</p> <pre><code>kubectl apply -f default-deny-dev.yaml   # deny-all Ingress Traffic to dev Namespace\n</code></pre> <p>Verify Policy:</p> <pre><code>kubectl describe netpol default-deny\n</code></pre> <p>Step 3 Verify that you can't access application from Ingress anymore:</p> <p>Visit the IP address in a web browser</p> <pre><code>$Ingress IP\n</code></pre>"},{"location":"ycit019_ass6_solution/#42-configure-network-policy-for-gowebapp-mysql","title":"4.2 Configure Network Policy for <code>gowebapp-mysql</code>","text":"<p>Step 1 Create a <code>backend-policy</code> network policy to allow access to <code>gowebapp-mysql</code> pod from the <code>gowebapp</code> pod</p> <pre><code>cd ~/$MY_REPO/deploy_a6/\n</code></pre> <pre><code>cat &gt; gowebapp-mysql-netpol.yaml &lt;&lt; EOF\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: backend-policy\nspec:\n  podSelector:\n    matchLabels:\n      run: gowebapp-mysql\n  ingress:\n    - from:\n      - podSelector:\n          matchLabels:\n            run: gowebapp\nEOF\n</code></pre> <p>Edit the required fields and save:</p> <pre><code>edit gowebapp-mysql-netpol.yaml\n</code></pre> <p>Step 2 Using Cilium Editor test you Policy for Ingress traffic only</p> <p>Open Browser, Upload created Policy YAML. </p> <p>Result</p> <p><code>mysql</code> pod can only communicate with <code>gowebapp</code> pod</p> <p>Step 3 Deploy <code>gowebapp-mysql-netpol</code> Policy app under ~/$MY_REPO/deploy_a6/</p> <pre><code>kubectl apply -f gowebapp-mysql-netpol.yaml   # `mysql` pod can be accessed from the `gowebapp` pod\n</code></pre> <p>Verify Policy:</p> <pre><code>kubectl describe netpol backend-policy\n</code></pre> <p>Result</p> <p><code>run=gowebapp-mysql</code> Allowing ingress traffic only From PodSelector: <code>run=gowebapp</code> pod</p>"},{"location":"ycit019_ass6_solution/#43-configure-network-policy-for-gowebapp","title":"4.3 Configure Network Policy for <code>gowebapp</code>","text":"<p>Step 1  Configure a <code>frontend-policy</code> Network Policy to allow access for the Healthcheck IP ranges needed for the Ingress Loadbalancer, and hence allow access through the Ingress Loadbalancer. The IP rangers you need to enable access from are <code>35.191.0.0/16</code> and <code>130.211.0.0/22</code></p> <pre><code>cd ~/$MY_REPO/deploy_a6/\n</code></pre> <pre><code>cat &gt; gowebapp-netpol.yaml &lt;&lt; EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: frontend-policy\nspec:\n  podSelector: \n    matchLabels:\n      run: gowebapp\n  ingress:\n  policyTypes:\n    - Ingress\n  ingress:\n  - from:\n    - ipBlock:\n        cidr: 35.191.0.0/16\n    - ipBlock:\n        cidr: 130.211.0.0/22\n  policyTypes:\n  - Ingress\nEOF\n</code></pre> <p>Edit the required fields and save:</p> <pre><code>edit gowebapp-netpol.yaml\n</code></pre> <p>Step 2 Using Cilium Editor test you Policy for Ingress traffic only</p> <p>Open Browser, Upload created Policy YAML. </p> <p>Result</p> <p><code>gowebapp</code> pod can only get ingress traffic from CIDRs: <code>35.191.0.0/16</code> and <code>130.211.0.0/22</code></p> <p>Step 3 Deploy <code>gowebapp-netpol</code> Policy app under ~/$MY_REPO/deploy_a6/</p> <pre><code>kubectl apply -f gowebapp-netpol.yaml  # `gowebapp` pod from Internet on CIDR: `35.191.0.0/16` and `130.211.0.0/22`\n</code></pre> <p>Verify Policy:</p> <pre><code>kubectl describe netpol frontend-policy\n</code></pre> <p>Result</p> <p><code>run=gowebapp</code>   Allowing ingress traffic from CIDR: 35.191.0.0/16 and 130.211.0.0/22</p> <p>Step 4  Test that application is reachable via <code>Ingress</code> after all Network Policy has been applied.</p> <p>Get the Ingress IP address, run the following command:</p> <pre><code>kubectl get ing gowebapp-ingress\n</code></pre> <p>In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser</p> <pre><code>$ADDRESS/*\n</code></pre> <p>Step 5 Verify that <code>NotePad</code> application is functional (e.g can login and create new entries)</p>"},{"location":"ycit019_ass6_solution/#5-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","title":"5 Commit K8s manifests to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git add .\ngit commit -m \"k8s manifests for Hands-on Assignment 6\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"ycit019_ass6_solution/#6-cleaning-up","title":"6 Cleaning Up","text":"<p>Step 1 Delete the cluster</p> <pre><code>gcloud container clusters delete k8s-scaling\n</code></pre>"},{"location":"ycit019_ass7/","title":"Deploy Applications on Kubernetes","text":"<p>Objective:</p> <ul> <li>Review process of creating K8s:<ul> <li>Network Policy</li> <li>Ingress</li> <li>PVC, PV</li> </ul> </li> </ul>"},{"location":"ycit019_ass7/#prepare-the-cloud-source-repository-environment-with-module-10-and-11-assignments","title":"Prepare the Cloud Source Repository Environment with Module 10 and 11 Assignments","text":"<p>This lab can be executed in you GCP Cloud Environment using Google Cloud Shell.</p> <p>Open the Google Cloud Shell by clicking on the icon on the top right of the screen:</p> <p></p> <p>Once opened, you can use it to run the instructions for this lab.</p> <p>Cloud Source Repositories: Qwick Start</p> <p>Step 1 Locate directory where kubernetes <code>YAML</code> manifest going to be stored.</p> <pre><code>cd ~/ycit019_2022/\ngit pull       # Pull latest Mod10_assignment\n</code></pre> <p>In case you don't have this folder clone it as following:</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit019_2022\ncd ~/ycit019_2022/Mod10_assignment/\nls\n</code></pre> <p>Step 2 Go into the local repository you've created:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <p>Important</p> <p>Replace above with your project_id student_name</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <p>Step 3 Copy <code>Mod10_assignment</code> folder to your repo:</p> <pre><code>git pull                              # Pull latest code from you repo\ncp -r ~/ycit019_2022/Mod10_assignment/ .\n</code></pre> <p>Step 4 Commit <code>Mod10_assignment</code> folder using the following Git commands:</p> <pre><code>git status \ngit add .\ngit commit -m \"adding `Mod10_assignment` with kubernetes YAML manifest\"\n</code></pre> <p>Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre> <p>Step 6 Review Cloud Source Repositories</p> <p>Use the <code>Google Cloud Source Repositories</code> code browser to view repository files.  You can filter your view to focus on a specific branch, tag, or comment.</p> <p>Browse the Mod10_assignment files you pushed to the repository by opening the Navigation menu and selecting Source Repositories:</p> <pre><code>Click Menu -&gt; Source Repositories &gt; Source Code.\n</code></pre> <p>Result</p> <p>The console shows the files in the master branch at the most recent commit.</p>"},{"location":"ycit019_ass7/#11-create-gke-cluster-with-cluster-network-policy-support","title":"1.1 Create GKE Cluster with Cluster Network Policy Support","text":"<p>Step 1 Enable the Google Kubernetes Engine API.</p> <pre><code>gcloud services enable container.googleapis.com\n</code></pre> <p>Step 2 From the cloud shell, run the following command to create a cluster with 1 node:</p> <pre><code>gcloud container clusters create k8s-networking \\\n--zone us-central1-c \\\n--enable-ip-alias \\\n--create-subnetwork=\"\" \\\n--network=default \\\n--enable-dataplane-v2 \\\n--num-nodes 2\n</code></pre> <p>Note</p> <p>Dataplane v2 CNI on GKE is Google's integration of Cilium Networking based on EBPF. If you prefer using Calico CNI you need to replace </p> <p>Output:</p> <pre><code>NAME: k8s-networking\nLOCATION: us-central1-c\nMASTER_VERSION: 1.22.8-gke.202\nMASTER_IP: 34.66.47.138\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.22.8-gke.202\nNUM_NODES: 2\nSTATUS: RUNNING\n</code></pre> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials k8s-networking --zone us-central1-c\n</code></pre>"},{"location":"ycit019_ass7/#2-configure-volumes","title":"2 Configure Volumes","text":""},{"location":"ycit019_ass7/#21-create-persistent-volume-for-gowebapp-mysql","title":"2.1 Create Persistent Volume for <code>gowebapp-mysql</code>","text":"<p>Step 1 Edit a Persistent Volume Claim (PVC) <code>gowebapp-mysql-pvc.yaml</code> manifest so that it will Dynamically creates 5G GCP PD Persistent Volume (PV), using  <code>balanced</code> persistent disk Provisioner with Access Mode <code>ReadWriteOnce</code>.</p> <pre><code>cd ~/$student_name-notepad/Mod10_assignment/deploy\nedit gowebapp-mysql-pvc.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: gowebapp-mysql-pvc\n  labels:\n    run: gowebapp-mysql\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <p>Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims</p> <p>Step 2 Apply the manifest for <code>gowebapp-mysql-pvc</code> to Dynamically provision Persistent Volume:</p> <pre><code>kubectl apply -f gowebapp-mysql-pvc.yaml\n</code></pre> <p>Step 3 Verify  <code>STATUS</code> of PVC</p> <pre><code>kubectl get pvc\n</code></pre> <p>Output:</p> <pre><code>NAME                 STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS \ngowebapp-mysql-pvc   Bound     pvc-xxx  5G         RWO            standard   \n</code></pre> <p>List PVs:</p> <pre><code>kubectl get pv\n</code></pre> <pre><code>NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                        STORAGECLASS\npvc-xxxx  5Gi        RWO            Delete           Bound    default/gowebapp-mysql-pvc   standard \n</code></pre> <p>List GCP Disks:</p> <pre><code>gcloud compute disks list\n</code></pre>"},{"location":"ycit019_ass7/#22-create-mysql-deployment","title":"2.2 Create Mysql deployment","text":"<p>Create Mysql deployment using created Persistent Volume (PV)</p> <p>Step 1  Update <code>gowebapp-mysql-deployment.yaml</code> and add Volume</p> <pre><code>cd ~/$student_name-notepad/Mod10_assignment/deploy\nedit gowebapp-mysql-deployment.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gowebapp-mysql\n  labels:\n    run: gowebapp-mysql\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: gowebapp-mysql\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        run: gowebapp-mysql\n    spec:\n      containers:\n      - env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: password\n        image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1\n        name: gowebapp-mysql\n        ports:\n        - containerPort: 3306\n        livenessProbe:\n          tcpSocket: \n            port: 3306\n          initialDelaySeconds: 30\n          timeoutSeconds: 2\n        readinessProbe: \n          tcpSocket:\n            port: 3306\n          initialDelaySeconds: 25\n          timeoutSeconds: 2\n        resources:\n          requests:\n            cpu: 20m\n            memory: 252M\n          limits:\n            cpu: 1000m\n            memory: 2G\n        #TODO add the definition for volumeMounts:\n        volumeMounts:\n        - #TODO: add mountPath as /var/lib/mysql\n        - mountPath: /var/lib/mysql\n          name: mysql\n      #TODO Configure Pods access to storage by using the claim as a volume\n      #TODO: define persistentVolumeClaim\n      #TODO: claimName is the name defined in gowebapp-mysql-pvc.yaml\n      #TODO Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes\n      volumes:\n        - name: mysql\n          persistentVolumeClaim:\n            claimName: gowebapp-mysql-pvc\n</code></pre> <p>Step 2 Deploy <code>gowebapp-mysql</code> app under <code>~/$MY_REPO/deploy/</code></p> <pre><code>cd ~/$student_name-notepad/Mod10_assignment/deploy/\nkubectl apply -f secret-mysql.yaml              #Create Secret\nkubectl apply -f gowebapp-mysql-service.yaml    #Create Service\nkubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment\n</code></pre> <p>Check Status of the <code>mysql</code> Pod:</p> <pre><code>kubectl get pods\n</code></pre> <p>Check Events from the <code>mysql</code> Pod:</p> <pre><code>kubectl describe $POD_NAME_FROM_PREVIOUS_COMMAND\n</code></pre> <p>Note</p> <p>Notice the sequence of events:</p> <ul> <li>Scheduler selects node</li> <li>Volume mounted to Pod</li> <li><code>kubelet</code> pulls image</li> <li><code>kubelet</code> creates and starts container</li> </ul> <p>Result</p> <p>Our Mysql Deployment is up and running with Volume mounted</p>"},{"location":"ycit019_ass7/#24-create-gowebapp-deployment","title":"2.4 Create GoWebApp deployment","text":"<p>Step 1: Create ConfigMap for gowebapp's config.json file</p> <pre><code>cd ~/$student_name-notepad/Mod10_assignment/gowebapp/config/\nkubectl create configmap gowebapp --from-file=webapp-config-json=config.json\nkubectl describe configmap gowebapp\n</code></pre> <p>Step 2 Deploy <code>gowebapp</code> app under <code>~/$MY_REPO/deploy/</code></p> <pre><code>cd ~/$student_name-notepad/Mod10_assignment/deploy/\nkubectl apply -f gowebapp-service.yaml    #Create Service\nkubectl apply -f gowebapp-deployment.yaml #Create Deployment\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p>Step 3 Access your application on Public IP via automatically created Loadbalancer  created for <code>gowebapp</code> service.</p> <p>To get the value of <code>Loadbalancer</code> run following command:</p> <pre><code>kubectl get svc gowebapp -o wide\n</code></pre> <p>Expected output:</p> <pre><code>NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\ngowebapp         Loadbalancer 10.107.15.39  XXXXXX        9000:32634/TCP   30m\ngowebapp-mysql   ClusterIP   None           &lt;none&gt;        3306/TCP         1h\n</code></pre> <p>Step 4 Access <code>Loadbalancer</code> IP via browser:</p> <pre><code>EXTERNAL-IP:9000\n</code></pre> <p>Result</p> <p>Congrats!!! You've deployed you code to Kubernetes</p>"},{"location":"ycit019_ass7/#31-ingress","title":"3.1 Ingress","text":""},{"location":"ycit019_ass7/#31-expose-gowebapp-with-ingress-resource","title":"3.1 Expose <code>gowebapp</code> with Ingress resource","text":"<p>Step 1 Delete the LoadBalancer gowebapp service.</p> <pre><code>#TODO delete the gowebapp service\n</code></pre> <p>Step 2 Modify gowebapp service manifest <code>gowebapp-service.yaml</code> to change the service to a NodePort service instead of LoadBalancer service.</p> <pre><code>cd ~/$student_name-notepad/Mod10_assignment/deploy/\nedit gowebapp-service.yaml\n</code></pre> <p>Result</p> <p>Cloud Shell Editor opens. Make sure to update the service and save it!</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: gowebapp\n  labels:\n    run: gowebapp\nspec:\n  ports:\n  - port: 9000\n    targetPort: 80\n  selector:\n    run: gowebapp\n  #TODO add the appropriate type\n</code></pre> <p>Go back to  Cloud Shell Terminal.</p> <pre><code>kubectl apply -f gowebapp-service.yaml   #Re-Create the service\n</code></pre> <p>Step 3 Update an Ingress resource for the <code>gowebapp</code> service to expose it externally at the path /*</p> <pre><code>edit gowebapp-ingress.yaml\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: gowebapp-ingress\nspec:\n  rules:\n  - http:\n      paths:\n      - path: #TODO add the correct path\n        pathType: #TODO add the pathType appropriate for a GKE Ingress\n        backend:\n          service:\n            name: #TODO add the appropriate service name\n            port:\n              number: #TODO add the appropriate service port\n</code></pre> <p>Step 4 Deploy <code>gowebapp-ingress</code> app under ~/$MY_REPO/deploy/</p> <pre><code>kubectl apply -f gowebapp-ingress.yaml   #Create Ingress\n</code></pre> <p>Step 5 Verify the Ingress</p> <p>After you have deployed a workload, grouped its Pods into a Service, and created an Ingress for the Service, you should verify that the Ingress has provisioned the container-native load balancer successfully.</p> <pre><code>kubectl describe ingress gowebapp-ingress\n</code></pre> <p>Step 6  Test that application is reachable via <code>Ingress</code></p> <p>Note</p> <p>Wait several minutes for the HTTP(S) load balancer to be configured.</p> <p>Get the Ingress IP address, run the following command:</p> <pre><code>kubectl get ing gowebapp-ingress\n</code></pre> <p>In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser</p> <pre><code>$ADDRESS/*\n</code></pre>"},{"location":"ycit019_ass7/#4-kubernetes-network-policy","title":"4 Kubernetes Network Policy","text":"<p>Let's secure our 2 tier application using Kubernetes Network Policy!</p> <p>Task: We need to ensure the following is configured</p> <ul> <li>Your namespace deny by default all <code>Ingress</code> traffic including from outside the cluster (Internet), With-in the namespace, and from inside the Cluster</li> <li><code>mysql</code> pod can be accessed from the <code>gowebapp</code> pod</li> <li><code>gowebapp</code> pod can be accessed from the Internet, in our case GKE Ingress (HTTPs Loadbalancer) and it must be configure to allow the appropriate HTTP(S) load balancer health check IP ranges FROM CIDR: <code>35.191.0.0/16</code> and <code>130.211.0.0/22</code> or   to make it less secure From any Endpoint CIDR: <code>0.0.0.0/0</code></li> </ul>"},{"location":"ycit019_ass7/#prerequisite","title":"Prerequisite","text":"<p>In order to ensure that Kubernetes can configure Network Policy we need to make sure our CNI supports this feature.  As we've learned from class Calico, WeaveNet and Cilium are CNIs that support network Policy.</p> <p>Our GKE cluster is already deployed using Cilium CNI. To check that calico is installed in your cluster, run:</p> <pre><code> kubectl -n kube-system get ds -l k8s-app=cilium -o wide\n</code></pre> <p>Output </p> <pre><code>NAME    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE   CONTAINERS     IMAGES                                  \nanetd   2         2         2       2            2           kubernetes.io/os=linux   45m   cilium-agent   gke.gcr.io/cilium/cilium:v1.11.1-gke3.3\n</code></pre> <p>Note</p> <p><code>anetd</code> is a Cilium agent DaemonSet that runs on each node.</p> <p>Result</p> <p>Our GKE cluster running <code>cilium:v1.11.1</code> which Cilium version 1.11.1</p>"},{"location":"ycit019_ass7/#41-configure-deny-all-network-policy-inside-the-namespace","title":"4.1 Configure <code>deny-all</code> Network Policy inside the Namespace","text":"<pre><code>cd ~/$student_name-notepad/Mod10_assignment/deploy/\n</code></pre> <p>Step 1 Create a policy so that you current namespace deny by default all <code>Ingress</code> traffic including from the Internet, with-in the namespace, and from inside the Cluster</p> <pre><code>edit default-deny.yaml\n</code></pre> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: default-deny\nspec:\n  podSelector:\n    matchLabels: #TODO add the appropriate match labels to block all traffic\n  policyTypes:\n    #TODO Default deny all ingress traffic https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-traffic\n</code></pre> <p>Edit the required fields and save</p> <p>Step 2 Deploy <code>default-deny</code> Policy app under ~/$MY_REPO/deploy/</p> <pre><code>kubectl apply -f default-deny.yaml   # deny-all Ingress Traffic inside Namespace\n</code></pre> <p>Verify Policy:</p> <pre><code>kubectl describe netpol default-deny\n</code></pre> <p>Result</p> <p> - blocking the specific traffic to all pods in this namespace <p>Step 3 Verify that you can't access the application through Ingress anymore:</p> <p>Visit the IP address in a web browser</p> <pre><code>$Ingress IP\n</code></pre> <p>Result</p> <p>can't access the app through ingress loadbalancer anymore.</p>"},{"location":"ycit019_ass7/#42-configure-network-policy-for-gowebapp-mysql","title":"4.2 Configure Network Policy for <code>gowebapp-mysql</code>","text":"<p>Step 1 Create a <code>backend-policy</code> network policy to allow access to <code>gowebapp-mysql</code> pod from the <code>gowebapp</code> pods</p> <pre><code>cd ~/$student_name-notepad/Mod10_assignment/deploy/\n</code></pre> <pre><code>edit gowebapp-mysql-netpol.yaml\n</code></pre> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: backend-policy\nspec:\n  podSelector:\n    matchLabels:\n      #TODO add the appropriate match labels\n  ingress:\n    - from:\n      - podSelector:\n          matchLabels:\n            #TODO add the appropriate match labels\n</code></pre> <p>Edit the required fields and save:</p> <pre><code>edit gowebapp-mysql-netpol.yaml\n</code></pre> <p>Step 2 Using Cilium Editor test you Policy for Ingress traffic only</p> <p>Open Browser, Upload created Policy YAML. </p> <p>Result</p> <p><code>mysql</code> pod can only communicate with <code>gowebapp</code> pod</p> <p>Step 3 Deploy <code>gowebapp-mysql-netpol</code> Policy app under ~/$MY_REPO/deploy/</p> <pre><code>kubectl apply -f gowebapp-mysql-netpol.yaml   # `mysql` pod can be accessed from the `gowebapp` pod\n</code></pre> <p>Verify Policy:</p> <pre><code>kubectl describe netpol backend-policy\n</code></pre> <p>Result</p> <p><code>run=gowebapp-mysql</code> Allowing ingress traffic only From PodSelector: <code>run=gowebapp</code> pod</p>"},{"location":"ycit019_ass7/#43-configure-network-policy-for-gowebapp","title":"4.3 Configure Network Policy for <code>gowebapp</code>","text":"<p>Step 1 Configure a Network Policy to allow access for the Healthcheck IP ranges needed for the Ingress Loadbalancer, and hence allow access through the Ingress Loadbalancer. The IP rangers you need to enable access from are <code>35.191.0.0/16</code> and <code>130.211.0.0/22</code></p> <pre><code>cd ~/$student_name-notepad/Mod10_assignment/deploy/\n</code></pre> <pre><code>edit gowebapp-netpol.yaml\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: frontend-policy\nspec:\n  podSelector: \n    matchLabels:\n      run: gowebapp\n  ingress:\n  #TODO add the appropriate rules to enable access from IP ranges\n  policyTypes:\n  - Ingress\n</code></pre> <p>Edit the required fields and save:</p> <pre><code>edit gowebapp-netpol.yaml\n</code></pre> <p>Step 2 Using Cilium Editor test you Policy for Ingress traffic only</p> <p>Open Browser, Upload created Policy YAML. </p> <p>Result</p> <p><code>gowebapp</code> pod can only get ingress traffic from CIDRs: <code>35.191.0.0/16</code> and <code>130.211.0.0/22</code></p> <p>Step 3 Deploy <code>gowebapp-netpol</code> Policy app under ~/$MY_REPO/deploy/</p> <pre><code>kubectl apply -f gowebapp-netpol.yaml  # `gowebapp` pod from Internet on CIDR: `35.191.0.0/16` and `130.211.0.0/22`\n</code></pre> <p>Verify Policy:</p> <pre><code>kubectl describe netpol frontend-policy\n</code></pre> <p>Result</p> <p><code>run=gowebapp</code> Allowing ingress traffic from CIDR: 35.191.0.0/16 and 130.211.0.0/22</p> <p>Step 4  Test that application is reachable via <code>Ingress</code> after all Network Policy has been applied.</p> <p>Get the Ingress IP address, run the following command:</p> <pre><code>kubectl get ing gowebapp-ingress\n</code></pre> <p>In the command output, the Ingress' IP address is displayed in the ADDRESS column. Visit the IP address in a web browser</p> <pre><code>$ADDRESS/*\n</code></pre> <p>Step 5 Verify that <code>NotePad</code> application is functional (e.g can login and create new entries)</p>"},{"location":"ycit019_ass7/#5-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","title":"5 Commit K8s manifests to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>deploy</code> folder using the following Git commands:</p> <pre><code>git add .\ngit commit -m \"k8s manifests for Hands-on Assignment 6\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"ycit019_ass7/#6-cleaning-up","title":"6 Cleaning Up","text":"<p>Step 1 Delete the cluster</p> <pre><code>gcloud container clusters delete k8s-networking\n</code></pre>"},{"location":"reveal/info/","title":"Info","text":"<p>reveal.js is an open source HTML presentation framework. It enables anyone with a web browser to create beautiful presentations for free. Check out the live demo at revealjs.com.</p> <p>The framework comes with a powerful feature set including nested slides, Markdown support, Auto-Animate, PDF export, speaker notes, LaTeX typesetting, syntax highlighted code and an extensive API.</p> <p>Want to create reveal.js presentation in a graphical editor? Try https://slides.com. It's made by the same people behind reveal.js.</p>"},{"location":"reveal/info/#getting-started","title":"Getting started","text":"<ul> <li>\ud83d\ude80 Install reveal.js</li> <li>\ud83d\udc40 View the demo presentation</li> <li>\ud83d\udcd6 Read the documentation</li> <li>\ud83d\udd8c Try the visual editor for reveal.js at Slides.com</li> <li>\ud83c\udfac Watch the reveal.js video course (paid)</li> </ul>    MIT licensed | Copyright \u00a9 2011-2024 Hakim El Hattab, https://hakim.se"},{"location":"reveal/css/theme/","title":"Index","text":""},{"location":"reveal/css/theme/#dependencies","title":"Dependencies","text":"<p>Themes are written using Sass to keep things modular and reduce the need for repeated selectors across files. Make sure that you have the reveal.js development environment installed before proceeding: https://revealjs.com/installation/#full-setup</p>"},{"location":"reveal/css/theme/#creating-a-theme","title":"Creating a Theme","text":"<p>To create your own theme, start by duplicating a <code>.scss</code> file in /css/theme/source. It will be automatically compiled from Sass to CSS (see the gulpfile) when you run <code>npm run build -- css-themes</code>.</p> <p>Each theme file does four things in the following order:</p> <ol> <li> <p>Include /css/theme/template/mixins.scss Shared utility functions.</p> </li> <li> <p>Include /css/theme/template/settings.scss Declares a set of custom variables that the template file (step 4) expects. Can be overridden in step 3.</p> </li> <li> <p>Override This is where you override the default theme. Either by specifying variables (see settings.scss for reference) or by adding any selectors and styles you please.</p> </li> <li> <p>Include /css/theme/template/theme.scss The template theme file which will generate final CSS output based on the currently defined variables.</p> </li> </ol>"},{"location":"reveal/examples/markdown/","title":"Markdown Demo","text":""},{"location":"reveal/examples/markdown/#external-11","title":"External 1.1","text":"<p>Content 1.1</p> <p>Note: This will only appear in the speaker notes window.</p>"},{"location":"reveal/examples/markdown/#external-12","title":"External 1.2","text":"<p>Content 1.2</p>"},{"location":"reveal/examples/markdown/#external-2","title":"External 2","text":"<p>Content 2.1</p>"},{"location":"reveal/examples/markdown/#external-31","title":"External 3.1","text":"<p>Content 3.1</p>"},{"location":"reveal/examples/markdown/#external-32","title":"External 3.2","text":"<p>Content 3.2</p>"},{"location":"reveal/examples/markdown/#external-33-image","title":"External 3.3 (Image)","text":""},{"location":"reveal/examples/markdown/#external-34-math","title":"External 3.4 (Math)","text":"<p><code>\\[ J(\\theta_0,\\theta_1) = \\sum_{i=0} \\]</code></p>"},{"location":"reveal/test/simple/","title":"Simple","text":""},{"location":"reveal/test/simple/#slide-11","title":"Slide 1.1","text":"<pre><code>var a = 1;\n</code></pre>"},{"location":"reveal/test/simple/#slide-12","title":"Slide 1.2","text":""},{"location":"reveal/test/simple/#slide-2","title":"Slide 2","text":""}]}