{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud Architecture \u00b6 Class Objectives: The power of containerization and its distinction from virtualization; features of the Linux kernel underpinning containerization; how to set up a Docker environment, build containers, and use an orchestration tool, namely, Kubernetes; additional tools for monitoring, sharing, and deploying applications.","title":"Home"},{"location":"#cloud-architecture","text":"Class Objectives: The power of containerization and its distinction from virtualization; features of the Linux kernel underpinning containerization; how to set up a Docker environment, build containers, and use an orchestration tool, namely, Kubernetes; additional tools for monitoring, sharing, and deploying applications.","title":"Cloud Architecture"},{"location":"Lab_10_Networking/","text":"K8s Networking \u00b6 Objective: Kubernetes Network Policy Run applications and expose them via service via Ingress resource. 0 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-networking \\ --zone us-central1-c \\ --enable-network-policy \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-networking --zone us-central1-c 1. Network Policy \u00b6 1.1 Basic Policy Demo \u00b6 This guide will deploy pods in a Kubernetes Namespaces. Let\u2019s create the Namespace object for this guide. Step 1 Create policy-demo Namespace: kubectl create ns policy-demo Step 2 Than create a policy-demo nginx deployment in policy-demo namespace: kubectl create deployment --namespace=policy-demo nginx --replicas=2 --image=nginx Then create a policy-demo service: kubectl expose --namespace=policy-demo deployment nginx --port=80 Step 3 Ensure nginx service is accessible. In order to do that create busybox pod and try to access the nginx service. kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh Output: Waiting for pod `policy-demo/access-472357175-y0m47` to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q nginx -O - Success You should see a response from nginx . Great! Our service is accessible. You can exit the Pod now. Result Pods in a given namespace can be accessed by anyone. Step 4 Now let\u2019s turn on isolation in our policy-demo Namespace. Calico will then prevent connections to pods in this Namespace. In order for Calico to prevent connection to pods in a namespace, we first need to enable network isolation in the namespace by creating following NetworkPolicy : kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny namespace: policy-demo spec: podSelector: matchLabels: {} EOF Note Current Lab is using Calico v3.0. Older versions of Calico (2.1 and prior) used namespace annotation to deny all traffic. Step 5 Verify that all access to the nginx Service is blocked. We can see the effect by trying to access the Service again. In order to do that, run a busybox deployment and try to access the nginx service. kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh Waiting for pod policy-demo/access-472357175-y0m47 to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q --timeout=5 nginx -O - Result download timed out The request should time out after 5 seconds. By enabling isolation on the Namespace, we\u2019ve prevented access to the Service. Step 6 Allow Access using a NetworkPolicy Now, let\u2019s enable access to the nginx Service using a NetworkPolicy. This will allow incoming connections from our access Pod, but not from anywhere else. Create a network policy access-nginx with the following contents: kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: access-nginx namespace: policy-demo spec: podSelector: matchLabels: app: nginx ingress: - from: - podSelector: matchLabels: run: access EOF Notice The NetworkPolicy allows traffic from Pods with the label run: access to Pods with the label app: nginx . The labels are automatically added by kubectl and are based on the name of the resource. Step 7: We should now be able to access the Service from the access Pod. In order to check that create busybox deployment and try to access the nginx Service. kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh Waiting for pod policy-demo/access-472357175-y0m47 to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q --timeout=5 nginx -O - Step 8 Without closing a prompt to a container, open a new terminal and run kubectl get pods --show-labels -n policy-demo Output: NAME READY STATUS RESTARTS AGE LABELS access 1/1 Running 0 5s run=access nginx-6799fc88d8-5r64l 1/1 Running 0 17m app=nginx,pod-template-hash=6799fc88d8 nginx-6799fc88d8-dbtgk 1/1 Running 0 17m app=nginx,pod-template-hash=6799fc88d8 Result We can see the Labels run=access and app=nginx , that has been defined in Network Policy. Step 9 However, we still cannot access the Service from a Pod without the label run: access: Once again run busybox deployment and try to access the nginx service. kubectl run --namespace=policy-demo cant-access --rm -ti --image busybox /bin/sh Waiting for pod policy-demo/cant-access-472357175-y0m47 to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q --timeout=5 nginx -O - wget: download timed out / # Step 9 You can clean up the demo by deleting the demo Namespace: kubectl delete ns policy-demo 1.2 (Demo) Stars Policy \u00b6 1.2.1 Deploy 3 tier-app \u00b6 Let's Deploy 3 tier-app: UI, frontend and backend service, as well as a client service. And configures network policy on each service. Step 1 Deploy Stars Namespace and management-ui apps inside it: kubectl create -f - <<EOF --- kind: Namespace apiVersion: v1 metadata: name: stars --- apiVersion: v1 kind: Namespace metadata: name: management-ui labels: role: management-ui --- apiVersion: v1 kind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 9001 targetPort: 9001 selector: role: management-ui --- apiVersion: apps/v1 kind: Deployment metadata: name: management-ui labels: role: management-ui namespace: management-ui spec: selector: matchLabels: role: management-ui replicas: 1 template: metadata: labels: role: management-ui spec: containers: - name: management-ui image: calico/star-collect:v0.1.0 imagePullPolicy: Always ports: - containerPort: 9001 EOF Step 2 Deploy backend application inside of the stars Namespace: Backend: kubectl create -f - <<EOF apiVersion: v1 kind: Service metadata: name: backend namespace: stars spec: ports: - port: 6379 targetPort: 6379 selector: role: backend --- apiVersion: apps/v1 kind: Deployment metadata: name: backend labels: role: backend namespace: stars spec: selector: matchLabels: role: backend replicas: 1 template: metadata: labels: role: backend spec: containers: - name: backend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=6379 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 6379 EOF Step 3 Deploy frontend application inside of the stars Namespace: kubectl create -f - <<EOF apiVersion: v1 kind: Service metadata: name: frontend namespace: stars spec: ports: - port: 80 targetPort: 80 selector: role: frontend --- apiVersion: apps/v1 kind: Deployment metadata: name: frontend labels: role: frontend namespace: stars spec: selector: matchLabels: role: frontend replicas: 1 template: metadata: labels: role: frontend spec: containers: - name: frontend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=80 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 80 EOF Step 4 Finally deploy client application inside of the client Namespace: kubectl create -f - <<EOF kind: Namespace apiVersion: v1 metadata: name: client labels: role: client --- apiVersion: apps/v1 kind: Deployment metadata: name: client labels: role: client namespace: client spec: selector: matchLabels: role: client replicas: 1 template: metadata: labels: role: client spec: containers: - name: client image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status ports: - containerPort: 9000 --- apiVersion: v1 kind: Service metadata: name: client namespace: client spec: ports: - port: 9000 targetPort: 9000 selector: role: client EOF Step 5 Wait for all the pods to enter Running state. kubectl get pods --all-namespaces --watch Note It may take several minutes to download the necessary Docker images for this demo. Step 6 Locate LoadBalancer IP: kubectl get svc -n management-ui Step 9 Open UI in you browser http://EXTERNAL-IP:9001 in a browser. Result Once all the pods are started, they should have full connectivity. You can see this by visiting the UI. Each service is represented by a single node in the graph. backend -> Node \"B\" frontend -> Node \"F\" client -> Node \"C\" 1.2.2 Enable isolation \u00b6 Step 1 Enable isolation Running the following commands will prevent all Ingress access to the frontend, backend, and client Services located in namespaces starts and client kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny namespace: stars spec: podSelector: matchLabels: {} ingress: [] --- kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny namespace: client spec: podSelector: matchLabels: {} policyTypes: - Ingress EOF Result Now that we've enabled isolation, the UI can no longer access the pods, and so they will no longer show up in the UI. Note You need to Refresh web-browser to see result. Step 2 Allow apps from stars and client namespace access UI service via NetworkPolicy rules: kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui --- kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: client name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui EOF Result refresh the UI - it should now show the Services, but they should not be able to access each other any more. Step 4 Create the NetworkPolicy to allow traffic from the frontend to the backend. kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 EOF Result Refresh the UI. You should see the following: The frontend can now access the backend (on TCP port 80 only). The backend cannot access the frontend at all. The client cannot access the frontend, nor can it access the backend. Step 5 Expose the frontend service to the client namespace. kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80 EOF Success We isolated our app using NetworkPolicy ingress rules so that: The client can now access the frontend, but not the backend. Neither the frontend nor the backend can initiate connections to the client. The frontend can still access the backend. 1.2.3 Cleanup \u00b6 Step 1 You can clean up the demo by deleting the demo Namespaces: kubectl delete ns client stars management-ui 2. Ingress \u00b6 In GKE, Ingress is implemented using Cloud Load Balancing. In other words, when yiu create an Ingress resource in your cluster, GKE creates an HTTP(S) LB for you and configure it. In this lab, we will create a fanout Ingress with two backends for two verisons of the hello-app application. Note If you want to experiment with a different type of Ingress, you can install Nginx Controller using Helm. You can also follow these [instructions] (https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md). 2.1 Deploy an Application \u00b6 Step 1 Deploy the web application version 1, and its service cat <<EOF > web-service-v1.yaml apiVersion: apps/v1 kind: Deployment metadata: name: web-v1 namespace: default spec: selector: matchLabels: run: web version: v1 template: metadata: labels: run: web version: v1 spec: containers: - image: gcr.io/google-samples/hello-app:1.0 imagePullPolicy: IfNotPresent name: web ports: - containerPort: 8080 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: web-v1 namespace: default spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: run: web version: v1 type: NodePort EOF Step 2 Apply the resources to the cluster kubectl apply -f web-service-v1.yaml Step 3 Deploy the web application version 2, and its service cat <<EOF > web-service-v2.yaml apiVersion: apps/v1 kind: Deployment metadata: name: web-v2 namespace: default spec: selector: matchLabels: run: web version: v2 template: metadata: labels: run: web version: v2 spec: containers: - image: gcr.io/google-samples/hello-app:2.0 imagePullPolicy: IfNotPresent name: web ports: - containerPort: 8080 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: web-v2 namespace: default spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: run: web version: v2 type: NodePort EOF Step 4 Apply the resources to the cluster kubectl apply -f web-service-v2.yaml Step 5 Take a look at the pods created and view their IPs kubectl get pod -o wide Step 6 Take a look at the endpoints created and notice how they match the IP for the Pods in the matching service. kubectl get ep 2.2 Creating Single Service Ingress rule \u00b6 Let's expose our web-v1 service using Ingress! Step 1 To start, let's create an Ingress resource that directs traffic to v1 of the web Service cat <<EOF > single-svc-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: single-svc-ingress spec: defaultBackend: service: name: web-v1 port: number: 8080 EOF This manifest defines a Single Service Ingress rule , which makes sure all HTTP requests that will hit the external IP for the Cloud LB created for the Ingress, will be proxied to the web-v1 service on port 8080 . Step 2 Create the Ingress resource kubectl apply -f single-svc-ingress.yaml Step 3 Verify Ingress Resource kubectl get ing Output NAME CLASS HOSTS ADDRESS PORTS AGE single-svc-ingress <none> * 34.117.137.144 80 110s Notice that the Ingress controllers and load balancers may take a minute or two to allocate an IP address. Bingo! Step 4 descirbe the Ingress object created and notice the Google Cloud LB resources created. kubectl describe ing single-svc-ingress Notice that you might need to give it some time to get a similar output to what I have below. Output: Name: single-svc-ingress Namespace: default Address: 34.117.137.144 Default backend: web-v1:8080 (10.32.0.10:8080) Rules: Host Path Backends ---- ---- -------- * * web-v1:8080 (10.32.0.10:8080) Annotations: ingress.kubernetes.io/backends: {\"k8s-be-30059--a2b52ac0331abe29\":\"HEALTHY\"} ingress.kubernetes.io/forwarding-rule: k8s2-fr-8rwy2qu4-default-single-svc-ingress-74hvjigm ingress.kubernetes.io/target-proxy: k8s2-tp-8rwy2qu4-default-single-svc-ingress-74hvjigm ingress.kubernetes.io/url-map: k8s2-um-8rwy2qu4-default-single-svc-ingress-74hvjigm Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 10m loadbalancer-controller UrlMap \"k8s2-um-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created Normal Sync 10m loadbalancer-controller TargetProxy \"k8s2-tp-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created Normal Sync 10m loadbalancer-controller ForwardingRule \"k8s2-fr-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created Normal IPChanged 10m loadbalancer-controller IP is now 34.117.137.144 Normal Sync 4m27s (x6 over 11m) loadbalancer-controller Scheduled for sync Step 5 Access deployed app via Ingress by navigating to the public IP of the ingress, which we got in step 3. Step 6 Finally execute the following command to cleanup the ingress. kubectl delete -f single-svc-ingress.yaml 2.3 Simple fanout \u00b6 Let's explore more sophisticated Ingress rules. This time we going to deploy Simple fanout type that allows for exposing multiple services on same host, but via different paths. This type is very handy when you running in CloudProvider and want to cut cost on creating LoadBalancers for each of you application. Or when running on prem. and it's required to expose multiple services via same host. Step 1 Let's start by a simple fanout with one service. Notice that we are not using a hostname here as for this lab, we don't want to go through setting up DNS. If you have a DNS setup, you can try with an actual hostname . Just make sure to update you DNS records with the Ingress external IP. cat <<EOF > fanout-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: fanout-ingress spec: rules: - http: paths: - path: /v1 pathType: ImplementationSpecific backend: service: name: web-v1 port: number: 8080 EOF Step 2 Create the Ingress resource kubectl apply -f fanout-ingress.yaml Step 3 Verify Ingress Resource kubectl get ing fanout-ingress Step 4 Verify that you can navigate to http://INGRESS_PUBLIC_IP/v1 and get the following Hello, world! Version: 1.0.0 Step 5 Verify that ingress is returning default backend - 404 page in a browser. Open the browser with public_ip: http://INGRESS_PUBLIC_IP/test Step 6 On your own modify the Ingress resource to add a second path so when we navigate to http://INGRESS_PUBLIC_IP/v2 users will get routed to web-v2 Step 7 Delete the demo Finally execute the following command to cleanup the application and ingress. kubectl delete -f fanout-ingress.yaml kubectl apply -f web-service-v1.yaml kubectl apply -f web-service-v2.yaml 2.4 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-networking","title":"Lab 10 Kubernetes Networking"},{"location":"Lab_10_Networking/#k8s-networking","text":"Objective: Kubernetes Network Policy Run applications and expose them via service via Ingress resource.","title":"K8s Networking"},{"location":"Lab_10_Networking/#0-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-networking \\ --zone us-central1-c \\ --enable-network-policy \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-networking --zone us-central1-c","title":"0 Create GKE Cluster"},{"location":"Lab_10_Networking/#1-network-policy","text":"","title":"1. Network Policy"},{"location":"Lab_10_Networking/#11-basic-policy-demo","text":"This guide will deploy pods in a Kubernetes Namespaces. Let\u2019s create the Namespace object for this guide. Step 1 Create policy-demo Namespace: kubectl create ns policy-demo Step 2 Than create a policy-demo nginx deployment in policy-demo namespace: kubectl create deployment --namespace=policy-demo nginx --replicas=2 --image=nginx Then create a policy-demo service: kubectl expose --namespace=policy-demo deployment nginx --port=80 Step 3 Ensure nginx service is accessible. In order to do that create busybox pod and try to access the nginx service. kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh Output: Waiting for pod `policy-demo/access-472357175-y0m47` to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q nginx -O - Success You should see a response from nginx . Great! Our service is accessible. You can exit the Pod now. Result Pods in a given namespace can be accessed by anyone. Step 4 Now let\u2019s turn on isolation in our policy-demo Namespace. Calico will then prevent connections to pods in this Namespace. In order for Calico to prevent connection to pods in a namespace, we first need to enable network isolation in the namespace by creating following NetworkPolicy : kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny namespace: policy-demo spec: podSelector: matchLabels: {} EOF Note Current Lab is using Calico v3.0. Older versions of Calico (2.1 and prior) used namespace annotation to deny all traffic. Step 5 Verify that all access to the nginx Service is blocked. We can see the effect by trying to access the Service again. In order to do that, run a busybox deployment and try to access the nginx service. kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh Waiting for pod policy-demo/access-472357175-y0m47 to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q --timeout=5 nginx -O - Result download timed out The request should time out after 5 seconds. By enabling isolation on the Namespace, we\u2019ve prevented access to the Service. Step 6 Allow Access using a NetworkPolicy Now, let\u2019s enable access to the nginx Service using a NetworkPolicy. This will allow incoming connections from our access Pod, but not from anywhere else. Create a network policy access-nginx with the following contents: kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: access-nginx namespace: policy-demo spec: podSelector: matchLabels: app: nginx ingress: - from: - podSelector: matchLabels: run: access EOF Notice The NetworkPolicy allows traffic from Pods with the label run: access to Pods with the label app: nginx . The labels are automatically added by kubectl and are based on the name of the resource. Step 7: We should now be able to access the Service from the access Pod. In order to check that create busybox deployment and try to access the nginx Service. kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh Waiting for pod policy-demo/access-472357175-y0m47 to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q --timeout=5 nginx -O - Step 8 Without closing a prompt to a container, open a new terminal and run kubectl get pods --show-labels -n policy-demo Output: NAME READY STATUS RESTARTS AGE LABELS access 1/1 Running 0 5s run=access nginx-6799fc88d8-5r64l 1/1 Running 0 17m app=nginx,pod-template-hash=6799fc88d8 nginx-6799fc88d8-dbtgk 1/1 Running 0 17m app=nginx,pod-template-hash=6799fc88d8 Result We can see the Labels run=access and app=nginx , that has been defined in Network Policy. Step 9 However, we still cannot access the Service from a Pod without the label run: access: Once again run busybox deployment and try to access the nginx service. kubectl run --namespace=policy-demo cant-access --rm -ti --image busybox /bin/sh Waiting for pod policy-demo/cant-access-472357175-y0m47 to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. / # wget -q --timeout=5 nginx -O - wget: download timed out / # Step 9 You can clean up the demo by deleting the demo Namespace: kubectl delete ns policy-demo","title":"1.1 Basic Policy Demo"},{"location":"Lab_10_Networking/#12-demo-stars-policy","text":"","title":"1.2 (Demo) Stars Policy"},{"location":"Lab_10_Networking/#121-deploy-3-tier-app","text":"Let's Deploy 3 tier-app: UI, frontend and backend service, as well as a client service. And configures network policy on each service. Step 1 Deploy Stars Namespace and management-ui apps inside it: kubectl create -f - <<EOF --- kind: Namespace apiVersion: v1 metadata: name: stars --- apiVersion: v1 kind: Namespace metadata: name: management-ui labels: role: management-ui --- apiVersion: v1 kind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 9001 targetPort: 9001 selector: role: management-ui --- apiVersion: apps/v1 kind: Deployment metadata: name: management-ui labels: role: management-ui namespace: management-ui spec: selector: matchLabels: role: management-ui replicas: 1 template: metadata: labels: role: management-ui spec: containers: - name: management-ui image: calico/star-collect:v0.1.0 imagePullPolicy: Always ports: - containerPort: 9001 EOF Step 2 Deploy backend application inside of the stars Namespace: Backend: kubectl create -f - <<EOF apiVersion: v1 kind: Service metadata: name: backend namespace: stars spec: ports: - port: 6379 targetPort: 6379 selector: role: backend --- apiVersion: apps/v1 kind: Deployment metadata: name: backend labels: role: backend namespace: stars spec: selector: matchLabels: role: backend replicas: 1 template: metadata: labels: role: backend spec: containers: - name: backend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=6379 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 6379 EOF Step 3 Deploy frontend application inside of the stars Namespace: kubectl create -f - <<EOF apiVersion: v1 kind: Service metadata: name: frontend namespace: stars spec: ports: - port: 80 targetPort: 80 selector: role: frontend --- apiVersion: apps/v1 kind: Deployment metadata: name: frontend labels: role: frontend namespace: stars spec: selector: matchLabels: role: frontend replicas: 1 template: metadata: labels: role: frontend spec: containers: - name: frontend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=80 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 80 EOF Step 4 Finally deploy client application inside of the client Namespace: kubectl create -f - <<EOF kind: Namespace apiVersion: v1 metadata: name: client labels: role: client --- apiVersion: apps/v1 kind: Deployment metadata: name: client labels: role: client namespace: client spec: selector: matchLabels: role: client replicas: 1 template: metadata: labels: role: client spec: containers: - name: client image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status ports: - containerPort: 9000 --- apiVersion: v1 kind: Service metadata: name: client namespace: client spec: ports: - port: 9000 targetPort: 9000 selector: role: client EOF Step 5 Wait for all the pods to enter Running state. kubectl get pods --all-namespaces --watch Note It may take several minutes to download the necessary Docker images for this demo. Step 6 Locate LoadBalancer IP: kubectl get svc -n management-ui Step 9 Open UI in you browser http://EXTERNAL-IP:9001 in a browser. Result Once all the pods are started, they should have full connectivity. You can see this by visiting the UI. Each service is represented by a single node in the graph. backend -> Node \"B\" frontend -> Node \"F\" client -> Node \"C\"","title":"1.2.1 Deploy 3 tier-app"},{"location":"Lab_10_Networking/#122-enable-isolation","text":"Step 1 Enable isolation Running the following commands will prevent all Ingress access to the frontend, backend, and client Services located in namespaces starts and client kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny namespace: stars spec: podSelector: matchLabels: {} ingress: [] --- kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny namespace: client spec: podSelector: matchLabels: {} policyTypes: - Ingress EOF Result Now that we've enabled isolation, the UI can no longer access the pods, and so they will no longer show up in the UI. Note You need to Refresh web-browser to see result. Step 2 Allow apps from stars and client namespace access UI service via NetworkPolicy rules: kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui --- kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: client name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui EOF Result refresh the UI - it should now show the Services, but they should not be able to access each other any more. Step 4 Create the NetworkPolicy to allow traffic from the frontend to the backend. kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 EOF Result Refresh the UI. You should see the following: The frontend can now access the backend (on TCP port 80 only). The backend cannot access the frontend at all. The client cannot access the frontend, nor can it access the backend. Step 5 Expose the frontend service to the client namespace. kubectl create -f - <<EOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80 EOF Success We isolated our app using NetworkPolicy ingress rules so that: The client can now access the frontend, but not the backend. Neither the frontend nor the backend can initiate connections to the client. The frontend can still access the backend.","title":"1.2.2 Enable isolation"},{"location":"Lab_10_Networking/#123-cleanup","text":"Step 1 You can clean up the demo by deleting the demo Namespaces: kubectl delete ns client stars management-ui","title":"1.2.3 Cleanup"},{"location":"Lab_10_Networking/#2-ingress","text":"In GKE, Ingress is implemented using Cloud Load Balancing. In other words, when yiu create an Ingress resource in your cluster, GKE creates an HTTP(S) LB for you and configure it. In this lab, we will create a fanout Ingress with two backends for two verisons of the hello-app application. Note If you want to experiment with a different type of Ingress, you can install Nginx Controller using Helm. You can also follow these [instructions] (https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md).","title":"2. Ingress"},{"location":"Lab_10_Networking/#21-deploy-an-application","text":"Step 1 Deploy the web application version 1, and its service cat <<EOF > web-service-v1.yaml apiVersion: apps/v1 kind: Deployment metadata: name: web-v1 namespace: default spec: selector: matchLabels: run: web version: v1 template: metadata: labels: run: web version: v1 spec: containers: - image: gcr.io/google-samples/hello-app:1.0 imagePullPolicy: IfNotPresent name: web ports: - containerPort: 8080 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: web-v1 namespace: default spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: run: web version: v1 type: NodePort EOF Step 2 Apply the resources to the cluster kubectl apply -f web-service-v1.yaml Step 3 Deploy the web application version 2, and its service cat <<EOF > web-service-v2.yaml apiVersion: apps/v1 kind: Deployment metadata: name: web-v2 namespace: default spec: selector: matchLabels: run: web version: v2 template: metadata: labels: run: web version: v2 spec: containers: - image: gcr.io/google-samples/hello-app:2.0 imagePullPolicy: IfNotPresent name: web ports: - containerPort: 8080 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: web-v2 namespace: default spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: run: web version: v2 type: NodePort EOF Step 4 Apply the resources to the cluster kubectl apply -f web-service-v2.yaml Step 5 Take a look at the pods created and view their IPs kubectl get pod -o wide Step 6 Take a look at the endpoints created and notice how they match the IP for the Pods in the matching service. kubectl get ep","title":"2.1 Deploy an Application"},{"location":"Lab_10_Networking/#22-creating-single-service-ingress-rule","text":"Let's expose our web-v1 service using Ingress! Step 1 To start, let's create an Ingress resource that directs traffic to v1 of the web Service cat <<EOF > single-svc-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: single-svc-ingress spec: defaultBackend: service: name: web-v1 port: number: 8080 EOF This manifest defines a Single Service Ingress rule , which makes sure all HTTP requests that will hit the external IP for the Cloud LB created for the Ingress, will be proxied to the web-v1 service on port 8080 . Step 2 Create the Ingress resource kubectl apply -f single-svc-ingress.yaml Step 3 Verify Ingress Resource kubectl get ing Output NAME CLASS HOSTS ADDRESS PORTS AGE single-svc-ingress <none> * 34.117.137.144 80 110s Notice that the Ingress controllers and load balancers may take a minute or two to allocate an IP address. Bingo! Step 4 descirbe the Ingress object created and notice the Google Cloud LB resources created. kubectl describe ing single-svc-ingress Notice that you might need to give it some time to get a similar output to what I have below. Output: Name: single-svc-ingress Namespace: default Address: 34.117.137.144 Default backend: web-v1:8080 (10.32.0.10:8080) Rules: Host Path Backends ---- ---- -------- * * web-v1:8080 (10.32.0.10:8080) Annotations: ingress.kubernetes.io/backends: {\"k8s-be-30059--a2b52ac0331abe29\":\"HEALTHY\"} ingress.kubernetes.io/forwarding-rule: k8s2-fr-8rwy2qu4-default-single-svc-ingress-74hvjigm ingress.kubernetes.io/target-proxy: k8s2-tp-8rwy2qu4-default-single-svc-ingress-74hvjigm ingress.kubernetes.io/url-map: k8s2-um-8rwy2qu4-default-single-svc-ingress-74hvjigm Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 10m loadbalancer-controller UrlMap \"k8s2-um-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created Normal Sync 10m loadbalancer-controller TargetProxy \"k8s2-tp-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created Normal Sync 10m loadbalancer-controller ForwardingRule \"k8s2-fr-8rwy2qu4-default-single-svc-ingress-74hvjigm\" created Normal IPChanged 10m loadbalancer-controller IP is now 34.117.137.144 Normal Sync 4m27s (x6 over 11m) loadbalancer-controller Scheduled for sync Step 5 Access deployed app via Ingress by navigating to the public IP of the ingress, which we got in step 3. Step 6 Finally execute the following command to cleanup the ingress. kubectl delete -f single-svc-ingress.yaml","title":"2.2 Creating Single Service Ingress rule"},{"location":"Lab_10_Networking/#23-simple-fanout","text":"Let's explore more sophisticated Ingress rules. This time we going to deploy Simple fanout type that allows for exposing multiple services on same host, but via different paths. This type is very handy when you running in CloudProvider and want to cut cost on creating LoadBalancers for each of you application. Or when running on prem. and it's required to expose multiple services via same host. Step 1 Let's start by a simple fanout with one service. Notice that we are not using a hostname here as for this lab, we don't want to go through setting up DNS. If you have a DNS setup, you can try with an actual hostname . Just make sure to update you DNS records with the Ingress external IP. cat <<EOF > fanout-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: fanout-ingress spec: rules: - http: paths: - path: /v1 pathType: ImplementationSpecific backend: service: name: web-v1 port: number: 8080 EOF Step 2 Create the Ingress resource kubectl apply -f fanout-ingress.yaml Step 3 Verify Ingress Resource kubectl get ing fanout-ingress Step 4 Verify that you can navigate to http://INGRESS_PUBLIC_IP/v1 and get the following Hello, world! Version: 1.0.0 Step 5 Verify that ingress is returning default backend - 404 page in a browser. Open the browser with public_ip: http://INGRESS_PUBLIC_IP/test Step 6 On your own modify the Ingress resource to add a second path so when we navigate to http://INGRESS_PUBLIC_IP/v2 users will get routed to web-v2 Step 7 Delete the demo Finally execute the following command to cleanup the application and ingress. kubectl delete -f fanout-ingress.yaml kubectl apply -f web-service-v1.yaml kubectl apply -f web-service-v2.yaml","title":"2.3 Simple fanout"},{"location":"Lab_10_Networking/#24-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-networking","title":"2.4 Cleaning Up"},{"location":"Lab_11_Storage/","text":"Kubernetes Storage Concepts \u00b6 Objective: Create a PersistentVolume (PV) referencing a disk in your environment. Learn how to dynamically provision volumes. Create a Single MySQL Deployment based on the Volume Claim Deploy a Replicated MySQL (Master/Slaves) with a StatefulSet controller. 0 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-storage \\ --zone us-central1-c \\ --enable-network-policy \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-storage us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-storage --zone us-central1-c 1 Dynamically Provision Volume \u00b6 Our Lab already has provisioned Default Storageclass created by Cluster Administrator. Step 1 Verify what storage class is used in our lab: kubectl get sc Output: NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE premium-rwo pd.csi.storage.gke.io Delete WaitForFirstConsumer true 5m2s standard (default) kubernetes.io/gce-pd Delete Immediate true 5m2s standard-rwo pd.csi.storage.gke.io Delete WaitForFirstConsumer true 5m2s Info The PROVISIONER field determines what volume plugin is used for provisioning PVs. standard provisions standard GCP PDs (In-tree volume plugin) standard-rwo provisions balanced GCP persistent disk (CSI based) premium-rwo provisions GCP SSD PDs (CSI based) Info The RECLAIMPOLICY field tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained , Recycled , or Deleted Delete reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk Retain reclaim policy allows for manual reclamation of the resource. When the persistent volume is released (this happens when you delete the claim that\u2019s bound to it), Kubernetes retains the volume. The cluster administrator must manually reclaim the volume. This is the default policy for manually created persistent volumes aka Static Provisioners Recycle - This option is deprecated and shouldn\u2019t be used as it may not be supported by the underlying volume plugin. This policy typically causes all files on the volume to be deleted and makes the persistent volume available again without the need to delete and recreate it. If no reclaimPolicy is specified when a StorageClass object is created, it will default to Delete. Info The VOLUMEBINDINGMODE field controls when volume binding and dynamic provisioning should occur. When unset, \"Immediate\" mode is used by default. Immediate The Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created. For storage backends that are topology-constrained and not globally accessible from all Nodes in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod's scheduling requirements. This may result in unschedulable Pods. WaitForFirstConsumer The volume is provisioned and bound to the claim when the first pod that uses this claim is created. This mode is used for topology-constrained volume types. The following plugins support WaitForFirstConsumer with dynamic provisioning: AWSElasticBlockStore GCEPersistentDisk AzureDisk kubectl describe sc Output: Name: premium-rwo IsDefaultClass: No Annotations: components.gke.io/component-name=pdcsi,components.gke.io/component-version=0.9.6,components.gke.io/layer=addon Provisioner: pd.csi.storage.gke.io Parameters: type=pd-ssd AllowVolumeExpansion: True MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: WaitForFirstConsumer Events: <none> Name: standard IsDefaultClass: Yes Annotations: storageclass.kubernetes.io/is-default-class=true Provisioner: kubernetes.io/gce-pd Parameters: type=pd-standard AllowVolumeExpansion: True MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> Name: standard-rwo IsDefaultClass: No Annotations: components.gke.io/layer=addon,storageclass.kubernetes.io/is-default-class=false Provisioner: pd.csi.storage.gke.io Parameters: type=pd-balanced AllowVolumeExpansion: True MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: WaitForFirstConsumer Events: <none> Summary The StorageClass resource specifies which provisioner should be used for provisioning the persistent volume when a persistent volume claim requests this storage class. The parameters defined in the storage class definition are passed to the provisioner and are specific to each provisioner plugin. Step 2 Let's create a new StorageClass for Regional PDs: cat > regionalpd-sc.yaml << EOF kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: regionalpd-storageclass provisioner: pd.csi.storage.gke.io parameters: type: pd-standard replication-type: regional-pd allowedTopologies: - matchLabelExpressions: - key: failure-domain.beta.kubernetes.io/zone values: - us-central1-b - us-central1-c EOF kubectl apply -f regionalpd-sc.yaml kubectl describe sc regionalpd-storageclass Result We've created a new StorageClass that uses GCP PD csi provisioner to create Regional Disks in GCP. Step 3 Create a Persistent Volume Claim (PVC) pvc-demo-ssd.yaml file that will Dynamically creates 30G GCP PD Persistent Volume (PV), using balanced GCP SSD disk Provisioner. cat > pvc-demo-ssd.yaml << EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: hello-web-disk spec: storageClassName: premium-rwo accessModes: - ReadWriteOnce resources: requests: storage: 30G EOF Step 3 Create a PVC: kubectl create -f pvc-demo-ssd.yaml Step 4 Verify STATUS of PVC kubectl get pvc Output: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongodb-pvc Pending standard-rwo 5s List PVs: kubectl get pv List GCP Disks: gcloud compute disks list Result What we see is that: PVC is in Pending PV is not created GCP PD is not created Question : why PVC is in Pending State ? Step 5 Let's review VolumeBindingMode of premium-rwo Storage Class: kubectl describe sc premium-rwo | grep VolumeBindingMode Info This StorageClass using VolumeBindingMode - WaitForFirstConsumer that creates PV only, when the first pod that uses this claim is created. Result Ok so if we want PV created we actually need to create a Pod first. This mode is especially important in the Cloud, as Pods can be created in different zones and so PV needs to be created in the correct zone as well. Step 6 Create a pod-volume-demo.yaml manifest that will create a Pod and mount Persistent Volume from hello-web-disk PVC . cat > pod-volume-demo.yaml << EOF kind: Pod apiVersion: v1 metadata: name: pvc-demo-pod spec: containers: - name: frontend image: nginx volumeMounts: - mountPath: \"/var/www/html\" name: pvc-demo-volume volumes: - name: pvc-demo-volume persistentVolumeClaim: claimName: hello-web-disk EOF Step 3 Create a Pod kubectl create -f pod-volume-demo.yaml Step 4 Verify STATUS of PVC now kubectl get pvc Output: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE hello-web-disk Bound pvc-4c006173-284e-4786-b752-028bdae768e9 28Gi RWO premium-rwo 15m Result PVC STATUS shows as Claim hello-web-disk as Bound , and that Claim has been attached to VOLUME pvc-4c006173-284e-4786-b752-028bdae768e9 with CAPACITY 28Gi and ACCESS MODES RWO via STORAGECLASS premium-rwo using SSD. List PVs: kubectl get pv Result PV STATUS shows as Bound to the CLAIM default/hello-web-disk, with RECLAIM POLICY Delete, meaning that SSD Disk will be deleted after PVC is deleted from Kubernetes. Output: NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-4c006173-284e-4786-b752-028bdae768e9 28Gi RWO Delete Bound default/hello-web-disk premium-rwo 14m List GCP Disks: gcloud compute disks list Output: pvc-4c006173-284e-4786-b752-028bdae768e9 us-central1-c zone 28 pd-ssd READ Result We can see that CSI Provisioner created SSD disk on GCP infrastructure. Step 5 Verify STATUS of Pod kubectl get pod Output: NAME READY STATUS RESTARTS AGE pvc-demo-pod 1/1 Running 0 21m Step 6 Delete PVC kubectl delete pod pvc-demo-pod kubeclt delete pvc hello-web-disk Step 6 Verify resources has been released: kubectl get pv,pvc,pods 2 Deploy Single MySQL Database with Volume \u00b6 You can run a stateful application by creating a Kubernetes Deployment and connecting it to an existing PersistentVolume using a PersistentVolumeClaim. Step 1 Below Manifest file going to creates 3 Kubernetes resources: PersistentVolumeClaim that looks for a 2G volume. This claim will be satisfied by dynamic provisioner general and appropriate PV going to be created Deployment that runs MySQL and references the PersistentVolumeClaim that is mounted in /var/lib/mysql. Service that depoyed as ClusterIP:None that lets the Service DNS name resolve directly to the Pod\u2019s IP kubectl create -f - <<EOF --- apiVersion: v1 kind: Service metadata: name: mysql spec: ports: - port: 3306 selector: app: mysql clusterIP: None --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pv-claim spec: storageClassName: general accessModes: - ReadWriteOnce resources: requests: storage: 2Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: mysql spec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6.37 name: mysql env: # Use secret in real use case - name: MYSQL_ROOT_PASSWORD value: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim EOF Note The password is defined inside of the Manifest as environment, which is not insecure. See Kubernetes Secrets for a secure solution. Result Single node MySQL database has been deployed with a Volume Step 3 Display information about the Deployment: kubectl describe deployment mysql Output: Name: mysql Namespace: default CreationTimestamp: Tue, 01 Nov 2016 11:18:45 -0700 Labels: app=mysql Annotations: deployment.kubernetes.io/revision=1 Selector: app=mysql Replicas: 1 desired | 1 updated | 1 total | 0 available | 1 unavailable StrategyType: Recreate MinReadySeconds: 0 Pod Template: Labels: app=mysql Containers: mysql: Image: mysql:5.6 Port: 3306/TCP Environment: MYSQL_ROOT_PASSWORD: password Mounts: /var/lib/mysql from mysql-persistent-storage (rw) Volumes: mysql-persistent-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: mysql-pv-claim ReadOnly: false Conditions: Type Status Reason ---- ------ ------ Available False MinimumReplicasUnavailable Progressing True ReplicaSetUpdated OldReplicaSets: <none> NewReplicaSet: mysql-63082529 (1/1 replicas created) Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 33s 33s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set mysql-63082529 to 1 Step 4 List the pods created by the Deployment: kubectl get pods -l app=mysql Output: NAME READY STATUS RESTARTS AGE mysql-63082529-2z3ki 1/1 Running 0 3m Step 5 Inspect the PersistentVolumeClaim: kubectl describe pvc mysql-pv-claim Output: Name: mysql-pv-claim Namespace: default StorageClass: Status: Bound Volume: mysql-pv Labels: <none> Annotations: pv.kubernetes.io/bind-completed=yes pv.kubernetes.io/bound-by-controller=yes Capacity: 20Gi Access Modes: RWO Events: <none> Step 6 Access the MySQL instance The Service option clusterIP: None lets the Service DNS name resolve directly to the Pod's IP address. This is optimal when you have only one Pod behind a Service and you don't intend to increase the number of Pods. Run a MySQL client to connect to the server: kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h mysql -ppassword This command creates a new Pod in the cluster running a MySQL client and connects it to the server through the Service. If it connects, you know your stateful MySQL database is up and running. Waiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | +--------------------+ 3 rows in set (0.00 sec) mysql> exit Step 7 Update the MySQL instance The image or any other part of the Deployment can be updated as usual with the kubectl apply command. Important Don't scale the app. This setup is for single-instance apps only. The underlying PersistentVolume can only be mounted to one Pod. For clustered stateful apps, see the StatefulSet documentation . Use strategy: type: Recreate in the Deployment configuration YAML file. This instructs Kubernetes to not use rolling updates. Rolling updates will not work, as you cannot have more than one Pod running at a time. The Recreate strategy will stop the first pod before creating a new one with the updated configuration. Step 7 Delete the MySQL instance Delete the deployed objects by name: kubectl delete deployment,svc mysql kubectl delete pvc mysql-pv-claim Since we used a dynamic provisioner, it automatically deletes the PersistentVolume when it sees that you deleted the PersistentVolumeClaim. Note If PersistentVolume was manually provisioned, it is requrire to manually delete it, as well as release the underlying resource. 3 Deploy StatefulSet \u00b6 3.1 Deploy Replicated MySQL (Master/Slaves) Cluster using StatefulSet. \u00b6 Our Replicated MySQL deployment going to consists of: 1 ConfigMap 2 Services 1 StatefulSet Step 1 Create the ConfigMap (just copy paste below): cat <<EOF | kubectl create -f - apiVersion: v1 kind: ConfigMap metadata: name: mysql labels: app: mysql data: master.cnf: | # Apply this config only on the master. [mysqld] log-bin slave.cnf: | # Apply this config only on slaves. [mysqld] super-read-only EOF Result This ConfigMap provides overrides that let you independently control configuration on the MySQL master and slaves. In this case: master going to be able to serve replication logs to slaves slaves to reject any writes that don't come via replication. There's nothing special about the ConfigMap itself that causes different portions to apply to different Pods. Each Pod decides which portion to look at as it's initializing, based on information provided by the StatefulSet controller. Step 2 Create 2 Services (just copy paste below): cat <<EOF | kubectl create -f - # Headless service for stable DNS entries of StatefulSet members. apiVersion: v1 kind: Service metadata: name: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- # Client service for connecting to any MySQL instance for reads. # For writes, you must instead connect to the master: mysql-0.mysql. apiVersion: v1 kind: Service metadata: name: mysql-read labels: app: mysql spec: ports: - name: mysql port: 3306 selector: app: mysql EOF Info The Headless Service provides a home for the DNS entries that the StatefulSet controller creates for each Pod that's part of the set. Because the Headless Service is named mysql, the Pods are accessible by resolving .mysql from within any other Pod in the same Kubernetes cluster and namespace. The Client Service, called mysql-read, is a normal Service with its own cluster IP that distributes connections across all MySQL Pods that report being Ready. The set of potential endpoints includes the MySQL master and all slaves. Note Only read queries can use the load-balanced Client Service. Because there is only one MySQL master, clients should connect directly to the MySQL master Pod (through its DNS entry within the Headless Service) to execute writes. Step 3 Create the StatefulSet from mysql-statefulset.yaml manifest: cat <<EOF | kubectl create -f - apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: mysql spec: serviceName: mysql replicas: 3 template: metadata: labels: app: mysql annotations: pod.beta.kubernetes.io/init-containers: '[ { \"name\": \"init-mysql\", \"image\": \"mysql:5.7\", \"command\": [\"bash\", \"-c\", \" set -ex\\n # Generate mysql server-id from pod ordinal index.\\n [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\\n ordinal=${BASH_REMATCH[1]}\\n echo [mysqld] > /mnt/conf.d/server-id.cnf\\n # Add an offset to avoid reserved server-id=0 value.\\n echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n # Copy appropriate conf.d files from config-map to emptyDir.\\n if [[ $ordinal -eq 0 ]]; then\\n cp /mnt/config-map/master.cnf /mnt/conf.d/\\n else\\n cp /mnt/config-map/slave.cnf /mnt/conf.d/\\n fi\\n \"], \"volumeMounts\": [ {\"name\": \"conf\", \"mountPath\": \"/mnt/conf.d\"}, {\"name\": \"config-map\", \"mountPath\": \"/mnt/config-map\"} ] }, { \"name\": \"clone-mysql\", \"image\": \"gcr.io/google-samples/xtrabackup:1.0\", \"command\": [\"bash\", \"-c\", \" set -ex\\n # Skip the clone if data already exists.\\n [[ -d /var/lib/mysql/mysql ]] && exit 0\\n # Skip the clone on master (ordinal index 0).\\n [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\\n ordinal=${BASH_REMATCH[1]}\\n [[ $ordinal -eq 0 ]] && exit 0\\n # Clone data from previous peer.\\n ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\\n # Prepare the backup.\\n xtrabackup --prepare --target-dir=/var/lib/mysql\\n \"], \"volumeMounts\": [ {\"name\": \"data\", \"mountPath\": \"/var/lib/mysql\", \"subPath\": \"mysql\"}, {\"name\": \"conf\", \"mountPath\": \"/etc/mysql/conf.d\"} ] } ]' spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ALLOW_EMPTY_PASSWORD value: \"1\" ports: - name: mysql containerPort: 3306 volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 400m memory: 500Mi livenessProbe: exec: command: [\"mysqladmin\", \"ping\"] initialDelaySeconds: 30 timeoutSeconds: 5 readinessProbe: exec: # Check we can execute queries over TCP (skip-networking is off). command: [\"mysql\", \"-h\", \"127.0.0.1\", \"-e\", \"SELECT 1\"] initialDelaySeconds: 5 timeoutSeconds: 1 - name: xtrabackup image: gcr.io/google-samples/xtrabackup:1.0 ports: - name: xtrabackup containerPort: 3307 command: - bash - \"-c\" - | set -ex cd /var/lib/mysql # Determine binlog position of cloned data, if any. if [[ -f xtrabackup_slave_info ]]; then # XtraBackup already generated a partial \"CHANGE MASTER TO\" query # because we're cloning from an existing slave. mv xtrabackup_slave_info change_master_to.sql.in # Ignore xtrabackup_binlog_info in this case (it's useless). rm -f xtrabackup_binlog_info elif [[ -f xtrabackup_binlog_info ]]; then # We're cloning directly from master. Parse binlog position. [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1 rm xtrabackup_binlog_info echo \"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\ MASTER_LOG_POS=${BASH_REMATCH[2]}\" > change_master_to.sql.in fi # Check if we need to complete a clone by starting replication. if [[ -f change_master_to.sql.in ]]; then echo \"Waiting for mysqld to be ready (accepting connections)\" until mysql -h 127.0.0.1 -e \"SELECT 1\"; do sleep 1; done echo \"Initializing replication from clone position\" # In case of container restart, attempt this at-most-once. mv change_master_to.sql.in change_master_to.sql.orig mysql -h 127.0.0.1 <<EOF $(<change_master_to.sql.orig), MASTER_HOST='mysql-0.mysql', MASTER_USER='root', MASTER_PASSWORD='', MASTER_CONNECT_RETRY=10; START SLAVE; EOF fi # Start a server to send backups when requested by peers. exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \\ \"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root\" volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 100m memory: 100Mi volumes: - name: conf emptyDir: {} - name: config-map configMap: name: mysql volumeClaimTemplates: - metadata: name: data annotations: volume.beta.kubernetes.io/storage-class: general spec: accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 5Gi EOF Step 4 Monitor Deployment Process/Sequence watch kubectl get statefulset,pvc,pv,pods -l app=mysql Press Ctrl+C to cancel the watch when all pods, pvc and statefulset provisioned. Result The StatefulSet controller started Pods one at a time, in order by their ordinal index . It waits until each Pod reports being Ready before starting the next one. In addition, the controller assigned each Pod a unique , stable name of the form <statefulset-name>-<ordinal-index> . In this case, that results in Pods named mysql-0 , mysql-1 , and mysql-2 . The Pod template in the above StatefulSet manifest takes advantage of these properties to perform orderly startup of MySQL replication. Generating configuration Before starting any of the containers in the Pod spec, the Pod first runs any [Init Containers] in the order defined. The first Init Container, named init-mysql , generates special MySQL config files based on the ordinal index. The script determines its own ordinal index by extracting it from the end of the Pod name, which is returned by the hostname command. Then it saves the ordinal (with a numeric offset to avoid reserved values) into a file called server-id.cnf in the MySQL conf.d directory. This translates the unique, stable identity provided by the StatefulSet controller into the domain of MySQL server IDs, which require the same properties. The script in the init-mysql container also applies either master.cnf or slave.cnf from the ConfigMap by copying the contents into conf.d . Because the example topology consists of a single MySQL master and any number of slaves, the script simply assigns ordinal 0 to be the master, and everyone else to be slaves. Combined with the StatefulSet controller's deployment order guarantee ensures the MySQL master is Ready before creating slaves, so they can begin replicating. Cloning existing data In general, when a new Pod joins the set as a slave, it must assume the MySQL master might already have data on it. It also must assume that the replication logs might not go all the way back to the beginning of time. These conservative assumptions are the key to allow a running StatefulSet to scale up and down over time, rather than being fixed at its initial size. The second Init Container, named clone-mysql , performs a clone operation on a slave Pod the first time it starts up on an empty PersistentVolume. That means it copies all existing data from another running Pod, so its local state is consistent enough to begin replicating from the master. MySQL itself does not provide a mechanism to do this, so the example uses a popular open-source tool called Percona XtraBackup. During the clone, the source MySQL server might suffer reduced performance. To minimize impact on the MySQL master, the script instructs each Pod to clone from the Pod whose ordinal index is one lower. This works because the StatefulSet controller always ensures Pod N is Ready before starting Pod N+1 . Starting replication After the Init Containers complete successfully, the regular containers run. The MySQL Pods consist of a mysql container that runs the actual mysqld server, and an xtrabackup container that acts as a sidecar . The xtrabackup sidecar looks at the cloned data files and determines if it's necessary to initialize MySQL replication on the slave. If so, it waits for mysqld to be ready and then executes the CHANGE MASTER TO and START SLAVE commands with replication parameters extracted from the XtraBackup clone files. Once a slave begins replication, it remembers its MySQL master and reconnects automatically if the server restarts or the connection dies. Also, because slaves look for the master at its stable DNS name ( mysql-0.mysql ), they automatically find the master even if it gets a new Pod IP due to being rescheduled. Lastly, after starting replication, the xtrabackup container listens for connections from other Pods requesting a data clone. This server remains up indefinitely in case the StatefulSet scales up, or in case the next Pod loses its PersistentVolumeClaim and needs to redo the clone. 3.2 Test the MySQL cluster app and running \u00b6 Step 1 Create Database, Table and message on Master MySQL database Send test queries to the MySQL master (hostname mysql-0.mysql ) by running a temporary container with the mysql:5.7 image and running the mysql client binary. kubectl run mysql-client --image = mysql:5.7 -i --rm --restart = Never -- \\ mysql -h mysql-0.mysql <<EOF CREATE DATABASE test; CREATE TABLE test.messages (message VARCHAR(250)); INSERT INTO test.messages VALUES ('hello'); EOF Step 2 Verify that recorded data has been replicated to the slaves: Use the hostname mysql-read to send test queries to any server that reports being Ready: kubectl run mysql-client --image = mysql:5.7 -i -t --rm --restart = Never -- \\ mysql -h mysql-read -e \"SELECT * FROM test.messages\" You should get output like this: Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false +---------+ | message | +---------+ | hello | +---------+ pod \"mysql-client\" deleted Step 3 Demonstrate that the mysql-read Service distributes connections across servers, you can run SELECT @@hostname in a loop: kubectl run mysql-client-loop --image = mysql:5.7 -i -t --rm --restart = Never -- \\ bash -ic \"while sleep 1; do mysql -h mysql-read -e 'SELECT @@hostname,NOW()'; done\" You should see the reported @@hostname change randomly, because a different endpoint might be selected upon each connection attempt: +-------------+---------------------+ | @@hostname | NOW() | +-------------+---------------------+ | 100 | 2006-01-02 15:04:05 | +-------------+---------------------+ +-------------+---------------------+ | @@hostname | NOW() | +-------------+---------------------+ | 102 | 2006-01-02 15:04:06 | +-------------+---------------------+ +-------------+---------------------+ | @@hostname | NOW() | +-------------+---------------------+ | 101 | 2006-01-02 15:04:07 | +-------------+---------------------+ You can press Ctrl+C when you want to stop the loop, but it's useful to keep it running in another window so you can see the effects of the following steps. 3.3 Delete Pods \u00b6 The StatefulSet recreates Pods if they're deleted, similar to what a ReplicaSet does for stateless Pods. Step 1 Try to fail Mysql cluster by deleting mysql-2 pod: kubectl delete pod mysql-2 The StatefulSet controller notices that no mysql-2 Pod exists anymore, and creates a new one with the same name and linked to the same PersistentVolumeClaim. Step 5 Monitor Deployment Process/Sequence watch kubectl get statefulset,pvc,pv,pods -l app=mysql Result You should see server ID 102 disappear from the loop output for a while and then return on its own. 3.4 Scaling the number of slaves \u00b6 With MySQL replication, you can scale your read query capacity by adding slaves. With StatefulSet, you can do this with a single command: Step 1 Scale up statefulset: kubectl scale statefulset mysql --replicas = 5 Step 2 Watch the new Pods come up by running: kubectl get pods -l app = mysql --watch Once they're up, you should see server IDs 103 and 104 start appearing in the SELECT @@server_id loop output. You can also verify that these new servers have the data you added before they existed: Step 3 Watch the new Pods come up by running: kubectl run mysql-client --image = mysql:5.7 -i -t --rm --restart = Never -- \\ mysql -h mysql-3.mysql -e \"SELECT * FROM test.messages\" Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false +---------+ | message | +---------+ | hello | +---------+ pod \"mysql-client\" deleted Step 4 Scaling back down is also seamless: kubectl scale statefulset mysql --replicas = 3 Note, however, that while scaling up creates new PersistentVolumeClaims automatically, scaling down does not automatically delete these PVCs. This gives you the choice to keep those initialized PVCs around to make scaling back up quicker, or to extract data before deleting them. You can see this by running: kubectl get pvc -l app = mysql Which shows that all 5 PVCs still exist, despite having scaled the StatefulSet down to 3: NAME STATUS VOLUME CAPACITY ACCESSMODES AGE data-mysql-0 Bound pvc-8acbf5dc-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-1 Bound pvc-8ad39820-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-2 Bound pvc-8ad69a6d-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-3 Bound pvc-50043c45-b1c5-11e6-93fa-42010a800002 10Gi RWO 2m data-mysql-4 Bound pvc-500a9957-b1c5-11e6-93fa-42010a800002 10Gi RWO 2m If you don't intend to reuse the extra PVCs, you can delete them: kubectl delete pvc data-mysql-3 kubectl delete pvc data-mysql-4 Step 5 Cancel the SELECT @@server_id loop by pressing Ctrl+C in its terminal, or running the following from another terminal: kubectl delete pod mysql-client-loop --now Step 6 Delete the StatefulSet. This also begins terminating the Pods. kubectl delete statefulset mysql 2.4 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-networking","title":"Lab 11 Kubernetes Storage"},{"location":"Lab_11_Storage/#kubernetes-storage-concepts","text":"Objective: Create a PersistentVolume (PV) referencing a disk in your environment. Learn how to dynamically provision volumes. Create a Single MySQL Deployment based on the Volume Claim Deploy a Replicated MySQL (Master/Slaves) with a StatefulSet controller.","title":"Kubernetes Storage Concepts"},{"location":"Lab_11_Storage/#0-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-storage \\ --zone us-central1-c \\ --enable-network-policy \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-storage us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-storage --zone us-central1-c","title":"0 Create GKE Cluster"},{"location":"Lab_11_Storage/#1-dynamically-provision-volume","text":"Our Lab already has provisioned Default Storageclass created by Cluster Administrator. Step 1 Verify what storage class is used in our lab: kubectl get sc Output: NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE premium-rwo pd.csi.storage.gke.io Delete WaitForFirstConsumer true 5m2s standard (default) kubernetes.io/gce-pd Delete Immediate true 5m2s standard-rwo pd.csi.storage.gke.io Delete WaitForFirstConsumer true 5m2s Info The PROVISIONER field determines what volume plugin is used for provisioning PVs. standard provisions standard GCP PDs (In-tree volume plugin) standard-rwo provisions balanced GCP persistent disk (CSI based) premium-rwo provisions GCP SSD PDs (CSI based) Info The RECLAIMPOLICY field tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained , Recycled , or Deleted Delete reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk Retain reclaim policy allows for manual reclamation of the resource. When the persistent volume is released (this happens when you delete the claim that\u2019s bound to it), Kubernetes retains the volume. The cluster administrator must manually reclaim the volume. This is the default policy for manually created persistent volumes aka Static Provisioners Recycle - This option is deprecated and shouldn\u2019t be used as it may not be supported by the underlying volume plugin. This policy typically causes all files on the volume to be deleted and makes the persistent volume available again without the need to delete and recreate it. If no reclaimPolicy is specified when a StorageClass object is created, it will default to Delete. Info The VOLUMEBINDINGMODE field controls when volume binding and dynamic provisioning should occur. When unset, \"Immediate\" mode is used by default. Immediate The Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created. For storage backends that are topology-constrained and not globally accessible from all Nodes in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod's scheduling requirements. This may result in unschedulable Pods. WaitForFirstConsumer The volume is provisioned and bound to the claim when the first pod that uses this claim is created. This mode is used for topology-constrained volume types. The following plugins support WaitForFirstConsumer with dynamic provisioning: AWSElasticBlockStore GCEPersistentDisk AzureDisk kubectl describe sc Output: Name: premium-rwo IsDefaultClass: No Annotations: components.gke.io/component-name=pdcsi,components.gke.io/component-version=0.9.6,components.gke.io/layer=addon Provisioner: pd.csi.storage.gke.io Parameters: type=pd-ssd AllowVolumeExpansion: True MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: WaitForFirstConsumer Events: <none> Name: standard IsDefaultClass: Yes Annotations: storageclass.kubernetes.io/is-default-class=true Provisioner: kubernetes.io/gce-pd Parameters: type=pd-standard AllowVolumeExpansion: True MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> Name: standard-rwo IsDefaultClass: No Annotations: components.gke.io/layer=addon,storageclass.kubernetes.io/is-default-class=false Provisioner: pd.csi.storage.gke.io Parameters: type=pd-balanced AllowVolumeExpansion: True MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: WaitForFirstConsumer Events: <none> Summary The StorageClass resource specifies which provisioner should be used for provisioning the persistent volume when a persistent volume claim requests this storage class. The parameters defined in the storage class definition are passed to the provisioner and are specific to each provisioner plugin. Step 2 Let's create a new StorageClass for Regional PDs: cat > regionalpd-sc.yaml << EOF kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: regionalpd-storageclass provisioner: pd.csi.storage.gke.io parameters: type: pd-standard replication-type: regional-pd allowedTopologies: - matchLabelExpressions: - key: failure-domain.beta.kubernetes.io/zone values: - us-central1-b - us-central1-c EOF kubectl apply -f regionalpd-sc.yaml kubectl describe sc regionalpd-storageclass Result We've created a new StorageClass that uses GCP PD csi provisioner to create Regional Disks in GCP. Step 3 Create a Persistent Volume Claim (PVC) pvc-demo-ssd.yaml file that will Dynamically creates 30G GCP PD Persistent Volume (PV), using balanced GCP SSD disk Provisioner. cat > pvc-demo-ssd.yaml << EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: hello-web-disk spec: storageClassName: premium-rwo accessModes: - ReadWriteOnce resources: requests: storage: 30G EOF Step 3 Create a PVC: kubectl create -f pvc-demo-ssd.yaml Step 4 Verify STATUS of PVC kubectl get pvc Output: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongodb-pvc Pending standard-rwo 5s List PVs: kubectl get pv List GCP Disks: gcloud compute disks list Result What we see is that: PVC is in Pending PV is not created GCP PD is not created Question : why PVC is in Pending State ? Step 5 Let's review VolumeBindingMode of premium-rwo Storage Class: kubectl describe sc premium-rwo | grep VolumeBindingMode Info This StorageClass using VolumeBindingMode - WaitForFirstConsumer that creates PV only, when the first pod that uses this claim is created. Result Ok so if we want PV created we actually need to create a Pod first. This mode is especially important in the Cloud, as Pods can be created in different zones and so PV needs to be created in the correct zone as well. Step 6 Create a pod-volume-demo.yaml manifest that will create a Pod and mount Persistent Volume from hello-web-disk PVC . cat > pod-volume-demo.yaml << EOF kind: Pod apiVersion: v1 metadata: name: pvc-demo-pod spec: containers: - name: frontend image: nginx volumeMounts: - mountPath: \"/var/www/html\" name: pvc-demo-volume volumes: - name: pvc-demo-volume persistentVolumeClaim: claimName: hello-web-disk EOF Step 3 Create a Pod kubectl create -f pod-volume-demo.yaml Step 4 Verify STATUS of PVC now kubectl get pvc Output: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE hello-web-disk Bound pvc-4c006173-284e-4786-b752-028bdae768e9 28Gi RWO premium-rwo 15m Result PVC STATUS shows as Claim hello-web-disk as Bound , and that Claim has been attached to VOLUME pvc-4c006173-284e-4786-b752-028bdae768e9 with CAPACITY 28Gi and ACCESS MODES RWO via STORAGECLASS premium-rwo using SSD. List PVs: kubectl get pv Result PV STATUS shows as Bound to the CLAIM default/hello-web-disk, with RECLAIM POLICY Delete, meaning that SSD Disk will be deleted after PVC is deleted from Kubernetes. Output: NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-4c006173-284e-4786-b752-028bdae768e9 28Gi RWO Delete Bound default/hello-web-disk premium-rwo 14m List GCP Disks: gcloud compute disks list Output: pvc-4c006173-284e-4786-b752-028bdae768e9 us-central1-c zone 28 pd-ssd READ Result We can see that CSI Provisioner created SSD disk on GCP infrastructure. Step 5 Verify STATUS of Pod kubectl get pod Output: NAME READY STATUS RESTARTS AGE pvc-demo-pod 1/1 Running 0 21m Step 6 Delete PVC kubectl delete pod pvc-demo-pod kubeclt delete pvc hello-web-disk Step 6 Verify resources has been released: kubectl get pv,pvc,pods","title":"1 Dynamically Provision Volume"},{"location":"Lab_11_Storage/#2-deploy-single-mysql-database-with-volume","text":"You can run a stateful application by creating a Kubernetes Deployment and connecting it to an existing PersistentVolume using a PersistentVolumeClaim. Step 1 Below Manifest file going to creates 3 Kubernetes resources: PersistentVolumeClaim that looks for a 2G volume. This claim will be satisfied by dynamic provisioner general and appropriate PV going to be created Deployment that runs MySQL and references the PersistentVolumeClaim that is mounted in /var/lib/mysql. Service that depoyed as ClusterIP:None that lets the Service DNS name resolve directly to the Pod\u2019s IP kubectl create -f - <<EOF --- apiVersion: v1 kind: Service metadata: name: mysql spec: ports: - port: 3306 selector: app: mysql clusterIP: None --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pv-claim spec: storageClassName: general accessModes: - ReadWriteOnce resources: requests: storage: 2Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: mysql spec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6.37 name: mysql env: # Use secret in real use case - name: MYSQL_ROOT_PASSWORD value: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim EOF Note The password is defined inside of the Manifest as environment, which is not insecure. See Kubernetes Secrets for a secure solution. Result Single node MySQL database has been deployed with a Volume Step 3 Display information about the Deployment: kubectl describe deployment mysql Output: Name: mysql Namespace: default CreationTimestamp: Tue, 01 Nov 2016 11:18:45 -0700 Labels: app=mysql Annotations: deployment.kubernetes.io/revision=1 Selector: app=mysql Replicas: 1 desired | 1 updated | 1 total | 0 available | 1 unavailable StrategyType: Recreate MinReadySeconds: 0 Pod Template: Labels: app=mysql Containers: mysql: Image: mysql:5.6 Port: 3306/TCP Environment: MYSQL_ROOT_PASSWORD: password Mounts: /var/lib/mysql from mysql-persistent-storage (rw) Volumes: mysql-persistent-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: mysql-pv-claim ReadOnly: false Conditions: Type Status Reason ---- ------ ------ Available False MinimumReplicasUnavailable Progressing True ReplicaSetUpdated OldReplicaSets: <none> NewReplicaSet: mysql-63082529 (1/1 replicas created) Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 33s 33s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set mysql-63082529 to 1 Step 4 List the pods created by the Deployment: kubectl get pods -l app=mysql Output: NAME READY STATUS RESTARTS AGE mysql-63082529-2z3ki 1/1 Running 0 3m Step 5 Inspect the PersistentVolumeClaim: kubectl describe pvc mysql-pv-claim Output: Name: mysql-pv-claim Namespace: default StorageClass: Status: Bound Volume: mysql-pv Labels: <none> Annotations: pv.kubernetes.io/bind-completed=yes pv.kubernetes.io/bound-by-controller=yes Capacity: 20Gi Access Modes: RWO Events: <none> Step 6 Access the MySQL instance The Service option clusterIP: None lets the Service DNS name resolve directly to the Pod's IP address. This is optimal when you have only one Pod behind a Service and you don't intend to increase the number of Pods. Run a MySQL client to connect to the server: kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h mysql -ppassword This command creates a new Pod in the cluster running a MySQL client and connects it to the server through the Service. If it connects, you know your stateful MySQL database is up and running. Waiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | +--------------------+ 3 rows in set (0.00 sec) mysql> exit Step 7 Update the MySQL instance The image or any other part of the Deployment can be updated as usual with the kubectl apply command. Important Don't scale the app. This setup is for single-instance apps only. The underlying PersistentVolume can only be mounted to one Pod. For clustered stateful apps, see the StatefulSet documentation . Use strategy: type: Recreate in the Deployment configuration YAML file. This instructs Kubernetes to not use rolling updates. Rolling updates will not work, as you cannot have more than one Pod running at a time. The Recreate strategy will stop the first pod before creating a new one with the updated configuration. Step 7 Delete the MySQL instance Delete the deployed objects by name: kubectl delete deployment,svc mysql kubectl delete pvc mysql-pv-claim Since we used a dynamic provisioner, it automatically deletes the PersistentVolume when it sees that you deleted the PersistentVolumeClaim. Note If PersistentVolume was manually provisioned, it is requrire to manually delete it, as well as release the underlying resource.","title":"2 Deploy Single MySQL Database with Volume"},{"location":"Lab_11_Storage/#3-deploy-statefulset","text":"","title":"3 Deploy StatefulSet"},{"location":"Lab_11_Storage/#31-deploy-replicated-mysql-masterslaves-cluster-using-statefulset","text":"Our Replicated MySQL deployment going to consists of: 1 ConfigMap 2 Services 1 StatefulSet Step 1 Create the ConfigMap (just copy paste below): cat <<EOF | kubectl create -f - apiVersion: v1 kind: ConfigMap metadata: name: mysql labels: app: mysql data: master.cnf: | # Apply this config only on the master. [mysqld] log-bin slave.cnf: | # Apply this config only on slaves. [mysqld] super-read-only EOF Result This ConfigMap provides overrides that let you independently control configuration on the MySQL master and slaves. In this case: master going to be able to serve replication logs to slaves slaves to reject any writes that don't come via replication. There's nothing special about the ConfigMap itself that causes different portions to apply to different Pods. Each Pod decides which portion to look at as it's initializing, based on information provided by the StatefulSet controller. Step 2 Create 2 Services (just copy paste below): cat <<EOF | kubectl create -f - # Headless service for stable DNS entries of StatefulSet members. apiVersion: v1 kind: Service metadata: name: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- # Client service for connecting to any MySQL instance for reads. # For writes, you must instead connect to the master: mysql-0.mysql. apiVersion: v1 kind: Service metadata: name: mysql-read labels: app: mysql spec: ports: - name: mysql port: 3306 selector: app: mysql EOF Info The Headless Service provides a home for the DNS entries that the StatefulSet controller creates for each Pod that's part of the set. Because the Headless Service is named mysql, the Pods are accessible by resolving .mysql from within any other Pod in the same Kubernetes cluster and namespace. The Client Service, called mysql-read, is a normal Service with its own cluster IP that distributes connections across all MySQL Pods that report being Ready. The set of potential endpoints includes the MySQL master and all slaves. Note Only read queries can use the load-balanced Client Service. Because there is only one MySQL master, clients should connect directly to the MySQL master Pod (through its DNS entry within the Headless Service) to execute writes. Step 3 Create the StatefulSet from mysql-statefulset.yaml manifest: cat <<EOF | kubectl create -f - apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: mysql spec: serviceName: mysql replicas: 3 template: metadata: labels: app: mysql annotations: pod.beta.kubernetes.io/init-containers: '[ { \"name\": \"init-mysql\", \"image\": \"mysql:5.7\", \"command\": [\"bash\", \"-c\", \" set -ex\\n # Generate mysql server-id from pod ordinal index.\\n [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\\n ordinal=${BASH_REMATCH[1]}\\n echo [mysqld] > /mnt/conf.d/server-id.cnf\\n # Add an offset to avoid reserved server-id=0 value.\\n echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n # Copy appropriate conf.d files from config-map to emptyDir.\\n if [[ $ordinal -eq 0 ]]; then\\n cp /mnt/config-map/master.cnf /mnt/conf.d/\\n else\\n cp /mnt/config-map/slave.cnf /mnt/conf.d/\\n fi\\n \"], \"volumeMounts\": [ {\"name\": \"conf\", \"mountPath\": \"/mnt/conf.d\"}, {\"name\": \"config-map\", \"mountPath\": \"/mnt/config-map\"} ] }, { \"name\": \"clone-mysql\", \"image\": \"gcr.io/google-samples/xtrabackup:1.0\", \"command\": [\"bash\", \"-c\", \" set -ex\\n # Skip the clone if data already exists.\\n [[ -d /var/lib/mysql/mysql ]] && exit 0\\n # Skip the clone on master (ordinal index 0).\\n [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\\n ordinal=${BASH_REMATCH[1]}\\n [[ $ordinal -eq 0 ]] && exit 0\\n # Clone data from previous peer.\\n ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\\n # Prepare the backup.\\n xtrabackup --prepare --target-dir=/var/lib/mysql\\n \"], \"volumeMounts\": [ {\"name\": \"data\", \"mountPath\": \"/var/lib/mysql\", \"subPath\": \"mysql\"}, {\"name\": \"conf\", \"mountPath\": \"/etc/mysql/conf.d\"} ] } ]' spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ALLOW_EMPTY_PASSWORD value: \"1\" ports: - name: mysql containerPort: 3306 volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 400m memory: 500Mi livenessProbe: exec: command: [\"mysqladmin\", \"ping\"] initialDelaySeconds: 30 timeoutSeconds: 5 readinessProbe: exec: # Check we can execute queries over TCP (skip-networking is off). command: [\"mysql\", \"-h\", \"127.0.0.1\", \"-e\", \"SELECT 1\"] initialDelaySeconds: 5 timeoutSeconds: 1 - name: xtrabackup image: gcr.io/google-samples/xtrabackup:1.0 ports: - name: xtrabackup containerPort: 3307 command: - bash - \"-c\" - | set -ex cd /var/lib/mysql # Determine binlog position of cloned data, if any. if [[ -f xtrabackup_slave_info ]]; then # XtraBackup already generated a partial \"CHANGE MASTER TO\" query # because we're cloning from an existing slave. mv xtrabackup_slave_info change_master_to.sql.in # Ignore xtrabackup_binlog_info in this case (it's useless). rm -f xtrabackup_binlog_info elif [[ -f xtrabackup_binlog_info ]]; then # We're cloning directly from master. Parse binlog position. [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1 rm xtrabackup_binlog_info echo \"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\ MASTER_LOG_POS=${BASH_REMATCH[2]}\" > change_master_to.sql.in fi # Check if we need to complete a clone by starting replication. if [[ -f change_master_to.sql.in ]]; then echo \"Waiting for mysqld to be ready (accepting connections)\" until mysql -h 127.0.0.1 -e \"SELECT 1\"; do sleep 1; done echo \"Initializing replication from clone position\" # In case of container restart, attempt this at-most-once. mv change_master_to.sql.in change_master_to.sql.orig mysql -h 127.0.0.1 <<EOF $(<change_master_to.sql.orig), MASTER_HOST='mysql-0.mysql', MASTER_USER='root', MASTER_PASSWORD='', MASTER_CONNECT_RETRY=10; START SLAVE; EOF fi # Start a server to send backups when requested by peers. exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \\ \"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root\" volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 100m memory: 100Mi volumes: - name: conf emptyDir: {} - name: config-map configMap: name: mysql volumeClaimTemplates: - metadata: name: data annotations: volume.beta.kubernetes.io/storage-class: general spec: accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 5Gi EOF Step 4 Monitor Deployment Process/Sequence watch kubectl get statefulset,pvc,pv,pods -l app=mysql Press Ctrl+C to cancel the watch when all pods, pvc and statefulset provisioned. Result The StatefulSet controller started Pods one at a time, in order by their ordinal index . It waits until each Pod reports being Ready before starting the next one. In addition, the controller assigned each Pod a unique , stable name of the form <statefulset-name>-<ordinal-index> . In this case, that results in Pods named mysql-0 , mysql-1 , and mysql-2 . The Pod template in the above StatefulSet manifest takes advantage of these properties to perform orderly startup of MySQL replication. Generating configuration Before starting any of the containers in the Pod spec, the Pod first runs any [Init Containers] in the order defined. The first Init Container, named init-mysql , generates special MySQL config files based on the ordinal index. The script determines its own ordinal index by extracting it from the end of the Pod name, which is returned by the hostname command. Then it saves the ordinal (with a numeric offset to avoid reserved values) into a file called server-id.cnf in the MySQL conf.d directory. This translates the unique, stable identity provided by the StatefulSet controller into the domain of MySQL server IDs, which require the same properties. The script in the init-mysql container also applies either master.cnf or slave.cnf from the ConfigMap by copying the contents into conf.d . Because the example topology consists of a single MySQL master and any number of slaves, the script simply assigns ordinal 0 to be the master, and everyone else to be slaves. Combined with the StatefulSet controller's deployment order guarantee ensures the MySQL master is Ready before creating slaves, so they can begin replicating. Cloning existing data In general, when a new Pod joins the set as a slave, it must assume the MySQL master might already have data on it. It also must assume that the replication logs might not go all the way back to the beginning of time. These conservative assumptions are the key to allow a running StatefulSet to scale up and down over time, rather than being fixed at its initial size. The second Init Container, named clone-mysql , performs a clone operation on a slave Pod the first time it starts up on an empty PersistentVolume. That means it copies all existing data from another running Pod, so its local state is consistent enough to begin replicating from the master. MySQL itself does not provide a mechanism to do this, so the example uses a popular open-source tool called Percona XtraBackup. During the clone, the source MySQL server might suffer reduced performance. To minimize impact on the MySQL master, the script instructs each Pod to clone from the Pod whose ordinal index is one lower. This works because the StatefulSet controller always ensures Pod N is Ready before starting Pod N+1 . Starting replication After the Init Containers complete successfully, the regular containers run. The MySQL Pods consist of a mysql container that runs the actual mysqld server, and an xtrabackup container that acts as a sidecar . The xtrabackup sidecar looks at the cloned data files and determines if it's necessary to initialize MySQL replication on the slave. If so, it waits for mysqld to be ready and then executes the CHANGE MASTER TO and START SLAVE commands with replication parameters extracted from the XtraBackup clone files. Once a slave begins replication, it remembers its MySQL master and reconnects automatically if the server restarts or the connection dies. Also, because slaves look for the master at its stable DNS name ( mysql-0.mysql ), they automatically find the master even if it gets a new Pod IP due to being rescheduled. Lastly, after starting replication, the xtrabackup container listens for connections from other Pods requesting a data clone. This server remains up indefinitely in case the StatefulSet scales up, or in case the next Pod loses its PersistentVolumeClaim and needs to redo the clone.","title":"3.1 Deploy Replicated MySQL (Master/Slaves) Cluster using StatefulSet."},{"location":"Lab_11_Storage/#32-test-the-mysql-cluster-app-and-running","text":"Step 1 Create Database, Table and message on Master MySQL database Send test queries to the MySQL master (hostname mysql-0.mysql ) by running a temporary container with the mysql:5.7 image and running the mysql client binary. kubectl run mysql-client --image = mysql:5.7 -i --rm --restart = Never -- \\ mysql -h mysql-0.mysql <<EOF CREATE DATABASE test; CREATE TABLE test.messages (message VARCHAR(250)); INSERT INTO test.messages VALUES ('hello'); EOF Step 2 Verify that recorded data has been replicated to the slaves: Use the hostname mysql-read to send test queries to any server that reports being Ready: kubectl run mysql-client --image = mysql:5.7 -i -t --rm --restart = Never -- \\ mysql -h mysql-read -e \"SELECT * FROM test.messages\" You should get output like this: Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false +---------+ | message | +---------+ | hello | +---------+ pod \"mysql-client\" deleted Step 3 Demonstrate that the mysql-read Service distributes connections across servers, you can run SELECT @@hostname in a loop: kubectl run mysql-client-loop --image = mysql:5.7 -i -t --rm --restart = Never -- \\ bash -ic \"while sleep 1; do mysql -h mysql-read -e 'SELECT @@hostname,NOW()'; done\" You should see the reported @@hostname change randomly, because a different endpoint might be selected upon each connection attempt: +-------------+---------------------+ | @@hostname | NOW() | +-------------+---------------------+ | 100 | 2006-01-02 15:04:05 | +-------------+---------------------+ +-------------+---------------------+ | @@hostname | NOW() | +-------------+---------------------+ | 102 | 2006-01-02 15:04:06 | +-------------+---------------------+ +-------------+---------------------+ | @@hostname | NOW() | +-------------+---------------------+ | 101 | 2006-01-02 15:04:07 | +-------------+---------------------+ You can press Ctrl+C when you want to stop the loop, but it's useful to keep it running in another window so you can see the effects of the following steps.","title":"3.2 Test the MySQL cluster app and running"},{"location":"Lab_11_Storage/#33-delete-pods","text":"The StatefulSet recreates Pods if they're deleted, similar to what a ReplicaSet does for stateless Pods. Step 1 Try to fail Mysql cluster by deleting mysql-2 pod: kubectl delete pod mysql-2 The StatefulSet controller notices that no mysql-2 Pod exists anymore, and creates a new one with the same name and linked to the same PersistentVolumeClaim. Step 5 Monitor Deployment Process/Sequence watch kubectl get statefulset,pvc,pv,pods -l app=mysql Result You should see server ID 102 disappear from the loop output for a while and then return on its own.","title":"3.3 Delete Pods"},{"location":"Lab_11_Storage/#34-scaling-the-number-of-slaves","text":"With MySQL replication, you can scale your read query capacity by adding slaves. With StatefulSet, you can do this with a single command: Step 1 Scale up statefulset: kubectl scale statefulset mysql --replicas = 5 Step 2 Watch the new Pods come up by running: kubectl get pods -l app = mysql --watch Once they're up, you should see server IDs 103 and 104 start appearing in the SELECT @@server_id loop output. You can also verify that these new servers have the data you added before they existed: Step 3 Watch the new Pods come up by running: kubectl run mysql-client --image = mysql:5.7 -i -t --rm --restart = Never -- \\ mysql -h mysql-3.mysql -e \"SELECT * FROM test.messages\" Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false +---------+ | message | +---------+ | hello | +---------+ pod \"mysql-client\" deleted Step 4 Scaling back down is also seamless: kubectl scale statefulset mysql --replicas = 3 Note, however, that while scaling up creates new PersistentVolumeClaims automatically, scaling down does not automatically delete these PVCs. This gives you the choice to keep those initialized PVCs around to make scaling back up quicker, or to extract data before deleting them. You can see this by running: kubectl get pvc -l app = mysql Which shows that all 5 PVCs still exist, despite having scaled the StatefulSet down to 3: NAME STATUS VOLUME CAPACITY ACCESSMODES AGE data-mysql-0 Bound pvc-8acbf5dc-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-1 Bound pvc-8ad39820-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-2 Bound pvc-8ad69a6d-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-3 Bound pvc-50043c45-b1c5-11e6-93fa-42010a800002 10Gi RWO 2m data-mysql-4 Bound pvc-500a9957-b1c5-11e6-93fa-42010a800002 10Gi RWO 2m If you don't intend to reuse the extra PVCs, you can delete them: kubectl delete pvc data-mysql-3 kubectl delete pvc data-mysql-4 Step 5 Cancel the SELECT @@server_id loop by pressing Ctrl+C in its terminal, or running the following from another terminal: kubectl delete pod mysql-client-loop --now Step 6 Delete the StatefulSet. This also begins terminating the Pods. kubectl delete statefulset mysql","title":"3.4 Scaling the number of slaves"},{"location":"Lab_11_Storage/#24-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-networking","title":"2.4 Cleaning Up"},{"location":"Lab_2_Docker_basics/","text":"Lab 2 Docker basics Objective: Practice to run Docker containers 1 Docker basics \u00b6 1.1 Show running containers \u00b6 Step 1 Run docker ps to show running containers: docker ps Step 2 The output shows that there are no running containers at the moment. Use the command docker ps -a to list all containers including the ones has been stopped: docker ps -a Output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e6db2a24a8e hello-world \"/hello\" 15 minutes ago Exited (0) 15 min dreamy_nobel Review the collumns CONTAINER ID , STATUS , COMMAND , PORTS , NAMES . In the previous section we started one container and the command docker ps -a shows it as Exited . Note You can name your own containers with --name when you use docker run. If you do not provide a name, Docker will generate a random one like the one you have. Question Why Docker names are random? How docker containers named? Step 3 Let\u2019s run the command docker images to show all the images on your local system: docker images As you see, there is only one image that was downloaded from the Docker Hub. 1.2 Specify a container main process \u00b6 Step 1 Let\u2019s run our own \"hello world\" container. For that we will use the official Ubuntu image : docker run ubuntu /bin/echo 'Hello world' Output: Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:latest Hello world As you see, Docker downloaded the image ubuntu because it was not on the local machine. Step 2 Let\u2019s run the command docker images again: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Step 3 If you run the same \"hello world\" container again, Docker will use a local copy of the image: docker run ubuntu /bin/echo 'Hello world' Output: Hello world Question Compare Ubuntu Docker image with ISO image or with Cloud VM image. Why the size is so different ? Summary Pulling docker images from Docker Hub takes sometime. This time depends on: How large is the image? How fast is the network to Internet ? However, it is still much faster than booting traditional OS with Ubuntu on VM. If image already pulled on local host it takes fraction of a second to start a container. Running application in docker containers considered as a best practice for running CI/CD pipelines as it considerably faster than using VMs and reduce time for deploying a test environments. 1.3 Specify an image version \u00b6 Step 1 As you see, Docker has downloaded the ubuntu:latest image. You can see Ubuntu version by running the following command: docker run ubuntu /bin/cat /etc/issue.net Output: Ubuntu 16.04 LTS Let\u2019s say you need a previous Ubuntu LTS release. In this case, you can specify the version you need: docker run ubuntu:14.04 /bin/cat /etc/issue.net Output: Unable to find image 'ubuntu:14.04' locally 14.04: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:14.04 Ubuntu 14.04.4 LTS Step 2 The docker images command should show that we have 3 Ubuntu images downloaded locally: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB ubuntu 14.04 0ccb13bf1954 11 days ago 188 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Tip Running CI/CD pipeline with Docker using latest tag considered as a Bad Practice. Instead consider using: Versioning SHA tagging. 1.4 Run an interactive container \u00b6 Step 1 Let\u2019s use the ubuntu image to run an interactive bash session and inspect what is running inside our docker image. To achive that we going to use -i and -t flags. The -i is shorthand for --interactive , which instructs Docker to keep stdin open so that we can send commands to the sprocess. The -t flag is short for --tty and allocates a pseudo-TTY or terminal inside of the session. docker run -it ubuntu /bin/bash root@17d8bdeda98e:/# Result We get a bash shell prompt inside of the container. Note Bash prompt is not availabe for all docker images. Step 2 Let's print the system information of the latest Ubuntu image: root@17d8bdeda98e:/# uname -a Linux 17d8bdeda98e 3.19.0-31-generic ... Step 3 Let's verify what Ubuntu version is run by latest image of ubuntu: root@17d8bdeda98e:/# lsb_release -a bash: lsb_release: command not found Failure Why the standard Ubuntu command that checks version of OS is not working as expeced ? Step 4 Let's verify Ubuntu version using alternative way by checking /etc/lsb-release file. root@8cbcbd0fe8d2:/# cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.3 LTS\" Step 5 Let's compare the number of executable binaries availabe inside of the docker image versus Cloud VM that we running our class environment. First, run ls command on /bin and /usr/bin directories inside of the running ubuntu container as well as dpkg --list command that shows total number of installed packages: root@8cbcbd0fe8d2:/# ls /bin | wc -l 86 root@8cbcbd0fe8d2:/# ls /usr/bin | wc -l 233 root@eb11cd0b4106:/# dpkg --list | wc -l 101 Step 6 Use the exit command or press Ctrl-D to exit the interactive bash session back to Cloud VM. root@eb11cd0b4106:/# exit Step 7 Now run ls command on /bin and /usr/bin directories on Cloud VM that we using as our class environment: cca-user@userx-docker-vm:~$ ls /bin | wc -l 171 cca-user@userx-docker-vm:~$ ls /usr/bin | wc -l 660 cca-user@userx-docker-vm:~$ dpkg --list | wc -l 463 Result Official Docker container has much less binaries and packages installed vs Ubuntu Cloud Image. Summary Some of the use cases running docker containers in interactive mode are: Troubleshooting containerized applications Deploying and running containerized application on the existing production systems without affecting it. We've also learned that an official Docker \"minimal\" ubuntu image, does not include lsb_release command, as well as many other commands and packages that can be found in Official Ubuntu ISO image . The docker images are ment to contain only required core system commands and functions to make Images as light as possible. That say you can still install required packages using apt-get install , however this may increase size of docker image considerably. Hint While Docker Ubuntu image we used so far or Docker Centos image are very familiar to users and can be good starting point for learning docker containers. Using them in production or development considered as a Bad Practice. This is due those images still considered as heavy and potentially contain a lot more valnurabilities compare to specialized images. To reduce image pull time from docker hub and follow the best secuirity practices consider using specialized images that works well with you underlining code (Node image for NodeJS applications and etc.). Examples of specialized images are: Alpine Linux Node Atomic In fact, not so long ago all the official Docker Images in Docker-Hub has been moved to use Alpine Image . Step 8 Finally let\u2019s check that when the shell process has finished, the container stops: docker ps 1.5 Run a container in a background \u00b6 Now we know how to connect to running container and execute commands in it. However in most cases you just want run a container in a background so it can do a specific action. Step 1 Run a container in a background using the -d command line argument: docker run -d ubuntu /bin/sh -c \"while true; do date; echo hello world; sleep 1; done\" Result Command should return the container ID. Step 2 Let\u2019s use the docker ps command to see running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac231579e57f ubuntu \"/bin/sh -c 'while tr\" 1 minute ago Up 11 minute evil_golick Note Container id is going to be different in your case Hint Instead of using full container-id when building commands, it is possible simply type first few characters of container-id, to make things nice and easy. Step 3 Let\u2019s use container-id to show the container standard output: docker logs <container-id> Thu Jan 26 00:23:45 UTC 2017 hello world Thu Jan 26 00:23:46 UTC 2017 hello world Thu Jan 26 00:23:47 UTC 2017 hello world ... As you can see, in the docker ps command output, the auto generated container name is evil_golick (your container can have a different name). Step 4 Now, instead of using docker contaier-id use container name to show the container standard output: docker logs <name> Thu Jan 26 00:23:51 UTC 2017 hello world Thu Jan 26 00:23:52 UTC 2017 hello world Thu Jan 26 00:23:53 UTC 2017 hello world ... Step 5 Finally, let\u2019s stop our container: docker stop <name> Step 6 Check, that there are no running containers: docker ps Summary docker logs is a very usefull command to troubleshoot containers, and going to be used very often both for Docker and Kubernertes troubleshooting. 1.6 Accessing Containers from the Internet \u00b6 Step 1 Let\u2019s run a simple web application. We will use the existing image training/webapp, which contains a Python Flask application: docker run -d -P training/webapp python app.py ... Status: Downloaded newer image for training/webapp:latest 6e88f42d3d853762edcbfe1fe73fdc5c48865275bc6df759b83b0939d5bd2456 In the command above we specified the main process (python app.py), the -d command line argument, which tells Docker to run the container in the background. The -P command line argument tells Docker to map any required network ports inside our container to our host. This allows us to access the web application in the container. Step 2 Use the docker ps command to list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e88f42d3d85 training/webapp \"python app.py\" 3 minutes ago Up 3 minutes 0.0.0.0:32768->5000/tcp determined_torvalds The PORTS column contains the mapped ports. In our case, Docker has exposed port 5000 (the default Python Flask port) on port 32768 (can be different in your case). Step 3 The docker port command shows the exposed port. We will use the container name (determined_torvalds in the example above, it can be different in your case): docker port <name> 5000 0.0.0.0:32768 Step 4 Let\u2019s check that we can access the web application exposed port: curl http://localhost:<port>/ Result Hello world! Step 5 Let\u2019s stop our web application for now: docker stop <name> Step 6 We want to manually specify the local port to expose (-p argument). Let\u2019s use the standard HTTP port 80. We also want to specify the container name (--name argument): docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 7 Let\u2019s check that the port 80 is exposed: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp curl http://localhost/ Result `Hello world!`` Step 8 You can also observe Hello world! webapp from you laptop, for that you need to use you public VM IP that can be gather from VMs list: Your_VM_Public_IP Than paste VM Public IP address in you browser. Result Our web-app can be accessed from Internet! 1.7 Restart a container \u00b6 Step 1 Let\u2019s stop the container with web application: docker stop webapp The main process inside of the container will receive SIGTERM, and after a grace period, SIGKILL. Step 2 You can start the container later using the docker start command: docker start webapp Step 3 Check that the web application works: curl http://localhost/ Hello world! Step 4 You also can restart the running container using the docker restart command. docker restart webapp Step 4 Run docker ps command and check STATUS field: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS 6e400179070f training/webapp \"python app.py\" 25 minutes ago Up 3 seconds 1.8 Ensuring Container Uptime \u00b6 Docker considers any containers to exit with a non-zero exit code to have crashed. By default a crashed container will remain stopped. Step 1 Start the container that outputs a message and then exits with code 1 to simulate a crash. docker run -d --name restart-default scrapbook/docker-restart-example docker ps -a | grep restart-default CONTAINER ID IMAGE CREATED STATUS NAMES c854289d2f39 scrapbook/docker-restart-example 5 seconds ago Exited 3 sec ago restart-default $ docker logs restart-default Sun Sep 17 20:34:55 UTC 2017 Booting up... Result Container crushed and exited. However, there are several ways to ensure that you container up and running even if it\u2019s restarts. Step 2 The option --restart=on-failure : allows you to say how many times Docker should try again: docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example docker logs restart-3 Thu Apr 20 14:01:27 UTC 2017 Booting up... Thu Apr 20 14:01:28 UTC 2017 Booting up... Thu Apr 20 14:01:29 UTC 2017 Booting up... Thu Apr 20 14:01:31 UTC 2017 Booting up... Step 3 Finally, Docker can always restart a failed container. In this case, Docker will keep trying until the container is explicitly told to stop. docker run -d --name restart-always --restart=always scrapbook/docker-restart-example docker logs restart-always Step 4 After sometime stop running docker container, as it will be keep failing and starting again: docker stop restart-always 1.9 Inspect a container \u00b6 Step 1 You can use the docker inspect command to see the configuration and status information for the specified container: docker inspect webapp [ { \"Id\": \"249476631f7d...\", \"Created\": \"2016-08-02T23:42:56.932135327Z\", \"Path\": \"python\", \"Args\": [ \"app.py\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 16055, \"ExitCode\": 0, \"Error\": \"\", ... Step 2 You can specify a filter (-f command line argument) to show only specific elements. For example: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp 172.17.0.2 The command returns the IP address of the container. 1.10 Interacting with containers \u00b6 In some cases using docker log is not enough to undertand issues and you want to login inside of running VM. Also sometimes you package you applicaiton and in order to run it you need to login inside of container and execute and leave it running in background. Below provded few ways to interacting with containers that can help to achive descrined use cases. 1.10.1 Detach from Interactive container \u00b6 In Module, 1.4 Run an interactive container we run an Ubuntu container with -it flag and able directly login inside of the container to interact with it, however after we exited contianer using Ctrl-D or exit command container stopped. However you can exit from Interactive mode without stoping a container. Let's demonstrate how this works: Step 1 Start Ubunu container in interactive mode: docker run -it ubuntu /bin/bash Step 2 Run watch date command inside running container in order to exit date command every 2 seconds. root@1d688a9f4ed4:/# watch date Step 3 Detach from a container and leave it running using the CTRL-p CTRL-q key sequence. Step 4 Verify that Ubuntu container is still running: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS NAMES 1d688a9f4ed4 ubuntu \"/bin/bash\" 1 minutes ago Up 1 minutes admiring_lovelace Result Great you were able to detach from Docker container without stopping it, while it is executing a process in it. What about attaching back to container ? Important CTRL-p CTRL-q sequence key only works if docker contaienr started with -it command! 1.11.2 Attach to a container \u00b6 Now let's get back and attach to our running Ubuntu image. For that docker provides docker attach command. docker attach <container name> Every 2.0s: date Mon Sep 18 00:08:57 2017 Summary docker attach attaches your contairs terminal\u2019s standard input, output, and error (or any combination of the 3) to a running container. This allows you to view its ongoing output or to control it interactively, as though the commands were running directly in your terminal. 1.11.3 Execute a process in a container \u00b6 Step 1 Let verify if webapp container is still running docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp If not running start it with following command: $ docker run -d -p 80:5000 --name webapp training/webapp python app.py other wise skip to next step . Step 2 Use the docker exec command to execute a command in the running container. For example: docker exec webapp ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 26 0.0 0.0 15572 2104 ? Rs 00:12 0:00 ps aux The same command with the -it command line argument can be used to run an interactive session in the container: docker exec -it webapp bash root@249476631f7d:/opt/webapp# ps auxw ps auxw USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 32 0.0 0.0 18144 3064 ? Ss 00:14 0:00 bash root 47 0.0 0.0 15572 2076 ? R+ 00:16 0:00 ps auxw Step 2 Use the exit command or press Ctrl-D to exit the interactive bash session: root@249476631f7d:/opt/webapp# exit Summary docker exec is one of the most usefull docker commands used for troubleshooting containers. 1.12 Copy files to/from container \u00b6 The docker cp command allows you to copy files from the container to the local machine or from the local file system to the container. This command works for a running or stopped container. Step 1 Let\u2019s copy the container\u2019s app.py file to the local machine: docker cp webapp:/opt/webapp/app.py . Step 2 Edit the local app.py file. For example, change the line return 'Hello '+provider+'!' to return 'Hello '+provider+'!!!'. Copy the modified file back and restart the container: docker cp app.py webapp:/opt/webapp/ docker restart webapp Step 3 Check that the modified web application works:: curl http://localhost/ Result `Hello world!!!`` 1.12 Remove containers \u00b6 Now let's clean up the environment and at the same time learn how delete containers. Step 1 First list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 81c4c66baaf9 training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp Step 2 Than try to delete running container using docker rm <container_id> docker rm $container_id Error response from daemon: You cannot remove a running container 81c4c66baaf9. Stop the container before attempting removal or force remove. Failure Docker containers needs to be first stopped or deleted using --force flag. docker rm $container_id -f Alternatively, you can run stop and rm in sequence: docker stop 81c4c66baaf9 docker rm 81c4c66baaf9 Summary We've learned a lot of docker commands which are very handy to know both when using Docker and Kubernetes. In the next Module we going to deep dive in to details of how networking and storage works in Docker.","title":"Lab 2 Docker Basics"},{"location":"Lab_2_Docker_basics/#1-docker-basics","text":"","title":"1 Docker basics"},{"location":"Lab_2_Docker_basics/#11-show-running-containers","text":"Step 1 Run docker ps to show running containers: docker ps Step 2 The output shows that there are no running containers at the moment. Use the command docker ps -a to list all containers including the ones has been stopped: docker ps -a Output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e6db2a24a8e hello-world \"/hello\" 15 minutes ago Exited (0) 15 min dreamy_nobel Review the collumns CONTAINER ID , STATUS , COMMAND , PORTS , NAMES . In the previous section we started one container and the command docker ps -a shows it as Exited . Note You can name your own containers with --name when you use docker run. If you do not provide a name, Docker will generate a random one like the one you have. Question Why Docker names are random? How docker containers named? Step 3 Let\u2019s run the command docker images to show all the images on your local system: docker images As you see, there is only one image that was downloaded from the Docker Hub.","title":"1.1 Show running containers"},{"location":"Lab_2_Docker_basics/#12-specify-a-container-main-process","text":"Step 1 Let\u2019s run our own \"hello world\" container. For that we will use the official Ubuntu image : docker run ubuntu /bin/echo 'Hello world' Output: Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:latest Hello world As you see, Docker downloaded the image ubuntu because it was not on the local machine. Step 2 Let\u2019s run the command docker images again: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Step 3 If you run the same \"hello world\" container again, Docker will use a local copy of the image: docker run ubuntu /bin/echo 'Hello world' Output: Hello world Question Compare Ubuntu Docker image with ISO image or with Cloud VM image. Why the size is so different ? Summary Pulling docker images from Docker Hub takes sometime. This time depends on: How large is the image? How fast is the network to Internet ? However, it is still much faster than booting traditional OS with Ubuntu on VM. If image already pulled on local host it takes fraction of a second to start a container. Running application in docker containers considered as a best practice for running CI/CD pipelines as it considerably faster than using VMs and reduce time for deploying a test environments.","title":"1.2 Specify a container main process"},{"location":"Lab_2_Docker_basics/#13-specify-an-image-version","text":"Step 1 As you see, Docker has downloaded the ubuntu:latest image. You can see Ubuntu version by running the following command: docker run ubuntu /bin/cat /etc/issue.net Output: Ubuntu 16.04 LTS Let\u2019s say you need a previous Ubuntu LTS release. In this case, you can specify the version you need: docker run ubuntu:14.04 /bin/cat /etc/issue.net Output: Unable to find image 'ubuntu:14.04' locally 14.04: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:14.04 Ubuntu 14.04.4 LTS Step 2 The docker images command should show that we have 3 Ubuntu images downloaded locally: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB ubuntu 14.04 0ccb13bf1954 11 days ago 188 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Tip Running CI/CD pipeline with Docker using latest tag considered as a Bad Practice. Instead consider using: Versioning SHA tagging.","title":"1.3 Specify an image version"},{"location":"Lab_2_Docker_basics/#14-run-an-interactive-container","text":"Step 1 Let\u2019s use the ubuntu image to run an interactive bash session and inspect what is running inside our docker image. To achive that we going to use -i and -t flags. The -i is shorthand for --interactive , which instructs Docker to keep stdin open so that we can send commands to the sprocess. The -t flag is short for --tty and allocates a pseudo-TTY or terminal inside of the session. docker run -it ubuntu /bin/bash root@17d8bdeda98e:/# Result We get a bash shell prompt inside of the container. Note Bash prompt is not availabe for all docker images. Step 2 Let's print the system information of the latest Ubuntu image: root@17d8bdeda98e:/# uname -a Linux 17d8bdeda98e 3.19.0-31-generic ... Step 3 Let's verify what Ubuntu version is run by latest image of ubuntu: root@17d8bdeda98e:/# lsb_release -a bash: lsb_release: command not found Failure Why the standard Ubuntu command that checks version of OS is not working as expeced ? Step 4 Let's verify Ubuntu version using alternative way by checking /etc/lsb-release file. root@8cbcbd0fe8d2:/# cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.3 LTS\" Step 5 Let's compare the number of executable binaries availabe inside of the docker image versus Cloud VM that we running our class environment. First, run ls command on /bin and /usr/bin directories inside of the running ubuntu container as well as dpkg --list command that shows total number of installed packages: root@8cbcbd0fe8d2:/# ls /bin | wc -l 86 root@8cbcbd0fe8d2:/# ls /usr/bin | wc -l 233 root@eb11cd0b4106:/# dpkg --list | wc -l 101 Step 6 Use the exit command or press Ctrl-D to exit the interactive bash session back to Cloud VM. root@eb11cd0b4106:/# exit Step 7 Now run ls command on /bin and /usr/bin directories on Cloud VM that we using as our class environment: cca-user@userx-docker-vm:~$ ls /bin | wc -l 171 cca-user@userx-docker-vm:~$ ls /usr/bin | wc -l 660 cca-user@userx-docker-vm:~$ dpkg --list | wc -l 463 Result Official Docker container has much less binaries and packages installed vs Ubuntu Cloud Image. Summary Some of the use cases running docker containers in interactive mode are: Troubleshooting containerized applications Deploying and running containerized application on the existing production systems without affecting it. We've also learned that an official Docker \"minimal\" ubuntu image, does not include lsb_release command, as well as many other commands and packages that can be found in Official Ubuntu ISO image . The docker images are ment to contain only required core system commands and functions to make Images as light as possible. That say you can still install required packages using apt-get install , however this may increase size of docker image considerably. Hint While Docker Ubuntu image we used so far or Docker Centos image are very familiar to users and can be good starting point for learning docker containers. Using them in production or development considered as a Bad Practice. This is due those images still considered as heavy and potentially contain a lot more valnurabilities compare to specialized images. To reduce image pull time from docker hub and follow the best secuirity practices consider using specialized images that works well with you underlining code (Node image for NodeJS applications and etc.). Examples of specialized images are: Alpine Linux Node Atomic In fact, not so long ago all the official Docker Images in Docker-Hub has been moved to use Alpine Image . Step 8 Finally let\u2019s check that when the shell process has finished, the container stops: docker ps","title":"1.4 Run an interactive container"},{"location":"Lab_2_Docker_basics/#15-run-a-container-in-a-background","text":"Now we know how to connect to running container and execute commands in it. However in most cases you just want run a container in a background so it can do a specific action. Step 1 Run a container in a background using the -d command line argument: docker run -d ubuntu /bin/sh -c \"while true; do date; echo hello world; sleep 1; done\" Result Command should return the container ID. Step 2 Let\u2019s use the docker ps command to see running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac231579e57f ubuntu \"/bin/sh -c 'while tr\" 1 minute ago Up 11 minute evil_golick Note Container id is going to be different in your case Hint Instead of using full container-id when building commands, it is possible simply type first few characters of container-id, to make things nice and easy. Step 3 Let\u2019s use container-id to show the container standard output: docker logs <container-id> Thu Jan 26 00:23:45 UTC 2017 hello world Thu Jan 26 00:23:46 UTC 2017 hello world Thu Jan 26 00:23:47 UTC 2017 hello world ... As you can see, in the docker ps command output, the auto generated container name is evil_golick (your container can have a different name). Step 4 Now, instead of using docker contaier-id use container name to show the container standard output: docker logs <name> Thu Jan 26 00:23:51 UTC 2017 hello world Thu Jan 26 00:23:52 UTC 2017 hello world Thu Jan 26 00:23:53 UTC 2017 hello world ... Step 5 Finally, let\u2019s stop our container: docker stop <name> Step 6 Check, that there are no running containers: docker ps Summary docker logs is a very usefull command to troubleshoot containers, and going to be used very often both for Docker and Kubernertes troubleshooting.","title":"1.5 Run a container in a background"},{"location":"Lab_2_Docker_basics/#16-accessing-containers-from-the-internet","text":"Step 1 Let\u2019s run a simple web application. We will use the existing image training/webapp, which contains a Python Flask application: docker run -d -P training/webapp python app.py ... Status: Downloaded newer image for training/webapp:latest 6e88f42d3d853762edcbfe1fe73fdc5c48865275bc6df759b83b0939d5bd2456 In the command above we specified the main process (python app.py), the -d command line argument, which tells Docker to run the container in the background. The -P command line argument tells Docker to map any required network ports inside our container to our host. This allows us to access the web application in the container. Step 2 Use the docker ps command to list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e88f42d3d85 training/webapp \"python app.py\" 3 minutes ago Up 3 minutes 0.0.0.0:32768->5000/tcp determined_torvalds The PORTS column contains the mapped ports. In our case, Docker has exposed port 5000 (the default Python Flask port) on port 32768 (can be different in your case). Step 3 The docker port command shows the exposed port. We will use the container name (determined_torvalds in the example above, it can be different in your case): docker port <name> 5000 0.0.0.0:32768 Step 4 Let\u2019s check that we can access the web application exposed port: curl http://localhost:<port>/ Result Hello world! Step 5 Let\u2019s stop our web application for now: docker stop <name> Step 6 We want to manually specify the local port to expose (-p argument). Let\u2019s use the standard HTTP port 80. We also want to specify the container name (--name argument): docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 7 Let\u2019s check that the port 80 is exposed: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp curl http://localhost/ Result `Hello world!`` Step 8 You can also observe Hello world! webapp from you laptop, for that you need to use you public VM IP that can be gather from VMs list: Your_VM_Public_IP Than paste VM Public IP address in you browser. Result Our web-app can be accessed from Internet!","title":"1.6 Accessing Containers from the Internet"},{"location":"Lab_2_Docker_basics/#17-restart-a-container","text":"Step 1 Let\u2019s stop the container with web application: docker stop webapp The main process inside of the container will receive SIGTERM, and after a grace period, SIGKILL. Step 2 You can start the container later using the docker start command: docker start webapp Step 3 Check that the web application works: curl http://localhost/ Hello world! Step 4 You also can restart the running container using the docker restart command. docker restart webapp Step 4 Run docker ps command and check STATUS field: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS 6e400179070f training/webapp \"python app.py\" 25 minutes ago Up 3 seconds","title":"1.7 Restart a container"},{"location":"Lab_2_Docker_basics/#18-ensuring-container-uptime","text":"Docker considers any containers to exit with a non-zero exit code to have crashed. By default a crashed container will remain stopped. Step 1 Start the container that outputs a message and then exits with code 1 to simulate a crash. docker run -d --name restart-default scrapbook/docker-restart-example docker ps -a | grep restart-default CONTAINER ID IMAGE CREATED STATUS NAMES c854289d2f39 scrapbook/docker-restart-example 5 seconds ago Exited 3 sec ago restart-default $ docker logs restart-default Sun Sep 17 20:34:55 UTC 2017 Booting up... Result Container crushed and exited. However, there are several ways to ensure that you container up and running even if it\u2019s restarts. Step 2 The option --restart=on-failure : allows you to say how many times Docker should try again: docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example docker logs restart-3 Thu Apr 20 14:01:27 UTC 2017 Booting up... Thu Apr 20 14:01:28 UTC 2017 Booting up... Thu Apr 20 14:01:29 UTC 2017 Booting up... Thu Apr 20 14:01:31 UTC 2017 Booting up... Step 3 Finally, Docker can always restart a failed container. In this case, Docker will keep trying until the container is explicitly told to stop. docker run -d --name restart-always --restart=always scrapbook/docker-restart-example docker logs restart-always Step 4 After sometime stop running docker container, as it will be keep failing and starting again: docker stop restart-always","title":"1.8 Ensuring Container Uptime"},{"location":"Lab_2_Docker_basics/#19-inspect-a-container","text":"Step 1 You can use the docker inspect command to see the configuration and status information for the specified container: docker inspect webapp [ { \"Id\": \"249476631f7d...\", \"Created\": \"2016-08-02T23:42:56.932135327Z\", \"Path\": \"python\", \"Args\": [ \"app.py\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 16055, \"ExitCode\": 0, \"Error\": \"\", ... Step 2 You can specify a filter (-f command line argument) to show only specific elements. For example: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp 172.17.0.2 The command returns the IP address of the container.","title":"1.9 Inspect a container"},{"location":"Lab_2_Docker_basics/#110-interacting-with-containers","text":"In some cases using docker log is not enough to undertand issues and you want to login inside of running VM. Also sometimes you package you applicaiton and in order to run it you need to login inside of container and execute and leave it running in background. Below provded few ways to interacting with containers that can help to achive descrined use cases.","title":"1.10 Interacting with containers"},{"location":"Lab_2_Docker_basics/#1101-detach-from-interactive-container","text":"In Module, 1.4 Run an interactive container we run an Ubuntu container with -it flag and able directly login inside of the container to interact with it, however after we exited contianer using Ctrl-D or exit command container stopped. However you can exit from Interactive mode without stoping a container. Let's demonstrate how this works: Step 1 Start Ubunu container in interactive mode: docker run -it ubuntu /bin/bash Step 2 Run watch date command inside running container in order to exit date command every 2 seconds. root@1d688a9f4ed4:/# watch date Step 3 Detach from a container and leave it running using the CTRL-p CTRL-q key sequence. Step 4 Verify that Ubuntu container is still running: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS NAMES 1d688a9f4ed4 ubuntu \"/bin/bash\" 1 minutes ago Up 1 minutes admiring_lovelace Result Great you were able to detach from Docker container without stopping it, while it is executing a process in it. What about attaching back to container ? Important CTRL-p CTRL-q sequence key only works if docker contaienr started with -it command!","title":"1.10.1 Detach from Interactive container"},{"location":"Lab_2_Docker_basics/#1112-attach-to-a-container","text":"Now let's get back and attach to our running Ubuntu image. For that docker provides docker attach command. docker attach <container name> Every 2.0s: date Mon Sep 18 00:08:57 2017 Summary docker attach attaches your contairs terminal\u2019s standard input, output, and error (or any combination of the 3) to a running container. This allows you to view its ongoing output or to control it interactively, as though the commands were running directly in your terminal.","title":"1.11.2 Attach to a container"},{"location":"Lab_2_Docker_basics/#1113-execute-a-process-in-a-container","text":"Step 1 Let verify if webapp container is still running docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp If not running start it with following command: $ docker run -d -p 80:5000 --name webapp training/webapp python app.py other wise skip to next step . Step 2 Use the docker exec command to execute a command in the running container. For example: docker exec webapp ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 26 0.0 0.0 15572 2104 ? Rs 00:12 0:00 ps aux The same command with the -it command line argument can be used to run an interactive session in the container: docker exec -it webapp bash root@249476631f7d:/opt/webapp# ps auxw ps auxw USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 32 0.0 0.0 18144 3064 ? Ss 00:14 0:00 bash root 47 0.0 0.0 15572 2076 ? R+ 00:16 0:00 ps auxw Step 2 Use the exit command or press Ctrl-D to exit the interactive bash session: root@249476631f7d:/opt/webapp# exit Summary docker exec is one of the most usefull docker commands used for troubleshooting containers.","title":"1.11.3 Execute a process in a container"},{"location":"Lab_2_Docker_basics/#112-copy-files-tofrom-container","text":"The docker cp command allows you to copy files from the container to the local machine or from the local file system to the container. This command works for a running or stopped container. Step 1 Let\u2019s copy the container\u2019s app.py file to the local machine: docker cp webapp:/opt/webapp/app.py . Step 2 Edit the local app.py file. For example, change the line return 'Hello '+provider+'!' to return 'Hello '+provider+'!!!'. Copy the modified file back and restart the container: docker cp app.py webapp:/opt/webapp/ docker restart webapp Step 3 Check that the modified web application works:: curl http://localhost/ Result `Hello world!!!``","title":"1.12 Copy files to/from container"},{"location":"Lab_2_Docker_basics/#112-remove-containers","text":"Now let's clean up the environment and at the same time learn how delete containers. Step 1 First list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 81c4c66baaf9 training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp Step 2 Than try to delete running container using docker rm <container_id> docker rm $container_id Error response from daemon: You cannot remove a running container 81c4c66baaf9. Stop the container before attempting removal or force remove. Failure Docker containers needs to be first stopped or deleted using --force flag. docker rm $container_id -f Alternatively, you can run stop and rm in sequence: docker stop 81c4c66baaf9 docker rm 81c4c66baaf9 Summary We've learned a lot of docker commands which are very handy to know both when using Docker and Kubernetes. In the next Module we going to deep dive in to details of how networking and storage works in Docker.","title":"1.12 Remove containers"},{"location":"Lab_3_Advanced_Docker/","text":"Lab 3 Docker Networking, Persistence, Monitoring and Logging Objective: Networks Docker basics User-defined private Networks Persistence Data Volumes 1 Docker Networking \u00b6 1.1 Docker Networking Basics \u00b6 Step 1: The Docker Network Command The docker network command is the main command for configuring and managing container networks. Run the docker network command from the first terminal. docker network Usage: docker network COMMAND Manage networks Options: --help Print usage Commands: connect Connect a container to a network create Create a network disconnect Disconnect a container from a network inspect Display detailed information on one or more networks ls List networks prune Remove all unused networks rm Remove one or more networks Run 'docker network COMMAND --help' for more information on a command. The command output shows how to use the command as well as all of the docker network sub-commands. As you can see from the output, the docker network command allows you to create new networks, list existing networks, inspect networks, and remove networks. It also allows you to connect and disconnect containers from networks. Step 2 Run a docker network ls command to view existing container networks on the current Docker host. docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local The output above shows the container networks that are created as part of a standard installation of Docker. New networks that you create will also show up in the output of the docker network ls command. You can see that each network gets a unique ID and NAME . Each network is also associated with a single driver. Notice that the \"bridge\" network and the \"host\" network have the same name as their respective drivers. Step 3: The docker network inspect command is used to view network configuration details. These details include; name, ID, driver, IPAM driver, subnet info, connected containers, and more. Use docker network inspect <network> to view configuration details of the container networks on your Docker host. The command below shows the details of the network called bridge . docker network inspect bridge [ { \"Name\": \"bridge\", \"Id\": \"3430ad6f20bf1486df2e5f64ddc93cc4ff95d81f59b6baea8a510ad500df2e57\", \"Created\": \"2017-04-03T16:49:58.6536278Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Containers\": {}, \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" }, \"Labels\": {} } ] Note The syntax of the docker network inspect command is docker network inspect <network> , where <network> can be either network name or network ID. In the example above we are showing the configuration details for the network called \"bridge\". Do not confuse this with the \"bridge\" driver. Step 4 Now, list Docker supported network driver plugins. For that run docker info command, that shows a lot of interesting information about a Docker installation. Run the docker info command and locate the list of network plugins. docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ee-3 Storage Driver: aufs <Snip> Plugins: Volume: local Network: bridge host macvlan null overlay Swarm: inactive Runtimes: runc <Snip> The output above shows the bridge , host , macvlan , null , and overlay drivers. Summary We've quickly reviewed available docker networking commands as well as found what drivers current docker setup supports. 1.2 Default bridge network \u00b6 Every clean installation of Docker comes with a pre-built network called Default bridge network . Let's explore in more details how it works. Step 1 Verify this with the docker network ls . docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local Result The output above shows that the bridge network is associated with the bridge driver. It's important to note that the network and the driver are connected, but they are not the same. In this example the network and the driver have the same name - but they are not the same thing! The output above also shows that the bridge network is scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the bridge driver - the bridge driver provides single-host networking. All networks created with the bridge driver are based on a Linux bridge (a.k.a. a virtual switch). Step 5 Start webapp in Default bridge network docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 6 Check that the webapp and db containers are running: Command: docker ps 1.3 User-defined Private Networks \u00b6 So far we\u2019ve learned how Docker networking works with Docker default bridge network . With the introduction of user-defined networking in Docker 1.9, it is now possible to create multiple Docker bridges to allow network segregation within the same host or multi-host networking to allow communicate Docker containers between hosts. The commands are available through the Docker Engine CLI are: docker network create docker network connect docker network ls docker network rm docker network disconnect docker network inspect Let's demonstrate how to create a custom bridge network. Step 1 By default, Docker runs containers in the bridge network. You may want to isolate one or more containers in a separate network. Let\u2019s create a new network: docker network create my-network \\ -d bridge \\ --subnet 172.19.0.0/16 The -d bridge command line argument specifies the bridge network driver and the --subnet command line argument specifies the network segment in CIDR format. If you do not specify a subnet when creating a network, then Docker assigns a subnet automatically, so it is a good idea to specify a subnet to avoid potential conflicts with the existing networks. Below are some other options that are available with the bridge Driver: com.docker.network.bridge.enable_ip_masquerade: This instructs the Docker host to hide or masquerade all containers in this network behind the Docker host's interfaces if the container attempts to route off the local host . com.docker.network.bridge.name: This is the name you wish to give to the bridge. com.docker.network.bridge.enable_icc: This turns on or off Inter-Container Connectivity (ICC) mode for the bridge. com.docker.network.bridge.host_binding_ipv4: This defines the host interface that should be used for port binding. com.docker.network.driver.mtu: This sets MTU for containers attached to this bridge. Step 2 To check that the new network is created, execute docker network ls: docker network ls NETWORK ID NAME DRIVER SCOPE d428e49e4869 bridge bridge local 0d1f78528cc5 host host local 56ef0481820d my-network bridge local 4a07cef84617 none null local Step 3 Let\u2019s inspect the new network: docker network inspect my-network [ { \"Name\": \"my-network\", \"Id\": \"56ef0481820d...\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"172.19.0.0/16\" } ] }, \"Internal\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ] Step 4 As expected, there are no containers connected to the my-network. Let\u2019s recreate the db container in the my-network: docker rm -f db docker run -d --network=my-network --name db training/postgres Step 5 Inspect the my-network again: docker network inspect my-network Output: \"Containers\": { \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... As you see, the db container is connected to the my-network and has 172.19.0.2 address. Step 6 Let\u2019s start an interactive session in the db container and ping the IP address of the webapp again: Note Quick reminder how to locate webapp ip: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp docker exec -it db bash Once inside of container run: root@c3afff20019a:/# ping -c 1 172.17.0.3 PING 172.17.0.3 (172.17.0.3) 56(84) bytes of data. --- 172.17.0.3 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms As expected, the webapp container is no longer accessible from the db container, because they are connected to different networks. Summary Using Multi-host networking provides network isolation within a Docker host via network namepsaces. This is can be used if you want to deploy different applications on same host for isolation or resource duplicate prevention. Step 7 Let\u2019s connect the webapp container to the my-network: docker network connect my-network webapp Step 8 Check that the webapp container now is connected to the my-network: docker network inspect my-network Output: ... \"Containers\": { \"62ed4a627356...\": { \"Name\": \"webapp\", \"EndpointID\": \"ae95b0103bbc...\", \"MacAddress\": \"02:42:ac:12:00:03\", \"IPv4Address\": \"172.19.0.3/16\", \"IPv6Address\": \"\" }, \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... The output shows that two containers are connected to the my-network and the webapp container has 172.19.0.3 address in that network. Step 9 Check that the webapp container is accessible from the db container using its new IP address: docker exec -it db bash root@c3afff20019a:/# ping -c 1 172.19.0.3 PING 172.19.0.3 (172.19.0.3) 56(84) bytes of data. 64 bytes from 172.19.0.3: icmp_seq=1 ttl=64 time=0.136 ms --- 172.19.0.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms Success As expected containers can communicate with each other. Step 10 You can now remove the existing container. You should stop the container before removing it. Alternatively you can use the -f command line argument: docker rm -f webapp docker rm -f db docker network rm my-network Hint Use below command to delete running containers in bulk : docker rm -f $(docker ps -q) Summary It is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses 1.4 Access containers from outside \u00b6 External Access to the Containers can be configured via publishing mechanism. Docker provides 2 options to publish ports: -P flag publishes all exposed ports -p flag allows you to specify specific ports and interfaces to use when mapping ports. The -p flag can take several different forms with the syntax looking like this: Specify the host port and container port: \u2013p <host port>:<container port> Specify the host interface, host port, and container port: \u2013p <host IP interface>:<host port>:<container port> Specify the host interface, have Docker choose a random host port, and specify the container port: \u2013p <host IP interface>::<container port> Specify only a container port and have Docker use a random host port: \u2013p <container port> Let's test exposing containers. For that let's start a new NGINX container and map port 8080 on the Docker host to port 80 inside of the container. This means that traffic that hits the Docker host on port 8080 will be passed on to port 80 inside the container. Note If you start a new container from the official NGINX image without specifying a command to run, the container will run a basic web server on port 80. Step 1 Start a new container based off the official NGINX image by running docker run --name web1 -d -p 8080:80 nginx . docker run --name web1 -d -p 8080:80 nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 6d827a3ef358: Pull complete b556b18c7952: Pull complete 03558b976e24: Pull complete 9abee7e1ef9d: Pull complete Digest: sha256:52f84ace6ea43f2f58937e5f9fc562e99ad6876e82b99d171916c1ece587c188 Status: Downloaded newer image for nginx:latest 4e0da45b0f169f18b0e1ee9bf779500cb0f756402c0a0821d55565f162741b3e Step 2 Review the container status and port mappings by running docker ps . docker ps CONTAINER ID IMAGE COMMAND PORTS NAMES 4e0da45b0f16 nginx \"nginx -g 'daemon ...\" 443/tcp, 0.0.0.0:8080->80/tcp web1 Result The top line shows the new web1 container running NGINX. Take note of the command the container is running as well as the port mapping - 0.0.0.0:8080->80/tcp maps port 8080 on all host interfaces to port 80 inside the web1 container. This port mapping is what effectively makes the containers web service accessible from external sources (via the Docker hosts IP address on port 8080). Step 3 Test connectivity to the NGINX web server, by pasting <Public_IP:8080> of VM to the browser. Note In order to locate Public IP see the list of VMs. Alternatively from inside of VM run curl 127.0.0.1:8080 command. curl 127.0.0.1:8080 <!DOCTYPE html> <html> <Snip> <head> <title>Welcome to nginx!</title> <Snip> <p><em>Thank you for using nginx.</em></p> </body> </html> Success Both CLI and UI method works! If you try and curl the IP address on a different port number it will fail. Summary Docker provides easy way to expose containers outside of the Docker Node. This can ber used for connecting containers between each other: Between networks on the same host Between networks on different host Accessing containers from outside (e.g web site) However, port mapping is implemented via port address translation (PAT) unlike in Kubernetes which we learn soon, exposes applications via service IPs and communicates via POD IPs using (NAT) Step 4 Cleanup environment docker rm -f $(docker ps -q) 2 Persistant Volumes \u00b6 2.1 Storage driver \u00b6 We've discussed several Storage drivers (graphdrivers) during the class. Let's find out what graphdriver is running in our Lab environment. docker info | grep Storage WARNING: No swap limit support Storage Driver: aufs Result Our Classroom is running aufs storage driver. Not a suprise as we running our Lab on Ubuntu VM. Summary Systems runnng Ubuntu or Debian ,going to run aufs graphdriver by default and will most likely meet the majority of your needs. In future overlay2 may replace aufs stay tunned! 2.2 Persisting Data Using Volumes \u00b6 Docker Volumes are created and assigned when containers are started. Data Volumes allow you to map a host directory to a container for sharing data. This mapping is bi-directional. It allows data stored on the host to be accessed from within the container. It also means data saved by the process inside the container is persisted on the host. 2.2.1 Create and manage volumes \u00b6 Step 1 Create a volume: docker volume create --name my-vol Step 2 List volumes: docker volume ls Output: local my-vol Step 3 Inspect a volume: docker volume inspect my-vol [ { \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\", \"Name\": \"my-vol\", \"Options\": {}, \"Scope\": \"local\" } ] Step 3 Add some data to the Mountpoint of the volume: sudo touch /var/lib/docker/volumes/my-vol/_data/test_vol sudo ls /var/lib/docker/volumes/my-vol/_data/ Step 4 Create a container busybox alpine image and attach created my-vol volume in to it: docker run -it -v my-vol:/world busybox / # ls /world test_vol / # Result Volume is mounted and test_vol file is under /world folder as expected Step 5 Try to delete the volume: docker volume rm my-vol Error response from daemon: unable to remove volume: remove my-vol: volume is in use - [6ef3055b516b306847150af8fcea796c02cd90578967802ac29c39d3a2c90102] Failure Deleting container that is attached is not permited. However you can delete with -f option Step 5 Busybox container stopped, howerver it is not deleted. Let's locate stopped busybox container and delete it: docker ps -a | grep busybox docker rm $docker_id Step 6 You can now delete my-vol Note Volume is still avaiable if needed to be reattached any time docker volume ls docker volume rm my-vol docker volume ls Summary Volumes can be craeted and managed separately from containers. 2.2.2 Start a container with a volume \u00b6 If you start a container with a volume that does not yet exist, Docker creates the volume for you. Step 1 Add a data volume to a container: docker run -d -P --name webapp -v /webapp training/webapp python app.py Result Command started a new container and created a new volume inside the container at /webapp. Step 2 Locate the volume on the host using the docker inspect command: docker inspect webapp | grep -A9 Mounts ``` **Output:** ``` \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\", \"Source\": \"/var/lib/docker/volumes/39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d/_data\", \"Destination\": \"/webapp\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" Step 3 List container docker volume ls Output: DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Step 5 Alternatively, you can specify a host directory you want to use as a data volume: mkdir db docker run -d --name db -v ~/db:/db training/postgres Step 2 Start an interactive session in the db container and create a new file in the /db directory: docker exec -it db bash Type inside docker containers console: root@9a7a4fbcc929:/# cd /db root@9a7a4fbcc929:/db# touch hello_from_db_container root@9a7a4fbcc929:/db# exit Step 4 Check that the local db directory contains the new file: ls db hello_from_db_container Step 5 Check that the data volume is persistent. Remove the db container: docker rm -f db Step 6 Create the db container again: docker run -d --name db -v ~/db:/db training/postgres Step 7 Check that its /db directory contains the hello_from_db_container file: docker exec -it db bash Run commands inside container: root@47a60c01590e:/# ls /db hello_from_db_container root@47a60c01590e:/# exit 2.2.3 Use a read-only volume \u00b6 Step 1 Mounting Volumes gives the container full read and write access to the directory. You can specify read-only permissions on the directory by adding the permissions :ro to the mount. If the container attempts to modify data within the directory it will error. docker run -d --name db1 -v ~/db:/db:ro training/postgres docker exec -it db1 bash cd db touch test Result touch: cannot touch 'test': Read-only file system $ exit Step 2 Clean up containers and volumes: docker rm -f $(docker ps -q) docker volume ls Output DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d docker volume rm 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Summary We've learned how to manage volumes with containers Hint If you Docker host has several Storage plugins configured (e.g. ceph, gluster) you can specify via --opt type=btrfs, nfs or --driver=glusterfs during docker volume creation.","title":"Lab 3 Advanced Docker"},{"location":"Lab_3_Advanced_Docker/#1-docker-networking","text":"","title":"1 Docker Networking"},{"location":"Lab_3_Advanced_Docker/#11-docker-networking-basics","text":"Step 1: The Docker Network Command The docker network command is the main command for configuring and managing container networks. Run the docker network command from the first terminal. docker network Usage: docker network COMMAND Manage networks Options: --help Print usage Commands: connect Connect a container to a network create Create a network disconnect Disconnect a container from a network inspect Display detailed information on one or more networks ls List networks prune Remove all unused networks rm Remove one or more networks Run 'docker network COMMAND --help' for more information on a command. The command output shows how to use the command as well as all of the docker network sub-commands. As you can see from the output, the docker network command allows you to create new networks, list existing networks, inspect networks, and remove networks. It also allows you to connect and disconnect containers from networks. Step 2 Run a docker network ls command to view existing container networks on the current Docker host. docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local The output above shows the container networks that are created as part of a standard installation of Docker. New networks that you create will also show up in the output of the docker network ls command. You can see that each network gets a unique ID and NAME . Each network is also associated with a single driver. Notice that the \"bridge\" network and the \"host\" network have the same name as their respective drivers. Step 3: The docker network inspect command is used to view network configuration details. These details include; name, ID, driver, IPAM driver, subnet info, connected containers, and more. Use docker network inspect <network> to view configuration details of the container networks on your Docker host. The command below shows the details of the network called bridge . docker network inspect bridge [ { \"Name\": \"bridge\", \"Id\": \"3430ad6f20bf1486df2e5f64ddc93cc4ff95d81f59b6baea8a510ad500df2e57\", \"Created\": \"2017-04-03T16:49:58.6536278Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Containers\": {}, \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" }, \"Labels\": {} } ] Note The syntax of the docker network inspect command is docker network inspect <network> , where <network> can be either network name or network ID. In the example above we are showing the configuration details for the network called \"bridge\". Do not confuse this with the \"bridge\" driver. Step 4 Now, list Docker supported network driver plugins. For that run docker info command, that shows a lot of interesting information about a Docker installation. Run the docker info command and locate the list of network plugins. docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ee-3 Storage Driver: aufs <Snip> Plugins: Volume: local Network: bridge host macvlan null overlay Swarm: inactive Runtimes: runc <Snip> The output above shows the bridge , host , macvlan , null , and overlay drivers. Summary We've quickly reviewed available docker networking commands as well as found what drivers current docker setup supports.","title":"1.1 Docker Networking Basics"},{"location":"Lab_3_Advanced_Docker/#12-default-bridge-network","text":"Every clean installation of Docker comes with a pre-built network called Default bridge network . Let's explore in more details how it works. Step 1 Verify this with the docker network ls . docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local Result The output above shows that the bridge network is associated with the bridge driver. It's important to note that the network and the driver are connected, but they are not the same. In this example the network and the driver have the same name - but they are not the same thing! The output above also shows that the bridge network is scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the bridge driver - the bridge driver provides single-host networking. All networks created with the bridge driver are based on a Linux bridge (a.k.a. a virtual switch). Step 5 Start webapp in Default bridge network docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 6 Check that the webapp and db containers are running: Command: docker ps","title":"1.2 Default bridge network"},{"location":"Lab_3_Advanced_Docker/#13-user-defined-private-networks","text":"So far we\u2019ve learned how Docker networking works with Docker default bridge network . With the introduction of user-defined networking in Docker 1.9, it is now possible to create multiple Docker bridges to allow network segregation within the same host or multi-host networking to allow communicate Docker containers between hosts. The commands are available through the Docker Engine CLI are: docker network create docker network connect docker network ls docker network rm docker network disconnect docker network inspect Let's demonstrate how to create a custom bridge network. Step 1 By default, Docker runs containers in the bridge network. You may want to isolate one or more containers in a separate network. Let\u2019s create a new network: docker network create my-network \\ -d bridge \\ --subnet 172.19.0.0/16 The -d bridge command line argument specifies the bridge network driver and the --subnet command line argument specifies the network segment in CIDR format. If you do not specify a subnet when creating a network, then Docker assigns a subnet automatically, so it is a good idea to specify a subnet to avoid potential conflicts with the existing networks. Below are some other options that are available with the bridge Driver: com.docker.network.bridge.enable_ip_masquerade: This instructs the Docker host to hide or masquerade all containers in this network behind the Docker host's interfaces if the container attempts to route off the local host . com.docker.network.bridge.name: This is the name you wish to give to the bridge. com.docker.network.bridge.enable_icc: This turns on or off Inter-Container Connectivity (ICC) mode for the bridge. com.docker.network.bridge.host_binding_ipv4: This defines the host interface that should be used for port binding. com.docker.network.driver.mtu: This sets MTU for containers attached to this bridge. Step 2 To check that the new network is created, execute docker network ls: docker network ls NETWORK ID NAME DRIVER SCOPE d428e49e4869 bridge bridge local 0d1f78528cc5 host host local 56ef0481820d my-network bridge local 4a07cef84617 none null local Step 3 Let\u2019s inspect the new network: docker network inspect my-network [ { \"Name\": \"my-network\", \"Id\": \"56ef0481820d...\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"172.19.0.0/16\" } ] }, \"Internal\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ] Step 4 As expected, there are no containers connected to the my-network. Let\u2019s recreate the db container in the my-network: docker rm -f db docker run -d --network=my-network --name db training/postgres Step 5 Inspect the my-network again: docker network inspect my-network Output: \"Containers\": { \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... As you see, the db container is connected to the my-network and has 172.19.0.2 address. Step 6 Let\u2019s start an interactive session in the db container and ping the IP address of the webapp again: Note Quick reminder how to locate webapp ip: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp docker exec -it db bash Once inside of container run: root@c3afff20019a:/# ping -c 1 172.17.0.3 PING 172.17.0.3 (172.17.0.3) 56(84) bytes of data. --- 172.17.0.3 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms As expected, the webapp container is no longer accessible from the db container, because they are connected to different networks. Summary Using Multi-host networking provides network isolation within a Docker host via network namepsaces. This is can be used if you want to deploy different applications on same host for isolation or resource duplicate prevention. Step 7 Let\u2019s connect the webapp container to the my-network: docker network connect my-network webapp Step 8 Check that the webapp container now is connected to the my-network: docker network inspect my-network Output: ... \"Containers\": { \"62ed4a627356...\": { \"Name\": \"webapp\", \"EndpointID\": \"ae95b0103bbc...\", \"MacAddress\": \"02:42:ac:12:00:03\", \"IPv4Address\": \"172.19.0.3/16\", \"IPv6Address\": \"\" }, \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... The output shows that two containers are connected to the my-network and the webapp container has 172.19.0.3 address in that network. Step 9 Check that the webapp container is accessible from the db container using its new IP address: docker exec -it db bash root@c3afff20019a:/# ping -c 1 172.19.0.3 PING 172.19.0.3 (172.19.0.3) 56(84) bytes of data. 64 bytes from 172.19.0.3: icmp_seq=1 ttl=64 time=0.136 ms --- 172.19.0.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms Success As expected containers can communicate with each other. Step 10 You can now remove the existing container. You should stop the container before removing it. Alternatively you can use the -f command line argument: docker rm -f webapp docker rm -f db docker network rm my-network Hint Use below command to delete running containers in bulk : docker rm -f $(docker ps -q) Summary It is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses","title":"1.3 User-defined Private Networks"},{"location":"Lab_3_Advanced_Docker/#14-access-containers-from-outside","text":"External Access to the Containers can be configured via publishing mechanism. Docker provides 2 options to publish ports: -P flag publishes all exposed ports -p flag allows you to specify specific ports and interfaces to use when mapping ports. The -p flag can take several different forms with the syntax looking like this: Specify the host port and container port: \u2013p <host port>:<container port> Specify the host interface, host port, and container port: \u2013p <host IP interface>:<host port>:<container port> Specify the host interface, have Docker choose a random host port, and specify the container port: \u2013p <host IP interface>::<container port> Specify only a container port and have Docker use a random host port: \u2013p <container port> Let's test exposing containers. For that let's start a new NGINX container and map port 8080 on the Docker host to port 80 inside of the container. This means that traffic that hits the Docker host on port 8080 will be passed on to port 80 inside the container. Note If you start a new container from the official NGINX image without specifying a command to run, the container will run a basic web server on port 80. Step 1 Start a new container based off the official NGINX image by running docker run --name web1 -d -p 8080:80 nginx . docker run --name web1 -d -p 8080:80 nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 6d827a3ef358: Pull complete b556b18c7952: Pull complete 03558b976e24: Pull complete 9abee7e1ef9d: Pull complete Digest: sha256:52f84ace6ea43f2f58937e5f9fc562e99ad6876e82b99d171916c1ece587c188 Status: Downloaded newer image for nginx:latest 4e0da45b0f169f18b0e1ee9bf779500cb0f756402c0a0821d55565f162741b3e Step 2 Review the container status and port mappings by running docker ps . docker ps CONTAINER ID IMAGE COMMAND PORTS NAMES 4e0da45b0f16 nginx \"nginx -g 'daemon ...\" 443/tcp, 0.0.0.0:8080->80/tcp web1 Result The top line shows the new web1 container running NGINX. Take note of the command the container is running as well as the port mapping - 0.0.0.0:8080->80/tcp maps port 8080 on all host interfaces to port 80 inside the web1 container. This port mapping is what effectively makes the containers web service accessible from external sources (via the Docker hosts IP address on port 8080). Step 3 Test connectivity to the NGINX web server, by pasting <Public_IP:8080> of VM to the browser. Note In order to locate Public IP see the list of VMs. Alternatively from inside of VM run curl 127.0.0.1:8080 command. curl 127.0.0.1:8080 <!DOCTYPE html> <html> <Snip> <head> <title>Welcome to nginx!</title> <Snip> <p><em>Thank you for using nginx.</em></p> </body> </html> Success Both CLI and UI method works! If you try and curl the IP address on a different port number it will fail. Summary Docker provides easy way to expose containers outside of the Docker Node. This can ber used for connecting containers between each other: Between networks on the same host Between networks on different host Accessing containers from outside (e.g web site) However, port mapping is implemented via port address translation (PAT) unlike in Kubernetes which we learn soon, exposes applications via service IPs and communicates via POD IPs using (NAT) Step 4 Cleanup environment docker rm -f $(docker ps -q)","title":"1.4 Access containers from outside"},{"location":"Lab_3_Advanced_Docker/#2-persistant-volumes","text":"","title":"2 Persistant Volumes"},{"location":"Lab_3_Advanced_Docker/#21-storage-driver","text":"We've discussed several Storage drivers (graphdrivers) during the class. Let's find out what graphdriver is running in our Lab environment. docker info | grep Storage WARNING: No swap limit support Storage Driver: aufs Result Our Classroom is running aufs storage driver. Not a suprise as we running our Lab on Ubuntu VM. Summary Systems runnng Ubuntu or Debian ,going to run aufs graphdriver by default and will most likely meet the majority of your needs. In future overlay2 may replace aufs stay tunned!","title":"2.1 Storage driver"},{"location":"Lab_3_Advanced_Docker/#22-persisting-data-using-volumes","text":"Docker Volumes are created and assigned when containers are started. Data Volumes allow you to map a host directory to a container for sharing data. This mapping is bi-directional. It allows data stored on the host to be accessed from within the container. It also means data saved by the process inside the container is persisted on the host.","title":"2.2 Persisting Data Using Volumes"},{"location":"Lab_3_Advanced_Docker/#221-create-and-manage-volumes","text":"Step 1 Create a volume: docker volume create --name my-vol Step 2 List volumes: docker volume ls Output: local my-vol Step 3 Inspect a volume: docker volume inspect my-vol [ { \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\", \"Name\": \"my-vol\", \"Options\": {}, \"Scope\": \"local\" } ] Step 3 Add some data to the Mountpoint of the volume: sudo touch /var/lib/docker/volumes/my-vol/_data/test_vol sudo ls /var/lib/docker/volumes/my-vol/_data/ Step 4 Create a container busybox alpine image and attach created my-vol volume in to it: docker run -it -v my-vol:/world busybox / # ls /world test_vol / # Result Volume is mounted and test_vol file is under /world folder as expected Step 5 Try to delete the volume: docker volume rm my-vol Error response from daemon: unable to remove volume: remove my-vol: volume is in use - [6ef3055b516b306847150af8fcea796c02cd90578967802ac29c39d3a2c90102] Failure Deleting container that is attached is not permited. However you can delete with -f option Step 5 Busybox container stopped, howerver it is not deleted. Let's locate stopped busybox container and delete it: docker ps -a | grep busybox docker rm $docker_id Step 6 You can now delete my-vol Note Volume is still avaiable if needed to be reattached any time docker volume ls docker volume rm my-vol docker volume ls Summary Volumes can be craeted and managed separately from containers.","title":"2.2.1  Create and manage volumes"},{"location":"Lab_3_Advanced_Docker/#222-start-a-container-with-a-volume","text":"If you start a container with a volume that does not yet exist, Docker creates the volume for you. Step 1 Add a data volume to a container: docker run -d -P --name webapp -v /webapp training/webapp python app.py Result Command started a new container and created a new volume inside the container at /webapp. Step 2 Locate the volume on the host using the docker inspect command: docker inspect webapp | grep -A9 Mounts ``` **Output:** ``` \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\", \"Source\": \"/var/lib/docker/volumes/39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d/_data\", \"Destination\": \"/webapp\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" Step 3 List container docker volume ls Output: DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Step 5 Alternatively, you can specify a host directory you want to use as a data volume: mkdir db docker run -d --name db -v ~/db:/db training/postgres Step 2 Start an interactive session in the db container and create a new file in the /db directory: docker exec -it db bash Type inside docker containers console: root@9a7a4fbcc929:/# cd /db root@9a7a4fbcc929:/db# touch hello_from_db_container root@9a7a4fbcc929:/db# exit Step 4 Check that the local db directory contains the new file: ls db hello_from_db_container Step 5 Check that the data volume is persistent. Remove the db container: docker rm -f db Step 6 Create the db container again: docker run -d --name db -v ~/db:/db training/postgres Step 7 Check that its /db directory contains the hello_from_db_container file: docker exec -it db bash Run commands inside container: root@47a60c01590e:/# ls /db hello_from_db_container root@47a60c01590e:/# exit","title":"2.2.2 Start a container with a volume"},{"location":"Lab_3_Advanced_Docker/#223-use-a-read-only-volume","text":"Step 1 Mounting Volumes gives the container full read and write access to the directory. You can specify read-only permissions on the directory by adding the permissions :ro to the mount. If the container attempts to modify data within the directory it will error. docker run -d --name db1 -v ~/db:/db:ro training/postgres docker exec -it db1 bash cd db touch test Result touch: cannot touch 'test': Read-only file system $ exit Step 2 Clean up containers and volumes: docker rm -f $(docker ps -q) docker volume ls Output DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d docker volume rm 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Summary We've learned how to manage volumes with containers Hint If you Docker host has several Storage plugins configured (e.g. ceph, gluster) you can specify via --opt type=btrfs, nfs or --driver=glusterfs during docker volume creation.","title":"2.2.3 Use a read-only volume"},{"location":"Lab_4_Docker_Images/","text":"Lab 4 Managing Docker Images Objective: Learn to build docker images using Dockerfiles. Store images in Docker Hub Learn alternative registry solutions (GCR) Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1 Distributing Docker images with Container Registry \u00b6 In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: https://github.com/Cloud-Architects-Program/ycit019/tree/main/Module4/flask-app 1.1 Create DOCKERFILE \u00b6 Step 1 Clone git repo on you laptop: git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\" , \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\" , \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\" , \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\" , \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\" , \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\" , \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\" , \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\" , \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\" , \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==2.0.0 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # Official Python Alpine Base image using Simple Tags # Image contains Python 3 and pip pre-installed, so no need to install them FROM python:3.9.5-alpine3.12 # Specify Working directory WORKDIR /usr/src/app # COPY requirements.txt /usr/src/app/ COPY requirements.txt ./ # Install Python Flask used by the Python app RUN pip install --no-cache-dir -r requirements.txt # Copy files required for the app to run COPY app.py ./ COPY templates/index.html ./templates/ # Make a record that the port number the container should be expose is: EXPOSE 5000 # run the application CMD [ \"python\" , \"./app.py\" ] 1.2 Build a Docker image \u00b6 Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step to your own Docker Hub. docker build -t <Docker-hub-user-name>/myfirstapp . Result Image has been buit Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 8080:5000 --name myfirstapp <Docker-hub-user-name>/myfirstapp Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Stop the container and remove it: docker rm -f myfirstapp 1.2.2 Publish Docker Image to Docker Hub \u00b6 One of the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <Docker-hub-user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry Note Image Tag of the created myfirstapp : docker images Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag <Docker-hub-user-name>/myfirstapp:v1 docker push <Docker-hub-user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <Docker-hub-user-name>/myfirstapp:latest docker pull <Docker-hub-user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <Docker-hub-user-name>/myfirstapp:v1 1.2.3 Pushing images to gcr.io \u00b6 In a similar manner we need to tag the image to prepare it to be pushed to gcr.io. We just need to change the registry, which is for gcr.io formatted as gcr.io/PROJECT_ID. Step 1 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 2 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 3 Tag the image: Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag gcr.io/${PROJECT_ID}/myfirstapp:v1 $docker_image_tag Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/myfirstapp:v1 Step 4 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 1.2.3 Pushing images to Local Repository \u00b6 First, we need to spin up a local docker registry. This could be a use case if you want to deploy basic registry On-Prem. This registry will luck security features such as Authentication, SSL, scanning. If you interested to use Enterprise ready solution On-Prem consider: Jfrog Artifactory, RedHa's Clair, Docker Enterprise or open source CNCF project Harbor. Step 1 Deploy local registry docker run -d -p 5000:5000 --name registry registry:2.7.1 Step 2 In order to upload an image to a registry, we need to tag it properly Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag localhost:5000/myfirstapp:v1 Step 3 Now that we have an image tagged correctly, we can push it to our local registry docker push localhost:5000/myfirstapp:v1 Step 4 Let\u2019s now delete the local image, and pull it again from the local registry To delete the image, we need to first remove the container that depends on that image. Run docker ps and get the Container_ID for the container that uses myfirstapp:v1 Kill and delete that container by running the following command, but make sure to replace CONTAINER_ID, with the actual ID. docker rm CONTAINER_ID Result: The command will print back the container ID, which is an indication it was successful. Step 5 Run docker images to validate docker images Step 6 Now we can delete the docker image docker rmi localhost:5000/myfirstapp:v1 Step 7 Although the image is deleted locally, it is still in the registry and we can pull it back, or use it to deploy containers. docker run -dp 8080:5000 --name myfirstapp localhost:5000/myfirstapp:v1 Run docker images again to check how the image is available locally again. docker images Step 8 Cleanup: docker rm -f myfirstapp 2 Follow Docker Best Practices \u00b6 2.1 Inspecting Dockerfiles with dockle \u00b6 Dockle - Container Image Linter for Security, Helping build the Best-Practice Docker Image, Easy to start Dockle helps you: Build Best Practice Docker images Build secure Docker images Checkpoints includes CIS Benchmarks Step 1 Install Dockle $ VERSION=$( curl --silent \"https://api.github.com/repos/goodwithtech/dockle/releases/latest\" | \\ grep '\"tag_name\":' | \\ sed -E 's/.*\"v([^\"]+)\".*/\\1/' \\ ) && curl -L -o dockle.deb https://github.com/goodwithtech/dockle/releases/download/v${VERSION}/dockle_${VERSION}_Linux-64bit.deb $ sudo dpkg -i dockle.deb && rm dockle.deb Step 2 Experiment with existing applications we've created in the class: $ dockle [YOUR_IMAGE_NAME] e.g. dockle archy/myfirstapp output: WARN - CIS-DI-0001: Create a user for the container * Last user should not be root WARN - DKL-DI-0006: Avoid latest tag * Avoid 'latest' tag INFO - CIS-DI-0005: Enable Content trust for Docker * export DOCKER_CONTENT_TRUST=1 before docker pull/build INFO - CIS-DI-0006: Add HEALTHCHECK instruction to the container image * not found HEALTHCHECK statement INFO - DKL-LI-0003: Only put necessary files * Suspicious directory : tmp 2.2 Automated Builds with Google Cloud Build \u00b6 Live Demo: GCR Image scanning Setting Up Docker Image Auto-Build with Google Cloud Build based on Push to Branch Auto Deployment of Image to Cloud Run","title":"Lab 4 Managing Docker Images"},{"location":"Lab_4_Docker_Images/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"Lab_4_Docker_Images/#1-distributing-docker-images-with-container-registry","text":"In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: https://github.com/Cloud-Architects-Program/ycit019/tree/main/Module4/flask-app","title":"1 Distributing Docker images with Container Registry"},{"location":"Lab_4_Docker_Images/#11-create-dockerfile","text":"Step 1 Clone git repo on you laptop: git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\" , \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\" , \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\" , \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\" , \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\" , \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\" , \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\" , \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\" , \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\" , \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==2.0.0 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # Official Python Alpine Base image using Simple Tags # Image contains Python 3 and pip pre-installed, so no need to install them FROM python:3.9.5-alpine3.12 # Specify Working directory WORKDIR /usr/src/app # COPY requirements.txt /usr/src/app/ COPY requirements.txt ./ # Install Python Flask used by the Python app RUN pip install --no-cache-dir -r requirements.txt # Copy files required for the app to run COPY app.py ./ COPY templates/index.html ./templates/ # Make a record that the port number the container should be expose is: EXPOSE 5000 # run the application CMD [ \"python\" , \"./app.py\" ]","title":"1.1 Create DOCKERFILE"},{"location":"Lab_4_Docker_Images/#12-build-a-docker-image","text":"Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step to your own Docker Hub. docker build -t <Docker-hub-user-name>/myfirstapp . Result Image has been buit Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 8080:5000 --name myfirstapp <Docker-hub-user-name>/myfirstapp Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Stop the container and remove it: docker rm -f myfirstapp","title":"1.2 Build a Docker image"},{"location":"Lab_4_Docker_Images/#122-publish-docker-image-to-docker-hub","text":"One of the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <Docker-hub-user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry Note Image Tag of the created myfirstapp : docker images Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag <Docker-hub-user-name>/myfirstapp:v1 docker push <Docker-hub-user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <Docker-hub-user-name>/myfirstapp:latest docker pull <Docker-hub-user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <Docker-hub-user-name>/myfirstapp:v1","title":"1.2.2 Publish Docker Image to Docker Hub"},{"location":"Lab_4_Docker_Images/#123-pushing-images-to-gcrio","text":"In a similar manner we need to tag the image to prepare it to be pushed to gcr.io. We just need to change the registry, which is for gcr.io formatted as gcr.io/PROJECT_ID. Step 1 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 2 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 3 Tag the image: Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag gcr.io/${PROJECT_ID}/myfirstapp:v1 $docker_image_tag Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/myfirstapp:v1 Step 4 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"1.2.3 Pushing images to gcr.io"},{"location":"Lab_4_Docker_Images/#123-pushing-images-to-local-repository","text":"First, we need to spin up a local docker registry. This could be a use case if you want to deploy basic registry On-Prem. This registry will luck security features such as Authentication, SSL, scanning. If you interested to use Enterprise ready solution On-Prem consider: Jfrog Artifactory, RedHa's Clair, Docker Enterprise or open source CNCF project Harbor. Step 1 Deploy local registry docker run -d -p 5000:5000 --name registry registry:2.7.1 Step 2 In order to upload an image to a registry, we need to tag it properly Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag localhost:5000/myfirstapp:v1 Step 3 Now that we have an image tagged correctly, we can push it to our local registry docker push localhost:5000/myfirstapp:v1 Step 4 Let\u2019s now delete the local image, and pull it again from the local registry To delete the image, we need to first remove the container that depends on that image. Run docker ps and get the Container_ID for the container that uses myfirstapp:v1 Kill and delete that container by running the following command, but make sure to replace CONTAINER_ID, with the actual ID. docker rm CONTAINER_ID Result: The command will print back the container ID, which is an indication it was successful. Step 5 Run docker images to validate docker images Step 6 Now we can delete the docker image docker rmi localhost:5000/myfirstapp:v1 Step 7 Although the image is deleted locally, it is still in the registry and we can pull it back, or use it to deploy containers. docker run -dp 8080:5000 --name myfirstapp localhost:5000/myfirstapp:v1 Run docker images again to check how the image is available locally again. docker images Step 8 Cleanup: docker rm -f myfirstapp","title":"1.2.3 Pushing images to Local Repository"},{"location":"Lab_4_Docker_Images/#2-follow-docker-best-practices","text":"","title":"2 Follow Docker Best Practices"},{"location":"Lab_4_Docker_Images/#21-inspecting-dockerfiles-with-dockle","text":"Dockle - Container Image Linter for Security, Helping build the Best-Practice Docker Image, Easy to start Dockle helps you: Build Best Practice Docker images Build secure Docker images Checkpoints includes CIS Benchmarks Step 1 Install Dockle $ VERSION=$( curl --silent \"https://api.github.com/repos/goodwithtech/dockle/releases/latest\" | \\ grep '\"tag_name\":' | \\ sed -E 's/.*\"v([^\"]+)\".*/\\1/' \\ ) && curl -L -o dockle.deb https://github.com/goodwithtech/dockle/releases/download/v${VERSION}/dockle_${VERSION}_Linux-64bit.deb $ sudo dpkg -i dockle.deb && rm dockle.deb Step 2 Experiment with existing applications we've created in the class: $ dockle [YOUR_IMAGE_NAME] e.g. dockle archy/myfirstapp output: WARN - CIS-DI-0001: Create a user for the container * Last user should not be root WARN - DKL-DI-0006: Avoid latest tag * Avoid 'latest' tag INFO - CIS-DI-0005: Enable Content trust for Docker * export DOCKER_CONTENT_TRUST=1 before docker pull/build INFO - CIS-DI-0006: Add HEALTHCHECK instruction to the container image * not found HEALTHCHECK statement INFO - DKL-LI-0003: Only put necessary files * Suspicious directory : tmp","title":"2.1 Inspecting Dockerfiles with dockle"},{"location":"Lab_4_Docker_Images/#22-automated-builds-with-google-cloud-build","text":"Live Demo: GCR Image scanning Setting Up Docker Image Auto-Build with Google Cloud Build based on Push to Branch Auto Deployment of Image to Cloud Run","title":"2.2 Automated Builds with Google Cloud Build"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/","text":"Lab 4 Managing Docker Images Objective: Learn to build docker images using Dockerfiles. Store images in Docker Hub Learn alternative registry solutions (Quya.io) Automate image build process with Docker Cloud 1 Building Docker Images \u00b6 In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a registry of Docker images. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: git clone https://github.com/archyufa/k8scanada 1.1 Create DOCKERFILE \u00b6 Step 1 Clone git repo on you laptop: git clone https://github.com/archyufa/k8scanada cd k8scanada/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26388-1381844103-11.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr01/15/9/anigif_enhanced-buzz-31540-1381844535-8.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26390-1381844163-18.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-1376-1381846217-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3391-1381844336-26.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-29111-1381845968-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3409-1381844582-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr02/15/9/anigif_enhanced-buzz-19667-1381844937-10.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26358-1381845043-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-18774-1381844645-6.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-25158-1381844793-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/10/anigif_enhanced-buzz-11980-1381846269-1.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==0.10.1 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # our base image FROM alpine:3.5 # Install python and pip RUN apk add --update py2-pip # upgrade pip RUN pip install --upgrade pip # install Python modules needed by the Python app COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt # copy files required for the app to run COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ # tell the port number the container should expose EXPOSE 5000 # run the application CMD [ \"python\" , \"/usr/src/app/app.py\" ] 1.2 Build a Docker image \u00b6 Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step. docker build -t <user-name>/myfirstapp . Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 80:5000 --name myfirstapp <user-name>/myfirstapp Step 4 Use your browser to open the address http:// and check that the application works. Step 5 Stop the container and remove it: docker stop myfirstapp docker rm myfirstapp myfirstapp 1.2.1 Share docker images with tar files \u00b6 Now ideally you want to share you freshly build docker image with someone or run it in different environment which for some reason don\u2019t have internet access so images can not be pulled from Online Docker registries. In that case Docker images can be shared as we share traditionally regular files by creating tarballs using docker save command as following Step 1 Create a tar file using docker save command: docker save <user-name>/myfirstapp > myfirstapp.tar Or docker save --output myfirstapp1.tar archyufa/myfirstapp ls -trh | grep tar Step 2 Transfer images to another environment using scp command. Hint You can also store this images in Object storages, e.g. Swift or Amazon S3 using version control. Step 3 Now you can restore this images using docker load, that will load a tarred repository from a file or the standard input stream. It restores both images and tags. docker load < myfirstapp.tar 23b9c7b43573: Loading layer [==================================================>] 4.23MB/4.23MB b3b5c1214f71: Loading layer [==================================================>] 52.87MB/52.87MB f877d8dd64d3: Loading layer [==================================================>] 8.636MB/8.636MB bba871f91589: Loading layer [==================================================>] 3.584kB/3.584kB 1c131e92eb5f: Loading layer [==================================================>] 5.053MB/5.053MB 3f6463bcb64c: Loading layer [==================================================>] 5.12kB/5.12kB 47c61110467a: Loading layer [==================================================>] 4.096kB/4.096kB Loaded image: archyufa/myfirstapp:latest docker images 1.2.2 Publish Docker Image to Docker Hub \u00b6 However the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry docker tag 5b45ce063cea <user-name>/myfirstapp:v1 docker push <user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <user-name>/myfirstapp:latest docker pull <user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <user-name>/myfirstapp:v1 1.2.3 Automated Builds with Docker Cloud \u00b6 Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud and notifications to slac Automated Tests with PR 1.2.4 Push Docker Images to quay.io \u00b6 Prerequisite: Register to quay.io with your Github user Step 1 Login to quay.io from CLI docker login quay.io Step 2 Build image with quay prefix docker build -t quay.io/archyufa/myfirstapp . Step 3 Push image to quay registry docker push quay.io/archyufa/myfirstapp 1.2.5 Demo quay.io \u00b6 Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud / Quai.io","title":"Lab 4 Docker Images Docker Hub Quya io"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#1-building-docker-images","text":"In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a registry of Docker images. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: git clone https://github.com/archyufa/k8scanada","title":"1 Building Docker Images"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#11-create-dockerfile","text":"Step 1 Clone git repo on you laptop: git clone https://github.com/archyufa/k8scanada cd k8scanada/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26388-1381844103-11.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr01/15/9/anigif_enhanced-buzz-31540-1381844535-8.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26390-1381844163-18.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-1376-1381846217-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3391-1381844336-26.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-29111-1381845968-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3409-1381844582-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr02/15/9/anigif_enhanced-buzz-19667-1381844937-10.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26358-1381845043-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-18774-1381844645-6.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-25158-1381844793-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/10/anigif_enhanced-buzz-11980-1381846269-1.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==0.10.1 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # our base image FROM alpine:3.5 # Install python and pip RUN apk add --update py2-pip # upgrade pip RUN pip install --upgrade pip # install Python modules needed by the Python app COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt # copy files required for the app to run COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ # tell the port number the container should expose EXPOSE 5000 # run the application CMD [ \"python\" , \"/usr/src/app/app.py\" ]","title":"1.1 Create DOCKERFILE"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#12-build-a-docker-image","text":"Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step. docker build -t <user-name>/myfirstapp . Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 80:5000 --name myfirstapp <user-name>/myfirstapp Step 4 Use your browser to open the address http:// and check that the application works. Step 5 Stop the container and remove it: docker stop myfirstapp docker rm myfirstapp myfirstapp","title":"1.2 Build a Docker image"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#121-share-docker-images-with-tar-files","text":"Now ideally you want to share you freshly build docker image with someone or run it in different environment which for some reason don\u2019t have internet access so images can not be pulled from Online Docker registries. In that case Docker images can be shared as we share traditionally regular files by creating tarballs using docker save command as following Step 1 Create a tar file using docker save command: docker save <user-name>/myfirstapp > myfirstapp.tar Or docker save --output myfirstapp1.tar archyufa/myfirstapp ls -trh | grep tar Step 2 Transfer images to another environment using scp command. Hint You can also store this images in Object storages, e.g. Swift or Amazon S3 using version control. Step 3 Now you can restore this images using docker load, that will load a tarred repository from a file or the standard input stream. It restores both images and tags. docker load < myfirstapp.tar 23b9c7b43573: Loading layer [==================================================>] 4.23MB/4.23MB b3b5c1214f71: Loading layer [==================================================>] 52.87MB/52.87MB f877d8dd64d3: Loading layer [==================================================>] 8.636MB/8.636MB bba871f91589: Loading layer [==================================================>] 3.584kB/3.584kB 1c131e92eb5f: Loading layer [==================================================>] 5.053MB/5.053MB 3f6463bcb64c: Loading layer [==================================================>] 5.12kB/5.12kB 47c61110467a: Loading layer [==================================================>] 4.096kB/4.096kB Loaded image: archyufa/myfirstapp:latest docker images","title":"1.2.1 Share docker images with tar files"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#122-publish-docker-image-to-docker-hub","text":"However the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry docker tag 5b45ce063cea <user-name>/myfirstapp:v1 docker push <user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <user-name>/myfirstapp:latest docker pull <user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <user-name>/myfirstapp:v1","title":"1.2.2 Publish Docker Image to Docker Hub"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#123-automated-builds-with-docker-cloud","text":"Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud and notifications to slac Automated Tests with PR","title":"1.2.3 Automated Builds with Docker Cloud"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#124-push-docker-images-to-quayio","text":"Prerequisite: Register to quay.io with your Github user Step 1 Login to quay.io from CLI docker login quay.io Step 2 Build image with quay prefix docker build -t quay.io/archyufa/myfirstapp . Step 3 Push image to quay registry docker push quay.io/archyufa/myfirstapp","title":"1.2.4 Push Docker Images to quay.io"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#125-demo-quayio","text":"Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud / Quai.io","title":"1.2.5 Demo quay.io"},{"location":"Lab_5_Docker_Compose/","text":"Lab 5 Docker Compose and Docker Security Objective: Practice to use Docker Compose, 1 Docker Security \u00b6 1.1 Scan images with Trivy \u00b6 Trivy (tri pronounced like trigger, vy pronounced like envy) is a simple and comprehensive vulnerability scanner for containers and other artifacts. A software vulnerability is a glitch, flaw, or weakness present in the software or in an Operating System. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and application dependencies (Bundler, Composer, npm, yarn, etc.). Trivy is easy to use. Just install the binary and you're ready to scan. All you need to do for scanning is to specify a target such as an image name of the container. Step 1 Install Trivy sudo apt-get install wget apt-transport-https gnupg lsb-release wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add - echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list sudo apt-get update sudo apt-get install trivy Step 2 Specify an image name (and a tag). $ trivy image [YOUR_IMAGE_NAME] For example: $ trivy image python:3.4-alpine 2019-05-16T01:20:43.180+0900 INFO Updating vulnerability database... 2019-05-16T01:20:53.029+0900 INFO Detecting Alpine vulnerabilities... python:3.4-alpine3.9 (alpine 3.9.2) =================================== Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0) +---------+------------------+----------+-------------------+---------------+--------------------------------+ | LIBRARY | VULNERABILITY ID | SEVERITY | INSTALLED VERSION | FIXED VERSION | TITLE | +---------+------------------+----------+-------------------+---------------+--------------------------------+ | openssl | CVE-2019-1543 | MEDIUM | 1.1.1a-r1 | 1.1.1b-r1 | openssl: ChaCha20-Poly1305 | | | | | | | with long nonces | +---------+------------------+----------+-------------------+---------------+--------------------------------+ Step 3 Explore local images in your environment. 2 Docker Compose \u00b6 In this module, will guide you through the process of building a multi-container application using docker compose. The application code is available at GitHub: https://github.com/Cloud-Architects-Program/ycit019 2.1 Deploy Guestbook app with Compose \u00b6 Let\u2019s build another application. This time we going to create famous Guestbook application. Guestbook consists of three services. A redis-master node, a set of redis-slave that can be scaled and find the redis-master via its DNS name. And a PHP frontend that exposes itself on port 80. The resulting application allows you to leave short messages which are stored in the redis cluster. Step 1 Change directory to the guestbook cd ~/ycit019/Module5/guestbook/ ls Step 2 Let\u2019s review the docker-guestbook.yml file version: \"2\" services: redis-master: image: gcr.io/google_containers/redis:e2e ports: - \"6379\" redis-slave: image: gcr.io/google_samples/gb-redisslave:v1 ports: - \"6379\" environment: - GET_HOSTS_FROM=dns frontend: image: gcr.io/google-samples/gb-frontend:v4 ports: - \"80:80\" environment: - GET_HOSTS_FROM=dns Step 3 Let\u2019s run docker-guestbook.yml with compose export LD_LIBRARY_PATH=/usr/local/lib docker-compose -f docker-guestbook.yml up -d Creating network \"examples_default\" with the default driver Creating examples_redis-slave_1 Creating examples_frontend_1 Creating examples_redis-master_1 Note -d - Detached mode: Run containers in the background, print new container names. -f - Specify an alternate compose file (default: docker-compose.yml) Step 4 Check that all containers are running: docker ps CONTAINER ID IMAGE COMMAND d1006d1beee5 gcr.io/google-samples/gb-frontend:v4 \"apache2-foreground\" fb3a15fde23f gcr.io/google_containers/redis:e2e \"redis-server /etc...\" 326b94d4cdd7 gcr.io/google_samples/gb-redisslave:v1 \"/entrypoint.sh /b...\" Step 5 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Success Nice you now have compose stuck up and running! Step 6 Cleanup environment: docker-compose -f docker-guestbook.yml down Stopping guestbook_frontend_1 ... done Stopping guestbook_redis-master_1 ... done Stopping guestbook_redis-slave_1 ... done Removing guestbook_frontend_1 ... done Removing guestbook_redis-master_1 ... done Removing guestbook_redis-slave_1 ... done Removing network guestbook_default 2.2 Deploy Voting App using Compose \u00b6 Step 1 Switch to Module5/example-voting-app folder : cd ~/ycit019/Module5/example-voting-app/ Step 2 The existing file docker-compose.yml defines several images: A voting-app container based on a Python image A result-app container based on a Node.js image A Redis container based on a redis image, to temporarily store the data. A worker app based on a dotnet image A Postgres container based on a postgres image App Architecture: Note that three of the containers are built from Dockerfiles, while the other two are images on Docker Hub. Let's review them closely: Step 3 Review files that going to be deployed with tree command. Alternatively view the files in gitrepo page here sudo apt install tree tree Step 5 Let\u2019s change the default port to expose. Edit the docker-compose.yml file and find the following lines: ports: - \"5000:80\" Change 5000 to 8080: ports: - \"8080:80\" Step 4 Verify Docker Compose version: docker-compose version Step 5 Use the docker-compose tool to launch your application: docker-compose up -d Step 6 Check that all containers are running, volumes created. Check compose state and logs : #Docker state docker ps docker volumes #Docker compose state docker-compose ps docker-compose logs Step 7 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 8 Cleanup up. docker-compose down Stopping examplevotingapp_worker_1 ... done Stopping examplevotingapp_redis_1 ... done Stopping examplevotingapp_result_1 ... done Stopping examplevotingapp_db_1 ... done Stopping examplevotingapp_vote_1 ... done Removing examplevotingapp_worker_1 ... done Removing examplevotingapp_redis_1 ... done Removing examplevotingapp_result_1 ... done Removing examplevotingapp_db_1 ... done Removing examplevotingapp_vote_1 ... done Removing network examplevotingapp_default Step 9 You Boss told you that the application has a bug. Update the the app by editing the vote/app.py file and change the following lines near the top of the file: vim vote/app.py Press 'i' option_a = os.getenv('OPTION_A', \"Cats\") option_b = os.getenv('OPTION_B', \"Dogs\") Step 10 Replace \u201cCats\u201d and \u201cDogs\u201d with two options of your choice. For example: option_a = os.getenv('OPTION_A', \"Java\") option_b = os.getenv('OPTION_B', \"Python\") Press 'wq!' Step 11 Use docker-compose tool to launch your Update application: docker-compose up -d Check the UI Bingo Let's see who wins the battle of Orchestrations! Step 8 Cleanup up docker-compose down Congratulations You are now docker expert! We were able to start 2 microservices application with docker compose. First microservice had 3 services. Second microservice had 5 servics written in 3 different languages and able to talk to each other. Summary So far we've learned docker-compose v2. docker-compose v3 is out of scope for this Lab. However you got the idea! Read the Docker-Compose documentation on new syntax. Also example of v3 version of voting-app is here for you reference.","title":"Lab 5 Docker Compose"},{"location":"Lab_5_Docker_Compose/#1-docker-security","text":"","title":"1 Docker Security"},{"location":"Lab_5_Docker_Compose/#11-scan-images-with-trivy","text":"Trivy (tri pronounced like trigger, vy pronounced like envy) is a simple and comprehensive vulnerability scanner for containers and other artifacts. A software vulnerability is a glitch, flaw, or weakness present in the software or in an Operating System. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and application dependencies (Bundler, Composer, npm, yarn, etc.). Trivy is easy to use. Just install the binary and you're ready to scan. All you need to do for scanning is to specify a target such as an image name of the container. Step 1 Install Trivy sudo apt-get install wget apt-transport-https gnupg lsb-release wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add - echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list sudo apt-get update sudo apt-get install trivy Step 2 Specify an image name (and a tag). $ trivy image [YOUR_IMAGE_NAME] For example: $ trivy image python:3.4-alpine 2019-05-16T01:20:43.180+0900 INFO Updating vulnerability database... 2019-05-16T01:20:53.029+0900 INFO Detecting Alpine vulnerabilities... python:3.4-alpine3.9 (alpine 3.9.2) =================================== Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0) +---------+------------------+----------+-------------------+---------------+--------------------------------+ | LIBRARY | VULNERABILITY ID | SEVERITY | INSTALLED VERSION | FIXED VERSION | TITLE | +---------+------------------+----------+-------------------+---------------+--------------------------------+ | openssl | CVE-2019-1543 | MEDIUM | 1.1.1a-r1 | 1.1.1b-r1 | openssl: ChaCha20-Poly1305 | | | | | | | with long nonces | +---------+------------------+----------+-------------------+---------------+--------------------------------+ Step 3 Explore local images in your environment.","title":"1.1 Scan images with Trivy"},{"location":"Lab_5_Docker_Compose/#2-docker-compose","text":"In this module, will guide you through the process of building a multi-container application using docker compose. The application code is available at GitHub: https://github.com/Cloud-Architects-Program/ycit019","title":"2 Docker Compose"},{"location":"Lab_5_Docker_Compose/#21-deploy-guestbook-app-with-compose","text":"Let\u2019s build another application. This time we going to create famous Guestbook application. Guestbook consists of three services. A redis-master node, a set of redis-slave that can be scaled and find the redis-master via its DNS name. And a PHP frontend that exposes itself on port 80. The resulting application allows you to leave short messages which are stored in the redis cluster. Step 1 Change directory to the guestbook cd ~/ycit019/Module5/guestbook/ ls Step 2 Let\u2019s review the docker-guestbook.yml file version: \"2\" services: redis-master: image: gcr.io/google_containers/redis:e2e ports: - \"6379\" redis-slave: image: gcr.io/google_samples/gb-redisslave:v1 ports: - \"6379\" environment: - GET_HOSTS_FROM=dns frontend: image: gcr.io/google-samples/gb-frontend:v4 ports: - \"80:80\" environment: - GET_HOSTS_FROM=dns Step 3 Let\u2019s run docker-guestbook.yml with compose export LD_LIBRARY_PATH=/usr/local/lib docker-compose -f docker-guestbook.yml up -d Creating network \"examples_default\" with the default driver Creating examples_redis-slave_1 Creating examples_frontend_1 Creating examples_redis-master_1 Note -d - Detached mode: Run containers in the background, print new container names. -f - Specify an alternate compose file (default: docker-compose.yml) Step 4 Check that all containers are running: docker ps CONTAINER ID IMAGE COMMAND d1006d1beee5 gcr.io/google-samples/gb-frontend:v4 \"apache2-foreground\" fb3a15fde23f gcr.io/google_containers/redis:e2e \"redis-server /etc...\" 326b94d4cdd7 gcr.io/google_samples/gb-redisslave:v1 \"/entrypoint.sh /b...\" Step 5 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Success Nice you now have compose stuck up and running! Step 6 Cleanup environment: docker-compose -f docker-guestbook.yml down Stopping guestbook_frontend_1 ... done Stopping guestbook_redis-master_1 ... done Stopping guestbook_redis-slave_1 ... done Removing guestbook_frontend_1 ... done Removing guestbook_redis-master_1 ... done Removing guestbook_redis-slave_1 ... done Removing network guestbook_default","title":"2.1 Deploy Guestbook app with Compose"},{"location":"Lab_5_Docker_Compose/#22-deploy-voting-app-using-compose","text":"Step 1 Switch to Module5/example-voting-app folder : cd ~/ycit019/Module5/example-voting-app/ Step 2 The existing file docker-compose.yml defines several images: A voting-app container based on a Python image A result-app container based on a Node.js image A Redis container based on a redis image, to temporarily store the data. A worker app based on a dotnet image A Postgres container based on a postgres image App Architecture: Note that three of the containers are built from Dockerfiles, while the other two are images on Docker Hub. Let's review them closely: Step 3 Review files that going to be deployed with tree command. Alternatively view the files in gitrepo page here sudo apt install tree tree Step 5 Let\u2019s change the default port to expose. Edit the docker-compose.yml file and find the following lines: ports: - \"5000:80\" Change 5000 to 8080: ports: - \"8080:80\" Step 4 Verify Docker Compose version: docker-compose version Step 5 Use the docker-compose tool to launch your application: docker-compose up -d Step 6 Check that all containers are running, volumes created. Check compose state and logs : #Docker state docker ps docker volumes #Docker compose state docker-compose ps docker-compose logs Step 7 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 8 Cleanup up. docker-compose down Stopping examplevotingapp_worker_1 ... done Stopping examplevotingapp_redis_1 ... done Stopping examplevotingapp_result_1 ... done Stopping examplevotingapp_db_1 ... done Stopping examplevotingapp_vote_1 ... done Removing examplevotingapp_worker_1 ... done Removing examplevotingapp_redis_1 ... done Removing examplevotingapp_result_1 ... done Removing examplevotingapp_db_1 ... done Removing examplevotingapp_vote_1 ... done Removing network examplevotingapp_default Step 9 You Boss told you that the application has a bug. Update the the app by editing the vote/app.py file and change the following lines near the top of the file: vim vote/app.py Press 'i' option_a = os.getenv('OPTION_A', \"Cats\") option_b = os.getenv('OPTION_B', \"Dogs\") Step 10 Replace \u201cCats\u201d and \u201cDogs\u201d with two options of your choice. For example: option_a = os.getenv('OPTION_A', \"Java\") option_b = os.getenv('OPTION_B', \"Python\") Press 'wq!' Step 11 Use docker-compose tool to launch your Update application: docker-compose up -d Check the UI Bingo Let's see who wins the battle of Orchestrations! Step 8 Cleanup up docker-compose down Congratulations You are now docker expert! We were able to start 2 microservices application with docker compose. First microservice had 3 services. Second microservice had 5 servics written in 3 different languages and able to talk to each other. Summary So far we've learned docker-compose v2. docker-compose v3 is out of scope for this Lab. However you got the idea! Read the Docker-Compose documentation on new syntax. Also example of v3 version of voting-app is here for you reference.","title":"2.2 Deploy Voting App using Compose"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/","text":"Deploy Kubernetes \u00b6 In this Lab, we are going to: Deploy Single Node Kubernetes cluster using kubeadm on a Google Compute Engine node Deploy an application to Kubernetes 1. Deploy Kubernetes and Calico with Kubeadm \u00b6 In general to deploy Kubernetes with kubeadm it is required to use following official Kubernetes documentation . 1.1 Create a VM, and ssh into it \u00b6 Step 1 Create the instance gcloud compute instances create k8s-cluster \\ --zone us-central1-c \\ --machine-type=e2-standard-4 \\ --image=ubuntu-1804-bionic-v20210514 \\ --image-project=ubuntu-os-cloud Step 2 Capture the private IP address of the VM Record the IP address of the node, as we will need later. Step 3 Once the vm is created, you can ssh into it gcloud compute ssh k8s-cluster --zone us-central1-c You can also ssh into the node through the console if you prefer that. Step 4 Define the NODE_IP as an environment variable. export NODE_IP=<REPLACE_WITH_NODE_PRIVATE_IP> 1.2 Let iptables see bridged traffic \u00b6 cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 1.3 Install Docker \u00b6 Step 1 Update the apt package index and install packages: sudo su apt-get update apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Step 2 Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Step 3 Setup the stable repository echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Step 4 Install Docker Engine and containerd apt-get update apt-get install docker-ce docker-ce-cli containerd.io Step 5 Configure the Docker daemon to use systemd for the management of the container's cgroups. mkdir /etc/docker cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF Step 6 Restart Docker and enable on boot: systemctl enable docker systemctl daemon-reload systemctl restart docker 1.4 Install kubeadm and prerequisite packages on each node \u00b6 The next step is to install kubeadm and prerequisite packages as showed here. Step 1 Deploy kubeadm and prerequisite packages apt-get update && apt-get install -y apt-transport-https ca-certificates curl curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list apt-get update && apt-get install -y kubelet=1.20.6-00 kubeadm=1.20.6-00 kubectl=1.20.6-00 apt-mark hold kubelet kubeadm kubectl Step 2 Verify kubeadm version kubeadm version Output: kubeadm version: &version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:26:21Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"} Result Latest available version of Kubernetes/kubeadm has been installed from (GitHub Kubernetes repo release page.)[https://github.com/kubernetes/kubernetes/releases] 1.5 'Kubeadm init' the Master \u00b6 Run On the Master node only: Step 1: Build kubeadm Custom Config cat <<EOF > kubeadm.conf kind: ClusterConfiguration apiVersion: kubeadm.k8s.io/v1beta2 apiServer: extraArgs: advertise-address: $NODE_IP kubernetesVersion: v1.20.6 --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd EOF Make sure the IP address was updated: cat kubeadm.conf Note To expose custom Config you can create a kubeadm.conf and specify during kubecadm init execution. For instance: * ControllerManager configs * Custom Subnet * Custom version * Apiserver configs such as authentication, authorization and etc. Step 2: Create a cluster kubeadm init --config=kubeadm.conf Result Once the command completes, configure the KUBECONFIG env variable with the path to admin.conf (recommend adding it to your .bashrc): mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config export KUBECONFIG=$HOME/.kube/config Let's validate that the installation was successful. You should now be able to run kubectl commands and see that all cluster Pods are running (except DNS one): watch kubectl get pods --all-namespaces -o wide To exit back to the terminal, press ctrl+c 1.6 Deploy Calico Networking (via self-hosted, as a daemon set) \u00b6 Step 1 Lets download the Calico manifest: curl https://docs.projectcalico.org/manifests/calico.yaml -O Note You can customize Calico deployments based on you needs. For instance by changing: (Optional) In the manifest, change CALICO_IPV4POOL_IPIP from \"always\" to \"cross-subnet\". (Optional) In the manifest, change FELIX_IPINIPMTU to match your ethernet interface mtu (Optional) If you want a different address range for containers, change CALICO_IPV4POOL_CIDR to the same cidr range used in kubeadm init, for e.g., \"10.6.0.0/16\" This time we are not going to modify defaults Calico values. Step 2 Now lets deploy Calico as a daemon set. kubectl apply -f calico.yaml Watch the Calico/node pod for the master get created (hopefully successfully) watch kubectl get pods --all-namespaces -o wide 1.7 Join worker node \u00b6 If you have other nodes around you can run the 'kubeadm join ...' command from the output of kubeadm init on each worker node (incl token). Watch the calico/node pods get created for each worker node automatically. e.g. kubeadm join --token **** For this lab, we are creating a one node kubernetes clusters, so in order to be able to deploy applications on the same node as the control plane, we need to remove the taint that prevent such deployment. kubectl taint nodes --all node-role.kubernetes.io/master- 1.8 Now lets create a test deployment with 2 replicas \u00b6 kubectl create deployment nginx --replicas=2 --image=nginx --port=8080 Lets get some more detail about the deployment: kubectl describe deployment nginx kubectl get deployment nginx And pods that has been created by nginx deployment: kubectl get pods Congrats. Now you have a working Kubernetes+Calico cluster. 2.1 Verify Kubernetes components deployed by kubeadm \u00b6 2.1.1 Check Kubernetes version \u00b6 Step 1 Verify that Kubernetes is deployed and working. kubectl get nodes Result Kubernetes has single node for workload scheduling. Kubernetes running version 1.20.6 Note At Kubernetes community, we define 3 types of Kubernetes releases: Major (x.0.0) Minor (x.x.0) Patch (x.x.x) Note At a single point of time, we develop the new \"Major\"/\"Minor\" version of Kubernetes (today - Kubernetes 1.21), and we support three existing releases as the \"Patch\" releases (today - 1.19.x, 1.20.x and 1.21.x). 2.1.2 Verify Cluster default namespaces. \u00b6 Step 1 Verify namespaces created in K8s systems $ kubectl get ns NAME STATUS AGE default Active 5h50m kube-node-lease Active 5h50m kube-public Active 5h50m kube-system Active 5h50m Info Namespaces are intendent to isolate groups/teams and give them access to a set of resources. They avoid name collisions between resources. Namespaces provides with a soft Multitenancy, meaning they not provide full isolation. Result By default Kubernetes deployed by kubeadm starts with 4 namespaces: default The default namespace for objects with no other namespace. When listing resources with the kubectl get command, we\u2019ve never specified the namespace explicitly, so kubectl always defaulted to the default namespace, showing us just the objects inside that namespace. kube-system The namespace for objects created by the Kubernetes system kube-public Readable by all users, and mostly reserved for cluster usage. kube-node-lease This namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales. 2.1.3 Verify kubelet \u00b6 Step 1 Verify that kubelet installed in K8s Cluster: systemctl -l | grep kubelet systemctl status kubelet Note Service and its config file can be found in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Step 2 Find manifests file for other master Node components: Once kubelet is deployed, all the rest master node components are deployed as a static pods on Kubernetes Master node. Setting --pod-manifest-path= specifies from where to read Static Pod manifests used for spinning up the control plane. Step 3 List K8s components manifest files that is going to be used for cluster deployment and run as Static Pods by kubelet : sudo ls /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml Result We see etcd, api-server, controller-manager and scheduler that has been used to deploy on this cluster and managed by kubelet . Step 4 Verify K8s Components deployed as containers on K8s: kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-6d7b4db76c-h242g 1/1 Running 0 27m calico-node-gwnng 1/1 Running 0 27m coredns-74ff55c5b-5s7rp 1/1 Running 0 5h59m coredns-74ff55c5b-l6hd4 1/1 Running 0 5h59m etcd-k8s-cluster 1/1 Running 0 5h59m kube-apiserver-k8s-cluster 1/1 Running 0 5h59m kube-controller-manager-k8s-cluster 1/1 Running 0 5h59m kube-proxy-f8647 1/1 Running 0 5h59m kube-scheduler-k8s-cluster 1/1 Running 0 5h59m Result We can see that Kubernetes components: etcd, api-server, controller-manager and scheduler deployed on K8s cluster via kubelet. Calico Networking including calico-etcd, calico-node, calico-policy-controller has been deployed as a last step of kubeadm installation 2.1.4 Verify etcd database deployment. \u00b6 Step 1 Verify etcd config file sudo cat /etc/kubernetes/manifests/etcd.yaml Step 2 Overview etcd pod deployed on K8s cluster: kubectl get pods -n kube-system | grep etcd kubectl describe pods/etcd-k8s-cluste -n kube-system Result etcd has been deployed as a static pod. Annotation Priority Class Name: system-node-critical tells to K8s that this Pod is critical and will have highest QOS . Step 3 Check the location of etcd db and snapshot dumps. sudo ls /var/lib/etcd/member Result The data directory has two sub-directories in it: wal: write ahead log files are stored here. snap: log snapshots are stored here. When first started, etcd stores its configuration into a data directory specified by the data-dir configuration parameter. Configuration is stored in the write ahead log and includes: the local member ID, cluster ID, and initial cluster configuration. The write ahead log and snapshot files are used during member operation and to recover after a restart. 2.1.5 Verify api-server deployment on the K8s cluster. \u00b6 Step 1 Review configuration file: sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml Step 2 Overview api-server pod and its parameters. kubectl describe pods/kube-apiserver-k8s-cluster -n kube-system 2.1.6 Verify Controller-manager and scheduler deployment. \u00b6 Step 1 Controller-manager and scheduler deployed on K8s cluster via kubelet the same way api-server . Verify both configuration files and pods running on K8s Cluster. Summary K8s is an orchestration system for containers. Since most of the k8s components are the go binaries that can be containerized, K8s has been designed to run itself. This makes system itself HA, easily deployable, scaleable and upgradable.","title":"Lab 6 Deploy Kubernetes Cluster with Kubeadm"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#deploy-kubernetes","text":"In this Lab, we are going to: Deploy Single Node Kubernetes cluster using kubeadm on a Google Compute Engine node Deploy an application to Kubernetes","title":"Deploy Kubernetes"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#1-deploy-kubernetes-and-calico-with-kubeadm","text":"In general to deploy Kubernetes with kubeadm it is required to use following official Kubernetes documentation .","title":"1. Deploy Kubernetes and Calico with Kubeadm"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#11-create-a-vm-and-ssh-into-it","text":"Step 1 Create the instance gcloud compute instances create k8s-cluster \\ --zone us-central1-c \\ --machine-type=e2-standard-4 \\ --image=ubuntu-1804-bionic-v20210514 \\ --image-project=ubuntu-os-cloud Step 2 Capture the private IP address of the VM Record the IP address of the node, as we will need later. Step 3 Once the vm is created, you can ssh into it gcloud compute ssh k8s-cluster --zone us-central1-c You can also ssh into the node through the console if you prefer that. Step 4 Define the NODE_IP as an environment variable. export NODE_IP=<REPLACE_WITH_NODE_PRIVATE_IP>","title":"1.1 Create a VM, and ssh into it"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#12-let-iptables-see-bridged-traffic","text":"cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system","title":"1.2 Let iptables see bridged traffic"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#13-install-docker","text":"Step 1 Update the apt package index and install packages: sudo su apt-get update apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Step 2 Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Step 3 Setup the stable repository echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Step 4 Install Docker Engine and containerd apt-get update apt-get install docker-ce docker-ce-cli containerd.io Step 5 Configure the Docker daemon to use systemd for the management of the container's cgroups. mkdir /etc/docker cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF Step 6 Restart Docker and enable on boot: systemctl enable docker systemctl daemon-reload systemctl restart docker","title":"1.3 Install Docker"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#14-install-kubeadm-and-prerequisite-packages-on-each-node","text":"The next step is to install kubeadm and prerequisite packages as showed here. Step 1 Deploy kubeadm and prerequisite packages apt-get update && apt-get install -y apt-transport-https ca-certificates curl curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list apt-get update && apt-get install -y kubelet=1.20.6-00 kubeadm=1.20.6-00 kubectl=1.20.6-00 apt-mark hold kubelet kubeadm kubectl Step 2 Verify kubeadm version kubeadm version Output: kubeadm version: &version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:26:21Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"} Result Latest available version of Kubernetes/kubeadm has been installed from (GitHub Kubernetes repo release page.)[https://github.com/kubernetes/kubernetes/releases]","title":"1.4 Install kubeadm and prerequisite packages on each node"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#15-kubeadm-init-the-master","text":"Run On the Master node only: Step 1: Build kubeadm Custom Config cat <<EOF > kubeadm.conf kind: ClusterConfiguration apiVersion: kubeadm.k8s.io/v1beta2 apiServer: extraArgs: advertise-address: $NODE_IP kubernetesVersion: v1.20.6 --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd EOF Make sure the IP address was updated: cat kubeadm.conf Note To expose custom Config you can create a kubeadm.conf and specify during kubecadm init execution. For instance: * ControllerManager configs * Custom Subnet * Custom version * Apiserver configs such as authentication, authorization and etc. Step 2: Create a cluster kubeadm init --config=kubeadm.conf Result Once the command completes, configure the KUBECONFIG env variable with the path to admin.conf (recommend adding it to your .bashrc): mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config export KUBECONFIG=$HOME/.kube/config Let's validate that the installation was successful. You should now be able to run kubectl commands and see that all cluster Pods are running (except DNS one): watch kubectl get pods --all-namespaces -o wide To exit back to the terminal, press ctrl+c","title":"1.5 'Kubeadm init' the Master"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#16-deploy-calico-networking-via-self-hosted-as-a-daemon-set","text":"Step 1 Lets download the Calico manifest: curl https://docs.projectcalico.org/manifests/calico.yaml -O Note You can customize Calico deployments based on you needs. For instance by changing: (Optional) In the manifest, change CALICO_IPV4POOL_IPIP from \"always\" to \"cross-subnet\". (Optional) In the manifest, change FELIX_IPINIPMTU to match your ethernet interface mtu (Optional) If you want a different address range for containers, change CALICO_IPV4POOL_CIDR to the same cidr range used in kubeadm init, for e.g., \"10.6.0.0/16\" This time we are not going to modify defaults Calico values. Step 2 Now lets deploy Calico as a daemon set. kubectl apply -f calico.yaml Watch the Calico/node pod for the master get created (hopefully successfully) watch kubectl get pods --all-namespaces -o wide","title":"1.6 Deploy Calico Networking (via self-hosted, as a daemon set)"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#17-join-worker-node","text":"If you have other nodes around you can run the 'kubeadm join ...' command from the output of kubeadm init on each worker node (incl token). Watch the calico/node pods get created for each worker node automatically. e.g. kubeadm join --token **** For this lab, we are creating a one node kubernetes clusters, so in order to be able to deploy applications on the same node as the control plane, we need to remove the taint that prevent such deployment. kubectl taint nodes --all node-role.kubernetes.io/master-","title":"1.7 Join worker node"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#18-now-lets-create-a-test-deployment-with-2-replicas","text":"kubectl create deployment nginx --replicas=2 --image=nginx --port=8080 Lets get some more detail about the deployment: kubectl describe deployment nginx kubectl get deployment nginx And pods that has been created by nginx deployment: kubectl get pods Congrats. Now you have a working Kubernetes+Calico cluster.","title":"1.8 Now lets create a test deployment with 2 replicas"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#21-verify-kubernetes-components-deployed-by-kubeadm","text":"","title":"2.1 Verify Kubernetes components deployed by kubeadm"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#211-check-kubernetes-version","text":"Step 1 Verify that Kubernetes is deployed and working. kubectl get nodes Result Kubernetes has single node for workload scheduling. Kubernetes running version 1.20.6 Note At Kubernetes community, we define 3 types of Kubernetes releases: Major (x.0.0) Minor (x.x.0) Patch (x.x.x) Note At a single point of time, we develop the new \"Major\"/\"Minor\" version of Kubernetes (today - Kubernetes 1.21), and we support three existing releases as the \"Patch\" releases (today - 1.19.x, 1.20.x and 1.21.x).","title":"2.1.1 Check Kubernetes version"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#212-verify-cluster-default-namespaces","text":"Step 1 Verify namespaces created in K8s systems $ kubectl get ns NAME STATUS AGE default Active 5h50m kube-node-lease Active 5h50m kube-public Active 5h50m kube-system Active 5h50m Info Namespaces are intendent to isolate groups/teams and give them access to a set of resources. They avoid name collisions between resources. Namespaces provides with a soft Multitenancy, meaning they not provide full isolation. Result By default Kubernetes deployed by kubeadm starts with 4 namespaces: default The default namespace for objects with no other namespace. When listing resources with the kubectl get command, we\u2019ve never specified the namespace explicitly, so kubectl always defaulted to the default namespace, showing us just the objects inside that namespace. kube-system The namespace for objects created by the Kubernetes system kube-public Readable by all users, and mostly reserved for cluster usage. kube-node-lease This namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales.","title":"2.1.2 Verify Cluster default namespaces."},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#213-verify-kubelet","text":"Step 1 Verify that kubelet installed in K8s Cluster: systemctl -l | grep kubelet systemctl status kubelet Note Service and its config file can be found in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Step 2 Find manifests file for other master Node components: Once kubelet is deployed, all the rest master node components are deployed as a static pods on Kubernetes Master node. Setting --pod-manifest-path= specifies from where to read Static Pod manifests used for spinning up the control plane. Step 3 List K8s components manifest files that is going to be used for cluster deployment and run as Static Pods by kubelet : sudo ls /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml Result We see etcd, api-server, controller-manager and scheduler that has been used to deploy on this cluster and managed by kubelet . Step 4 Verify K8s Components deployed as containers on K8s: kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-6d7b4db76c-h242g 1/1 Running 0 27m calico-node-gwnng 1/1 Running 0 27m coredns-74ff55c5b-5s7rp 1/1 Running 0 5h59m coredns-74ff55c5b-l6hd4 1/1 Running 0 5h59m etcd-k8s-cluster 1/1 Running 0 5h59m kube-apiserver-k8s-cluster 1/1 Running 0 5h59m kube-controller-manager-k8s-cluster 1/1 Running 0 5h59m kube-proxy-f8647 1/1 Running 0 5h59m kube-scheduler-k8s-cluster 1/1 Running 0 5h59m Result We can see that Kubernetes components: etcd, api-server, controller-manager and scheduler deployed on K8s cluster via kubelet. Calico Networking including calico-etcd, calico-node, calico-policy-controller has been deployed as a last step of kubeadm installation","title":"2.1.3 Verify kubelet"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#214-verify-etcd-database-deployment","text":"Step 1 Verify etcd config file sudo cat /etc/kubernetes/manifests/etcd.yaml Step 2 Overview etcd pod deployed on K8s cluster: kubectl get pods -n kube-system | grep etcd kubectl describe pods/etcd-k8s-cluste -n kube-system Result etcd has been deployed as a static pod. Annotation Priority Class Name: system-node-critical tells to K8s that this Pod is critical and will have highest QOS . Step 3 Check the location of etcd db and snapshot dumps. sudo ls /var/lib/etcd/member Result The data directory has two sub-directories in it: wal: write ahead log files are stored here. snap: log snapshots are stored here. When first started, etcd stores its configuration into a data directory specified by the data-dir configuration parameter. Configuration is stored in the write ahead log and includes: the local member ID, cluster ID, and initial cluster configuration. The write ahead log and snapshot files are used during member operation and to recover after a restart.","title":"2.1.4 Verify etcd database deployment."},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#215-verify-api-server-deployment-on-the-k8s-cluster","text":"Step 1 Review configuration file: sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml Step 2 Overview api-server pod and its parameters. kubectl describe pods/kube-apiserver-k8s-cluster -n kube-system","title":"2.1.5 Verify api-server deployment on the K8s cluster."},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#216-verify-controller-manager-and-scheduler-deployment","text":"Step 1 Controller-manager and scheduler deployed on K8s cluster via kubelet the same way api-server . Verify both configuration files and pods running on K8s Cluster. Summary K8s is an orchestration system for containers. Since most of the k8s components are the go binaries that can be containerized, K8s has been designed to run itself. This makes system itself HA, easily deployable, scaleable and upgradable.","title":"2.1.6 Verify Controller-manager and scheduler deployment."},{"location":"Lab_7_Kubernetes_Concepts/","text":"Kubernetes Concepts \u00b6 Objective: Learn basic Kubernetes concepts: Create a GKE Cluster Pods Labels, Selectors and Annotations Create Deployments Create Services namespaces 0 Create GKE Cluster \u00b6 Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c 1 Pods \u00b6 1.1 Create a Pod with manifest \u00b6 Reference: Pod Overview Step 1 Printout explanation of the object and lists of attributes: kubectl explain pods See all possible fields available for the pods: kubectl explain pods.spec --recursive Note It's not require to provide all possible fields for the Pods or any other resources. Most of the fields will be added by default if not specified. For the Pods at minimum it is required to specify image , name , ports inside of spec.containers. Step 2 Define a new pod in the file echoserver-pod.yaml : cat <<EOF > echoserver-pod.yaml apiVersion: v1 kind: Pod metadata: name: echoserver labels: app: echoserver spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.10 ports: - containerPort: 8080 EOF Here, we use the existing image echoserver . This is a simple server that responds with the http headers it received. It runs on nginx server and implemented using lua in the nginx configuration: https://github.com/kubernetes/contrib/tree/master/ingress/echoheaders Step 3 Create the echoserver pod: kubectl apply -f echoserver-pod.yaml Step 4 Use kubectl get pods to watch the pod get created: kubectl get pods Result: NAME READY STATUS RESTARTS AGE echoserver 1/1 Running 0 5s Step 5 Use kubectl describe pods/podname to watch the details about scheduled pod: kubectl describe pods/echoserver Note Review and discuss the following fields: Namespace Status Containers QoS Class Events Step 6 Now let\u2019s get the pod definition back from Kubernetes: kubectl get pods echoserver -o yaml > echoserver-pod-created.yaml cat echoserver-pod-created.yaml Compare echoserver-pod.yaml and echoserver-pod-created.yaml to see additional properties that have been added to the original pod definition. 2 Labels & Selectors \u00b6 Organizing pods and other resources with labels. 2.1 Label and Select Pods \u00b6 Reference: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ Step 1 Label Pod hello-world with label app=hello and env=test kubectl label pods echoserver dep=sales kubectl label pods echoserver env=test Step 2 See all Pods and all their Labels. kubectl get pods --show-labels Step 3 Select all Pods with labels env=test kubectl get pods -l env=test 2.2 Label Nodes \u00b6 Reference: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ Step 1 List available nodes kubectl get nodes Step 2 List a detailed view of nodes kubectl get nodes -o wide Step 3 List Nodes and their labels kubectl get nodes --show-labels Step 4 Label the node as size: small . Make sure to replace YOUR_NODE_NAME with one of the nodes you have. kubectl label node YOUR_NODE_NAME size=small Step 5 Check the labels for this node kubectl get node YOUR_NODE_NAME --show-labels | grep size Note In the upcoming classes we will use node labels to make sure our applications run on eligible nodes only. 3 Services \u00b6 3.1 Create a Service \u00b6 We have three running echoserver pods, but we cannot access them yet, because the container ports are not accessible. Let\u2019s define a new service that will expose echoserver ports and make them accessible. Step 1 Create a new file echoserver-service.yaml with the following content: cat <<EOF > echoserver-service.yaml apiVersion: v1 kind: Service metadata: name: echoserver spec: selector: app: echoserver type: \"NodePort\" ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: echoserver EOF Step 2 Create a new service: kubectl create -f echoserver-service.yaml Step 3 Check the service details: kubectl describe services/echoserver Output: Name: echoserver Namespace: default Labels: <none> Selector: app=echoserver Type: NodePort IP: ... Port: <unset> 8080/TCP NodePort: <unset> 30366/TCP Endpoints: ...:8080,...:8080,..:8080 Session Affinity: None No events. Note The above output contains one endpoint and a node port, 30366, but it can be different in your case. Remember this port to use it in the next step. Step 4 We need to open the node port on one of the cluster nodes to be able to access the service externally. Let's first find the exteran IP address of one of the nodes. kubectl get nodes -o wide Output: NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gke-k8s-concepts-default-pool-ad96fd50-1rf1 Ready <none> 20m v1.19.9-gke.1400 10.128.0.32 34.136.1.22 gke-k8s-concepts-default-pool-ad96fd50-jpd2 Ready <none> 20m v1.19.9-gke.1400 10.128.0.31 34.69.114.67 Step 5 Create a firewall rule to allow TCP traffic on your node port. Make sure to replace YOUR_NODE_PORT. gcloud compute firewall-rules create echoserver-node-port --allow tcp:YOUR_NODE_PORT Step 6 To access a service exposed via a node port, specify the node port from the previous step and use one of the IP addresses of the cluster nodes. Make sure to replace both NODE_IP and YOUR_NODE_PORT curl http://NODE_IP:YOUR_NODE_PORT 3.2 Cleanup Services and Pods \u00b6 Step 1 Before diving into Kubernetes deployment, let\u2019s delete our service and pods. To delete the service execute the following command: kubectl delete service echoserver Step 2 delete the pod kubectl delete pod echoserver Step 3 Check that there are no running pods: kubectl get pods 4 Deployments \u00b6 4.1 Deploy hello-app on Kubernetes using Deployments \u00b6 4.1.1 Create a Deployment \u00b6 Step 1 The simplest way to create a new deployment for a single-container pod is to use kubectl run : kubectl create deployment hello-app \\ --image=gcr.io/google-samples/hello-app:1.0 \\ --port=8080 \\ --replicas=2 Note --port Deployment opens port 8080 for use by the Pods. --replicas number of replicas. Step 2 Check pods: kubectl get pods Step 3 To access the hello-app deployment, create a new service of type LoadBalancer this time using kubectl expose deployment : kubectl expose deployment hello-app --type=LoadBalancer To get the external IP for the loadbalancer that got created: kubectl get services/hello-app The Loadbalancer might take few minutes to get created, and it'll show pending status. Step 4 Check that the hello-app is accessible: Make sure to replace the LB_IP. curl http://LB_IP:8080 Output: Hello, world! Version: 1.0.0 Hostname: hello-app-76f778987d-rdhr7 Step 5 You can open the app in the browser by navigating to LB_IP:8080 Summary We learned how to create a deployment and expose our container. 4.1.2 Scale a Deployment \u00b6 Now, let's scale our application as our website get popular. Step 1 Deployments using replica set (RS) to scale the containers. Let's check how replica set (RS) looks like: kubectl get rs,deploy NAME DESIRED CURRENT READY AGE replicaset.apps/hello-app-76f778987d 2 2 2 5m12s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hello-app 2/2 2 2 5m12s Step 2 Let\u2019s scale number of pods in replica for the deployment. Use going to use kubectl scale to change the number of replicas to 5: kubectl scale deployment hello-app --replicas=5 Step 3 View the deployment details: kubectl describe deployment hello-app Step 4 Check that there are 5 running pods: kubectl get pods 4.1.3 Rolling Update of Containers \u00b6 To perform rolling upgrade we need a new version of our application and then perform Rolling Upgrade using deployments Step 4 Use kubectl rollout history deployment to see revisions of the deployment: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> Result Since we've just deployed there is only 1 revision that currenly running. Step 5 Now we want to replace our hello-app with a new implementation. We want to use a new version of hello-app image. We are going to use kubectl set command this time around. Hint kubectl set used only to change image name/version. You can use this for command for CI/CD pipeline. Suppose that we want to update the webk8sbirthday Pods to use the hello-app:2.0 image instead of the hello-app:1.0 image. kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record kubectl get pods Note It is a good practice to paste --record at the end of the rolling upgrade command as it will record the action in the rollout history Result: We can see that the Rolling Upgraded was recorded: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> 2 kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record=true Step 6 Refresh browser and see new version of app deployed http://LB_IP:8080 Step 7 Let's assume there was something wrong with this new version and we need to rollback with kubectl rollout undo our deployment: kubectl rollout undo deployment/hello-app Refresh the browser again to see how we rolledback to version 1.0.0 We have successfully rolled back the deployment and our pods are based on the hello-app:1.0.0 image again. Step 8 Let's delete the deployment and the service: kubectl delete deployment hello-app kubectl delete services/hello-app Success You are now up to speed with Kubernetes Concepts such as Pods, Services and Deployments. Let's move on to Kubernetes Features to learn what else Kubernetes is capable of! 5 NameSpace \u00b6 Namespace can be used for: Splitting complex systems with several components into smaller groups Separating resources in a multi-tenant env: production, development and QA environments Separating resources per production Separating per-user or per-department or any other logical group Some other rules and regulations: Resource names only need to be unique within a namespace. Two different namespaces can contain resources of the same name. Most of the Kubernetes resources (e.g. pods, svc, rcs, and others) are namespaced. However, some resource can be cluster-wide e.g nodes, persistentVolumes and PodSecurityPolicy. 5.1 Viewing namespaces \u00b6 Step 1 List the current namespaces in a cluster using: kubectl get ns Output: NAME STATUS AGE default Active 71m kube-node-lease Active 71m kube-public Active 71m kube-system Active 71m Step 2 You can also get the summary of a specific namespace using: kubectl get namespaces <name> Or you can get detailed information with: kubectl describe namespaces <name> A namespace can be in one of two phases: Active the namespace is in use Terminating the namespace is being deleted, and can not be used for new objects Note These details show both resource quota (if present) as well as resource and limit ranges. Resource quota tracks aggregate usage of resources in the Namespace and allows cluster operators to define Hard resource usage limits that a Namespace may consume. A limit range defines min/max constraints on the amount of resources a single entity can consume in a Namespace . Step 3 Let\u2019s have a look at the pods that belong to the kube-system namespace, by telling kubectl to list pods in that namespace: kubectl get po --namespace kube-system Output: NAME READY STATUS RESTARTS AGE event-exporter-gke-67986489c8-5fsdv 2/2 Running 0 71m fluentbit-gke-fqcsx 2/2 Running 0 71m fluentbit-gke-ppb9j 2/2 Running 0 71m gke-metrics-agent-5vl7t 1/1 Running 0 71m gke-metrics-agent-bxt2r 1/1 Running 0 71m kube-dns-5d54b45645-9srx6 4/4 Running 0 71m kube-dns-5d54b45645-b7njm 4/4 Running 0 71m kube-dns-autoscaler-58cbd4f75c-2scrv 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-1rf1 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-jpd2 1/1 Running 0 71m l7-default-backend-66579f5d7-dsbdt 1/1 Running 0 71m metrics-server-v0.3.6-6c47ffd7d7-mtls4 2/2 Running 0 71m pdcsi-node-knlqp 2/2 Running 0 71m pdcsi-node-vh4tx 2/2 Running 0 71m stackdriver-metadata-agent-cluster-level-6f7d66dc98-zcd25 2/2 Running 0 71m Tip You can also use -n instead of --namespace Yot may already know some of the pods, the rest we will cover later. It\u2019s clear from the name of the namespace, that resources inside kube-system related to the Kubernetes system itself. By having them in this separate namespace, it keeps everything nicely organized. If they were all in the default namespace, mixed in with the resources we create ourselves, we\u2019d have a hard time seeing what belongs where and we might inadvertently delete some system resources. Step 4 Now you know how to view resources in specific namespaces. Additionally, it is also possible to view list all resources in all namespaces. For example below is example to list all pods in all namespaces: kubectl get pods --all-namespaces 5.2 Creating Namespaces \u00b6 A namespace is a Kubernetes resource, therefore it is possible to create it by posting a YAML file to the Kubernetes API server or using kubectl create ns . Step 1 First, create a custom-namespace.yaml file with the following content: cat <<EOF > custom-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: custom-namespace EOF Step 2 Than, use kubectl to post the file to the Kubernetes API server: kubectl create -f custom-namespace.yaml Step 3 A much easier and faster way to create a namespaces using kubectl create ns command, as shown below: kubectl create namespace custom-namespace2 5.3 Setting the namespace preference. \u00b6 By default, a Kubernetes cluster will instantiate a default namespace when provisioning the cluster to hold the default set of Pods, Services, and Deployments used by the cluster. So by default all kubectl calls such as list or create resources will end up in default namespace . However sometimes you want to list or create resources in other namespaces than default namespace . As we discussed in previous exercise this can be done by specifying -n or --namespace to point in which namespaces action has to be done. However it is not convenient to do this action every time. Below example will show how to create 2 namespaces dev and prod and switch between each other. Step 1 kubectl API uses so called kubeconfig context where you can controls which namespace, user or cluster needs to be accessed. In order to display which context is currently in use run: KUBECONFIG=~/.kube/config kubectl config current-context Result We running in kubernetes-admin@kubernetes context, which is default for lab environment. Step 3 To see full view of the kubeconfig context run: kubectl config view Result Our context named as kubernetes-admin@kubernetes uses cluster cluster , and user kubernetes-admin Step 4 The result of the above command comes from kubeconfig file, in our case we defined it under ~/.kube/config . echo $KUBECONFIG Result KUBECONFIG is configured to use following ~/.kube/config file cat ~/.kube/config Note The KUBECONFIG environment variable is a list of paths to configuration files. The list is colon-delimited for Linux and Mac, and semicolon-delimited for Windows. We already set KUBECONFIG environment variable in a first step of exercise to be ~/.kube/config Tip You can use use multiple kubeconfig files at the same time and view merged config: $ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view Step 5 Create two new namespaces dev : kubectl create namespace dev And prod namespace: kubectl create namespace prod Step 7 Let\u2019s switch to operate in the development namespace: kubectl config set-context --current --namespace=dev We can see now that our current context is switched to dev: kubectl config view | grep namespace Output: namespace: dev Result At this point, all requests we make to the Kubernetes cluster from the command line are scoped to the development namespace. Step 8 Let's test that all resources going to be created in dev namespace. kubectl run devsnowflake --image=nginx Step 9 Verify result of creation: kubectl get pods Success Developers are able to do what they want, and they do not have to worry about affecting content in the production namespace. Step 10 Now switch to the production namespace and show how resources in one namespace are hidden from the other. kubectl config set-context --current --namespace=prod The production namespace should be empty, and the following commands should return nothing. kubectl get pods Step 11 Let's create some production workloads: kubectl run prodapp --image=nginx kubectl get pods -n prod Summary At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace. 6.4 Deleting Namespaces \u00b6 Step 1 Delete a namespace with kubectl delete namespaces custom-namespace kubectl delete namespaces dev kubectl delete namespaces prod Warning Unlike with OpenStack where when you delete a project/tenant, underlining resources will still exist as zombies and not deleted. In Kubernetes when you delete namespace it deletes everything under it (pods, svc, rc, and etc.)! This is called resource garbage collection in Kubernetes. Delete process is asynchronous, so you may see Terminating state for some time. 6.5 Create a pod in a different namespace \u00b6 Create test namespace: kubectl create ns test Create a pod in this namespaces: cat <<EOF > echoserver-pod_ns.yaml apiVersion: v1 kind: Pod metadata: name: echoserverns spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.4 ports: - containerPort: 8080 EOF Create Pod in namespaces: kubectl create -f echoserver-pod_ns.yaml -n test Verify Pods created in specified namespaces: kubectl get pods -n test 6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts Step 2 Delete the firewall rule gcloud compute firewall-rules delete echoserver-node-port","title":"Lab 7 Kubernetes Concepts"},{"location":"Lab_7_Kubernetes_Concepts/#kubernetes-concepts","text":"Objective: Learn basic Kubernetes concepts: Create a GKE Cluster Pods Labels, Selectors and Annotations Create Deployments Create Services namespaces","title":"Kubernetes Concepts"},{"location":"Lab_7_Kubernetes_Concepts/#0-create-gke-cluster","text":"Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c","title":"0 Create GKE Cluster"},{"location":"Lab_7_Kubernetes_Concepts/#1-pods","text":"","title":"1 Pods"},{"location":"Lab_7_Kubernetes_Concepts/#11-create-a-pod-with-manifest","text":"Reference: Pod Overview Step 1 Printout explanation of the object and lists of attributes: kubectl explain pods See all possible fields available for the pods: kubectl explain pods.spec --recursive Note It's not require to provide all possible fields for the Pods or any other resources. Most of the fields will be added by default if not specified. For the Pods at minimum it is required to specify image , name , ports inside of spec.containers. Step 2 Define a new pod in the file echoserver-pod.yaml : cat <<EOF > echoserver-pod.yaml apiVersion: v1 kind: Pod metadata: name: echoserver labels: app: echoserver spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.10 ports: - containerPort: 8080 EOF Here, we use the existing image echoserver . This is a simple server that responds with the http headers it received. It runs on nginx server and implemented using lua in the nginx configuration: https://github.com/kubernetes/contrib/tree/master/ingress/echoheaders Step 3 Create the echoserver pod: kubectl apply -f echoserver-pod.yaml Step 4 Use kubectl get pods to watch the pod get created: kubectl get pods Result: NAME READY STATUS RESTARTS AGE echoserver 1/1 Running 0 5s Step 5 Use kubectl describe pods/podname to watch the details about scheduled pod: kubectl describe pods/echoserver Note Review and discuss the following fields: Namespace Status Containers QoS Class Events Step 6 Now let\u2019s get the pod definition back from Kubernetes: kubectl get pods echoserver -o yaml > echoserver-pod-created.yaml cat echoserver-pod-created.yaml Compare echoserver-pod.yaml and echoserver-pod-created.yaml to see additional properties that have been added to the original pod definition.","title":"1.1 Create a Pod with manifest"},{"location":"Lab_7_Kubernetes_Concepts/#2-labels-selectors","text":"Organizing pods and other resources with labels.","title":"2 Labels &amp; Selectors"},{"location":"Lab_7_Kubernetes_Concepts/#21-label-and-select-pods","text":"Reference: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ Step 1 Label Pod hello-world with label app=hello and env=test kubectl label pods echoserver dep=sales kubectl label pods echoserver env=test Step 2 See all Pods and all their Labels. kubectl get pods --show-labels Step 3 Select all Pods with labels env=test kubectl get pods -l env=test","title":"2.1 Label and Select Pods"},{"location":"Lab_7_Kubernetes_Concepts/#22-label-nodes","text":"Reference: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ Step 1 List available nodes kubectl get nodes Step 2 List a detailed view of nodes kubectl get nodes -o wide Step 3 List Nodes and their labels kubectl get nodes --show-labels Step 4 Label the node as size: small . Make sure to replace YOUR_NODE_NAME with one of the nodes you have. kubectl label node YOUR_NODE_NAME size=small Step 5 Check the labels for this node kubectl get node YOUR_NODE_NAME --show-labels | grep size Note In the upcoming classes we will use node labels to make sure our applications run on eligible nodes only.","title":"2.2 Label Nodes"},{"location":"Lab_7_Kubernetes_Concepts/#3-services","text":"","title":"3 Services"},{"location":"Lab_7_Kubernetes_Concepts/#31-create-a-service","text":"We have three running echoserver pods, but we cannot access them yet, because the container ports are not accessible. Let\u2019s define a new service that will expose echoserver ports and make them accessible. Step 1 Create a new file echoserver-service.yaml with the following content: cat <<EOF > echoserver-service.yaml apiVersion: v1 kind: Service metadata: name: echoserver spec: selector: app: echoserver type: \"NodePort\" ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: echoserver EOF Step 2 Create a new service: kubectl create -f echoserver-service.yaml Step 3 Check the service details: kubectl describe services/echoserver Output: Name: echoserver Namespace: default Labels: <none> Selector: app=echoserver Type: NodePort IP: ... Port: <unset> 8080/TCP NodePort: <unset> 30366/TCP Endpoints: ...:8080,...:8080,..:8080 Session Affinity: None No events. Note The above output contains one endpoint and a node port, 30366, but it can be different in your case. Remember this port to use it in the next step. Step 4 We need to open the node port on one of the cluster nodes to be able to access the service externally. Let's first find the exteran IP address of one of the nodes. kubectl get nodes -o wide Output: NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gke-k8s-concepts-default-pool-ad96fd50-1rf1 Ready <none> 20m v1.19.9-gke.1400 10.128.0.32 34.136.1.22 gke-k8s-concepts-default-pool-ad96fd50-jpd2 Ready <none> 20m v1.19.9-gke.1400 10.128.0.31 34.69.114.67 Step 5 Create a firewall rule to allow TCP traffic on your node port. Make sure to replace YOUR_NODE_PORT. gcloud compute firewall-rules create echoserver-node-port --allow tcp:YOUR_NODE_PORT Step 6 To access a service exposed via a node port, specify the node port from the previous step and use one of the IP addresses of the cluster nodes. Make sure to replace both NODE_IP and YOUR_NODE_PORT curl http://NODE_IP:YOUR_NODE_PORT","title":"3.1 Create a Service"},{"location":"Lab_7_Kubernetes_Concepts/#32-cleanup-services-and-pods","text":"Step 1 Before diving into Kubernetes deployment, let\u2019s delete our service and pods. To delete the service execute the following command: kubectl delete service echoserver Step 2 delete the pod kubectl delete pod echoserver Step 3 Check that there are no running pods: kubectl get pods","title":"3.2 Cleanup Services and Pods"},{"location":"Lab_7_Kubernetes_Concepts/#4-deployments","text":"","title":"4 Deployments"},{"location":"Lab_7_Kubernetes_Concepts/#41-deploy-hello-app-on-kubernetes-using-deployments","text":"","title":"4.1 Deploy hello-app on Kubernetes using Deployments"},{"location":"Lab_7_Kubernetes_Concepts/#411-create-a-deployment","text":"Step 1 The simplest way to create a new deployment for a single-container pod is to use kubectl run : kubectl create deployment hello-app \\ --image=gcr.io/google-samples/hello-app:1.0 \\ --port=8080 \\ --replicas=2 Note --port Deployment opens port 8080 for use by the Pods. --replicas number of replicas. Step 2 Check pods: kubectl get pods Step 3 To access the hello-app deployment, create a new service of type LoadBalancer this time using kubectl expose deployment : kubectl expose deployment hello-app --type=LoadBalancer To get the external IP for the loadbalancer that got created: kubectl get services/hello-app The Loadbalancer might take few minutes to get created, and it'll show pending status. Step 4 Check that the hello-app is accessible: Make sure to replace the LB_IP. curl http://LB_IP:8080 Output: Hello, world! Version: 1.0.0 Hostname: hello-app-76f778987d-rdhr7 Step 5 You can open the app in the browser by navigating to LB_IP:8080 Summary We learned how to create a deployment and expose our container.","title":"4.1.1 Create a Deployment"},{"location":"Lab_7_Kubernetes_Concepts/#412-scale-a-deployment","text":"Now, let's scale our application as our website get popular. Step 1 Deployments using replica set (RS) to scale the containers. Let's check how replica set (RS) looks like: kubectl get rs,deploy NAME DESIRED CURRENT READY AGE replicaset.apps/hello-app-76f778987d 2 2 2 5m12s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hello-app 2/2 2 2 5m12s Step 2 Let\u2019s scale number of pods in replica for the deployment. Use going to use kubectl scale to change the number of replicas to 5: kubectl scale deployment hello-app --replicas=5 Step 3 View the deployment details: kubectl describe deployment hello-app Step 4 Check that there are 5 running pods: kubectl get pods","title":"4.1.2 Scale a Deployment"},{"location":"Lab_7_Kubernetes_Concepts/#413-rolling-update-of-containers","text":"To perform rolling upgrade we need a new version of our application and then perform Rolling Upgrade using deployments Step 4 Use kubectl rollout history deployment to see revisions of the deployment: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> Result Since we've just deployed there is only 1 revision that currenly running. Step 5 Now we want to replace our hello-app with a new implementation. We want to use a new version of hello-app image. We are going to use kubectl set command this time around. Hint kubectl set used only to change image name/version. You can use this for command for CI/CD pipeline. Suppose that we want to update the webk8sbirthday Pods to use the hello-app:2.0 image instead of the hello-app:1.0 image. kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record kubectl get pods Note It is a good practice to paste --record at the end of the rolling upgrade command as it will record the action in the rollout history Result: We can see that the Rolling Upgraded was recorded: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> 2 kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record=true Step 6 Refresh browser and see new version of app deployed http://LB_IP:8080 Step 7 Let's assume there was something wrong with this new version and we need to rollback with kubectl rollout undo our deployment: kubectl rollout undo deployment/hello-app Refresh the browser again to see how we rolledback to version 1.0.0 We have successfully rolled back the deployment and our pods are based on the hello-app:1.0.0 image again. Step 8 Let's delete the deployment and the service: kubectl delete deployment hello-app kubectl delete services/hello-app Success You are now up to speed with Kubernetes Concepts such as Pods, Services and Deployments. Let's move on to Kubernetes Features to learn what else Kubernetes is capable of!","title":"4.1.3 Rolling Update of Containers"},{"location":"Lab_7_Kubernetes_Concepts/#5-namespace","text":"Namespace can be used for: Splitting complex systems with several components into smaller groups Separating resources in a multi-tenant env: production, development and QA environments Separating resources per production Separating per-user or per-department or any other logical group Some other rules and regulations: Resource names only need to be unique within a namespace. Two different namespaces can contain resources of the same name. Most of the Kubernetes resources (e.g. pods, svc, rcs, and others) are namespaced. However, some resource can be cluster-wide e.g nodes, persistentVolumes and PodSecurityPolicy.","title":"5 NameSpace"},{"location":"Lab_7_Kubernetes_Concepts/#51-viewing-namespaces","text":"Step 1 List the current namespaces in a cluster using: kubectl get ns Output: NAME STATUS AGE default Active 71m kube-node-lease Active 71m kube-public Active 71m kube-system Active 71m Step 2 You can also get the summary of a specific namespace using: kubectl get namespaces <name> Or you can get detailed information with: kubectl describe namespaces <name> A namespace can be in one of two phases: Active the namespace is in use Terminating the namespace is being deleted, and can not be used for new objects Note These details show both resource quota (if present) as well as resource and limit ranges. Resource quota tracks aggregate usage of resources in the Namespace and allows cluster operators to define Hard resource usage limits that a Namespace may consume. A limit range defines min/max constraints on the amount of resources a single entity can consume in a Namespace . Step 3 Let\u2019s have a look at the pods that belong to the kube-system namespace, by telling kubectl to list pods in that namespace: kubectl get po --namespace kube-system Output: NAME READY STATUS RESTARTS AGE event-exporter-gke-67986489c8-5fsdv 2/2 Running 0 71m fluentbit-gke-fqcsx 2/2 Running 0 71m fluentbit-gke-ppb9j 2/2 Running 0 71m gke-metrics-agent-5vl7t 1/1 Running 0 71m gke-metrics-agent-bxt2r 1/1 Running 0 71m kube-dns-5d54b45645-9srx6 4/4 Running 0 71m kube-dns-5d54b45645-b7njm 4/4 Running 0 71m kube-dns-autoscaler-58cbd4f75c-2scrv 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-1rf1 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-jpd2 1/1 Running 0 71m l7-default-backend-66579f5d7-dsbdt 1/1 Running 0 71m metrics-server-v0.3.6-6c47ffd7d7-mtls4 2/2 Running 0 71m pdcsi-node-knlqp 2/2 Running 0 71m pdcsi-node-vh4tx 2/2 Running 0 71m stackdriver-metadata-agent-cluster-level-6f7d66dc98-zcd25 2/2 Running 0 71m Tip You can also use -n instead of --namespace Yot may already know some of the pods, the rest we will cover later. It\u2019s clear from the name of the namespace, that resources inside kube-system related to the Kubernetes system itself. By having them in this separate namespace, it keeps everything nicely organized. If they were all in the default namespace, mixed in with the resources we create ourselves, we\u2019d have a hard time seeing what belongs where and we might inadvertently delete some system resources. Step 4 Now you know how to view resources in specific namespaces. Additionally, it is also possible to view list all resources in all namespaces. For example below is example to list all pods in all namespaces: kubectl get pods --all-namespaces","title":"5.1 Viewing namespaces"},{"location":"Lab_7_Kubernetes_Concepts/#52-creating-namespaces","text":"A namespace is a Kubernetes resource, therefore it is possible to create it by posting a YAML file to the Kubernetes API server or using kubectl create ns . Step 1 First, create a custom-namespace.yaml file with the following content: cat <<EOF > custom-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: custom-namespace EOF Step 2 Than, use kubectl to post the file to the Kubernetes API server: kubectl create -f custom-namespace.yaml Step 3 A much easier and faster way to create a namespaces using kubectl create ns command, as shown below: kubectl create namespace custom-namespace2","title":"5.2 Creating Namespaces"},{"location":"Lab_7_Kubernetes_Concepts/#53-setting-the-namespace-preference","text":"By default, a Kubernetes cluster will instantiate a default namespace when provisioning the cluster to hold the default set of Pods, Services, and Deployments used by the cluster. So by default all kubectl calls such as list or create resources will end up in default namespace . However sometimes you want to list or create resources in other namespaces than default namespace . As we discussed in previous exercise this can be done by specifying -n or --namespace to point in which namespaces action has to be done. However it is not convenient to do this action every time. Below example will show how to create 2 namespaces dev and prod and switch between each other. Step 1 kubectl API uses so called kubeconfig context where you can controls which namespace, user or cluster needs to be accessed. In order to display which context is currently in use run: KUBECONFIG=~/.kube/config kubectl config current-context Result We running in kubernetes-admin@kubernetes context, which is default for lab environment. Step 3 To see full view of the kubeconfig context run: kubectl config view Result Our context named as kubernetes-admin@kubernetes uses cluster cluster , and user kubernetes-admin Step 4 The result of the above command comes from kubeconfig file, in our case we defined it under ~/.kube/config . echo $KUBECONFIG Result KUBECONFIG is configured to use following ~/.kube/config file cat ~/.kube/config Note The KUBECONFIG environment variable is a list of paths to configuration files. The list is colon-delimited for Linux and Mac, and semicolon-delimited for Windows. We already set KUBECONFIG environment variable in a first step of exercise to be ~/.kube/config Tip You can use use multiple kubeconfig files at the same time and view merged config: $ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view Step 5 Create two new namespaces dev : kubectl create namespace dev And prod namespace: kubectl create namespace prod Step 7 Let\u2019s switch to operate in the development namespace: kubectl config set-context --current --namespace=dev We can see now that our current context is switched to dev: kubectl config view | grep namespace Output: namespace: dev Result At this point, all requests we make to the Kubernetes cluster from the command line are scoped to the development namespace. Step 8 Let's test that all resources going to be created in dev namespace. kubectl run devsnowflake --image=nginx Step 9 Verify result of creation: kubectl get pods Success Developers are able to do what they want, and they do not have to worry about affecting content in the production namespace. Step 10 Now switch to the production namespace and show how resources in one namespace are hidden from the other. kubectl config set-context --current --namespace=prod The production namespace should be empty, and the following commands should return nothing. kubectl get pods Step 11 Let's create some production workloads: kubectl run prodapp --image=nginx kubectl get pods -n prod Summary At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace.","title":"5.3 Setting the namespace preference."},{"location":"Lab_7_Kubernetes_Concepts/#64-deleting-namespaces","text":"Step 1 Delete a namespace with kubectl delete namespaces custom-namespace kubectl delete namespaces dev kubectl delete namespaces prod Warning Unlike with OpenStack where when you delete a project/tenant, underlining resources will still exist as zombies and not deleted. In Kubernetes when you delete namespace it deletes everything under it (pods, svc, rc, and etc.)! This is called resource garbage collection in Kubernetes. Delete process is asynchronous, so you may see Terminating state for some time.","title":"6.4 Deleting Namespaces"},{"location":"Lab_7_Kubernetes_Concepts/#65-create-a-pod-in-a-different-namespace","text":"Create test namespace: kubectl create ns test Create a pod in this namespaces: cat <<EOF > echoserver-pod_ns.yaml apiVersion: v1 kind: Pod metadata: name: echoserverns spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.4 ports: - containerPort: 8080 EOF Create Pod in namespaces: kubectl create -f echoserver-pod_ns.yaml -n test Verify Pods created in specified namespaces: kubectl get pods -n test","title":"6.5 Create a pod in a different namespace"},{"location":"Lab_7_Kubernetes_Concepts/#6-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts Step 2 Delete the firewall rule gcloud compute firewall-rules delete echoserver-node-port","title":"6 Cleaning Up"},{"location":"Lab_8_Kubernetes_Features/","text":"Lab 8 Kubernetes Features Objective Use Liveness Probes to healthcheck you application while it is running Learn about secrets and configmaps Deploy a Daemonset and jobs 0 Create GKE Cluster \u00b6 Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-features \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-features us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-features --zone us-central1-c 1 Kubernetes Features \u00b6 1.1 Using Liveness Probes \u00b6 Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes p rovides liveness probes to detect and remedy such situations. As we already discussed Kubernetes provides 3 types of Probes to perform Liveness checks: HTTP GET EXEC tcpSocket In below example we are going to use HTTP GET probe for a Pod that runs a container based on the gcr.io/google_containers/liveness image. Step 1 Create http-liveness.yaml manifest with below content: cat <<EOF > http-liveness.yaml apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: gcr.io/google_containers/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 EOF Based on the manifest, we can see that the Pod has a single Container. The periodSeconds field specifies that the kubelet should perform a liveness probe every 3 seconds. The initialDelaySeconds field tells the kubelet that it should wait 3 seconds before performing the first probe. To perform a probe, the kubelet sends an HTTP GET request to the server that is running in the Container and listening on port 8080. If the handler for the server's /healthz path returns a success code, the kubelet considers the Container to be alive and healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it. Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure. Full code for for a reference in server.go . For the first 10 seconds that the Container is alive, the /healthz handler returns a status of 200. After that, the handler returns a status of 500. http . HandleFunc ( \"/healthz\" , func ( w http . ResponseWriter , r * http . Request ) { duration := time . Now (). Sub ( started ) if duration . Seconds () > 10 { w . WriteHeader ( 500 ) w . Write ([] byte ( fmt . Sprintf ( \"error: %v\" , duration . Seconds ()))) } else { w . WriteHeader ( 200 ) w . Write ([] byte ( \"ok\" )) } }) The kubelet starts performing health checks 3 seconds after the Container starts. So the first couple of health checks will succeed. But after 10 seconds, the health checks will fail, and the kubelet will kill and restart the Container. Step 2 Let's create a Pod and see how HTTP liveness check works: kubectl create -f http-liveness.yaml Step 3 Monitor the Pod watch kubectl get pod Result After 10 seconds Pods has beed restarted. Exit the shell session by using ctrl+c Step 4 Verify status of the Pod and review the Events happened after restart kubectl describe pod liveness-http Result Pod events shows that liveness probes have failed and the Container has been restarted. Step 5 Clean up kubectl delete -f http-liveness.yaml 1.2 Using ConfigMaps \u00b6 In Kubernetes ConfigMaps could be use in several cases: Storing configuration values as key-values in ConfigMap and referencing them in a Pod as environment variables Storing configurations as a file inside of ConfigMap and referencing it in a Pod as a Volume Let's try second option and deploy nginx pod while storing its config in a ConfigMap. Step 1 Create nginx my-nginx-config.conf config file as below: cat <<EOF > my-nginx-config.conf server { listen 80; server_name www.cloudnative.tech; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } EOF Step 2 Create ConfigMap from this file kubectl create configmap nginxconfig --from-file=my-nginx-config.conf Step 3 Review the ConfigMap kubectl describe cm nginxconfig Result: ``` Name: nginxconfig Namespace: default Labels: <none> Annotations: <none> Data ==== my-nginx-config.conf: ---- server { listen 80; server_name _; gzip off; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } Events: <none> ``` Step 4 Create Nginx Pod website.yaml file, where ConfigMap referenced as a Volume cat <<EOF > website.yaml apiVersion: v1 kind: Pod metadata: name: website spec: containers: - image: nginx:alpine name: website volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: false - name: config mountPath: /etc/nginx/conf.d readOnly: true volumes: - name: html emptyDir: {} - name: config configMap: name: nginxconfig EOF Step 5 Deploy Nginx Pod website.yaml kubectl apply -f website.yaml You can expose a service as we usually do with NodePort or LoadBalancer or by using the port-forward technicque in the next step Step 6 Open second SSH terminal by pressing \"+\" icon in the cloud shell and run following command kubectl port-forward website 8080:80 Result This opens a tunnel and expose our application on port '80' to localhost:8080. Step 7 Test that website is running Navigate back to the first terminal tab and run the following curl localhost:8080 Result: ``` <html> <head><title>403 Forbidden</title></head> <body> <center><h1>403 Forbidden</h1></center> <hr><center>nginx/1.21.0</center> </body> </html> ``` Success You've just learned how to use ConfigMaps. You can also use the preview feature with the console Step 8 start a shell in the pod by using exec kubectl exec website -it -- sh Your terminal now should have /# Step 9 view the content of the folder where we added the volume ls /etc/nginx/conf.d Step 10 check the content of the file in that folder and notice that it's the same as the config file for nginx we mounted. cat /etc/nginx/conf.d/my-nginx-config.conf Step 11 Exit the shell exit Make sure you're back to the cloud shell terminal. 1.3 Using Secrets \u00b6 1.3.1 Kubernetes Secrets \u00b6 Kubernetes secrets allow users to define sensitive information outside of containers and expose that information to containers through environment variables as well as files within Pods. In this section we will declare and create secrets to hold our database connection information that will be used by Wordpress to connect to its backend database. Step 1 Open up two terminal windows. We will use one window to generate encoded strings that will contain our sensitive data. The other window will be used to create the secrets YAML declaration. Step 2 In the first terminal window, execute the following commands to encode our strings: echo -n \"admin\" | base64 echo -n \"t0p-Secret\" | base64 Step 3 create the secret. For this lab, we added the encoded values for you. Feel free to change the username and password, and replace the values in the file below: cat <<EOF > app-secrets.yaml apiVersion: v1 kind: Secret metadata: name: app-secret type: Opaque data: username: YWRtaW4= password: dDBwLVNlY3JldA== EOF Step 4 Create the secret: kubectl create -f app-secrets.yaml Step 5 Verify secret creation and get details: kubectl get secrets Step 6 Get details of this secret: kubectl describe secrets/app-secrets Result: Name: app-secret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== password: 10 bytes username: 5 bytes Step 7 Create a pod that will reference the secret as a volume. Use either nano or vim to create secret-pod.yaml, and copy the following to it apiVersion: v1 kind: Pod metadata: name: secret-test-pod spec: containers: - name: test-container image: alpine command: [\"/bin/sh\", \"-ec\", \"export LOGIN=$(cat /etc/secret-volume/username);export PWD=$(cat /etc/secret-volume/password);while true; do echo hello $LOGIN your password is $PWD; sleep 10; done\"] volumeMounts: - name: secret-volume mountPath: /etc/secret-volume volumes: - name: secret-volume secret: secretName: app-secret restartPolicy: Never Step 8 View the logs of the pod. kubectl logs secret-test-pod Notice how we were able to pull the content of the secret using a command. Not very secure, right? Step 9 Delete both the pod and the secret kubectl delete pod secret-test-pod kubectl delete secret app-secret Summary Secrets has been created and they not visiable when you view them via kubectl describe This does not make Kubernetes secrets secure as we have experienced. Additonal measures need to be in place to protect secrets. 1.4 Jobs \u00b6 A Job creates one or more pods and ensures that a specified number of them successfully complete. A job keeps track of successful completion of a pod. When the specified number of pods have successfully completed, the job itself is complete. The job will start a new pod if the pod fails or is deleted due to hardware failure. A successful completion of the specified number of pods means the job is complete. This is different from a replica set or a deployment which ensures that a certain number of pods are always running. So if a pod in a replica set or deployment terminates, then it is restarted again. This makes replica set or deployment as long-running processes. This is well suited for a web server, such as NGINX. But a job is completed if the specified number of pods successfully completes. This is well suited for tasks that need to run only once. For example, a job may convert an image format from one to another. Restarting this pod in replication controller would not only cause redundant work but may be harmful in certain cases. Jobs are complementary to Replica Set. A Replica Set manages pods which are not expected to terminate (e.g. web servers), and a Job manages pods that are expected to terminate (e.g. batch jobs). 1.4.1 Non-parallel Job \u00b6 Only one pod per job is started, unless the pod fails. Job is complete as soon as the pod terminates successfully. Use the image \"busybox\" and have it sleep for 10 seconds and then complete. Run your job to be sure it works. Step 1 Create a Job Manifest cat <<EOF > busybox.yaml apiVersion: batch/v1 kind: Job metadata: name: busybox spec: template: spec: containers: - name: busybox image: busybox command: [\"sleep\", \"20\"] restartPolicy: Never EOF Step 2 Run a Job kubectl create -f busybox.yaml Step 3 Look at the job: kubectl get jobs Output: NAME DESIRED SUCCESSFUL AGE busybox 1 0 0s Result The output shows that the job is not successful yet. Step 4 Watch the pod status kubectl get -w pods Output: NAME READY STATUS RESTARTS AGE busybox-lk49x 1/1 Running 0 7s busybox-lk49x 0/1 Completed 0 24s Result It starts with pod for the job is Running . Then pod successfully exits after a few seconds and shows the Completed status. Step 5 Watch the job status again: kubectl get jobs Output: NAME COMPLETIONS DURATION AGE busybox 1/1 21s 1m Step 6 Delete a Job kubectl delete -f busybox.yaml 1.4.2 Parallel Job \u00b6 Non-parallel jobs run only one pod per job. This API is used to run multiple pods in parallel for the job. The number of pods to complete is defined by .spec.completions attribute in the configuration file. The number of pods to run in parallel is defined by .spec.parallelism attribute in the configuration file. The default value for both of these attributes is 1. The job is complete when there is one successful pod for each value in the range in 1 to .spec.completions . For that reason, it is also called as fixed completion count job. Step 1 Create a Job Manifest cat <<EOF > job-parallel.yaml apiVersion: batch/v1 kind: Job metadata: name: wait spec: completions: 6 parallelism: 2 template: metadata: name: wait spec: containers: - name: wait image: ubuntu command: [\"sleep\", \"10\"] restartPolicy: Never EOF Note This job specification is similar to the non-parallel job specification above. However it has two new attributes added: .spec.completions and .spec.parallelism . This means the job will be complete when six pods have successfully completed. A maximum of two pods will run in parallel at a given time. Step 2 Create a parallel job using the command: kubectl apply -f job-parallel.yaml Step 3 Watch the status of the job as shown: kubectl get -w jobs Output: NAME COMPLETIONS DURATION AGE wait 0/6 6s 6s wait 1/6 12s 12s wait 2/6 12s 12s wait 3/6 24s 24s wait 4/6 24s 24s wait 5/6 36s 36s wait 6/6 36s 36s Results The output shows that 2 pods are created about every 12 seconds. Step 4 In another terminal window, watch the status of pods created: kubectl get -w pods -l job-name=wait Output: NAME READY STATUS RESTARTS AGE wait-5blwm 0/1 Completed 0 17s wait-stmk4 0/1 Completed 0 17s wait-ts6xt 1/1 Running 0 5s wait-xlhl6 1/1 Running 0 5s wait-xlhl6 0/1 Completed 0 12s wait-rq6z5 0/1 Pending 0 0s wait-ts6xt 0/1 Completed 0 12s wait-rq6z5 0/1 Pending 0 0s wait-rq6z5 0/1 ContainerCreating 0 0s wait-f85bj 0/1 Pending 0 0s wait-f85bj 0/1 Pending 0 0s wait-f85bj 0/1 ContainerCreating 0 0s wait-rq6z5 1/1 Running 0 2s wait-f85bj 1/1 Running 0 2s wait-f85bj 0/1 Completed 0 12s wait-rq6z5 0/1 Completed 0 12s Step 6 Once the job is completed, you can get the list of completed pods kubectl get pods -l job-name=wait Result: NAME READY STATUS RESTARTS AGE wait-5blwm 0/1 Completed 0 2m55s wait-f85bj 0/1 Completed 0 2m31s wait-rq6z5 0/1 Completed 0 2m31s wait-stmk4 0/1 Completed 0 2m55s wait-ts6xt 0/1 Completed 0 2m43s wait-xlhl6 0/1 Completed 0 2m43s Step 5 Similarly, kubectl get jobs shows the status of the job after it has completed: kubectl get jobs Result: NAME COMPLETIONS DURATION AGE wait 6/6 36s 3m54s Step 6 Deleting a job deletes all the pods as well. Delete the job as: kubectl delete -f job-parallel.yaml 1.5 Cron Jobs \u00b6 A Cron Job is a job that runs on a given schedule, written in Cron format. There are two primary use cases: Run jobs once at a specified point in time Repeatedly at a specified point in time 1.5.1 Create Cron Job \u00b6 Step 1 Create CronJob manifest that prints the current timestamp and the message \" Hello World \" every minute. cat <<EOF > cronjob.yaml apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: metadata: labels: app: hello-cronpod spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello World! restartPolicy: OnFailure EOF Step 2 Create the Cron Job as shown in the command: kubectl create -f cronjob.yaml Step 3 Watch the status of the job as shown: kubectl get -w cronjobs Output : NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hello */1 * * * * False 0 <none> 6s hello */1 * * * * False 1 5s 39s hello */1 * * * * False 0 15s 49s hello */1 * * * * False 1 5s 99s hello */1 * * * * False 0 15s 109s hello */1 * * * * False 1 6s 2m40s hello */1 * * * * False 0 16s 2m50s Step 4 In another terminal window, watch the status of pods created: kubectl get -w pods -l app=hello-cronpod Output : NAME READY STATUS RESTARTS AGE hello-1622584020-kc46c 0/1 Completed 0 118s hello-1622584080-c2pcq 0/1 Completed 0 58s hello-1622584140-hxnv2 0/1 Pending 0 0s hello-1622584140-hxnv2 0/1 Pending 0 0s hello-1622584140-hxnv2 0/1 ContainerCreating 0 0s hello-1622584140-hxnv2 1/1 Running 0 1s hello-1622584140-hxnv2 0/1 Completed 0 2s Step 5 Get logs from one of the pods: kubectl logs hello-1622584140-hxnv2 Output : Tue Jun 1 21:49:07 UTC 2021 Hello World! Step 6 Delete Cron Job kubectl delete -f cronjob.yaml 1.6 Daemon Set \u00b6 Daemon Set ensures that a copy of the pod runs on a selected set of nodes. By default, all nodes in the cluster are selected. A selection critieria may be specified to select a limited number of nodes. As new nodes are added to the cluster, pods are started on them. As nodes are removed, pods are removed through garbage collection. The following is an example DaemonSet that runs a Prometheus exporter container that used for collecting machine metrics from each node. Step 1 Create a DaemonSet manifest cat <<EOF > daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: prometheus-daemonset spec: selector: matchLabels: tier: monitoring name: prometheus-exporter template: metadata: labels: tier: monitoring name: prometheus-exporter spec: containers: - name: prometheus image: prom/node-exporter ports: - containerPort: 80 EOF Step 2 Run the following command to create the ReplicaSet and pods: kubectl create -f daemonset.yaml --record Note The --record flag will track changes made through each revision. Step 3 Get basic details about the DaemonSet: kubectl get daemonsets/prometheus-daemonset Output: NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE-SELECTOR AGE prometheus-daemonset 1 1 1 1 1 <none> 5s Step 4 Get more details about the DaemonSet: kubectl describe daemonset/prometheus-daemonset Output: Name: prometheus-daemonset Selector: name=prometheus-exporter,tier=monitoring Node-Selector: <none> Labels: <none> Annotations: deprecated.daemonset.template.generation: 1 kubernetes.io/change-cause: kubectl create --filename=daemonset.yaml --record=true Desired Number of Nodes Scheduled: 2 Current Number of Nodes Scheduled: 2 Number of Nodes Scheduled with Up-to-date Pods: 2 Number of Nodes Scheduled with Available Pods: 2 Number of Nodes Misscheduled: 0 Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: name=prometheus-exporter tier=monitoring Containers: prometheus: Image: prom/node-exporter Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 9m daemonset-controller Created pod: prometheus-daemonset-f4xl2 Normal SuccessfulCreate 2m18s daemonset-controller Created pod: prometheus-daemonset-w596z Step 5 Get pods in the DaemonSet: kubectl get pods -l name=prometheus-exporter Output: NAME READY STATUS RESTARTS AGE prometheus-daemonset-f4xl2 1/1 Running 0 8m27s prometheus-daemonset-w596z 1/1 Running 0 105s Step 6 Verify that the Prometheus pod was successfully deployed to the cluster nodes: kubectl get pods -o wide Output: NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES prometheus-daemonset-f4xl2 1/1 Running 0 6m51s 10.0.0.19 gke-k8s-features-default-pool-73e09df7-m2lf <none> <none> prometheus-daemonset-w596z 1/1 Running 0 9s 10.0.1.2 gke-k8s-features-default-pool-73e09df7-k7hj <none> <none> Notes It is possible to Limit DaemonSets to specific nodes by changing the spec.template.spec to include a nodeSelector to matche node label. Step 7 Delete a DaemonSet kubectl delete -f daemonset.yaml 1.7 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"Lab 8 Kubernetes Features"},{"location":"Lab_8_Kubernetes_Features/#0-create-gke-cluster","text":"Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-features \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-features us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-features --zone us-central1-c","title":"0 Create GKE Cluster"},{"location":"Lab_8_Kubernetes_Features/#1-kubernetes-features","text":"","title":"1 Kubernetes Features"},{"location":"Lab_8_Kubernetes_Features/#11-using-liveness-probes","text":"Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes p rovides liveness probes to detect and remedy such situations. As we already discussed Kubernetes provides 3 types of Probes to perform Liveness checks: HTTP GET EXEC tcpSocket In below example we are going to use HTTP GET probe for a Pod that runs a container based on the gcr.io/google_containers/liveness image. Step 1 Create http-liveness.yaml manifest with below content: cat <<EOF > http-liveness.yaml apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: gcr.io/google_containers/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 EOF Based on the manifest, we can see that the Pod has a single Container. The periodSeconds field specifies that the kubelet should perform a liveness probe every 3 seconds. The initialDelaySeconds field tells the kubelet that it should wait 3 seconds before performing the first probe. To perform a probe, the kubelet sends an HTTP GET request to the server that is running in the Container and listening on port 8080. If the handler for the server's /healthz path returns a success code, the kubelet considers the Container to be alive and healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it. Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure. Full code for for a reference in server.go . For the first 10 seconds that the Container is alive, the /healthz handler returns a status of 200. After that, the handler returns a status of 500. http . HandleFunc ( \"/healthz\" , func ( w http . ResponseWriter , r * http . Request ) { duration := time . Now (). Sub ( started ) if duration . Seconds () > 10 { w . WriteHeader ( 500 ) w . Write ([] byte ( fmt . Sprintf ( \"error: %v\" , duration . Seconds ()))) } else { w . WriteHeader ( 200 ) w . Write ([] byte ( \"ok\" )) } }) The kubelet starts performing health checks 3 seconds after the Container starts. So the first couple of health checks will succeed. But after 10 seconds, the health checks will fail, and the kubelet will kill and restart the Container. Step 2 Let's create a Pod and see how HTTP liveness check works: kubectl create -f http-liveness.yaml Step 3 Monitor the Pod watch kubectl get pod Result After 10 seconds Pods has beed restarted. Exit the shell session by using ctrl+c Step 4 Verify status of the Pod and review the Events happened after restart kubectl describe pod liveness-http Result Pod events shows that liveness probes have failed and the Container has been restarted. Step 5 Clean up kubectl delete -f http-liveness.yaml","title":"1.1 Using Liveness Probes"},{"location":"Lab_8_Kubernetes_Features/#12-using-configmaps","text":"In Kubernetes ConfigMaps could be use in several cases: Storing configuration values as key-values in ConfigMap and referencing them in a Pod as environment variables Storing configurations as a file inside of ConfigMap and referencing it in a Pod as a Volume Let's try second option and deploy nginx pod while storing its config in a ConfigMap. Step 1 Create nginx my-nginx-config.conf config file as below: cat <<EOF > my-nginx-config.conf server { listen 80; server_name www.cloudnative.tech; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } EOF Step 2 Create ConfigMap from this file kubectl create configmap nginxconfig --from-file=my-nginx-config.conf Step 3 Review the ConfigMap kubectl describe cm nginxconfig Result: ``` Name: nginxconfig Namespace: default Labels: <none> Annotations: <none> Data ==== my-nginx-config.conf: ---- server { listen 80; server_name _; gzip off; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } Events: <none> ``` Step 4 Create Nginx Pod website.yaml file, where ConfigMap referenced as a Volume cat <<EOF > website.yaml apiVersion: v1 kind: Pod metadata: name: website spec: containers: - image: nginx:alpine name: website volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: false - name: config mountPath: /etc/nginx/conf.d readOnly: true volumes: - name: html emptyDir: {} - name: config configMap: name: nginxconfig EOF Step 5 Deploy Nginx Pod website.yaml kubectl apply -f website.yaml You can expose a service as we usually do with NodePort or LoadBalancer or by using the port-forward technicque in the next step Step 6 Open second SSH terminal by pressing \"+\" icon in the cloud shell and run following command kubectl port-forward website 8080:80 Result This opens a tunnel and expose our application on port '80' to localhost:8080. Step 7 Test that website is running Navigate back to the first terminal tab and run the following curl localhost:8080 Result: ``` <html> <head><title>403 Forbidden</title></head> <body> <center><h1>403 Forbidden</h1></center> <hr><center>nginx/1.21.0</center> </body> </html> ``` Success You've just learned how to use ConfigMaps. You can also use the preview feature with the console Step 8 start a shell in the pod by using exec kubectl exec website -it -- sh Your terminal now should have /# Step 9 view the content of the folder where we added the volume ls /etc/nginx/conf.d Step 10 check the content of the file in that folder and notice that it's the same as the config file for nginx we mounted. cat /etc/nginx/conf.d/my-nginx-config.conf Step 11 Exit the shell exit Make sure you're back to the cloud shell terminal.","title":"1.2 Using ConfigMaps"},{"location":"Lab_8_Kubernetes_Features/#13-using-secrets","text":"","title":"1.3 Using Secrets"},{"location":"Lab_8_Kubernetes_Features/#131-kubernetes-secrets","text":"Kubernetes secrets allow users to define sensitive information outside of containers and expose that information to containers through environment variables as well as files within Pods. In this section we will declare and create secrets to hold our database connection information that will be used by Wordpress to connect to its backend database. Step 1 Open up two terminal windows. We will use one window to generate encoded strings that will contain our sensitive data. The other window will be used to create the secrets YAML declaration. Step 2 In the first terminal window, execute the following commands to encode our strings: echo -n \"admin\" | base64 echo -n \"t0p-Secret\" | base64 Step 3 create the secret. For this lab, we added the encoded values for you. Feel free to change the username and password, and replace the values in the file below: cat <<EOF > app-secrets.yaml apiVersion: v1 kind: Secret metadata: name: app-secret type: Opaque data: username: YWRtaW4= password: dDBwLVNlY3JldA== EOF Step 4 Create the secret: kubectl create -f app-secrets.yaml Step 5 Verify secret creation and get details: kubectl get secrets Step 6 Get details of this secret: kubectl describe secrets/app-secrets Result: Name: app-secret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== password: 10 bytes username: 5 bytes Step 7 Create a pod that will reference the secret as a volume. Use either nano or vim to create secret-pod.yaml, and copy the following to it apiVersion: v1 kind: Pod metadata: name: secret-test-pod spec: containers: - name: test-container image: alpine command: [\"/bin/sh\", \"-ec\", \"export LOGIN=$(cat /etc/secret-volume/username);export PWD=$(cat /etc/secret-volume/password);while true; do echo hello $LOGIN your password is $PWD; sleep 10; done\"] volumeMounts: - name: secret-volume mountPath: /etc/secret-volume volumes: - name: secret-volume secret: secretName: app-secret restartPolicy: Never Step 8 View the logs of the pod. kubectl logs secret-test-pod Notice how we were able to pull the content of the secret using a command. Not very secure, right? Step 9 Delete both the pod and the secret kubectl delete pod secret-test-pod kubectl delete secret app-secret Summary Secrets has been created and they not visiable when you view them via kubectl describe This does not make Kubernetes secrets secure as we have experienced. Additonal measures need to be in place to protect secrets.","title":"1.3.1 Kubernetes Secrets"},{"location":"Lab_8_Kubernetes_Features/#14-jobs","text":"A Job creates one or more pods and ensures that a specified number of them successfully complete. A job keeps track of successful completion of a pod. When the specified number of pods have successfully completed, the job itself is complete. The job will start a new pod if the pod fails or is deleted due to hardware failure. A successful completion of the specified number of pods means the job is complete. This is different from a replica set or a deployment which ensures that a certain number of pods are always running. So if a pod in a replica set or deployment terminates, then it is restarted again. This makes replica set or deployment as long-running processes. This is well suited for a web server, such as NGINX. But a job is completed if the specified number of pods successfully completes. This is well suited for tasks that need to run only once. For example, a job may convert an image format from one to another. Restarting this pod in replication controller would not only cause redundant work but may be harmful in certain cases. Jobs are complementary to Replica Set. A Replica Set manages pods which are not expected to terminate (e.g. web servers), and a Job manages pods that are expected to terminate (e.g. batch jobs).","title":"1.4 Jobs"},{"location":"Lab_8_Kubernetes_Features/#141-non-parallel-job","text":"Only one pod per job is started, unless the pod fails. Job is complete as soon as the pod terminates successfully. Use the image \"busybox\" and have it sleep for 10 seconds and then complete. Run your job to be sure it works. Step 1 Create a Job Manifest cat <<EOF > busybox.yaml apiVersion: batch/v1 kind: Job metadata: name: busybox spec: template: spec: containers: - name: busybox image: busybox command: [\"sleep\", \"20\"] restartPolicy: Never EOF Step 2 Run a Job kubectl create -f busybox.yaml Step 3 Look at the job: kubectl get jobs Output: NAME DESIRED SUCCESSFUL AGE busybox 1 0 0s Result The output shows that the job is not successful yet. Step 4 Watch the pod status kubectl get -w pods Output: NAME READY STATUS RESTARTS AGE busybox-lk49x 1/1 Running 0 7s busybox-lk49x 0/1 Completed 0 24s Result It starts with pod for the job is Running . Then pod successfully exits after a few seconds and shows the Completed status. Step 5 Watch the job status again: kubectl get jobs Output: NAME COMPLETIONS DURATION AGE busybox 1/1 21s 1m Step 6 Delete a Job kubectl delete -f busybox.yaml","title":"1.4.1 Non-parallel Job"},{"location":"Lab_8_Kubernetes_Features/#142-parallel-job","text":"Non-parallel jobs run only one pod per job. This API is used to run multiple pods in parallel for the job. The number of pods to complete is defined by .spec.completions attribute in the configuration file. The number of pods to run in parallel is defined by .spec.parallelism attribute in the configuration file. The default value for both of these attributes is 1. The job is complete when there is one successful pod for each value in the range in 1 to .spec.completions . For that reason, it is also called as fixed completion count job. Step 1 Create a Job Manifest cat <<EOF > job-parallel.yaml apiVersion: batch/v1 kind: Job metadata: name: wait spec: completions: 6 parallelism: 2 template: metadata: name: wait spec: containers: - name: wait image: ubuntu command: [\"sleep\", \"10\"] restartPolicy: Never EOF Note This job specification is similar to the non-parallel job specification above. However it has two new attributes added: .spec.completions and .spec.parallelism . This means the job will be complete when six pods have successfully completed. A maximum of two pods will run in parallel at a given time. Step 2 Create a parallel job using the command: kubectl apply -f job-parallel.yaml Step 3 Watch the status of the job as shown: kubectl get -w jobs Output: NAME COMPLETIONS DURATION AGE wait 0/6 6s 6s wait 1/6 12s 12s wait 2/6 12s 12s wait 3/6 24s 24s wait 4/6 24s 24s wait 5/6 36s 36s wait 6/6 36s 36s Results The output shows that 2 pods are created about every 12 seconds. Step 4 In another terminal window, watch the status of pods created: kubectl get -w pods -l job-name=wait Output: NAME READY STATUS RESTARTS AGE wait-5blwm 0/1 Completed 0 17s wait-stmk4 0/1 Completed 0 17s wait-ts6xt 1/1 Running 0 5s wait-xlhl6 1/1 Running 0 5s wait-xlhl6 0/1 Completed 0 12s wait-rq6z5 0/1 Pending 0 0s wait-ts6xt 0/1 Completed 0 12s wait-rq6z5 0/1 Pending 0 0s wait-rq6z5 0/1 ContainerCreating 0 0s wait-f85bj 0/1 Pending 0 0s wait-f85bj 0/1 Pending 0 0s wait-f85bj 0/1 ContainerCreating 0 0s wait-rq6z5 1/1 Running 0 2s wait-f85bj 1/1 Running 0 2s wait-f85bj 0/1 Completed 0 12s wait-rq6z5 0/1 Completed 0 12s Step 6 Once the job is completed, you can get the list of completed pods kubectl get pods -l job-name=wait Result: NAME READY STATUS RESTARTS AGE wait-5blwm 0/1 Completed 0 2m55s wait-f85bj 0/1 Completed 0 2m31s wait-rq6z5 0/1 Completed 0 2m31s wait-stmk4 0/1 Completed 0 2m55s wait-ts6xt 0/1 Completed 0 2m43s wait-xlhl6 0/1 Completed 0 2m43s Step 5 Similarly, kubectl get jobs shows the status of the job after it has completed: kubectl get jobs Result: NAME COMPLETIONS DURATION AGE wait 6/6 36s 3m54s Step 6 Deleting a job deletes all the pods as well. Delete the job as: kubectl delete -f job-parallel.yaml","title":"1.4.2 Parallel Job"},{"location":"Lab_8_Kubernetes_Features/#15-cron-jobs","text":"A Cron Job is a job that runs on a given schedule, written in Cron format. There are two primary use cases: Run jobs once at a specified point in time Repeatedly at a specified point in time","title":"1.5 Cron Jobs"},{"location":"Lab_8_Kubernetes_Features/#151-create-cron-job","text":"Step 1 Create CronJob manifest that prints the current timestamp and the message \" Hello World \" every minute. cat <<EOF > cronjob.yaml apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: metadata: labels: app: hello-cronpod spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello World! restartPolicy: OnFailure EOF Step 2 Create the Cron Job as shown in the command: kubectl create -f cronjob.yaml Step 3 Watch the status of the job as shown: kubectl get -w cronjobs Output : NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hello */1 * * * * False 0 <none> 6s hello */1 * * * * False 1 5s 39s hello */1 * * * * False 0 15s 49s hello */1 * * * * False 1 5s 99s hello */1 * * * * False 0 15s 109s hello */1 * * * * False 1 6s 2m40s hello */1 * * * * False 0 16s 2m50s Step 4 In another terminal window, watch the status of pods created: kubectl get -w pods -l app=hello-cronpod Output : NAME READY STATUS RESTARTS AGE hello-1622584020-kc46c 0/1 Completed 0 118s hello-1622584080-c2pcq 0/1 Completed 0 58s hello-1622584140-hxnv2 0/1 Pending 0 0s hello-1622584140-hxnv2 0/1 Pending 0 0s hello-1622584140-hxnv2 0/1 ContainerCreating 0 0s hello-1622584140-hxnv2 1/1 Running 0 1s hello-1622584140-hxnv2 0/1 Completed 0 2s Step 5 Get logs from one of the pods: kubectl logs hello-1622584140-hxnv2 Output : Tue Jun 1 21:49:07 UTC 2021 Hello World! Step 6 Delete Cron Job kubectl delete -f cronjob.yaml","title":"1.5.1 Create Cron Job"},{"location":"Lab_8_Kubernetes_Features/#16-daemon-set","text":"Daemon Set ensures that a copy of the pod runs on a selected set of nodes. By default, all nodes in the cluster are selected. A selection critieria may be specified to select a limited number of nodes. As new nodes are added to the cluster, pods are started on them. As nodes are removed, pods are removed through garbage collection. The following is an example DaemonSet that runs a Prometheus exporter container that used for collecting machine metrics from each node. Step 1 Create a DaemonSet manifest cat <<EOF > daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: prometheus-daemonset spec: selector: matchLabels: tier: monitoring name: prometheus-exporter template: metadata: labels: tier: monitoring name: prometheus-exporter spec: containers: - name: prometheus image: prom/node-exporter ports: - containerPort: 80 EOF Step 2 Run the following command to create the ReplicaSet and pods: kubectl create -f daemonset.yaml --record Note The --record flag will track changes made through each revision. Step 3 Get basic details about the DaemonSet: kubectl get daemonsets/prometheus-daemonset Output: NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE-SELECTOR AGE prometheus-daemonset 1 1 1 1 1 <none> 5s Step 4 Get more details about the DaemonSet: kubectl describe daemonset/prometheus-daemonset Output: Name: prometheus-daemonset Selector: name=prometheus-exporter,tier=monitoring Node-Selector: <none> Labels: <none> Annotations: deprecated.daemonset.template.generation: 1 kubernetes.io/change-cause: kubectl create --filename=daemonset.yaml --record=true Desired Number of Nodes Scheduled: 2 Current Number of Nodes Scheduled: 2 Number of Nodes Scheduled with Up-to-date Pods: 2 Number of Nodes Scheduled with Available Pods: 2 Number of Nodes Misscheduled: 0 Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: name=prometheus-exporter tier=monitoring Containers: prometheus: Image: prom/node-exporter Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 9m daemonset-controller Created pod: prometheus-daemonset-f4xl2 Normal SuccessfulCreate 2m18s daemonset-controller Created pod: prometheus-daemonset-w596z Step 5 Get pods in the DaemonSet: kubectl get pods -l name=prometheus-exporter Output: NAME READY STATUS RESTARTS AGE prometheus-daemonset-f4xl2 1/1 Running 0 8m27s prometheus-daemonset-w596z 1/1 Running 0 105s Step 6 Verify that the Prometheus pod was successfully deployed to the cluster nodes: kubectl get pods -o wide Output: NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES prometheus-daemonset-f4xl2 1/1 Running 0 6m51s 10.0.0.19 gke-k8s-features-default-pool-73e09df7-m2lf <none> <none> prometheus-daemonset-w596z 1/1 Running 0 9s 10.0.1.2 gke-k8s-features-default-pool-73e09df7-k7hj <none> <none> Notes It is possible to Limit DaemonSets to specific nodes by changing the spec.template.spec to include a nodeSelector to matche node label. Step 7 Delete a DaemonSet kubectl delete -f daemonset.yaml","title":"1.6 Daemon Set"},{"location":"Lab_8_Kubernetes_Features/#17-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"1.7 Cleaning Up"},{"location":"Lab_9_Kubernetes_Scaling/","text":"1 Kubernetes Autoscaling \u00b6 Objective Resource & Limits Scheduling HPA VPA Cluster Autoscaling Node Auto provisioning (NAP) 0 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-scaling \\ --zone us-central1-c \\ --enable-vertical-pod-autoscaling \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-scaling --zone us-central1-c 1.1 Resource and Limits \u00b6 Step 1: Inspecting a node\u2019s capacity kubectl describe nodes | grep -A15 Capacity: The output shows two sets of amounts related to the available resources on the node: the node\u2019s capacity and allocatable resources. The capacity represents the total resources of a node, which may not all be available to pods. Certain resources may be reserved for Kubernetes and/or system components. The Scheduler bases its decisions only on the allocatable resource amounts. Step 2: Show metrics for a given node kubectl top nodes kubectl top pods -n kube-system Result CPU and Memory information is available for pods and node through the metrics API. Step 3 Create a deployment best_effort.yaml as showed below. This is regular deployment with resources configured cat <<EOF > best_effort.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs EOF Step 4 Deploy application kubectl create -f best_effort.yaml Step 5 Verify what is the QOS for this pod: kubectl describe pods | grep QoS Result If you don't specify request/limits K8s provides Best Effort QOS Step 6 Cleanup kubectl delete -f best_effort.yaml Step 7 Create a deployment guaranteed.yaml as showed below. This is regular deployment with resources configured cat <<EOF > guaranteed.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs resources: requests: cpu: 100m memory: 200Mi limits: cpu: 100m memory: 200Mi EOF Step 8 Deploy application kubectl create -f guaranteed.yaml Step 9 Verify what is the QOS for this pod: kubectl describe pods | grep QoS Result If you request = limits K8s provides guaranteed QOS Step 10 Cleanup kubectl delete -f guaranteed.yaml Step 11 Create a deployment burstable.yaml as showed below. This is regular deployment with resources configured cat <<EOF > burstable.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs resources: requests: cpu: 3000 EOF Step 12 Deploy application kubectl create -f burstable.yaml Step 13 Verify what is the QOS for this pod: kubectl describe pods | grep QoS Result If you specify request > or < limits K8s provides Burstable QOS Step 14 Check status of the Pods kubectl get pods Pending Why the deployment failed ??? Step 15 Cleanup kubectl delete -f burstable.yaml 1.2 Creating a Horizontal Pod Autoscaler based on CPU usage \u00b6 Prerequisites: Ensure metrics api is running in your cluster. kubectl get pod -n kube-system Check the status of metrics-server-***** pod status. It should be Running kubectl top nodes kubectl top pods -n kube-system Result CPU and Memory information is available for pods and node through the metrics API. Let\u2019s create a horizontal pod autoscaler now and configure it to scale pods based on their CPU utilization. Step 1 Create a deployment.yaml as showed below. This is regular deployment with resources configured cat <<EOF > deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs resources: requests: cpu: 100m EOF Step 2 Deploy application kubectl create -f deployment.yaml Step 3 After creating the deployment, to enable horizontal autoscaling of its pods, you need to create a HorizontalPodAutoscaler (HPA) object and point it to the deployment. kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5 Note This creates the HPA object for us and sets the deployment called kubia as the scaling target. We\u2019re setting the target CPU utilization of the pods to 30% and specifying the minimum and maximum number of replicas. The autoscaler will thus constantly keep adjusting the number of replicas to keep their CPU utilization around 30%, but it will never scale down to less than 1 or scale up to more than 5 replicas. Step 4 Verify definition of the Horizontal Pod Autoscaler resource to gain a better understanding of it: kubectl get hpa kubia -o yaml Result: apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler ... spec: maxReplicas: 5 minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: kubia targetCPUUtilizationPercentage: 30 status: currentReplicas: 0 desiredReplicas: 0 Step 5 Take a closer look at the HPA and notice that it is still not ready to do the autoscaling. kubectl describe hpa kubia Results Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedGetResourceMetric 2m29s horizontal-pod-autoscaler unable to get metrics for resource cpu: no metrics returned from resource metrics API Warning FailedComputeMetricsReplicas 2m29s horizontal-pod-autoscaler failed to compute desired number of replicas based on listed metrics for Deployment/default/kubia: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API Warning FailedGetResourceMetric 118s (x3 over 2m14s) horizontal-pod-autoscaler did not receive metrics for any ready pods Warning FailedComputeMetricsReplicas 118s (x3 over 2m14s) horizontal-pod-autoscaler failed to compute desired number of replicas based on listed metrics for Deployment/default/kubia: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: did not receive metrics for any ready pods Given that historical data is not available yet, you will see the above in the events section. Give it a minute or so and try again. Eventually, you will see the following in the Events section. Normal SuccessfulRescale 41s horizontal-pod-autoscaler New size: 1; reason: All metrics below target If you take a look at the kubia deployment, you will see it was scaled down from 3 pods to 1 pod. Step 6 Create a service kubectl expose deployment kubia --port=80 --target-port=8080 Step 7 Start another terminal session and run: watch -n 1 kubectl get hpa,deployment Step 8 Generate load to the Application kubectl run -it --rm --restart=Never loadgenerator --image=busybox \\ -- sh -c \"while true; do wget -O - -q http://kubia.default; done\" Step 9 Observe autoscaling In the other terminal you will start noticing that the deployment is being scaled up. Step 10 Terminate both sessions by pressing Ctrl+c 1.3 Scale size of pods with Vertical Pod Autoscaling \u00b6 Step 1 Verify that Vertical Pod Autoscaling has already been enabled on the cluster. We enabled VPA when we created the cluster, by using --enable-vertical-pod-autoscaling . This command can be handy if you want to check VPA on an existing cluster. gcloud container clusters describe k8s-scaling --zone us-central1-c | grep ^verticalPodAutoscaling -A 1 Step 2 Apply the hello-server deployment to your cluster kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:2.0 Step 3 Ensure the deployment was successfully created kubectl get deployment hello-server Step 4 Assign a CPU resource request of 100m to the deployment kubectl set resources deployment hello-server --requests=cpu=100m Step 5 Inspect the container specifics of the hello-server pods, find Requests section, and notice that this pod is currently requesting the 450m CPU we assigned. kubectl describe pod hello-server | sed -n \"/Containers:$/,/Conditions:/p\" Output Containers: hello-app: Image: gcr.io/google-samples/hello-app:2.0 Port: <none> Host Port: <none> Requests: cpu: 100m Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-rw2gr (ro) Conditions: Containers: hello-app: Container ID: containerd://e9bb428186f5d6a6572e81a5c0a9c37118fd2855f22173aa791d8429f35169a6 Image: gcr.io/google-samples/hello-app:2.0 Image ID: gcr.io/google-samples/hello-app@sha256:37e5287945774f27b418ce567cd77f4bbc9ef44a1bcd1a2312369f31f9cce567 Port: <none> Host Port: <none> State: Running Started: Wed, 09 Jun 2021 11:34:15 +0000 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-rw2gr (ro) Conditions: Step 6 Create a manifest for you Vertical Pod Autoscale cat << EOF > hello-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: hello-server-vpa spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: hello-server updatePolicy: updateMode: \"Off\" EOF Step 7 Apply the manifest for hello-vpa kubectl apply -f hello-vpa.yaml Step 8 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa hello-server-vpa Step 9 Locate the \"Container Recommendations\" at the end of the output from the describe command. If you don't see it, wait a little longer and try the previous command again. When it appears, you'll see several different recommendation types, each with values for CPU and memory: Lower Bound: this is the lower bound number VPA looks at for triggering a resize. If your pod utilization goes below this, VPA will delete the pod and scale it down. Target: this is the value VPA will use when resizing the pod. Uncapped Target: if no minimum or maximum capacity is assigned to the VPA, this will be the target utilization for VPA. Upper Bound: this is the upper bound number VPA looks at for triggering a resize. If your pod utilization goes above this, VPA will delete the pod and scale it up. Notice that the VPA is recommending new values for CPU instead of what we set, and also giving you a suggested number for how much memory should be requested. We can at this point manually apply these suggestions, or allow VPA to apply them. Step 10 Update the manifest to set the policy to Auto and apply the configuration sed -i 's/Off/Auto/g' hello-vpa.yaml kubectl apply -f hello-vpa.yaml In order to resize a pod, Vertical Pod Autoscaler will need to delete that pod and recreate it with the new size. By default, to avoid downtime, VPA will not delete and resize the last active pod. Because of this, you will need at least 2 replicas to see VPA make any changes. Step 11 Scale hello-server deployment to 2 replicas: kubectl scale deployment hello-server --replicas=2 Step 12 Watch your pods kubectl get pods -w Step 13 The VPA should have resized your pods in the hello-server deployment. Inspect your pods: kubectl describe pod hello-server | sed -n \"/Containers:$/,/Conditions:/p\" 1.7 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"Lab 9 Kubernetes Autoscaling"},{"location":"Lab_9_Kubernetes_Scaling/#1-kubernetes-autoscaling","text":"Objective Resource & Limits Scheduling HPA VPA Cluster Autoscaling Node Auto provisioning (NAP)","title":"1 Kubernetes Autoscaling"},{"location":"Lab_9_Kubernetes_Scaling/#0-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-scaling \\ --zone us-central1-c \\ --enable-vertical-pod-autoscaling \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-scaling --zone us-central1-c","title":"0 Create GKE Cluster"},{"location":"Lab_9_Kubernetes_Scaling/#11-resource-and-limits","text":"Step 1: Inspecting a node\u2019s capacity kubectl describe nodes | grep -A15 Capacity: The output shows two sets of amounts related to the available resources on the node: the node\u2019s capacity and allocatable resources. The capacity represents the total resources of a node, which may not all be available to pods. Certain resources may be reserved for Kubernetes and/or system components. The Scheduler bases its decisions only on the allocatable resource amounts. Step 2: Show metrics for a given node kubectl top nodes kubectl top pods -n kube-system Result CPU and Memory information is available for pods and node through the metrics API. Step 3 Create a deployment best_effort.yaml as showed below. This is regular deployment with resources configured cat <<EOF > best_effort.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs EOF Step 4 Deploy application kubectl create -f best_effort.yaml Step 5 Verify what is the QOS for this pod: kubectl describe pods | grep QoS Result If you don't specify request/limits K8s provides Best Effort QOS Step 6 Cleanup kubectl delete -f best_effort.yaml Step 7 Create a deployment guaranteed.yaml as showed below. This is regular deployment with resources configured cat <<EOF > guaranteed.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs resources: requests: cpu: 100m memory: 200Mi limits: cpu: 100m memory: 200Mi EOF Step 8 Deploy application kubectl create -f guaranteed.yaml Step 9 Verify what is the QOS for this pod: kubectl describe pods | grep QoS Result If you request = limits K8s provides guaranteed QOS Step 10 Cleanup kubectl delete -f guaranteed.yaml Step 11 Create a deployment burstable.yaml as showed below. This is regular deployment with resources configured cat <<EOF > burstable.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs resources: requests: cpu: 3000 EOF Step 12 Deploy application kubectl create -f burstable.yaml Step 13 Verify what is the QOS for this pod: kubectl describe pods | grep QoS Result If you specify request > or < limits K8s provides Burstable QOS Step 14 Check status of the Pods kubectl get pods Pending Why the deployment failed ??? Step 15 Cleanup kubectl delete -f burstable.yaml","title":"1.1 Resource and Limits"},{"location":"Lab_9_Kubernetes_Scaling/#12-creating-a-horizontal-pod-autoscaler-based-on-cpu-usage","text":"Prerequisites: Ensure metrics api is running in your cluster. kubectl get pod -n kube-system Check the status of metrics-server-***** pod status. It should be Running kubectl top nodes kubectl top pods -n kube-system Result CPU and Memory information is available for pods and node through the metrics API. Let\u2019s create a horizontal pod autoscaler now and configure it to scale pods based on their CPU utilization. Step 1 Create a deployment.yaml as showed below. This is regular deployment with resources configured cat <<EOF > deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: selector: matchLabels: app: kubia replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs resources: requests: cpu: 100m EOF Step 2 Deploy application kubectl create -f deployment.yaml Step 3 After creating the deployment, to enable horizontal autoscaling of its pods, you need to create a HorizontalPodAutoscaler (HPA) object and point it to the deployment. kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5 Note This creates the HPA object for us and sets the deployment called kubia as the scaling target. We\u2019re setting the target CPU utilization of the pods to 30% and specifying the minimum and maximum number of replicas. The autoscaler will thus constantly keep adjusting the number of replicas to keep their CPU utilization around 30%, but it will never scale down to less than 1 or scale up to more than 5 replicas. Step 4 Verify definition of the Horizontal Pod Autoscaler resource to gain a better understanding of it: kubectl get hpa kubia -o yaml Result: apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler ... spec: maxReplicas: 5 minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: kubia targetCPUUtilizationPercentage: 30 status: currentReplicas: 0 desiredReplicas: 0 Step 5 Take a closer look at the HPA and notice that it is still not ready to do the autoscaling. kubectl describe hpa kubia Results Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedGetResourceMetric 2m29s horizontal-pod-autoscaler unable to get metrics for resource cpu: no metrics returned from resource metrics API Warning FailedComputeMetricsReplicas 2m29s horizontal-pod-autoscaler failed to compute desired number of replicas based on listed metrics for Deployment/default/kubia: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API Warning FailedGetResourceMetric 118s (x3 over 2m14s) horizontal-pod-autoscaler did not receive metrics for any ready pods Warning FailedComputeMetricsReplicas 118s (x3 over 2m14s) horizontal-pod-autoscaler failed to compute desired number of replicas based on listed metrics for Deployment/default/kubia: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: did not receive metrics for any ready pods Given that historical data is not available yet, you will see the above in the events section. Give it a minute or so and try again. Eventually, you will see the following in the Events section. Normal SuccessfulRescale 41s horizontal-pod-autoscaler New size: 1; reason: All metrics below target If you take a look at the kubia deployment, you will see it was scaled down from 3 pods to 1 pod. Step 6 Create a service kubectl expose deployment kubia --port=80 --target-port=8080 Step 7 Start another terminal session and run: watch -n 1 kubectl get hpa,deployment Step 8 Generate load to the Application kubectl run -it --rm --restart=Never loadgenerator --image=busybox \\ -- sh -c \"while true; do wget -O - -q http://kubia.default; done\" Step 9 Observe autoscaling In the other terminal you will start noticing that the deployment is being scaled up. Step 10 Terminate both sessions by pressing Ctrl+c","title":"1.2 Creating a Horizontal Pod Autoscaler based on CPU usage"},{"location":"Lab_9_Kubernetes_Scaling/#13-scale-size-of-pods-with-vertical-pod-autoscaling","text":"Step 1 Verify that Vertical Pod Autoscaling has already been enabled on the cluster. We enabled VPA when we created the cluster, by using --enable-vertical-pod-autoscaling . This command can be handy if you want to check VPA on an existing cluster. gcloud container clusters describe k8s-scaling --zone us-central1-c | grep ^verticalPodAutoscaling -A 1 Step 2 Apply the hello-server deployment to your cluster kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:2.0 Step 3 Ensure the deployment was successfully created kubectl get deployment hello-server Step 4 Assign a CPU resource request of 100m to the deployment kubectl set resources deployment hello-server --requests=cpu=100m Step 5 Inspect the container specifics of the hello-server pods, find Requests section, and notice that this pod is currently requesting the 450m CPU we assigned. kubectl describe pod hello-server | sed -n \"/Containers:$/,/Conditions:/p\" Output Containers: hello-app: Image: gcr.io/google-samples/hello-app:2.0 Port: <none> Host Port: <none> Requests: cpu: 100m Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-rw2gr (ro) Conditions: Containers: hello-app: Container ID: containerd://e9bb428186f5d6a6572e81a5c0a9c37118fd2855f22173aa791d8429f35169a6 Image: gcr.io/google-samples/hello-app:2.0 Image ID: gcr.io/google-samples/hello-app@sha256:37e5287945774f27b418ce567cd77f4bbc9ef44a1bcd1a2312369f31f9cce567 Port: <none> Host Port: <none> State: Running Started: Wed, 09 Jun 2021 11:34:15 +0000 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-rw2gr (ro) Conditions: Step 6 Create a manifest for you Vertical Pod Autoscale cat << EOF > hello-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: hello-server-vpa spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: hello-server updatePolicy: updateMode: \"Off\" EOF Step 7 Apply the manifest for hello-vpa kubectl apply -f hello-vpa.yaml Step 8 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa hello-server-vpa Step 9 Locate the \"Container Recommendations\" at the end of the output from the describe command. If you don't see it, wait a little longer and try the previous command again. When it appears, you'll see several different recommendation types, each with values for CPU and memory: Lower Bound: this is the lower bound number VPA looks at for triggering a resize. If your pod utilization goes below this, VPA will delete the pod and scale it down. Target: this is the value VPA will use when resizing the pod. Uncapped Target: if no minimum or maximum capacity is assigned to the VPA, this will be the target utilization for VPA. Upper Bound: this is the upper bound number VPA looks at for triggering a resize. If your pod utilization goes above this, VPA will delete the pod and scale it up. Notice that the VPA is recommending new values for CPU instead of what we set, and also giving you a suggested number for how much memory should be requested. We can at this point manually apply these suggestions, or allow VPA to apply them. Step 10 Update the manifest to set the policy to Auto and apply the configuration sed -i 's/Off/Auto/g' hello-vpa.yaml kubectl apply -f hello-vpa.yaml In order to resize a pod, Vertical Pod Autoscaler will need to delete that pod and recreate it with the new size. By default, to avoid downtime, VPA will not delete and resize the last active pod. Because of this, you will need at least 2 replicas to see VPA make any changes. Step 11 Scale hello-server deployment to 2 replicas: kubectl scale deployment hello-server --replicas=2 Step 12 Watch your pods kubectl get pods -w Step 13 The VPA should have resized your pods in the hello-server deployment. Inspect your pods: kubectl describe pod hello-server | sed -n \"/Containers:$/,/Conditions:/p\"","title":"1.3 Scale size of pods with Vertical Pod Autoscaling"},{"location":"Lab_9_Kubernetes_Scaling/#17-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"1.7 Cleaning Up"},{"location":"ass1/","text":"1 Containerize Applications \u00b6 Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1.1 Overview of the Sample Application \u00b6 This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application. 1.1 Build Dockers image for frontend application \u00b6 Step 1 Locate and review the go source code: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Setup vim editor with appropriate color schema Note Skip this step if you use another editor. echo \"colo elflord\" > ~/.vimrc Step 3 Write Dockerfile for your frontend application cd ~/ycit019/Assignment1/gowebapp Create a file named Dockerfile in this directory for the frontend Go app. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1 1.2 Build Docker image for backend application \u00b6 Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally 1.3 Test application by running with Docker Engine. \u00b6 Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network named gowebapp with subnet range 172.19.0.0/16 in which to run the frontend and backend containers: #TODO docker xxx Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Container needs to run on network: `gowebapp` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine #TODO Container needs to run on network: `gowebapp` Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit; 1.5 Cleanup running applications and unused networks \u00b6 ### TODO docker xxx","title":"Assignment1"},{"location":"ass1/#1-containerize-applications","text":"Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process","title":"1 Containerize Applications"},{"location":"ass1/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ass1/#11-overview-of-the-sample-application","text":"This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application.","title":"1.1 Overview of the Sample Application"},{"location":"ass1/#11-build-dockers-image-for-frontend-application","text":"Step 1 Locate and review the go source code: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Setup vim editor with appropriate color schema Note Skip this step if you use another editor. echo \"colo elflord\" > ~/.vimrc Step 3 Write Dockerfile for your frontend application cd ~/ycit019/Assignment1/gowebapp Create a file named Dockerfile in this directory for the frontend Go app. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1","title":"1.1 Build Dockers image for frontend application"},{"location":"ass1/#12-build-docker-image-for-backend-application","text":"Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally","title":"1.2 Build Docker image for backend application"},{"location":"ass1/#13-test-application-by-running-with-docker-engine","text":"Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network named gowebapp with subnet range 172.19.0.0/16 in which to run the frontend and backend containers: #TODO docker xxx Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Container needs to run on network: `gowebapp` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine #TODO Container needs to run on network: `gowebapp` Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit;","title":"1.3 Test application by running with Docker Engine."},{"location":"ass1/#15-cleanup-running-applications-and-unused-networks","text":"### TODO docker xxx","title":"1.5 Cleanup running applications and unused networks"},{"location":"ass1_sol/","text":"1 Containerize Applications \u00b6 Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1.1 Overview of the Sample Application \u00b6 This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application. 1.1 Build Dockers image for frontend application \u00b6 Step 1 Locate and review the go source code folders: https://github.com/Cloud-Architects-Program/k8s cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Write Dockerfile for your frontend application Create a file named Dockerfile in this directory for the frontend Go application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: cd ~/ycit019/Assignment1/gowebapp vim Dockerfile FROM golang:1.15.11 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 3 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. docker build -t <user-name>/gowebapp:v1 . 1.2 Build Docker image for backend application \u00b6 Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1//gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Build gowebapp-mysql Docker image locally docker build -t <user-name>/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally 1.3 Store images in the Dockerhub \u00b6 docker login docker push <user-name>/gowebapp-mysql docker push <user-name>/gowebapp 1.4 Test application by running with Docker Engine. \u00b6 Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd <user-name>/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp <user-name>/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: docker exec -it gowebapp-mysql bash Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit; 1.5 Cleanup running applications and unused networks \u00b6 docker rm -f $(docker ps -q)","title":"Assignment1 - Solution"},{"location":"ass1_sol/#1-containerize-applications","text":"Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process","title":"1 Containerize Applications"},{"location":"ass1_sol/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ass1_sol/#11-overview-of-the-sample-application","text":"This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application.","title":"1.1 Overview of the Sample Application"},{"location":"ass1_sol/#11-build-dockers-image-for-frontend-application","text":"Step 1 Locate and review the go source code folders: https://github.com/Cloud-Architects-Program/k8s cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Write Dockerfile for your frontend application Create a file named Dockerfile in this directory for the frontend Go application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: cd ~/ycit019/Assignment1/gowebapp vim Dockerfile FROM golang:1.15.11 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 3 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. docker build -t <user-name>/gowebapp:v1 .","title":"1.1 Build Dockers image for frontend application"},{"location":"ass1_sol/#12-build-docker-image-for-backend-application","text":"Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1//gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Build gowebapp-mysql Docker image locally docker build -t <user-name>/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally","title":"1.2 Build Docker image for backend application"},{"location":"ass1_sol/#13-store-images-in-the-dockerhub","text":"docker login docker push <user-name>/gowebapp-mysql docker push <user-name>/gowebapp","title":"1.3 Store images in the Dockerhub"},{"location":"ass1_sol/#14-test-application-by-running-with-docker-engine","text":"Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd <user-name>/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp <user-name>/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: docker exec -it gowebapp-mysql bash Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit;","title":"1.4 Test application by running with Docker Engine."},{"location":"ass1_sol/#15-cleanup-running-applications-and-unused-networks","text":"docker rm -f $(docker ps -q)","title":"1.5 Cleanup running applications and unused networks"},{"location":"ass2/","text":"1 Containerize Applications \u00b6 Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1 Configure Cloud Source Repository \u00b6 Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console. 1.1 Create a repository with Cloud Source Repository \u00b6 Step 1 Start a new session in Cloud Shell and clone repository with Assignment2: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 Result Two folders with go app and mysql config has been reviewed. Step 2 Run the following command to create a new Cloud Source Repository named $student_id-notepad, where $student_id - is you mcgill student-ID: gcloud source repos create $student_id-notepad You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: MY_REPO=$student_id-notepad gcloud source repos clone $MY_REPO The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: MY_REPO=$student_id-notepad cd $MY_REPO Step 4 Copy Assignment 2 code to you repo: cp -r ~/ycit019/Assignment2/* . Step 5 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" git add . git commit -m \"Repo Structure\" Output: [master (root-commit) 48c4f03] Repo Structure 82 files changed, 3686 insertions(+) ... Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Git pushes the sample application files from the master branch to the origin remote: Output: Enumerating objects: 119, done. Counting objects: 100% (119/119), done. Delta compression using up to 4 threads Compressing objects: 100% (104/104), done. Writing objects: 100% (119/119), 224.31 KiB | 5.47 MiB/s, done. Total 119 (delta 22), reused 0 (delta 0) remote: Resolving deltas: 100% (22/22) To https://source.developers.google.com/p/mcgil-stundent/r/notepad-gowebapp * [new branch] master -> master Browse files in the Google Cloud Source repository Step 7 Browse files in the Google Cloud Source repository Use the Google Cloud Source Repositories source code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Assignment 2 files you pushed to the repository by opening the Navigation menu and selecting Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. Step 8 View a file in the Google Cloud repository Click $MY_REPO > gowebapp to view the file's contents in the source code browser Click $MY_REPO > gowebapp-mysql to view the file's contents in the source code browser 2 Build and push Docker images to Google Container Registry (GCR) \u00b6 2.1 Build and push gowebapp-mysql Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 2.2 Build and push gowebapp Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 2.3 Test application by running with Docker Engine. \u00b6 Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. 2.4 Cleanup running applications and unused networks \u00b6 ### TODO docker xxx 3 Docker Compose \u00b6 3.1 Test application locally with Docker Compose \u00b6 Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v2 documentations Note Starting from Docker Compose 1.27.0+, v2 and v3 format has been merged, there for if you using latest version above 1.27.0+ you do NOT need to specify version of docker-compose, otherwise you need to specify version 2 or 3. Where version 3 mainly used for docker swarm deployment. Implementation Ensure that mysql start first and then webapp services Ensure that mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Use version 2 or 3 of compose Step 1 Create compose file cd ~/$MY_REPO/ vim docker-compose.yaml Create structure as following, replace #TODO with correct values. version: '2.4' services: gowebapp-mysql: build: #TODO ... gowebapp: build: #TODO ... networks: #TODO docker-compose up -d Step 3 Test application Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!! 3.2 Commit docker-compose file to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.3 Grant viewing permissions for a repository to Instructors/Teachers \u00b6 Reference document gcloud projects add-iam-policy-binding mcgil-stundent --member='user:ayrat.khayretdinov@gmail.com' --role=roles/viewer gcloud projects add-iam-policy-binding mcgil-stundent --member='user:dima.kassab@gmail.com' --role=roles/viewer Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"Assignment2"},{"location":"ass2/#1-containerize-applications","text":"Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose","title":"1 Containerize Applications"},{"location":"ass2/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ass2/#1-configure-cloud-source-repository","text":"Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console.","title":"1 Configure Cloud Source Repository"},{"location":"ass2/#11-create-a-repository-with-cloud-source-repository","text":"Step 1 Start a new session in Cloud Shell and clone repository with Assignment2: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 Result Two folders with go app and mysql config has been reviewed. Step 2 Run the following command to create a new Cloud Source Repository named $student_id-notepad, where $student_id - is you mcgill student-ID: gcloud source repos create $student_id-notepad You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: MY_REPO=$student_id-notepad gcloud source repos clone $MY_REPO The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: MY_REPO=$student_id-notepad cd $MY_REPO Step 4 Copy Assignment 2 code to you repo: cp -r ~/ycit019/Assignment2/* . Step 5 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" git add . git commit -m \"Repo Structure\" Output: [master (root-commit) 48c4f03] Repo Structure 82 files changed, 3686 insertions(+) ... Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Git pushes the sample application files from the master branch to the origin remote: Output: Enumerating objects: 119, done. Counting objects: 100% (119/119), done. Delta compression using up to 4 threads Compressing objects: 100% (104/104), done. Writing objects: 100% (119/119), 224.31 KiB | 5.47 MiB/s, done. Total 119 (delta 22), reused 0 (delta 0) remote: Resolving deltas: 100% (22/22) To https://source.developers.google.com/p/mcgil-stundent/r/notepad-gowebapp * [new branch] master -> master Browse files in the Google Cloud Source repository Step 7 Browse files in the Google Cloud Source repository Use the Google Cloud Source Repositories source code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Assignment 2 files you pushed to the repository by opening the Navigation menu and selecting Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. Step 8 View a file in the Google Cloud repository Click $MY_REPO > gowebapp to view the file's contents in the source code browser Click $MY_REPO > gowebapp-mysql to view the file's contents in the source code browser","title":"1.1 Create a repository with Cloud Source Repository"},{"location":"ass2/#2-build-and-push-docker-images-to-google-container-registry-gcr","text":"","title":"2 Build and push Docker images to Google Container Registry (GCR)"},{"location":"ass2/#21-build-and-push-gowebapp-mysql-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"2.1 Build and push gowebapp-mysql Image to GCR"},{"location":"ass2/#22-build-and-push-gowebapp-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"2.2 Build and push gowebapp Image to GCR"},{"location":"ass2/#23-test-application-by-running-with-docker-engine","text":"Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed.","title":"2.3 Test application by running with Docker Engine."},{"location":"ass2/#24-cleanup-running-applications-and-unused-networks","text":"### TODO docker xxx","title":"2.4 Cleanup running applications and unused networks"},{"location":"ass2/#3-docker-compose","text":"","title":"3 Docker Compose"},{"location":"ass2/#31-test-application-locally-with-docker-compose","text":"Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v2 documentations Note Starting from Docker Compose 1.27.0+, v2 and v3 format has been merged, there for if you using latest version above 1.27.0+ you do NOT need to specify version of docker-compose, otherwise you need to specify version 2 or 3. Where version 3 mainly used for docker swarm deployment. Implementation Ensure that mysql start first and then webapp services Ensure that mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Use version 2 or 3 of compose Step 1 Create compose file cd ~/$MY_REPO/ vim docker-compose.yaml Create structure as following, replace #TODO with correct values. version: '2.4' services: gowebapp-mysql: build: #TODO ... gowebapp: build: #TODO ... networks: #TODO docker-compose up -d Step 3 Test application Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!!","title":"3.1 Test application locally with Docker Compose"},{"location":"ass2/#32-commit-docker-compose-file-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"3.2 Commit docker-compose file to repository and share it with Instructor/Teacher"},{"location":"ass2/#33-grant-viewing-permissions-for-a-repository-to-instructorsteachers","text":"Reference document gcloud projects add-iam-policy-binding mcgil-stundent --member='user:ayrat.khayretdinov@gmail.com' --role=roles/viewer gcloud projects add-iam-policy-binding mcgil-stundent --member='user:dima.kassab@gmail.com' --role=roles/viewer Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"3.3 Grant viewing permissions for a repository to Instructors/Teachers"},{"location":"ass2_solution/","text":"1 Containerize Applications \u00b6 Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. Cloud Source Repositories: Qwik Start 1 Configure Cloud Source Repository \u00b6 Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console. 1.1 Create a repository with Cloud Source Repository \u00b6 Step 1 Start a new session in Cloud Shell and clone repository with Assignment2: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 Result Two folders with go app and mysql config has been reviewed. Step 2 Run the following command to create a new Cloud Source Repository named $student_id-notepad, where $student_id - is you mcgill student-ID: gcloud source repos create $student_id-notepad You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: MY_REPO=$student_id-notepad gcloud source repos clone $MY_REPO The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: cd $MY_REPO Step 4 Copy Assignment 2 code to you repo: cp -r ~/ycit019/Assignment2/* . Step 5 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" git add . git commit -m \"Repo Structure\" Output: [master (root-commit) 48c4f03] Repo Structure 82 files changed, 3686 insertions(+) ... Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Git pushes the sample application files from the master branch to the origin remote: Output: Enumerating objects: 119, done. Counting objects: 100% (119/119), done. Delta compression using up to 4 threads Compressing objects: 100% (104/104), done. Writing objects: 100% (119/119), 224.31 KiB | 5.47 MiB/s, done. Total 119 (delta 22), reused 0 (delta 0) remote: Resolving deltas: 100% (22/22) To https://source.developers.google.com/p/mcgil-stundent/r/notepad-gowebapp * [new branch] master -> master Browse files in the Google Cloud Source repository Step 7 Browse files in the Google Cloud Source repository Use the Google Cloud Source Repositories source code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Assignment 2 files you pushed to the repository by opening the Navigation menu and selecting Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. Step 8 View a file in the Google Cloud repository Click $MY_REPO > gowebapp to view the file's contents in the source code browser Click $MY_REPO > gowebapp-mysql to view the file's contents in the source code browser 2 Build and push Docker images to Google Container Registry (GCR) \u00b6 2.1 Build and push gowebapp-mysql Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 2.1 Build and push gowebapp Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 2.3 Test application by running with Docker Engine. \u00b6 Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. 2.4 Cleanup running applications and unused networks \u00b6 ### TODO docker xxx 3 Docker Compose \u00b6 3.1 Test application locally with Docker Compose \u00b6 Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v2 documentations Note Starting from Docker Compose 1.27.0+, v2 and v3 format has been merged, there for if you using latest version above 1.27.0+ you do need to specify version of docker-compose. Implementation Ensure that Mysql start first and then webapp services Ensure that Mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Step 1 Create compose file cd ~/$MY_REPO/ vim docker-compose.yaml Create structure as following: version: '2.4' services: gowebapp-mysql: build: ./gowebapp-mysql environment: MYSQL_ROOT_PASSWORD: rootpasswd container_name: gowebapp-mysql healthcheck: test: \"/usr/bin/mysql --user=root --password=rootpasswd --execute \\\"SHOW DATABASES;\\\"\" interval: 2s timeout: 5s retries: 30 networks: - gowebapp1 gowebapp: build: ./gowebapp container_name: gowebapp ports: - \"8080:80\" depends_on: gowebapp-mysql: condition: service_healthy networks: - gowebapp1 networks: gowebapp1: driver: bridge Note In this case we using Compose v2.4 version due to support of healthcheck feature. Step 2 Run compose file export CLOUDSDK_PYTHON=/usr/bin/python # https://github.com/google-github-actions/setup-gcloud/issues/128 docker-compose up -d Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!! 3.2 Commit docker-compose file to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.3 Grant viewing permissions for a repository to Instructors/Teachers \u00b6 Reference document gcloud projects add-iam-policy-binding mcgil-stundent --member='user:ayrat.khayretdinov@gmail.com' --role=roles/viewer gcloud projects add-iam-policy-binding mcgil-stundent --member='user:dima.kassab@gmail.com' --role=roles/viewer Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"Assignment2 - Solution"},{"location":"ass2_solution/#1-containerize-applications","text":"Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose","title":"1 Containerize Applications"},{"location":"ass2_solution/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. Cloud Source Repositories: Qwik Start","title":"Prepare Lab Environment"},{"location":"ass2_solution/#1-configure-cloud-source-repository","text":"Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console.","title":"1 Configure Cloud Source Repository"},{"location":"ass2_solution/#11-create-a-repository-with-cloud-source-repository","text":"Step 1 Start a new session in Cloud Shell and clone repository with Assignment2: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 Result Two folders with go app and mysql config has been reviewed. Step 2 Run the following command to create a new Cloud Source Repository named $student_id-notepad, where $student_id - is you mcgill student-ID: gcloud source repos create $student_id-notepad You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: MY_REPO=$student_id-notepad gcloud source repos clone $MY_REPO The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: cd $MY_REPO Step 4 Copy Assignment 2 code to you repo: cp -r ~/ycit019/Assignment2/* . Step 5 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" git add . git commit -m \"Repo Structure\" Output: [master (root-commit) 48c4f03] Repo Structure 82 files changed, 3686 insertions(+) ... Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Git pushes the sample application files from the master branch to the origin remote: Output: Enumerating objects: 119, done. Counting objects: 100% (119/119), done. Delta compression using up to 4 threads Compressing objects: 100% (104/104), done. Writing objects: 100% (119/119), 224.31 KiB | 5.47 MiB/s, done. Total 119 (delta 22), reused 0 (delta 0) remote: Resolving deltas: 100% (22/22) To https://source.developers.google.com/p/mcgil-stundent/r/notepad-gowebapp * [new branch] master -> master Browse files in the Google Cloud Source repository Step 7 Browse files in the Google Cloud Source repository Use the Google Cloud Source Repositories source code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Assignment 2 files you pushed to the repository by opening the Navigation menu and selecting Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. Step 8 View a file in the Google Cloud repository Click $MY_REPO > gowebapp to view the file's contents in the source code browser Click $MY_REPO > gowebapp-mysql to view the file's contents in the source code browser","title":"1.1 Create a repository with Cloud Source Repository"},{"location":"ass2_solution/#2-build-and-push-docker-images-to-google-container-registry-gcr","text":"","title":"2 Build and push Docker images to Google Container Registry (GCR)"},{"location":"ass2_solution/#21-build-and-push-gowebapp-mysql-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"2.1 Build and push gowebapp-mysql Image to GCR"},{"location":"ass2_solution/#21-build-and-push-gowebapp-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"2.1 Build and push gowebapp Image to GCR"},{"location":"ass2_solution/#23-test-application-by-running-with-docker-engine","text":"Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed.","title":"2.3 Test application by running with Docker Engine."},{"location":"ass2_solution/#24-cleanup-running-applications-and-unused-networks","text":"### TODO docker xxx","title":"2.4 Cleanup running applications and unused networks"},{"location":"ass2_solution/#3-docker-compose","text":"","title":"3 Docker Compose"},{"location":"ass2_solution/#31-test-application-locally-with-docker-compose","text":"Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v2 documentations Note Starting from Docker Compose 1.27.0+, v2 and v3 format has been merged, there for if you using latest version above 1.27.0+ you do need to specify version of docker-compose. Implementation Ensure that Mysql start first and then webapp services Ensure that Mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Step 1 Create compose file cd ~/$MY_REPO/ vim docker-compose.yaml Create structure as following: version: '2.4' services: gowebapp-mysql: build: ./gowebapp-mysql environment: MYSQL_ROOT_PASSWORD: rootpasswd container_name: gowebapp-mysql healthcheck: test: \"/usr/bin/mysql --user=root --password=rootpasswd --execute \\\"SHOW DATABASES;\\\"\" interval: 2s timeout: 5s retries: 30 networks: - gowebapp1 gowebapp: build: ./gowebapp container_name: gowebapp ports: - \"8080:80\" depends_on: gowebapp-mysql: condition: service_healthy networks: - gowebapp1 networks: gowebapp1: driver: bridge Note In this case we using Compose v2.4 version due to support of healthcheck feature. Step 2 Run compose file export CLOUDSDK_PYTHON=/usr/bin/python # https://github.com/google-github-actions/setup-gcloud/issues/128 docker-compose up -d Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!!","title":"3.1 Test application locally with Docker Compose"},{"location":"ass2_solution/#32-commit-docker-compose-file-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"3.2 Commit docker-compose file to repository and share it with Instructor/Teacher"},{"location":"ass2_solution/#33-grant-viewing-permissions-for-a-repository-to-instructorsteachers","text":"Reference document gcloud projects add-iam-policy-binding mcgil-stundent --member='user:ayrat.khayretdinov@gmail.com' --role=roles/viewer gcloud projects add-iam-policy-binding mcgil-stundent --member='user:dima.kassab@gmail.com' --role=roles/viewer Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"3.3 Grant viewing permissions for a repository to Instructors/Teachers"},{"location":"ass3/","text":"1 Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating NameSpaces Review process of changing Context Review process of creating K8s: Services Labels, Selectors Deployments Rolling Updates 1.1 Development Tools \u00b6 Note This steps is optional Step 1 Choose one of the following option to develop the YAML manifests: Option 1: You can develop in Google Cloud Shell Editor Option 2: You can also develop locally on your laptop using VCScode . We recommend to use it in conjunction with VSC YAML extension from Redhat Option 3: Use your preferred text editor on Linux VM (vim, nano). 2.1 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c 2.2 Setup KUBECTL AUTOCOMPLETE \u00b6 Since we going to use a lot of kubectl cli let's setup autocomplete. source <(kubectl completion bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 2.3 Create 'dev' namespace and make it default. \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl #TO DO(create namespace `dev`) Step 2 Use dev context to create K8s resources inside this namespace. kubectl #TO DO (make `dev` context default) Step 3 Verify current context: kubectl config current-context Result dev 2.4 Create Service Object for MySQL \u00b6 Step 1 Locate directory where kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment3/ ls Step 2 Go into the local repository you've created: cd ~/$MY_REPO Step 3 Copy Assignment 3 deploy folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019/Assignment3/deploy . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests in deploy folder\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Define a Kubernetes Service object for the backend MySQL database. cd ~/$MY_REPO/deploy Follow instructions below to populate gowebapp-mysql-service.yaml For reference, please see Service docs : https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service Additionally, you can use kubectl built-in docs for any type of resources: kubectl explain service vim gowebapp-mysql-service.yaml Note You can also use VCS or Cloud Code to work with yaml manifest. #TODO: Specify Kubernetes API apiVersion #TODO: Identify the kind of Object metadata: #TODO: Give the service a name: \"gowebapp-mysql\" labels: #TODO: Add a label KV \"run: gowebapp-mysql\" spec: #TODO: leave the clusterIP to None. We allow k8s to assign clusterIP. ports: #TODO: Define a \"port\" as 3306 #TODO: Define a \"targetPort\" as 3306 #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp-mysql\" Step 3 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 4 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\" 2.5 Create Deployment object for the backend MySQL database \u00b6 Step 1 Follow instructions below to populate gowebapp-mysql-deployment.yaml For reference, please see Deployment doc: https://kubernetes.io/docs/concepts/workloads/controllers/deployment vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 #TODO: Identify the type of Object metadata: #TODO: Give the Deployment a name \"gowebapp-mysql\" labels: #TODO: Add a label KV \"run: gowebapp-mysql\" #TODO: give the Deployment a label: tier: backend spec: #TODO: Define number of replicas, set it to 1 #TODO: Starting from Deplloyment v1 selectors are mandatory #add selector KV \"run: gowebapp-mysql\" strategy: type: # Set strategy type as `Recreate` template: metadata: #TODO: Add a label called \"run\" with the name of the service: \"gowebapp-mysql\" spec: containers: - env: - name: # TODO add MYSQL_ROOT_PASSWORD env value image: #TODO define mysql image created in previous assignment, located in gcr registry name: gowebapp-mysql ports: #TODO: define containerPort: 3306 Step 2 Create a Deployment object for MySQL kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp-mysql\" Step 3 Check mysql pod logs: List mysql Pods and note the name the pod : kubectl get pods -l \"run=gowebapp-mysql\" Ensure Mysql is up by looking at pod logs: kubectl logs <Pod_name> Result We have created Service and Deployment for backend application. 2.5 Create a K8s Service for the frontend gowebapp. \u00b6 Step 1 Follow instructions below to populate gowebapp-service.yaml vim gowebapp-service.yaml apiVersion: v1 kind: Service metadata: name: gowebapp labels: #TODO: give the Service a label: run: gowebapp #TODO: give the Service a label: tier: frontend spec: #TODO: Define a \"ports\" array with the \"port\" attribute: 9000 and \"targetPort\" attributes: 80 #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp\" #TODO: Add a Service Type of LoadBalancer #If you need help, see reference: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types Step 2 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 3 Check to make sure it worked kubectl get service -l \"run=gowebapp\" 2.6 Create a K8s Deployment object for the frontend gowebapp \u00b6 Step 1 Follow instructions below to populate gowebapp-deployment.yaml vim gowebapp-deployment.yaml apiVersion: apps/v1 #TODO: define the kind of object as Deployment metadata: #TODO: Add a name attribute for the service as \"gowebapp\" labels: #TODO: give the Deployment a label: run: gowebapp #TODO: give the Deployment a label: tier: frontend spec: #TODO: Define number of replicas, set it to 2 #TODO: add selector KV \"run: gowebapp\" template: metadata: labels: run: gowebapp tier: frontend spec: containers: - env: - #TODO: define name as MYSQL_ROOT_PASSWORD #TODO: define value as cloudops #TODO: Replace <user-name> with value you Docker-Hub ID. image: #TODO define gowebapp image created in previous assignment, located in gcr registry name: gowebapp ports: - #TODO: define the container port as 80 Step 2 Create a Deployment object for gowebapp kubectl apply -f gowebapp-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp\" Step 4 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 5 Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes 2.7 Fix gowebapp code bugs and build a new image. \u00b6 Task: As you've noticed gowebapp frontend app has YCIT019 logo in it. Since you may want to use application for you personal needs, let's change YCIT019 logo to you Name . Step 1 Modify gowebapp frontend so that it has name of you company and link to company web page e.g. vim ~/$MY_REPO/gowebapp/code/template/partial/footer.tmpl Step 2 Build a new version of Image cd ~/$MY_REPO/gowebapp docker build -t gcr.io/${PROJECT_ID}/gowebapp:v2 . docker push gcr.io/${PROJECT_ID}/gowebapp:v2 . 2.7 Rolling Upgrade \u00b6 For gowebapp frontend deployment manifest we've not specified any upgrade strategy type. It means application will use default Upgrade strategy called RollingUpdate . RollingUpdate strategy - updates Pods in a rolling update fashion. maxUnavailable - is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. By default, it ensures that at least 25% less than the desired number of Pods are up (25% max unavailable). Max Surge - is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. By default, it ensures that at most 25% more than the desired number of Pods are up (25% max surge). Step 1 Locate directory with manifest cd ~/$MY_REPO/deploy Step 2 Trigger rolling upgrade using kubectl set command #TO DO Step 3 Verify rollout history #TO DO Step 4 Perform Rollback to v1 #TO DO 2.8 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 2.9 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"Assignment3"},{"location":"ass3/#1-deploy-applications-on-kubernetes","text":"Objective: Review process of creating NameSpaces Review process of changing Context Review process of creating K8s: Services Labels, Selectors Deployments Rolling Updates","title":"1 Deploy Applications on Kubernetes"},{"location":"ass3/#11-development-tools","text":"Note This steps is optional Step 1 Choose one of the following option to develop the YAML manifests: Option 1: You can develop in Google Cloud Shell Editor Option 2: You can also develop locally on your laptop using VCScode . We recommend to use it in conjunction with VSC YAML extension from Redhat Option 3: Use your preferred text editor on Linux VM (vim, nano).","title":"1.1 Development Tools"},{"location":"ass3/#21-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c","title":"2.1 Create GKE Cluster"},{"location":"ass3/#22-setup-kubectl-autocomplete","text":"Since we going to use a lot of kubectl cli let's setup autocomplete. source <(kubectl completion bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"2.2 Setup KUBECTL AUTOCOMPLETE"},{"location":"ass3/#23-create-dev-namespace-and-make-it-default","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl #TO DO(create namespace `dev`) Step 2 Use dev context to create K8s resources inside this namespace. kubectl #TO DO (make `dev` context default) Step 3 Verify current context: kubectl config current-context Result dev","title":"2.3 Create 'dev' namespace and make it default."},{"location":"ass3/#24-create-service-object-for-mysql","text":"Step 1 Locate directory where kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment3/ ls Step 2 Go into the local repository you've created: cd ~/$MY_REPO Step 3 Copy Assignment 3 deploy folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019/Assignment3/deploy . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests in deploy folder\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Define a Kubernetes Service object for the backend MySQL database. cd ~/$MY_REPO/deploy Follow instructions below to populate gowebapp-mysql-service.yaml For reference, please see Service docs : https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service Additionally, you can use kubectl built-in docs for any type of resources: kubectl explain service vim gowebapp-mysql-service.yaml Note You can also use VCS or Cloud Code to work with yaml manifest. #TODO: Specify Kubernetes API apiVersion #TODO: Identify the kind of Object metadata: #TODO: Give the service a name: \"gowebapp-mysql\" labels: #TODO: Add a label KV \"run: gowebapp-mysql\" spec: #TODO: leave the clusterIP to None. We allow k8s to assign clusterIP. ports: #TODO: Define a \"port\" as 3306 #TODO: Define a \"targetPort\" as 3306 #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp-mysql\" Step 3 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 4 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\"","title":"2.4 Create Service Object for MySQL"},{"location":"ass3/#25-create-deployment-object-for-the-backend-mysql-database","text":"Step 1 Follow instructions below to populate gowebapp-mysql-deployment.yaml For reference, please see Deployment doc: https://kubernetes.io/docs/concepts/workloads/controllers/deployment vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 #TODO: Identify the type of Object metadata: #TODO: Give the Deployment a name \"gowebapp-mysql\" labels: #TODO: Add a label KV \"run: gowebapp-mysql\" #TODO: give the Deployment a label: tier: backend spec: #TODO: Define number of replicas, set it to 1 #TODO: Starting from Deplloyment v1 selectors are mandatory #add selector KV \"run: gowebapp-mysql\" strategy: type: # Set strategy type as `Recreate` template: metadata: #TODO: Add a label called \"run\" with the name of the service: \"gowebapp-mysql\" spec: containers: - env: - name: # TODO add MYSQL_ROOT_PASSWORD env value image: #TODO define mysql image created in previous assignment, located in gcr registry name: gowebapp-mysql ports: #TODO: define containerPort: 3306 Step 2 Create a Deployment object for MySQL kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp-mysql\" Step 3 Check mysql pod logs: List mysql Pods and note the name the pod : kubectl get pods -l \"run=gowebapp-mysql\" Ensure Mysql is up by looking at pod logs: kubectl logs <Pod_name> Result We have created Service and Deployment for backend application.","title":"2.5 Create Deployment object for the backend MySQL database"},{"location":"ass3/#25-create-a-k8s-service-for-the-frontend-gowebapp","text":"Step 1 Follow instructions below to populate gowebapp-service.yaml vim gowebapp-service.yaml apiVersion: v1 kind: Service metadata: name: gowebapp labels: #TODO: give the Service a label: run: gowebapp #TODO: give the Service a label: tier: frontend spec: #TODO: Define a \"ports\" array with the \"port\" attribute: 9000 and \"targetPort\" attributes: 80 #TODO: Add a selector for our pods as label \"run\" with value \"gowebapp\" #TODO: Add a Service Type of LoadBalancer #If you need help, see reference: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types Step 2 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 3 Check to make sure it worked kubectl get service -l \"run=gowebapp\"","title":"2.5 Create a K8s Service for the frontend gowebapp."},{"location":"ass3/#26-create-a-k8s-deployment-object-for-the-frontend-gowebapp","text":"Step 1 Follow instructions below to populate gowebapp-deployment.yaml vim gowebapp-deployment.yaml apiVersion: apps/v1 #TODO: define the kind of object as Deployment metadata: #TODO: Add a name attribute for the service as \"gowebapp\" labels: #TODO: give the Deployment a label: run: gowebapp #TODO: give the Deployment a label: tier: frontend spec: #TODO: Define number of replicas, set it to 2 #TODO: add selector KV \"run: gowebapp\" template: metadata: labels: run: gowebapp tier: frontend spec: containers: - env: - #TODO: define name as MYSQL_ROOT_PASSWORD #TODO: define value as cloudops #TODO: Replace <user-name> with value you Docker-Hub ID. image: #TODO define gowebapp image created in previous assignment, located in gcr registry name: gowebapp ports: - #TODO: define the container port as 80 Step 2 Create a Deployment object for gowebapp kubectl apply -f gowebapp-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp\" Step 4 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 5 Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes","title":"2.6 Create a K8s Deployment object for the frontend gowebapp"},{"location":"ass3/#27-fix-gowebapp-code-bugs-and-build-a-new-image","text":"Task: As you've noticed gowebapp frontend app has YCIT019 logo in it. Since you may want to use application for you personal needs, let's change YCIT019 logo to you Name . Step 1 Modify gowebapp frontend so that it has name of you company and link to company web page e.g. vim ~/$MY_REPO/gowebapp/code/template/partial/footer.tmpl Step 2 Build a new version of Image cd ~/$MY_REPO/gowebapp docker build -t gcr.io/${PROJECT_ID}/gowebapp:v2 . docker push gcr.io/${PROJECT_ID}/gowebapp:v2 .","title":"2.7 Fix gowebapp code bugs and build a new image."},{"location":"ass3/#27-rolling-upgrade","text":"For gowebapp frontend deployment manifest we've not specified any upgrade strategy type. It means application will use default Upgrade strategy called RollingUpdate . RollingUpdate strategy - updates Pods in a rolling update fashion. maxUnavailable - is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. By default, it ensures that at least 25% less than the desired number of Pods are up (25% max unavailable). Max Surge - is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. By default, it ensures that at most 25% more than the desired number of Pods are up (25% max surge). Step 1 Locate directory with manifest cd ~/$MY_REPO/deploy Step 2 Trigger rolling upgrade using kubectl set command #TO DO Step 3 Verify rollout history #TO DO Step 4 Perform Rollback to v1 #TO DO","title":"2.7 Rolling Upgrade"},{"location":"ass3/#28-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"2.8 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ass3/#29-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"2.9 Cleaning Up"},{"location":"ass3_solution/","text":"1 Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating NameSpaces Review process of changing Context Review process of creating K8s: Services Labels, Selectors Deployments Rolling Updates 1.1 Development Tools \u00b6 Note This steps is optional Step 1 Choose one of the following option to develop the YAML manifests: Option 1: You can develop in Google Cloud Shell Editor Option 2: You can also develop locally on your laptop using VCScode . We recommend to use it in conjunction with VSC YAML extension from Redhat Option 3: Use your preferred text editor on Linux VM (vim, nano). 2.1 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c 2.2 Setup KUBECTL AUTOCOMPLETE \u00b6 Since we going to use a lot of kubectl cli let's setup autocomplete. source <(kubectl completion bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc 2.3 Create 'dev' namespace and make it default. \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 2 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 3 Verify current context: kubectl config view | grep namespace Result dev 2.4 Create Service Object for MySQL \u00b6 Step 1 Locate directory where kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment3/ ls Step 2 Go into the local repository you've created: cd ~/$MY_REPO Step 3 Copy Assignment 3 deploy folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019/Assignment3/deploy . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests in deploy folder\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Define a Kubernetes Service object for the backend MySQL database. cd ~/$MY_REPO/deploy Follow instructions below to populate gowebapp-mysql-service.yaml For reference, please see Service docs : https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service Additionally, you can use kubectl built-in docs for any type of resources: kubectl explain service vim gowebapp-mysql-service.yaml Note You can also use VCS or Cloud Code to work with yaml manifest. apiVersion: v1 kind: Service metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: clusterIP: None ports: - port: 3306 targetPort: 3306 selector: run: gowebapp-mysql Step 3 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 4 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\" 2.5 Create Deployment object for the backend MySQL database \u00b6 Step 1 Follow instructions below to populate gowebapp-mysql-deployment.yaml For reference, please see Deployment doc: https://kubernetes.io/docs/concepts/workloads/controllers/deployment vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: rootpasswd image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 Step 2 Create a Deployment object for MySQL kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp-mysql\" Step 3 Check mysql pod logs: List mysql Pods and note the name the pod : kubectl get pods -l \"run=gowebapp-mysql\" Ensure Mysql is up by looking at pod logs: kubectl logs <Pod_name> Result We have created Service and Deployment for backend application. 2.5 Create a K8s Service for the frontend gowebapp. \u00b6 Step 1 Follow instructions below to populate gowebapp-service.yaml vim gowebapp-service.yaml apiVersion: v1 kind: Service metadata: name: gowebapp labels: run: gowebapp spec: ports: - port: 9000 targetPort: 80 selector: run: gowebapp type: LoadBalancer Step 2 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 3 Check to make sure it worked kubectl get service -l \"run=gowebapp\" 2.6 Create a K8s Deployment object for the frontend gowebapp \u00b6 Step 1 Follow instructions below to populate gowebapp-deployment.yaml vim gowebapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: rootpasswd image: gcr.io/${PROJECT_ID}/gowebapp:v1 name: gowebapp ports: - containerPort: 80 Step 2 Create a Deployment object for gowebapp kubectl apply -f gowebapp-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp\" Step 4 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 5 Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes 2.7 Fix gowebapp code bugs and build a new image. \u00b6 Task: As you've noticed gowebapp frontend app has YCIT019 logo in it. Since you may want to use application for you personal needs, let's change YCIT019 logo to you Name . Step 1 Modify gowebapp frontend so that it has name of you company and link to company web page e.g. vim ~/$MY_REPO/gowebapp/code/template/partial/footer.tmpl Step 2 Build a new version of Image cd ~/$MY_REPO/gowebapp docker build -t gcr.io/${PROJECT_ID}/gowebapp:v2 . docker push gcr.io/${PROJECT_ID}/gowebapp:v2 . 2.7 Rolling Upgrade \u00b6 For gowebapp frontend deployment manifest we've not specified any upgrade strategy type. It means application will use default Upgrade strategy called RollingUpdate . RollingUpdate strategy - updates Pods in a rolling update fashion. maxUnavailable - is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. By default, it ensures that at least 25% less than the desired number of Pods are up (25% max unavailable). Max Surge - is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. By default, it ensures that at most 25% more than the desired number of Pods are up (25% max surge). Step 1 Locate directory with manifest cd ~/$MY_REPO/deploy Step 2 Trigger rolling upgrade using kubectl set command kubectl set image deployments/gowebapp gowebapp=gcr.io/${PROJECT_ID}/gowebapp:v2 Step 3 Verify rollout history kubectl rollout history deployment/gowebapp Step 4 Perform Rollback to v1 kubectl rollout undo deployment/gowebapp --to-revision=1 2.8 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 2.9 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"Assignment3 - Solution"},{"location":"ass3_solution/#1-deploy-applications-on-kubernetes","text":"Objective: Review process of creating NameSpaces Review process of changing Context Review process of creating K8s: Services Labels, Selectors Deployments Rolling Updates","title":"1 Deploy Applications on Kubernetes"},{"location":"ass3_solution/#11-development-tools","text":"Note This steps is optional Step 1 Choose one of the following option to develop the YAML manifests: Option 1: You can develop in Google Cloud Shell Editor Option 2: You can also develop locally on your laptop using VCScode . We recommend to use it in conjunction with VSC YAML extension from Redhat Option 3: Use your preferred text editor on Linux VM (vim, nano).","title":"1.1 Development Tools"},{"location":"ass3_solution/#21-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c","title":"2.1 Create GKE Cluster"},{"location":"ass3_solution/#22-setup-kubectl-autocomplete","text":"Since we going to use a lot of kubectl cli let's setup autocomplete. source <(kubectl completion bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc","title":"2.2 Setup KUBECTL AUTOCOMPLETE"},{"location":"ass3_solution/#23-create-dev-namespace-and-make-it-default","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 2 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 3 Verify current context: kubectl config view | grep namespace Result dev","title":"2.3 Create 'dev' namespace and make it default."},{"location":"ass3_solution/#24-create-service-object-for-mysql","text":"Step 1 Locate directory where kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment3/ ls Step 2 Go into the local repository you've created: cd ~/$MY_REPO Step 3 Copy Assignment 3 deploy folder to your repo: git pull # Pull latest code from you repo cp -r ~/ycit019/Assignment3/deploy . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests in deploy folder\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Step 6 Define a Kubernetes Service object for the backend MySQL database. cd ~/$MY_REPO/deploy Follow instructions below to populate gowebapp-mysql-service.yaml For reference, please see Service docs : https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service Additionally, you can use kubectl built-in docs for any type of resources: kubectl explain service vim gowebapp-mysql-service.yaml Note You can also use VCS or Cloud Code to work with yaml manifest. apiVersion: v1 kind: Service metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: clusterIP: None ports: - port: 3306 targetPort: 3306 selector: run: gowebapp-mysql Step 3 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 4 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\"","title":"2.4 Create Service Object for MySQL"},{"location":"ass3_solution/#25-create-deployment-object-for-the-backend-mysql-database","text":"Step 1 Follow instructions below to populate gowebapp-mysql-deployment.yaml For reference, please see Deployment doc: https://kubernetes.io/docs/concepts/workloads/controllers/deployment vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: rootpasswd image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 Step 2 Create a Deployment object for MySQL kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp-mysql\" Step 3 Check mysql pod logs: List mysql Pods and note the name the pod : kubectl get pods -l \"run=gowebapp-mysql\" Ensure Mysql is up by looking at pod logs: kubectl logs <Pod_name> Result We have created Service and Deployment for backend application.","title":"2.5 Create Deployment object for the backend MySQL database"},{"location":"ass3_solution/#25-create-a-k8s-service-for-the-frontend-gowebapp","text":"Step 1 Follow instructions below to populate gowebapp-service.yaml vim gowebapp-service.yaml apiVersion: v1 kind: Service metadata: name: gowebapp labels: run: gowebapp spec: ports: - port: 9000 targetPort: 80 selector: run: gowebapp type: LoadBalancer Step 2 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 3 Check to make sure it worked kubectl get service -l \"run=gowebapp\"","title":"2.5 Create a K8s Service for the frontend gowebapp."},{"location":"ass3_solution/#26-create-a-k8s-deployment-object-for-the-frontend-gowebapp","text":"Step 1 Follow instructions below to populate gowebapp-deployment.yaml vim gowebapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: rootpasswd image: gcr.io/${PROJECT_ID}/gowebapp:v1 name: gowebapp ports: - containerPort: 80 Step 2 Create a Deployment object for gowebapp kubectl apply -f gowebapp-deployment.yaml --record Step 3 Check to make sure it worked kubectl get deployment -l \"run=gowebapp\" Step 4 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step 5 Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes","title":"2.6 Create a K8s Deployment object for the frontend gowebapp"},{"location":"ass3_solution/#27-fix-gowebapp-code-bugs-and-build-a-new-image","text":"Task: As you've noticed gowebapp frontend app has YCIT019 logo in it. Since you may want to use application for you personal needs, let's change YCIT019 logo to you Name . Step 1 Modify gowebapp frontend so that it has name of you company and link to company web page e.g. vim ~/$MY_REPO/gowebapp/code/template/partial/footer.tmpl Step 2 Build a new version of Image cd ~/$MY_REPO/gowebapp docker build -t gcr.io/${PROJECT_ID}/gowebapp:v2 . docker push gcr.io/${PROJECT_ID}/gowebapp:v2 .","title":"2.7 Fix gowebapp code bugs and build a new image."},{"location":"ass3_solution/#27-rolling-upgrade","text":"For gowebapp frontend deployment manifest we've not specified any upgrade strategy type. It means application will use default Upgrade strategy called RollingUpdate . RollingUpdate strategy - updates Pods in a rolling update fashion. maxUnavailable - is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. By default, it ensures that at least 25% less than the desired number of Pods are up (25% max unavailable). Max Surge - is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. By default, it ensures that at most 25% more than the desired number of Pods are up (25% max surge). Step 1 Locate directory with manifest cd ~/$MY_REPO/deploy Step 2 Trigger rolling upgrade using kubectl set command kubectl set image deployments/gowebapp gowebapp=gcr.io/${PROJECT_ID}/gowebapp:v2 Step 3 Verify rollout history kubectl rollout history deployment/gowebapp Step 4 Perform Rollback to v1 kubectl rollout undo deployment/gowebapp --to-revision=1","title":"2.7 Rolling Upgrade"},{"location":"ass3_solution/#28-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"2.8 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ass3_solution/#29-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"2.9 Cleaning Up"},{"location":"ass4/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Startup Probes Liveness Probes Readiness Probes Secrets ConfigMaps Externalize Web Application Configuration 1.1 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c 1.2 Locate Assignment 4 \u00b6 Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment4/ ls Result You can see 4 Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 4 deploy_a4 folder to your repo: cp -r ~/ycit019/Assignment4/deploy_a4 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 4\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 2.1 Externalize Web Application Configuration \u00b6 Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~ mkdir $MY_REPO/gowebapp/config cp $MY_REPO/gowebapp/code/config/config.json \\ $MY_REPO/gowebapp/config Remove config folder that is located in code directory rm -rf $MY_REPO/gowebapp/code/config Result Your gowebapp folder should look like following: $ ls gowebapp code config Dockerfile Step 2: Modify app to support setting DB password through environment variable Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor of your choice (vim, VS code) to modify: vim ~/$MY_REPO/gowebapp/code/vendor/app/shared/database/database.go Note To see the line number in vim you can enable them by running :set number Add an import for the \"os\" package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") } 2.2 Build new Docker image for your frontend application \u00b6 Step 1: Update Dockerfile for your gowebapp frontend application: cd ~/$MY_REPO/gowebapp FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go #TODO --- add an environment variable declaration for a default DB_PASSWORD of \"rootpasswd\" #https://docs.docker.com/engine/reference/builder/#env COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- add a volume declaration for the container configuration path we # want to mount at runtime from the host file system: # $GOPATH/src/gowebapp/config #https://docs.docker.com/engine/reference/builder/#volume ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd ~/$MY_REPO/gowebapp Build and push the gowebapp image to GCR. Make sure to include \u201c.\u201c at the end of build command. docker build -t gcr.io/${PROJECT_ID}/gowebapp:v3 . docker push gcr.io/${PROJECT_ID}/gowebapp:v3 2.3 Run and test new Docker image locally \u00b6 Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker network create gowebapp -d bridge docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 8080:80 \\ -v ~/$MY_REPO/gowebapp/config:/go/src/gowebapp/config \\ --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v3 Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes Step 5 Cleanup environment docker rm -f $(docker ps -q) docker network rm gowebapp 2.4 Update DockerCompose file to support changes \u00b6 Step 1 Edit docker-compose file cd ~/$MY_REPO/ vim docker-compose.yml #TODO Define gowebapp configuration as volume #Ref: https://docs.docker.com/compose/compose-file/compose-file-v2/#volumes #Path Hint: ./gowebapp/config:/go/src/gowebapp/config Step 3 Test application export CLOUDSDK_PYTHON=/usr/bin/python docker-compose up -d Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Tear down environment docker-compose down 3.1 Create a Namespace dev \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 3.2 Create a Secret \u00b6 Step 1 Base64 Encode MySQL password rootpasswd . See Lab 8 for more details. Step 2 Edit a secret for the MySQL password in dev namespaces. cd ~/$MY_REPO/deploy_a4 vim secret-mysql.yaml kind: Secret apiVersion: v1 # TODO1 Create secret name: mysql # TODO2 Secret should be using arbitrary user defined type stringData: https://kubernetes.io/docs/concepts/configuration/secret/#secret-types # TODO3 Define Mysql Password in base64 encoded format kubectl apply -f secret-mysql.yaml kubectl describe secret mysql Step 2 Update gowebapp-mysql-deployment.yaml under ~/$MY_REPO/deploy_a4 vim ~/$MY_REPO/deploy_a4/gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: #TODO: replace value: rootpasswd with secretKeyRef #TODO: name is mysql #TODO: key is password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 #TODO add a livenessProbe which performs tcpSocket probe #aginst port 3306 with an initial # deay of 30 seconds, and a timeout of 2 seconds #TODO add a readinessProbe for tcpSocket port 3306 with a 25 second #initial delay, and a timeout of 2 seconds Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Check if pods are running kubectl get pods Step 5 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 6 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\" 3.3 Create ConfigMap and Probes for gowebapp \u00b6 Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json 3.4 Deploy webapp by Referencing Secret, ConfigMap and define Probes \u00b6 Step 1: Update gowebapp-deployment.yaml under ~/$MY_REPO/deploy_a4/ cd ~/$MY_REPO/deploy_a4/ vim gowebapp-deployment.yaml In this exercise, we will add liveness/readiness probes to our deployments. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: name: DB_PASSWORD #TODO: replace value: `rootpasswd` with valueFrom: secretKeyRef: #TODO: name is mysql #TODO: key is password image: gcr.io/${PROJECT_ID}/gowebapp:v3 name: gowebapp ports: - containerPort: 80 livenessProbe: #TODO add a livenessProbe which performs httpGet #aginst the /register endpoint on port 80 with an initial # deay of 15 seconds, and a timeout of 5 seconds #TODO add a livenessProbe which performs httpGet # aginst the /register endpoint on port 80 with an initial # deay of 25 seconds, and a timeout of 5 seconds volumeMounts: - #TODO: give the volume a name:config-volume #TODO: specify the mountPath: /go/src/gowebapp/config volumes: - #TODO: define volume name: config-volume configMap: #TODO: identify your ConfigMap name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6 Check if pods are running kubectl get pods Step 7 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 8 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes 3.5 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 4\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"Assignment4"},{"location":"ass4/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Startup Probes Liveness Probes Readiness Probes Secrets ConfigMaps Externalize Web Application Configuration","title":"Deploy Applications on Kubernetes"},{"location":"ass4/#11-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c","title":"1.1 Create GKE Cluster"},{"location":"ass4/#12-locate-assignment-4","text":"Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment4/ ls Result You can see 4 Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 4 deploy_a4 folder to your repo: cp -r ~/ycit019/Assignment4/deploy_a4 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 4\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"1.2 Locate Assignment 4"},{"location":"ass4/#21-externalize-web-application-configuration","text":"Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~ mkdir $MY_REPO/gowebapp/config cp $MY_REPO/gowebapp/code/config/config.json \\ $MY_REPO/gowebapp/config Remove config folder that is located in code directory rm -rf $MY_REPO/gowebapp/code/config Result Your gowebapp folder should look like following: $ ls gowebapp code config Dockerfile Step 2: Modify app to support setting DB password through environment variable Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor of your choice (vim, VS code) to modify: vim ~/$MY_REPO/gowebapp/code/vendor/app/shared/database/database.go Note To see the line number in vim you can enable them by running :set number Add an import for the \"os\" package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") }","title":"2.1 Externalize Web Application Configuration"},{"location":"ass4/#22-build-new-docker-image-for-your-frontend-application","text":"Step 1: Update Dockerfile for your gowebapp frontend application: cd ~/$MY_REPO/gowebapp FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go #TODO --- add an environment variable declaration for a default DB_PASSWORD of \"rootpasswd\" #https://docs.docker.com/engine/reference/builder/#env COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- add a volume declaration for the container configuration path we # want to mount at runtime from the host file system: # $GOPATH/src/gowebapp/config #https://docs.docker.com/engine/reference/builder/#volume ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd ~/$MY_REPO/gowebapp Build and push the gowebapp image to GCR. Make sure to include \u201c.\u201c at the end of build command. docker build -t gcr.io/${PROJECT_ID}/gowebapp:v3 . docker push gcr.io/${PROJECT_ID}/gowebapp:v3","title":"2.2 Build new Docker image for your frontend application"},{"location":"ass4/#23-run-and-test-new-docker-image-locally","text":"Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker network create gowebapp -d bridge docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 8080:80 \\ -v ~/$MY_REPO/gowebapp/config:/go/src/gowebapp/config \\ --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v3 Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes Step 5 Cleanup environment docker rm -f $(docker ps -q) docker network rm gowebapp","title":"2.3 Run and test new Docker image locally"},{"location":"ass4/#24-update-dockercompose-file-to-support-changes","text":"Step 1 Edit docker-compose file cd ~/$MY_REPO/ vim docker-compose.yml #TODO Define gowebapp configuration as volume #Ref: https://docs.docker.com/compose/compose-file/compose-file-v2/#volumes #Path Hint: ./gowebapp/config:/go/src/gowebapp/config Step 3 Test application export CLOUDSDK_PYTHON=/usr/bin/python docker-compose up -d Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Tear down environment docker-compose down","title":"2.4 Update DockerCompose file to support changes"},{"location":"ass4/#31-create-a-namespace-dev","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"3.1 Create a Namespace dev"},{"location":"ass4/#32-create-a-secret","text":"Step 1 Base64 Encode MySQL password rootpasswd . See Lab 8 for more details. Step 2 Edit a secret for the MySQL password in dev namespaces. cd ~/$MY_REPO/deploy_a4 vim secret-mysql.yaml kind: Secret apiVersion: v1 # TODO1 Create secret name: mysql # TODO2 Secret should be using arbitrary user defined type stringData: https://kubernetes.io/docs/concepts/configuration/secret/#secret-types # TODO3 Define Mysql Password in base64 encoded format kubectl apply -f secret-mysql.yaml kubectl describe secret mysql Step 2 Update gowebapp-mysql-deployment.yaml under ~/$MY_REPO/deploy_a4 vim ~/$MY_REPO/deploy_a4/gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: #TODO: replace value: rootpasswd with secretKeyRef #TODO: name is mysql #TODO: key is password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 #TODO add a livenessProbe which performs tcpSocket probe #aginst port 3306 with an initial # deay of 30 seconds, and a timeout of 2 seconds #TODO add a readinessProbe for tcpSocket port 3306 with a 25 second #initial delay, and a timeout of 2 seconds Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Check if pods are running kubectl get pods Step 5 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 6 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\"","title":"3.2 Create a Secret"},{"location":"ass4/#33-create-configmap-and-probes-for-gowebapp","text":"Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json","title":"3.3 Create ConfigMap and Probes for gowebapp"},{"location":"ass4/#34-deploy-webapp-by-referencing-secret-configmap-and-define-probes","text":"Step 1: Update gowebapp-deployment.yaml under ~/$MY_REPO/deploy_a4/ cd ~/$MY_REPO/deploy_a4/ vim gowebapp-deployment.yaml In this exercise, we will add liveness/readiness probes to our deployments. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: name: DB_PASSWORD #TODO: replace value: `rootpasswd` with valueFrom: secretKeyRef: #TODO: name is mysql #TODO: key is password image: gcr.io/${PROJECT_ID}/gowebapp:v3 name: gowebapp ports: - containerPort: 80 livenessProbe: #TODO add a livenessProbe which performs httpGet #aginst the /register endpoint on port 80 with an initial # deay of 15 seconds, and a timeout of 5 seconds #TODO add a livenessProbe which performs httpGet # aginst the /register endpoint on port 80 with an initial # deay of 25 seconds, and a timeout of 5 seconds volumeMounts: - #TODO: give the volume a name:config-volume #TODO: specify the mountPath: /go/src/gowebapp/config volumes: - #TODO: define volume name: config-volume configMap: #TODO: identify your ConfigMap name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6 Check if pods are running kubectl get pods Step 7 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 8 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes","title":"3.4 Deploy webapp by Referencing Secret, ConfigMap and define Probes"},{"location":"ass4/#35-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 4\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"3.5 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ass4/#36-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"3.6 Cleaning Up"},{"location":"ass4_sol/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Startup Probes Liveness Probes Readiness Probes Secrets ConfigMaps Externalize Web Application Configuration 1.1 Create GKE Cluster \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c 1.2 Locate Assignment 4 \u00b6 Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment4/ ls Result You can see 4 Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 4 deploy_a4 folder to your repo: cp -r ~/ycit019/Assignment4/deploy_a4 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 4\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 2.1 Externalize Web Application Configuration \u00b6 Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~ mkdir $MY_REPO/gowebapp/config cp $MY_REPO/gowebapp/code/config/config.json \\ $MY_REPO/gowebapp/config Remove config folder that is located in code directory rm -rf $MY_REPO/gowebapp/code/config Result Your gowebapp folder should look like following: $ ls gowebapp code config Dockerfile Step 2: Modify app to support setting DB password through environment variable Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor of your choice (vim, VS code) to modify: vim ~/$MY_REPO/gowebapp/code/vendor/app/shared/database/database.go Note To see the line number in vim you can enable them by running :set number Add an import for the \"os\" package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") } 2.2 Build new Docker image for your frontend application \u00b6 Step 1: Update Dockerfile for your gowebapp frontend application: cd ~/$MY_REPO/gowebapp FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go ENV PASSWORD=rootpasswd COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install VOLUME $GOPATH/src/gowebapp/config ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd ~/$MY_REPO/gowebapp Build and push the gowebapp image to GCR. Make sure to include \u201c.\u201c at the end of build command. docker build -t gcr.io/${PROJECT_ID}/gowebapp:v3 . docker push gcr.io/${PROJECT_ID}/gowebapp:v3 2.3 Run and test new Docker image locally \u00b6 Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker network create gowebapp -d bridge docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 8080:80 \\ -v ~/$MY_REPO/gowebapp/config:/go/src/gowebapp/config \\ --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v3 Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes Step 5 Cleanup environment docker rm -f $(docker ps -q) docker network rm gowebapp 2.4 Update DockerCompose file to support changes \u00b6 Step 1 Edit docker-compose file cd ~/$MY_REPO/ vim docker-compose.yml version: '2.4' services: gowebapp-mysql: container_name: gowebapp-mysql build: ./gowebapp-mysql environment: - MYSQL_ROOT_PASSWORD=rootpasswd healthcheck: test: [ \"CMD\", \"mysqladmin\" ,\"ping\", \"-h\", \"localhost\" ] interval: 30s timeout: 5s retries: 3 networks: - gowebapp gowebapp: container_name: gowebapp build: ./gowebapp ports: - 8080:80 depends_on: gowebapp-mysql: condition: service_healthy volumes: - ./gowebapp/config:/go/src/gowebapp/config networks: - gowebapp networks: gowebapp1: driver: bridge Step 3 Test application export CLOUDSDK_PYTHON=/usr/bin/python docker-compose up -d Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Tear down environment docker-compose down 3.1 Create a Namespace dev \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 3.2 Create a Secret \u00b6 Step 1 Base64 Encode MySQL password rootpasswd . See Lab 8 for more details. Step 2 Edit a secret for the MySQL password in dev namespaces. cd ~/$MY_REPO/deploy_a4 vim secret-mysql.yaml kind: Secret apiVersion: v1 metadata: name: mysql type: Opaque data: password: cm9vdHBhc3N3ZA== kubectl apply -f secret-mysql.yaml kubectl describe secret mysql Step 2 Update gowebapp-mysql-deployment.yaml under ~/$MY_REPO/deploy_a4 vim ~/$MY_REPO/deploy_a4/gowebapp-mysql-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: replicas: 1 strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Check if pods are running kubectl get pods Step 5 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 6 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\" 3.3 Create ConfigMap and Probes for gowebapp \u00b6 Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json 3.4 Deploy webapp by Referencing Secret, ConfigMap and define Probes \u00b6 Step 1: Update gowebapp-deployment.yaml under ~/$MY_REPO/deploy_a4/ cd ~/$MY_REPO/deploy_a4/ vim gowebapp-deployment.yaml In this exercise, we will add liveness/readiness probes to our deployments. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6 Check if pods are running kubectl get pods Step 7 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 8 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes 3.5 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 4\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"Assignment4 - Solution"},{"location":"ass4_sol/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Startup Probes Liveness Probes Readiness Probes Secrets ConfigMaps Externalize Web Application Configuration","title":"Deploy Applications on Kubernetes"},{"location":"ass4_sol/#11-create-gke-cluster","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c","title":"1.1 Create GKE Cluster"},{"location":"ass4_sol/#12-locate-assignment-4","text":"Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement3 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment4/ ls Result You can see 4 Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 4 deploy_a4 folder to your repo: cp -r ~/ycit019/Assignment4/deploy_a4 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 4\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"1.2 Locate Assignment 4"},{"location":"ass4_sol/#21-externalize-web-application-configuration","text":"Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~ mkdir $MY_REPO/gowebapp/config cp $MY_REPO/gowebapp/code/config/config.json \\ $MY_REPO/gowebapp/config Remove config folder that is located in code directory rm -rf $MY_REPO/gowebapp/code/config Result Your gowebapp folder should look like following: $ ls gowebapp code config Dockerfile Step 2: Modify app to support setting DB password through environment variable Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor of your choice (vim, VS code) to modify: vim ~/$MY_REPO/gowebapp/code/vendor/app/shared/database/database.go Note To see the line number in vim you can enable them by running :set number Add an import for the \"os\" package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") }","title":"2.1 Externalize Web Application Configuration"},{"location":"ass4_sol/#22-build-new-docker-image-for-your-frontend-application","text":"Step 1: Update Dockerfile for your gowebapp frontend application: cd ~/$MY_REPO/gowebapp FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go ENV PASSWORD=rootpasswd COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install VOLUME $GOPATH/src/gowebapp/config ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd ~/$MY_REPO/gowebapp Build and push the gowebapp image to GCR. Make sure to include \u201c.\u201c at the end of build command. docker build -t gcr.io/${PROJECT_ID}/gowebapp:v3 . docker push gcr.io/${PROJECT_ID}/gowebapp:v3","title":"2.2 Build new Docker image for your frontend application"},{"location":"ass4_sol/#23-run-and-test-new-docker-image-locally","text":"Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker network create gowebapp -d bridge docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 8080:80 \\ -v ~/$MY_REPO/gowebapp/config:/go/src/gowebapp/config \\ --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v3 Step 3 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 4 Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes Step 5 Cleanup environment docker rm -f $(docker ps -q) docker network rm gowebapp","title":"2.3 Run and test new Docker image locally"},{"location":"ass4_sol/#24-update-dockercompose-file-to-support-changes","text":"Step 1 Edit docker-compose file cd ~/$MY_REPO/ vim docker-compose.yml version: '2.4' services: gowebapp-mysql: container_name: gowebapp-mysql build: ./gowebapp-mysql environment: - MYSQL_ROOT_PASSWORD=rootpasswd healthcheck: test: [ \"CMD\", \"mysqladmin\" ,\"ping\", \"-h\", \"localhost\" ] interval: 30s timeout: 5s retries: 3 networks: - gowebapp gowebapp: container_name: gowebapp build: ./gowebapp ports: - 8080:80 depends_on: gowebapp-mysql: condition: service_healthy volumes: - ./gowebapp/config:/go/src/gowebapp/config networks: - gowebapp networks: gowebapp1: driver: bridge Step 3 Test application export CLOUDSDK_PYTHON=/usr/bin/python docker-compose up -d Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Tear down environment docker-compose down","title":"2.4 Update DockerCompose file to support changes"},{"location":"ass4_sol/#31-create-a-namespace-dev","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"3.1 Create a Namespace dev"},{"location":"ass4_sol/#32-create-a-secret","text":"Step 1 Base64 Encode MySQL password rootpasswd . See Lab 8 for more details. Step 2 Edit a secret for the MySQL password in dev namespaces. cd ~/$MY_REPO/deploy_a4 vim secret-mysql.yaml kind: Secret apiVersion: v1 metadata: name: mysql type: Opaque data: password: cm9vdHBhc3N3ZA== kubectl apply -f secret-mysql.yaml kubectl describe secret mysql Step 2 Update gowebapp-mysql-deployment.yaml under ~/$MY_REPO/deploy_a4 vim ~/$MY_REPO/deploy_a4/gowebapp-mysql-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: replicas: 1 strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Check if pods are running kubectl get pods Step 5 Create a Service object for MySQL kubectl apply -f gowebapp-mysql-service.yaml --record Step 6 Check to make sure it worked kubectl get service -l \"run=gowebapp-mysql\"","title":"3.2 Create a Secret"},{"location":"ass4_sol/#33-create-configmap-and-probes-for-gowebapp","text":"Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json","title":"3.3 Create ConfigMap and Probes for gowebapp"},{"location":"ass4_sol/#34-deploy-webapp-by-referencing-secret-configmap-and-define-probes","text":"Step 1: Update gowebapp-deployment.yaml under ~/$MY_REPO/deploy_a4/ cd ~/$MY_REPO/deploy_a4/ vim gowebapp-deployment.yaml In this exercise, we will add liveness/readiness probes to our deployments. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6 Check if pods are running kubectl get pods Step 7 Create a Service object for gowebapp kubectl apply -f gowebapp-service.yaml --record Step 8 Access your application on Public IP via automatically created Loadbalancer created for gowebapp service. To get the value of Loadbalancer run following command: kubectl get svc gowebapp -o wide Expected output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gowebapp Loadbalancer 10.107.15.39 XXXXXX 9000:32634/TCP 30m gowebapp-mysql ClusterIP None <none> 3306/TCP 1h Step Access Loadbalancer IP via browser: Result Congrats!!! You've deployed you code to Kubernetes","title":"3.4 Deploy webapp by Referencing Secret, ConfigMap and define Probes"},{"location":"ass4_sol/#35-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 4\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"3.5 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ass4_sol/#36-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts","title":"3.6 Cleaning Up"},{"location":"ass4_solution/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Secrets ConfigMaps Persistent Volumes (PV) Persisten Volume Claims (PVC) Jobs CronJobs HPA Externalize Web Application Configuration 3.1 Externalize Web Application Configuration \u00b6 Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~/k8scanada/ git pull mkdir $HOME/k8scanada/Assignment3/gowebapp/config cp $HOME/k8scanada/Assignment3/gowebapp/code/config/config.json \\ $HOME/k8scanada/Assignment3/gowebapp/config rm -rf $HOME/k8scanada/Assignment3/gowebapp/code/config/ Step 2: Modify app to support setting DB password through environment variable. Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor to modify: vim $HOME/k8scanada/Assignment3/gowebapp/code/vendor/app/shared/database/database.go Add an import for the os package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") } 3.2 Build new Docker image for your frontend application \u00b6 Step 1: Update Dockerfile for your frontend application FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go ENV PASSWORD=rootpasswd COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install VOLUME $GOPATH/src/gowebapp/config ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd $HOME/k8scanada/Assignment3/gowebapp/ Build the gowebapp image locally. Make sure to include \u201c.\u201c at the end. Notice the new version label. docker build -t gowebapp:v2 . 3.3 Run and test new Docker image locally \u00b6 Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=cloudops user-name/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 30005:80 \\ -v $HOME/k8scanada/Assignment3/gowebapp/config:/go/src/gowebapp/config \\ -e DB_PASSWORD=cloudops --net gowebapp -d --name gowebapp \\ --hostname gowebapp user-name/gowebapp:v2 Now that we\u2019ve launched the application containers, let\u2019s try to test the web application locally. You should be able to access the application at http://Public_IP:30005. Step 3: Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes 3.4 Publish New Image \u00b6 Step 1 We built the second version of gowebapp from the last exercise and tested it locally. Now we can tag and push gowebapp:v2 to private repository. docker tag gowebapp:v2 <user-name>/gowebapp:v2 docker push <user-name>/gowebapp:v2 3.5 Create a Secret \u00b6 Step 1 Create a secret for the MySQL password kind: Secret apiVersion: v1 metadata: name: mysql type: Opaque data: password: cm9vdHBhc3N3ZA== 3.6 Create a Volume for Mysql \u00b6 Step 1: Dynamically provisioning Persistent Volume Define a Persistent Volume Claim object to use with MySQL in a file named pvc.yaml under $HOME/k8scanada/Assignment3/deploy . In this case, we are not explicitly defining a Persistent Volume (pv) object for this PVC to use. This approach is more portable than explicitly defining and hard-wiring volume types. Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims cd $HOME/k8scanada/Assignment3/deploy vim pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysqlpvc labels: run: gowebapp-mysql spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi Step 2: Create PVC object kubectl apply -f pvc.yaml --record Step 3: View your PVC metadata kubectl get pvc mysqlpvc kubectl describe pv 3.6 Create Mysql deployment \u00b6 Create Mysql deployment using Secrets, liveness, readiness, resources, limits and Persistent Volume. Step 1 Update gowebapp-mysql-deployment.yaml under $HOME/k8scanada/Assignment3/deploy vim $HOME/k8scanada/Assignment3/deploy/gowebapp-mysql-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: replicas: 1 strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: archyufa/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 256m memory: 256Mi limits: cpu: 512m memory: 512Mi volumeMounts: - mountPath: /var/lib/mysql name: mysql volumes: - name: mysql persistentVolumeClaim: claimName: mysqlpvc Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 5 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 6 Get rollout history details for specific revision (use number show in output to previous command) Notice the value for MYSQL_ROOT_PASSWORD kubectl rollout history deploy gowebapp-mysql \\ --revision=<latest_version_number> 3.5 Create ConfigMap and Resources and Probes to MySQL \u00b6 Step 1: create ConfigMap for gowebapp's config.json file kubectl create configmap gowebapp --from-file=webapp-config-json=/home/cca-user/k8scanada/Assignment3/gowebapp/config/config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json Step 2: Update gowebapp-deployment.yaml under /home/cca-user/k8scanada/Assignment3/deploy In this exercise, we will add liveness/readiness probes to our deployments, as well as resource limits. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: <#TODO_user-name>/gowebapp:v1 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: request: cpu: 200m memory: 128Mi limits: cpu: 250m memory: 256Mi volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6: Access your application: <http://VM_Public_IP:NodePort. Register for an account, login and use the Notepad. If you need to lookup the VM_Public_IP for your environment, use the following command: kubectl get svc gowebapp -o wide ## 3.6 Define a Job to purge data from the web application database Create a Job object definition in a file called reset-db.yaml which connects to the MySQL database and deletes all the data from the note and user tables. Replace the TODOs below with the missing elements of the definition. If you need more help, refer to: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion Step 1: Create job reset-db.yaml under /home/cca-user/k8scanada/Assignment3/deploy vim reset-db.yaml apVersion: batch/v1 kind: Job metadata: name: reset-db labels: run: reset-db spec: activeDeadlineSeconds: 10 template: metadata: name: reset-db spec: restartPolicy: OnFailure containers: - name: database-cleaner image: mysql:5.6 env: - name: DB_USER value: root - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password - name: DB_HOST value: gowebapp-mysql - name: DB value: gowebapp command: - /bin/sh args: - -c - mysql -h $(DB_HOST) -u $(DB_USER) -p$(DB_PASSWORD) $(DB) -e 'delete from note; delete from user;' Step 2: Create the Job in Kubernetes kubectl apply -f reset-db.yaml --record Step 3: View Job metadata and status The job should run and complete immediately. kubectl get job reset-db kubectl describe job reset-db Step 4: Verify that data has been deleted Open the web application at http://Public_IP:NodePort and verify that the accounts and notes you created above no longer exist.","title":"Deploy Applications on Kubernetes"},{"location":"ass4_solution/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Secrets ConfigMaps Persistent Volumes (PV) Persisten Volume Claims (PVC) Jobs CronJobs HPA Externalize Web Application Configuration","title":"Deploy Applications on Kubernetes"},{"location":"ass4_solution/#31-externalize-web-application-configuration","text":"Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~/k8scanada/ git pull mkdir $HOME/k8scanada/Assignment3/gowebapp/config cp $HOME/k8scanada/Assignment3/gowebapp/code/config/config.json \\ $HOME/k8scanada/Assignment3/gowebapp/config rm -rf $HOME/k8scanada/Assignment3/gowebapp/code/config/ Step 2: Modify app to support setting DB password through environment variable. Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor to modify: vim $HOME/k8scanada/Assignment3/gowebapp/code/vendor/app/shared/database/database.go Add an import for the os package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") }","title":"3.1 Externalize Web Application Configuration"},{"location":"ass4_solution/#32-build-new-docker-image-for-your-frontend-application","text":"Step 1: Update Dockerfile for your frontend application FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go ENV PASSWORD=rootpasswd COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install VOLUME $GOPATH/src/gowebapp/config ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd $HOME/k8scanada/Assignment3/gowebapp/ Build the gowebapp image locally. Make sure to include \u201c.\u201c at the end. Notice the new version label. docker build -t gowebapp:v2 .","title":"3.2 Build new Docker image for your frontend application"},{"location":"ass4_solution/#33-run-and-test-new-docker-image-locally","text":"Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=cloudops user-name/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 30005:80 \\ -v $HOME/k8scanada/Assignment3/gowebapp/config:/go/src/gowebapp/config \\ -e DB_PASSWORD=cloudops --net gowebapp -d --name gowebapp \\ --hostname gowebapp user-name/gowebapp:v2 Now that we\u2019ve launched the application containers, let\u2019s try to test the web application locally. You should be able to access the application at http://Public_IP:30005. Step 3: Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes","title":"3.3 Run and test new Docker image locally"},{"location":"ass4_solution/#34-publish-new-image","text":"Step 1 We built the second version of gowebapp from the last exercise and tested it locally. Now we can tag and push gowebapp:v2 to private repository. docker tag gowebapp:v2 <user-name>/gowebapp:v2 docker push <user-name>/gowebapp:v2","title":"3.4 Publish New Image"},{"location":"ass4_solution/#35-create-a-secret","text":"Step 1 Create a secret for the MySQL password kind: Secret apiVersion: v1 metadata: name: mysql type: Opaque data: password: cm9vdHBhc3N3ZA==","title":"3.5 Create a Secret"},{"location":"ass4_solution/#36-create-a-volume-for-mysql","text":"Step 1: Dynamically provisioning Persistent Volume Define a Persistent Volume Claim object to use with MySQL in a file named pvc.yaml under $HOME/k8scanada/Assignment3/deploy . In this case, we are not explicitly defining a Persistent Volume (pv) object for this PVC to use. This approach is more portable than explicitly defining and hard-wiring volume types. Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims cd $HOME/k8scanada/Assignment3/deploy vim pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysqlpvc labels: run: gowebapp-mysql spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi Step 2: Create PVC object kubectl apply -f pvc.yaml --record Step 3: View your PVC metadata kubectl get pvc mysqlpvc kubectl describe pv","title":"3.6 Create a Volume for Mysql"},{"location":"ass4_solution/#36-create-mysql-deployment","text":"Create Mysql deployment using Secrets, liveness, readiness, resources, limits and Persistent Volume. Step 1 Update gowebapp-mysql-deployment.yaml under $HOME/k8scanada/Assignment3/deploy vim $HOME/k8scanada/Assignment3/deploy/gowebapp-mysql-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: replicas: 1 strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: archyufa/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 256m memory: 256Mi limits: cpu: 512m memory: 512Mi volumeMounts: - mountPath: /var/lib/mysql name: mysql volumes: - name: mysql persistentVolumeClaim: claimName: mysqlpvc Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 5 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 6 Get rollout history details for specific revision (use number show in output to previous command) Notice the value for MYSQL_ROOT_PASSWORD kubectl rollout history deploy gowebapp-mysql \\ --revision=<latest_version_number>","title":"3.6 Create Mysql deployment"},{"location":"ass4_solution/#35-create-configmap-and-resources-and-probes-to-mysql","text":"Step 1: create ConfigMap for gowebapp's config.json file kubectl create configmap gowebapp --from-file=webapp-config-json=/home/cca-user/k8scanada/Assignment3/gowebapp/config/config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json Step 2: Update gowebapp-deployment.yaml under /home/cca-user/k8scanada/Assignment3/deploy In this exercise, we will add liveness/readiness probes to our deployments, as well as resource limits. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: <#TODO_user-name>/gowebapp:v1 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: request: cpu: 200m memory: 128Mi limits: cpu: 250m memory: 256Mi volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6: Access your application: <http://VM_Public_IP:NodePort. Register for an account, login and use the Notepad. If you need to lookup the VM_Public_IP for your environment, use the following command: kubectl get svc gowebapp -o wide ## 3.6 Define a Job to purge data from the web application database Create a Job object definition in a file called reset-db.yaml which connects to the MySQL database and deletes all the data from the note and user tables. Replace the TODOs below with the missing elements of the definition. If you need more help, refer to: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion Step 1: Create job reset-db.yaml under /home/cca-user/k8scanada/Assignment3/deploy vim reset-db.yaml apVersion: batch/v1 kind: Job metadata: name: reset-db labels: run: reset-db spec: activeDeadlineSeconds: 10 template: metadata: name: reset-db spec: restartPolicy: OnFailure containers: - name: database-cleaner image: mysql:5.6 env: - name: DB_USER value: root - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password - name: DB_HOST value: gowebapp-mysql - name: DB value: gowebapp command: - /bin/sh args: - -c - mysql -h $(DB_HOST) -u $(DB_USER) -p$(DB_PASSWORD) $(DB) -e 'delete from note; delete from user;' Step 2: Create the Job in Kubernetes kubectl apply -f reset-db.yaml --record Step 3: View Job metadata and status The job should run and complete immediately. kubectl get job reset-db kubectl describe job reset-db Step 4: Verify that data has been deleted Open the web application at http://Public_IP:NodePort and verify that the accounts and notes you created above no longer exist.","title":"3.5 Create ConfigMap and Resources and Probes to MySQL"},{"location":"ass5/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Resource Limits HPA VPA Limit Range 0 Create GKE Cluster with Cluster and Vertical Autoscaling Support \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-scaling \\ --zone us-central1-c \\ --enable-vertical-pod-autoscaling \\ --num-nodes 2 \\ --enable-autoscaling --min-nodes 1 --max-nodes 3 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-scaling --zone us-central1-c 1.2 Locate Assignment 5 \u00b6 Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement5 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment5/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 5 deploy_a5 folder to your repo: cp -r ~/ycit019/Assignment5/deploy_a5 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 5\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 1.3 Create a Namespace dev \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 2.1 Configure VPA for gowebapp-mysql to find optimal Resource Request and Limits values \u00b6 requests and limits is the way Kubernetes set's QoS for Pods, as well as enable's features like HPA, CA, Resource Quota's and more. However setting best values for resource requests and limits is hard, VPA is here to help. Set VPA for gowebapp and observe usage recommendation for requests and limits Step 1 Deploy gowebapp-mysql app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 2 Edit a manifest for gowebapp-mysql Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp-mysql #TODO: Configure VPA with updateMode:OFF Step 3 Apply the manifest for gowebapp-mysql-vpa kubectl apply -f gowebapp-mysql-vpa.yaml Step 4 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp-mysql Note If you don't see it, wait a little longer and try the previous command again. Step 5 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value. 2.2 Set Recommended Request and Limits values to gowebapp-mysql \u00b6 Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 #TODO define a resource request and limits based on VPA Recommender Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-mysql-deployment.yaml kubectl apply -f gowebapp-mysql-deployment.yaml 2.3 Configure VPA for gowebapp to find optimal Resource Request and Limits values \u00b6 Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 3 Edit a manifest for gowebapp Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp #TODO: Ref: https://cloud.google.com/kubernetes-engine/docs/how-to/vertical-pod-autoscaling #TODO: Configure VPA with updateMode:OFF Step 4 Apply the manifest for gowebapp-vpa kubectl apply -f gowebapp-vpa.yaml Step 5 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp Note If you don't see it, wait a little longer and try the previous command again. Step 6 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value. 2.4 Set Recommended Request and Limits values to gowebapp \u00b6 Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp:v3 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: #TODO define a resource request and limits based on VPA Recommender volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-deployment.yaml kubectl apply -f gowebapp-deployment.yaml 2.5 Configure HPA for gowebapp \u00b6 Our NotePad Application is going to Production soon. To make sure our application can scale based on requests we will set HPA for our deployment resource using Horizontal Pod Autoscaler. Step 1 Create HPA for gowebapp based on CPU with minReplicas 1 and maxReplicas 5 with target 50. cd ~/$MY_REPO/deploy_a5 vim gowebapp-hpa.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: gowebapp-hpa spec: scaleTargetRef: #TODO: Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/ #TODO Create HPA based on CPU with minReplicas `1` and maxReplicas `5` with target 50. Step 2 Apply the manifest for gowebapp-hpa kubectl apply -f gowebapp-hpa.yaml Step 3 Take a closer look at the HPA and observe autoscaling or downscaling if any. kubectl describe hpa kubia Note It will take some time to collect metrics information about current cpu usage and since our does't have real load it might not trigger any scaling 2.6 Configure LimitRanges for Namespace dev \u00b6 In order to prevent developers accidentally forget to set values for request and limits . Ops team decide to create a Configuration LimitRange , that will enforce some default values for request and limits if they have not been set, as well as Minimum and maximum requests/limits a container can have to prevent resources abuse. Step 1 Edit a manifest for limit-range that can be used for dev namespace with following requirements: cd ~/$MY_REPO/deploy_a5/ vim limit-range.yaml apiVersion: v1 kind: LimitRange metadata: name: gowebapp-system #TODO Specify Namespace spec: #TODO Ref: https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/ #TODO If not specified the Container's `memory` limit is set to 256Mi, which is the default memory limit for the namespace. #TODO If not specified default limit `cpu` is 250m per container #TODO If not specified set the Container's Default memory requests to 256Mi #TODO If not specified set the Container's Default cpu requests to 250m #TODO Configure Minimum `20m` and Maximum `800m` CPU Constraints for a Namespace 2.7 Viewing cluster autoscaler events \u00b6 Our cluster was configured to use Cluster Autoscaler, verify if during you assignment, cluster did went through autoscaling process? To view the logs, perform the following: Step 1: In the Cloud Console, go to the Logs Viewer page. Step 2: Search for the logs using the basic or advanced query interface. To search for logs using the basic query interface, perform the following: a. From the resources drop-down list, select Kubernetes Cluster, then select the location of your cluster, and the name of your cluster. b. From the logs type drop-down list, select container.googleapis.com/cluster-autoscaler-visibility. c. From the time-range drop-down list, select the desired time range. OR search for logs using the advanced query interface, apply the following advanced filter: resource.type=\"k8s_cluster\" resource.labels.location=\"cluster-location\" resource.labels.cluster_name=\"cluster-name\" logName=\"projects/project-id/logs/container.googleapis.com%2Fcluster-autoscaler-visibility\" Reference link 2.7 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 5\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"Assignment5"},{"location":"ass5/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Resource Limits HPA VPA Limit Range","title":"Deploy Applications on Kubernetes"},{"location":"ass5/#0-create-gke-cluster-with-cluster-and-vertical-autoscaling-support","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-scaling \\ --zone us-central1-c \\ --enable-vertical-pod-autoscaling \\ --num-nodes 2 \\ --enable-autoscaling --min-nodes 1 --max-nodes 3 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-scaling --zone us-central1-c","title":"0 Create GKE Cluster with Cluster and Vertical Autoscaling Support"},{"location":"ass5/#12-locate-assignment-5","text":"Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement5 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment5/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 5 deploy_a5 folder to your repo: cp -r ~/ycit019/Assignment5/deploy_a5 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 5\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"1.2 Locate Assignment 5"},{"location":"ass5/#13-create-a-namespace-dev","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"1.3 Create a Namespace dev"},{"location":"ass5/#21-configure-vpa-for-gowebapp-mysql-to-find-optimal-resource-request-and-limits-values","text":"requests and limits is the way Kubernetes set's QoS for Pods, as well as enable's features like HPA, CA, Resource Quota's and more. However setting best values for resource requests and limits is hard, VPA is here to help. Set VPA for gowebapp and observe usage recommendation for requests and limits Step 1 Deploy gowebapp-mysql app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 2 Edit a manifest for gowebapp-mysql Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp-mysql #TODO: Configure VPA with updateMode:OFF Step 3 Apply the manifest for gowebapp-mysql-vpa kubectl apply -f gowebapp-mysql-vpa.yaml Step 4 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp-mysql Note If you don't see it, wait a little longer and try the previous command again. Step 5 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value.","title":"2.1 Configure VPA for gowebapp-mysql to find optimal Resource Request and Limits values"},{"location":"ass5/#22-set-recommended-request-and-limits-values-to-gowebapp-mysql","text":"Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 #TODO define a resource request and limits based on VPA Recommender Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-mysql-deployment.yaml kubectl apply -f gowebapp-mysql-deployment.yaml","title":"2.2 Set Recommended Request and Limits values to gowebapp-mysql"},{"location":"ass5/#23-configure-vpa-for-gowebapp-to-find-optimal-resource-request-and-limits-values","text":"Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 3 Edit a manifest for gowebapp Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp #TODO: Ref: https://cloud.google.com/kubernetes-engine/docs/how-to/vertical-pod-autoscaling #TODO: Configure VPA with updateMode:OFF Step 4 Apply the manifest for gowebapp-vpa kubectl apply -f gowebapp-vpa.yaml Step 5 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp Note If you don't see it, wait a little longer and try the previous command again. Step 6 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value.","title":"2.3 Configure VPA for gowebapp to find optimal Resource Request and Limits values"},{"location":"ass5/#24-set-recommended-request-and-limits-values-to-gowebapp","text":"Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp:v3 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: #TODO define a resource request and limits based on VPA Recommender volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-deployment.yaml kubectl apply -f gowebapp-deployment.yaml","title":"2.4 Set Recommended Request and Limits values to gowebapp"},{"location":"ass5/#25-configure-hpa-for-gowebapp","text":"Our NotePad Application is going to Production soon. To make sure our application can scale based on requests we will set HPA for our deployment resource using Horizontal Pod Autoscaler. Step 1 Create HPA for gowebapp based on CPU with minReplicas 1 and maxReplicas 5 with target 50. cd ~/$MY_REPO/deploy_a5 vim gowebapp-hpa.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: gowebapp-hpa spec: scaleTargetRef: #TODO: Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/ #TODO Create HPA based on CPU with minReplicas `1` and maxReplicas `5` with target 50. Step 2 Apply the manifest for gowebapp-hpa kubectl apply -f gowebapp-hpa.yaml Step 3 Take a closer look at the HPA and observe autoscaling or downscaling if any. kubectl describe hpa kubia Note It will take some time to collect metrics information about current cpu usage and since our does't have real load it might not trigger any scaling","title":"2.5 Configure HPA for gowebapp"},{"location":"ass5/#26-configure-limitranges-for-namespace-dev","text":"In order to prevent developers accidentally forget to set values for request and limits . Ops team decide to create a Configuration LimitRange , that will enforce some default values for request and limits if they have not been set, as well as Minimum and maximum requests/limits a container can have to prevent resources abuse. Step 1 Edit a manifest for limit-range that can be used for dev namespace with following requirements: cd ~/$MY_REPO/deploy_a5/ vim limit-range.yaml apiVersion: v1 kind: LimitRange metadata: name: gowebapp-system #TODO Specify Namespace spec: #TODO Ref: https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/ #TODO If not specified the Container's `memory` limit is set to 256Mi, which is the default memory limit for the namespace. #TODO If not specified default limit `cpu` is 250m per container #TODO If not specified set the Container's Default memory requests to 256Mi #TODO If not specified set the Container's Default cpu requests to 250m #TODO Configure Minimum `20m` and Maximum `800m` CPU Constraints for a Namespace","title":"2.6 Configure LimitRanges for Namespace dev"},{"location":"ass5/#27-viewing-cluster-autoscaler-events","text":"Our cluster was configured to use Cluster Autoscaler, verify if during you assignment, cluster did went through autoscaling process? To view the logs, perform the following: Step 1: In the Cloud Console, go to the Logs Viewer page. Step 2: Search for the logs using the basic or advanced query interface. To search for logs using the basic query interface, perform the following: a. From the resources drop-down list, select Kubernetes Cluster, then select the location of your cluster, and the name of your cluster. b. From the logs type drop-down list, select container.googleapis.com/cluster-autoscaler-visibility. c. From the time-range drop-down list, select the desired time range. OR search for logs using the advanced query interface, apply the following advanced filter: resource.type=\"k8s_cluster\" resource.labels.location=\"cluster-location\" resource.labels.cluster_name=\"cluster-name\" logName=\"projects/project-id/logs/container.googleapis.com%2Fcluster-autoscaler-visibility\" Reference link","title":"2.7 Viewing cluster autoscaler events"},{"location":"ass5/#27-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 5\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"2.7 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ass5/#36-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"3.6 Cleaning Up"},{"location":"ass5_solution/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Resource Limits HPA VPA Limit Range 0 Create GKE Cluster with Cluster and Vertical Autoscaling Support \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-scaling \\ --zone us-central1-c \\ --enable-vertical-pod-autoscaling \\ --num-nodes 2 \\ --enable-autoscaling --min-nodes 1 --max-nodes 3 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-scaling --zone us-central1-c 1.2 Locate Assignment 5 \u00b6 Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement5 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment5/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 5 deploy_a5 folder to your repo: cp -r ~/ycit019/Assignment5/deploy_a5 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 5\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master 1.3 Create a Namespace dev \u00b6 Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev 2.1 Configure VPA for gowebapp-mysql to find optimal Resource Request and Limits values \u00b6 requests and limits is the way Kubernetes set's QoS for Pods, as well as enable's features like HPA, CA, Resource Quota's and more. However setting best values for resource requests and limits is hard, VPA is here to help. Set VPA for gowebapp and observe usage recommendation for requests and limits Step 1 Deploy gowebapp-mysql app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 2 Edit a manifest for gowebapp-mysql Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp-mysql #TODO: Configure VPA with updateMode:OFF spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: gowebapp-mysql updatePolicy: updateMode: \"Off\" Step 3 Apply the manifest for gowebapp-mysql-vpa kubectl apply -f gowebapp-mysql-vpa.yaml Step 4 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp-mysql Note If you don't see it, wait a little longer and try the previous command again. Step 5 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value. 2.2 Set Recommended Request and Limits values to gowebapp-mysql \u00b6 Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 #TODO define a resource request and limits based on VPA Recommender Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-mysql-deployment.yaml kubectl apply -f gowebapp-mysql-deployment.yaml 2.3 Configure VPA for gowebapp to find optimal Resource Request and Limits values \u00b6 Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 3 Edit a manifest for gowebapp Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp #TODO: Ref: https://cloud.google.com/kubernetes-engine/docs/how-to/vertical-pod-autoscaling #TODO: Configure VPA with updateMode:OFF spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: gowebapp updatePolicy: updateMode: \"Off\" Step 4 Apply the manifest for gowebapp-vpa kubectl apply -f gowebapp-vpa.yaml Step 5 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp Note If you don't see it, wait a little longer and try the previous command again. Step 6 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value. 2.4 Set Recommended Request and Limits values to gowebapp \u00b6 Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp:v3 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: #TODO define a resource request and limits based on VPA Recommender volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-deployment.yaml kubectl apply -f gowebapp-deployment.yaml 2.5 Configure HPA for gowebapp \u00b6 Our NotePad Application is going to Production soon. To make sure our application can scale based on requests we will set HPA for our deployment resource using Horizontal Pod Autoscaler. Step 1 Create HPA for gowebapp based on CPU with minReplicas 1 and maxReplicas 5 with target 50. cd ~/$MY_REPO/deploy_a5 vim gowebapp-hpa.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: gowebapp-hpa spec: scaleTargetRef: #TODO Create HPA based on CPU with minReplicas `1` and maxReplicas `5` with target 50. apiVersion: apps/v1 kind: Deployment name: nginx minReplicas: 1 maxReplicas: 5 targetCPUUtilizationPercentage: 50 Step 2 Apply the manifest for gowebapp-hpa kubectl apply -f gowebapp-hpa.yaml Step 3 Take a closer look at the HPA and observe autoscaling or downscaling if any. kubectl describe hpa kubia Note It will take some time to collect metrics information about current cpu usage and since our does't have real load it might not trigger any scaling 2.6 Configure LimitRanges for Namespace dev \u00b6 In order to prevent developers accidentally forget to set values for request and limits . Ops team decide to create a Configuration LimitRange , that will enforce some default values for request and limits if they have not been set, as well as Minimum and maximum requests/limits a container can have to prevent resources abuse. Step 1 Edit a manifest for limit-range that can be used for dev namespace with following requirements: cd ~/$MY_REPO/deploy_a5/ vim limit-range.yaml apiVersion: v1 kind: LimitRange metadata: name: gowebapp-system namespace: dev spec: limits: - default: cpu: \"250m\" # If not specified default limit is 500m per container memory: \"256Mi\" # If not specified the Container's memory limit is set to 512Mi defaultRequest: cpu: \"250m\" # If not specified default it will take from whatever specified in limits.default.cpu memory: \"256Mi\" #If not specified default it will take from whatever specified in limits.default.memory max: cpu: 800m min: cpu: 200m type: Container 2.7 Viewing cluster autoscaler events \u00b6 Our cluster was configured to use Cluster Autoscaler, verify if during you assignment, cluster did went through autoscaling process? To view the logs, perform the following: Step 1: In the Cloud Console, go to the Logs Viewer page. Step 2: Search for the logs using the basic or advanced query interface. To search for logs using the basic query interface, perform the following: a. From the resources drop-down list, select Kubernetes Cluster, then select the location of your cluster, and the name of your cluster. b. From the logs type drop-down list, select container.googleapis.com/cluster-autoscaler-visibility. c. From the time-range drop-down list, select the desired time range. OR search for logs using the advanced query interface, apply the following advanced filter: resource.type=\"k8s_cluster\" resource.labels.location=\"cluster-location\" resource.labels.cluster_name=\"cluster-name\" logName=\"projects/project-id/logs/container.googleapis.com%2Fcluster-autoscaler-visibility\" Reference link 2.7 Commit K8s manifests to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 5\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"Deploy Applications on Kubernetes"},{"location":"ass5_solution/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Resource Limits HPA VPA Limit Range","title":"Deploy Applications on Kubernetes"},{"location":"ass5_solution/#0-create-gke-cluster-with-cluster-and-vertical-autoscaling-support","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-scaling \\ --zone us-central1-c \\ --enable-vertical-pod-autoscaling \\ --num-nodes 2 \\ --enable-autoscaling --min-nodes 1 --max-nodes 3 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-scaling us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-scaling --zone us-central1-c","title":"0 Create GKE Cluster with Cluster and Vertical Autoscaling Support"},{"location":"ass5_solution/#12-locate-assignment-5","text":"Step 1 Locate directory where Kubernetes manifests going to be stored. cd ~/ycit019/ git pull # Pull latest assignement5 In case you don't have this folder clone it as following: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment5/ ls Result You can see Kubernetes manifests with Assignment tasks. Step 2 Go into your personal Google Cloud Source Repository: MY_REPO=your_student_id-notepad Note Replace $student_id with your ID cd ~/$MY_REPO git pull # Pull latest code from you repo Step 3 Copy Assignment 5 deploy_a5 folder to your repo: cp -r ~/ycit019/Assignment5/deploy_a5 . Step 4 Commit deploy folder using the following Git commands: git status git add . git commit -m \"adding K8s manifests for assignment 5\" Step 5 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master","title":"1.2 Locate Assignment 5"},{"location":"ass5_solution/#13-create-a-namespace-dev","text":"Step 1 Create 'dev' namespace that's going to be used to develop and deploy Notestaker application on Kubernetes using kubetl CLI. kubectl create ns dev Step 3 Use dev context to create K8s resources inside this namespace. kubectl config set-context --current --namespace=dev Step 4 Verify current context: kubectl config view | grep namespace Result dev","title":"1.3 Create a Namespace dev"},{"location":"ass5_solution/#21-configure-vpa-for-gowebapp-mysql-to-find-optimal-resource-request-and-limits-values","text":"requests and limits is the way Kubernetes set's QoS for Pods, as well as enable's features like HPA, CA, Resource Quota's and more. However setting best values for resource requests and limits is hard, VPA is here to help. Set VPA for gowebapp and observe usage recommendation for requests and limits Step 1 Deploy gowebapp-mysql app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f secret-mysql.yaml #Create Secret kubectl apply -f gowebapp-mysql-service.yaml #Create Service kubectl apply -f gowebapp-mysql-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 2 Edit a manifest for gowebapp-mysql Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp-mysql #TODO: Configure VPA with updateMode:OFF spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: gowebapp-mysql updatePolicy: updateMode: \"Off\" Step 3 Apply the manifest for gowebapp-mysql-vpa kubectl apply -f gowebapp-mysql-vpa.yaml Step 4 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp-mysql Note If you don't see it, wait a little longer and try the previous command again. Step 5 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value.","title":"2.1 Configure VPA for gowebapp-mysql to find optimal Resource Request and Limits values"},{"location":"ass5_solution/#22-set-recommended-request-and-limits-values-to-gowebapp-mysql","text":"Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql tier: backend spec: replicas: 1 selector: matchLabels: run: gowebapp-mysql strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 #TODO define a resource request and limits based on VPA Recommender Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-mysql-deployment.yaml kubectl apply -f gowebapp-mysql-deployment.yaml","title":"2.2 Set Recommended Request and Limits values to gowebapp-mysql"},{"location":"ass5_solution/#23-configure-vpa-for-gowebapp-to-find-optimal-resource-request-and-limits-values","text":"Step 1: Create ConfigMap for gowebapp's config.json file cd ~/$MY_REPO/gowebapp/config/ kubectl create configmap gowebapp --from-file=webapp-config-json=config.json kubectl describe configmap gowebapp Step 2 Deploy gowebapp app under ~/$MY_REPO/deploy_a5/ cd ~/$MY_REPO/deploy_a5/ kubectl apply -f gowebapp-service.yaml #Create Service kubectl apply -f gowebapp-deployment.yaml #Create Deployment kubectl get deploy Result Our Deployment is up, however without request and limits it will be treated as Best Effort QoS resource on the Cluster. Step 3 Edit a manifest for gowebapp Vertical Pod Autoscaler resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-vpa.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: gowebapp #TODO: Ref: https://cloud.google.com/kubernetes-engine/docs/how-to/vertical-pod-autoscaling #TODO: Configure VPA with updateMode:OFF spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: gowebapp updatePolicy: updateMode: \"Off\" Step 4 Apply the manifest for gowebapp-vpa kubectl apply -f gowebapp-vpa.yaml Step 5 Wait a minute, and then view the VerticalPodAutoscaler kubectl describe vpa gowebapp Note If you don't see it, wait a little longer and try the previous command again. Step 6 Locate the \"Container Recommendations\" at the end of the output from the describe command. Result We will be using Lower Bound values to set our request value and Upper Bound as our limits value.","title":"2.3 Configure VPA for gowebapp to find optimal Resource Request and Limits values"},{"location":"ass5_solution/#24-set-recommended-request-and-limits-values-to-gowebapp","text":"Step 1 Edit a manifest for gowebapp deployment resource: cd ~/$MY_REPO/deploy_a5 vim gowebapp-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: gcr.io/${PROJECT_ID}/gowebapp:v3 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: #TODO define a resource request and limits based on VPA Recommender volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json Step 2 Redeploy our application with defined resource request and limits cd ~/$MY_REPO/deploy_a5/ kubectl delete -f gowebapp-deployment.yaml kubectl apply -f gowebapp-deployment.yaml","title":"2.4 Set Recommended Request and Limits values to gowebapp"},{"location":"ass5_solution/#25-configure-hpa-for-gowebapp","text":"Our NotePad Application is going to Production soon. To make sure our application can scale based on requests we will set HPA for our deployment resource using Horizontal Pod Autoscaler. Step 1 Create HPA for gowebapp based on CPU with minReplicas 1 and maxReplicas 5 with target 50. cd ~/$MY_REPO/deploy_a5 vim gowebapp-hpa.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: gowebapp-hpa spec: scaleTargetRef: #TODO Create HPA based on CPU with minReplicas `1` and maxReplicas `5` with target 50. apiVersion: apps/v1 kind: Deployment name: nginx minReplicas: 1 maxReplicas: 5 targetCPUUtilizationPercentage: 50 Step 2 Apply the manifest for gowebapp-hpa kubectl apply -f gowebapp-hpa.yaml Step 3 Take a closer look at the HPA and observe autoscaling or downscaling if any. kubectl describe hpa kubia Note It will take some time to collect metrics information about current cpu usage and since our does't have real load it might not trigger any scaling","title":"2.5 Configure HPA for gowebapp"},{"location":"ass5_solution/#26-configure-limitranges-for-namespace-dev","text":"In order to prevent developers accidentally forget to set values for request and limits . Ops team decide to create a Configuration LimitRange , that will enforce some default values for request and limits if they have not been set, as well as Minimum and maximum requests/limits a container can have to prevent resources abuse. Step 1 Edit a manifest for limit-range that can be used for dev namespace with following requirements: cd ~/$MY_REPO/deploy_a5/ vim limit-range.yaml apiVersion: v1 kind: LimitRange metadata: name: gowebapp-system namespace: dev spec: limits: - default: cpu: \"250m\" # If not specified default limit is 500m per container memory: \"256Mi\" # If not specified the Container's memory limit is set to 512Mi defaultRequest: cpu: \"250m\" # If not specified default it will take from whatever specified in limits.default.cpu memory: \"256Mi\" #If not specified default it will take from whatever specified in limits.default.memory max: cpu: 800m min: cpu: 200m type: Container","title":"2.6 Configure LimitRanges for Namespace dev"},{"location":"ass5_solution/#27-viewing-cluster-autoscaler-events","text":"Our cluster was configured to use Cluster Autoscaler, verify if during you assignment, cluster did went through autoscaling process? To view the logs, perform the following: Step 1: In the Cloud Console, go to the Logs Viewer page. Step 2: Search for the logs using the basic or advanced query interface. To search for logs using the basic query interface, perform the following: a. From the resources drop-down list, select Kubernetes Cluster, then select the location of your cluster, and the name of your cluster. b. From the logs type drop-down list, select container.googleapis.com/cluster-autoscaler-visibility. c. From the time-range drop-down list, select the desired time range. OR search for logs using the advanced query interface, apply the following advanced filter: resource.type=\"k8s_cluster\" resource.labels.location=\"cluster-location\" resource.labels.cluster_name=\"cluster-name\" logName=\"projects/project-id/logs/container.googleapis.com%2Fcluster-autoscaler-visibility\" Reference link","title":"2.7 Viewing cluster autoscaler events"},{"location":"ass5_solution/#27-commit-k8s-manifests-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit deploy folder using the following Git commands: git add . git commit -m \"k8s manifests for Hands-on Assignment 5\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"2.7 Commit K8s manifests to repository and share it with Instructor/Teacher"},{"location":"ass5_solution/#36-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-scaling","title":"3.6 Cleaning Up"},{"location":"assignment1/","text":"1 Containerize Applications \u00b6 Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1.1 Overview of the Sample Application \u00b6 This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application. 1.1 Build Dockers image for frontend application \u00b6 Step 1 Locate and review the go source code: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Setup vim editor with appropriate color schema Note Skip this step if you use another editor. echo \"colo elflord\" > ~/.vimrc Step 3 Write Dockerfile for your frontend application cd ~/ycit019/Assignment1/gowebapp Create a file named Dockerfile in this directory for the frontend Go app. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1 1.2 Build Docker image for backend application \u00b6 Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally 1.3 Test application by running with Docker Engine. \u00b6 Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network named gowebapp with subnet range 172.19.0.0/16 in which to run the frontend and backend containers: #TODO docker xxx Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Container needs to run on network: `gowebapp` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine #TODO Container needs to run on network: `gowebapp` Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit; 1.5 Cleanup running applications and unused networks \u00b6 ### TODO docker xxx","title":"1 Containerize Applications"},{"location":"assignment1/#1-containerize-applications","text":"Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process","title":"1 Containerize Applications"},{"location":"assignment1/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"assignment1/#11-overview-of-the-sample-application","text":"This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application.","title":"1.1 Overview of the Sample Application"},{"location":"assignment1/#11-build-dockers-image-for-frontend-application","text":"Step 1 Locate and review the go source code: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Setup vim editor with appropriate color schema Note Skip this step if you use another editor. echo \"colo elflord\" > ~/.vimrc Step 3 Write Dockerfile for your frontend application cd ~/ycit019/Assignment1/gowebapp Create a file named Dockerfile in this directory for the frontend Go app. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1","title":"1.1 Build Dockers image for frontend application"},{"location":"assignment1/#12-build-docker-image-for-backend-application","text":"Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally","title":"1.2 Build Docker image for backend application"},{"location":"assignment1/#13-test-application-by-running-with-docker-engine","text":"Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network named gowebapp with subnet range 172.19.0.0/16 in which to run the frontend and backend containers: #TODO docker xxx Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Container needs to run on network: `gowebapp` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine #TODO Container needs to run on network: `gowebapp` Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit;","title":"1.3 Test application by running with Docker Engine."},{"location":"assignment1/#15-cleanup-running-applications-and-unused-networks","text":"### TODO docker xxx","title":"1.5 Cleanup running applications and unused networks"},{"location":"assignment3_sol/","text":"Deploy Applications on Kubernetes \u00b6 Objective: Review process of creating K8s: Secrets ConfigMaps Persistent Volumes (PV) Persisten Volume Claims (PVC) Jobs CronJobs HPA Externalize Web Application Configuration 3.1 Externalize Web Application Configuration \u00b6 Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~/k8scanada/ git pull mkdir $HOME/k8scanada/Assignment3/gowebapp/config cp $HOME/k8scanada/Assignment3/gowebapp/code/config/config.json \\ $HOME/k8scanada/Assignment3/gowebapp/config rm -rf $HOME/k8scanada/Assignment3/gowebapp/code/config/ Step 2: Modify app to support setting DB password through environment variable. Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor to modify: vim $HOME/k8scanada/Assignment3/gowebapp/code/vendor/app/shared/database/database.go Add an import for the os package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") } 3.2 Build new Docker image for your frontend application \u00b6 Step 1: Update Dockerfile for your frontend application FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go ENV PASSWORD=rootpasswd COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install VOLUME $GOPATH/src/gowebapp/config ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd $HOME/k8scanada/Assignment3/gowebapp/ Build the gowebapp image locally. Make sure to include \u201c.\u201c at the end. Notice the new version label. docker build -t gowebapp:v2 . 3.3 Run and test new Docker image locally \u00b6 Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=cloudops user-name/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 30005:80 \\ -v $HOME/k8scanada/Assignment3/gowebapp/config:/go/src/gowebapp/config \\ -e DB_PASSWORD=cloudops --net gowebapp -d --name gowebapp \\ --hostname gowebapp user-name/gowebapp:v2 Now that we\u2019ve launched the application containers, let\u2019s try to test the web application locally. You should be able to access the application at http://Public_IP:30005. Step 3: Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes 3.4 Publish New Image \u00b6 Step 1 We built the second version of gowebapp from the last exercise and tested it locally. Now we can tag and push gowebapp:v2 to private repository. docker tag gowebapp:v2 <user-name>/gowebapp:v2 docker push <user-name>/gowebapp:v2 3.5 Create a Secret \u00b6 Step 1 Create a secret for the MySQL password kind: Secret apiVersion: v1 metadata: name: mysql type: Opaque data: password: cm9vdHBhc3N3ZA== 3.6 Create a Volume for Mysql \u00b6 Step 1: Dynamically provisioning Persistent Volume Define a Persistent Volume Claim object to use with MySQL in a file named pvc.yaml under $HOME/k8scanada/Assignment3/deploy . In this case, we are not explicitly defining a Persistent Volume (pv) object for this PVC to use. This approach is more portable than explicitly defining and hard-wiring volume types. Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims cd $HOME/k8scanada/Assignment3/deploy vim pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysqlpvc labels: run: gowebapp-mysql spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi Step 2: Create PVC object kubectl apply -f pvc.yaml --record Step 3: View your PVC metadata kubectl get pvc mysqlpvc kubectl describe pv 3.6 Create Mysql deployment \u00b6 Create Mysql deployment using Secrets, liveness, readiness, resources, limits and Persistent Volume. Step 1 Update gowebapp-mysql-deployment.yaml under $HOME/k8scanada/Assignment3/deploy vim $HOME/k8scanada/Assignment3/deploy/gowebapp-mysql-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: replicas: 1 strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: archyufa/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 256m memory: 256Mi limits: cpu: 512m memory: 512Mi volumeMounts: - mountPath: /var/lib/mysql name: mysql volumes: - name: mysql persistentVolumeClaim: claimName: mysqlpvc Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 5 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 6 Get rollout history details for specific revision (use number show in output to previous command) Notice the value for MYSQL_ROOT_PASSWORD kubectl rollout history deploy gowebapp-mysql \\ --revision=<latest_version_number> 3.5 Create ConfigMap and Resources and Probes to MySQL \u00b6 Step 1: create ConfigMap for gowebapp's config.json file kubectl create configmap gowebapp --from-file=webapp-config-json=/home/cca-user/k8scanada/Assignment3/gowebapp/config/config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json Step 2: Update gowebapp-deployment.yaml under /home/cca-user/k8scanada/Assignment3/deploy In this exercise, we will add liveness/readiness probes to our deployments, as well as resource limits. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: <#TODO_user-name>/gowebapp:v1 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: request: cpu: 200m memory: 128Mi limits: cpu: 250m memory: 256Mi volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6: Access your application: <http://VM_Public_IP:NodePort. Register for an account, login and use the Notepad. If you need to lookup the VM_Public_IP for your environment, use the following command: kubectl get svc gowebapp -o wide ## 3.6 Define a Job to purge data from the web application database Create a Job object definition in a file called reset-db.yaml which connects to the MySQL database and deletes all the data from the note and user tables. Replace the TODOs below with the missing elements of the definition. If you need more help, refer to: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion Step 1: Create job reset-db.yaml under /home/cca-user/k8scanada/Assignment3/deploy vim reset-db.yaml apVersion: batch/v1 kind: Job metadata: name: reset-db labels: run: reset-db spec: activeDeadlineSeconds: 10 template: metadata: name: reset-db spec: restartPolicy: OnFailure containers: - name: database-cleaner image: mysql:5.6 env: - name: DB_USER value: root - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password - name: DB_HOST value: gowebapp-mysql - name: DB value: gowebapp command: - /bin/sh args: - -c - mysql -h $(DB_HOST) -u $(DB_USER) -p$(DB_PASSWORD) $(DB) -e 'delete from note; delete from user;' Step 2: Create the Job in Kubernetes kubectl apply -f reset-db.yaml --record Step 3: View Job metadata and status The job should run and complete immediately. kubectl get job reset-db kubectl describe job reset-db Step 4: Verify that data has been deleted Open the web application at http://Public_IP:NodePort and verify that the accounts and notes you created above no longer exist.","title":"Deploy Applications on Kubernetes"},{"location":"assignment3_sol/#deploy-applications-on-kubernetes","text":"Objective: Review process of creating K8s: Secrets ConfigMaps Persistent Volumes (PV) Persisten Volume Claims (PVC) Jobs CronJobs HPA Externalize Web Application Configuration","title":"Deploy Applications on Kubernetes"},{"location":"assignment3_sol/#31-externalize-web-application-configuration","text":"Let\u2019s make some minor modifications to the web application to externalize its configuration, and make it easier to manage and update at deployment time. Step 1: Move config file outside compiled application First, let\u2019s move the web application\u2019s configuration into a folder outside the main compilation path cd ~/k8scanada/ git pull mkdir $HOME/k8scanada/Assignment3/gowebapp/config cp $HOME/k8scanada/Assignment3/gowebapp/code/config/config.json \\ $HOME/k8scanada/Assignment3/gowebapp/config rm -rf $HOME/k8scanada/Assignment3/gowebapp/code/config/ Step 2: Modify app to support setting DB password through environment variable. Next, let\u2019s make a minor modification to the Go application code to allow setting the DB password through an environment variable. This will make it easier to dynamically inject this value at deployment time. Use a text editor to modify: vim $HOME/k8scanada/Assignment3/gowebapp/code/vendor/app/shared/database/database.go Add an import for the os package at line 8. After making this change, your imports list will look like the following: import ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"os\" \"github.com/boltdb/bolt\" _ \"github.com/go-sql-driver/mysql\" // MySQL driver \"github.com/jmoiron/sqlx\" \"gopkg.in/mgo.v2\" ) Add the following code at line 89 after var err error : // Check for MySQL Password environment variable and update configuration if present if os.Getenv(\"DB_PASSWORD\") != \"\" { d.MySQL.Password = os.Getenv(\"DB_PASSWORD\") }","title":"3.1 Externalize Web Application Configuration"},{"location":"assignment3_sol/#32-build-new-docker-image-for-your-frontend-application","text":"Step 1: Update Dockerfile for your frontend application FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go ENV PASSWORD=rootpasswd COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install VOLUME $GOPATH/src/gowebapp/config ENTRYPOINT $GOPATH/bin/gowebapp Step 2: Build updated gowebapp Docker image locally cd $HOME/k8scanada/Assignment3/gowebapp/ Build the gowebapp image locally. Make sure to include \u201c.\u201c at the end. Notice the new version label. docker build -t gowebapp:v2 .","title":"3.2 Build new Docker image for your frontend application"},{"location":"assignment3_sol/#33-run-and-test-new-docker-image-locally","text":"Before deploying to Kubernetes, let\u2019s test the updated gowebapp Docker image locally, to ensure that the frontend and backend containers run and integrate properly. Step 1: Launch frontend and backend containers First, we launch the backend database container, using a previously created Docker image, as it will take a bit longer to startup, and the frontend container depends on it. Note Update user-name with command below with you docker-hub id docker run --net gowebapp --name gowebapp-mysql --hostname \\ gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=cloudops user-name/gowebapp-mysql:v1 Step 2: Now launch a frontend container using the updated gowebapp image, mapping the container port 80 - where the web application is exposed - to port 30005 on the host machine. Notice how we're mapping a host volume into the container for configuration, and setting a container environment variable with the MySQL DB password: Note Update user-name with command below with you docker-hub id docker run -p 30005:80 \\ -v $HOME/k8scanada/Assignment3/gowebapp/config:/go/src/gowebapp/config \\ -e DB_PASSWORD=cloudops --net gowebapp -d --name gowebapp \\ --hostname gowebapp user-name/gowebapp:v2 Now that we\u2019ve launched the application containers, let\u2019s try to test the web application locally. You should be able to access the application at http://Public_IP:30005. Step 3: Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Result By externalizing application configuration, you have made it easier to manage and modify your application configuration at deployment time. This will be very helpful as we deploy our applications to Kubernetes","title":"3.3 Run and test new Docker image locally"},{"location":"assignment3_sol/#34-publish-new-image","text":"Step 1 We built the second version of gowebapp from the last exercise and tested it locally. Now we can tag and push gowebapp:v2 to private repository. docker tag gowebapp:v2 <user-name>/gowebapp:v2 docker push <user-name>/gowebapp:v2","title":"3.4 Publish New Image"},{"location":"assignment3_sol/#35-create-a-secret","text":"Step 1 Create a secret for the MySQL password kind: Secret apiVersion: v1 metadata: name: mysql type: Opaque data: password: cm9vdHBhc3N3ZA==","title":"3.5 Create a Secret"},{"location":"assignment3_sol/#36-create-a-volume-for-mysql","text":"Step 1: Dynamically provisioning Persistent Volume Define a Persistent Volume Claim object to use with MySQL in a file named pvc.yaml under $HOME/k8scanada/Assignment3/deploy . In this case, we are not explicitly defining a Persistent Volume (pv) object for this PVC to use. This approach is more portable than explicitly defining and hard-wiring volume types. Please see reference for PVC creation here: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims cd $HOME/k8scanada/Assignment3/deploy vim pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysqlpvc labels: run: gowebapp-mysql spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi Step 2: Create PVC object kubectl apply -f pvc.yaml --record Step 3: View your PVC metadata kubectl get pvc mysqlpvc kubectl describe pv","title":"3.6 Create a Volume for Mysql"},{"location":"assignment3_sol/#36-create-mysql-deployment","text":"Create Mysql deployment using Secrets, liveness, readiness, resources, limits and Persistent Volume. Step 1 Update gowebapp-mysql-deployment.yaml under $HOME/k8scanada/Assignment3/deploy vim $HOME/k8scanada/Assignment3/deploy/gowebapp-mysql-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: gowebapp-mysql labels: run: gowebapp-mysql spec: replicas: 1 strategy: type: Recreate template: metadata: labels: run: gowebapp-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: archyufa/gowebapp-mysql:v1 name: gowebapp-mysql ports: - containerPort: 3306 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 30 timeoutSeconds: 2 readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 25 timeoutSeconds: 2 resources: requests: cpu: 256m memory: 256Mi limits: cpu: 512m memory: 512Mi volumeMounts: - mountPath: /var/lib/mysql name: mysql volumes: - name: mysql persistentVolumeClaim: claimName: mysqlpvc Step 2 Start the rolling upgrade and record the command used in the rollout history: kubectl apply -f gowebapp-mysql-deployment.yaml --record Step 3 Verify that rollout was successful kubectl rollout status deploy gowebapp-mysql Step 4 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 5 Get rollout history kubectl rollout history deploy gowebapp-mysql Step 6 Get rollout history details for specific revision (use number show in output to previous command) Notice the value for MYSQL_ROOT_PASSWORD kubectl rollout history deploy gowebapp-mysql \\ --revision=<latest_version_number>","title":"3.6 Create Mysql deployment"},{"location":"assignment3_sol/#35-create-configmap-and-resources-and-probes-to-mysql","text":"Step 1: create ConfigMap for gowebapp's config.json file kubectl create configmap gowebapp --from-file=webapp-config-json=/home/cca-user/k8scanada/Assignment3/gowebapp/config/config.json kubectl describe configmap gowebapp Note The entire file contents from config.json are stored under the key webapp-config-json Step 2: Update gowebapp-deployment.yaml under /home/cca-user/k8scanada/Assignment3/deploy In this exercise, we will add liveness/readiness probes to our deployments, as well as resource limits. For more information, see here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ apiVersion: apps/v1 kind: Deployment metadata: name: gowebapp labels: run: gowebapp spec: replicas: 2 selector: matchLabels: run: gowebapp template: metadata: labels: run: gowebapp spec: containers: - env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password image: <#TODO_user-name>/gowebapp:v1 name: gowebapp ports: - containerPort: 80 livenessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /register port: 80 initialDelaySeconds: 25 timeoutSeconds: 5 resources: request: cpu: 200m memory: 128Mi limits: cpu: 250m memory: 256Mi volumeMounts: - name: config-volume mountPath: /go/src/gowebapp/config volumes: - name: config-volume configMap: name: gowebapp items: - key: webapp-config-json path: config.json kubectl apply -f gowebapp-deployment.yaml --record Result This will start the rolling upgrade and record the command used in the rollout history Step 3: Verify that rollout was successful kubectl rollout status deploy gowebapp Step 4: Get rollout history kubectl rollout history deploy gowebapp Step 5: Get rollout history details for specific revision (use number show in output to previous command) kubectl rollout history deploy gowebapp --revision=<latest_version_number Step 6: Access your application: <http://VM_Public_IP:NodePort. Register for an account, login and use the Notepad. If you need to lookup the VM_Public_IP for your environment, use the following command: kubectl get svc gowebapp -o wide ## 3.6 Define a Job to purge data from the web application database Create a Job object definition in a file called reset-db.yaml which connects to the MySQL database and deletes all the data from the note and user tables. Replace the TODOs below with the missing elements of the definition. If you need more help, refer to: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion Step 1: Create job reset-db.yaml under /home/cca-user/k8scanada/Assignment3/deploy vim reset-db.yaml apVersion: batch/v1 kind: Job metadata: name: reset-db labels: run: reset-db spec: activeDeadlineSeconds: 10 template: metadata: name: reset-db spec: restartPolicy: OnFailure containers: - name: database-cleaner image: mysql:5.6 env: - name: DB_USER value: root - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysql key: password - name: DB_HOST value: gowebapp-mysql - name: DB value: gowebapp command: - /bin/sh args: - -c - mysql -h $(DB_HOST) -u $(DB_USER) -p$(DB_PASSWORD) $(DB) -e 'delete from note; delete from user;' Step 2: Create the Job in Kubernetes kubectl apply -f reset-db.yaml --record Step 3: View Job metadata and status The job should run and complete immediately. kubectl get job reset-db kubectl describe job reset-db Step 4: Verify that data has been deleted Open the web application at http://Public_IP:NodePort and verify that the accounts and notes you created above no longer exist.","title":"3.5 Create ConfigMap and Resources and Probes to MySQL"}]}