{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cloud Architecture Course (019, 020)","text":""},{"location":"020_Module_2_Lab_Terraform_Fundamentals/","title":"Module 2 Terraform Fundamentals","text":"<p>Lab 2 Terraform Fundamentals</p> <p>Objective:</p> <ul> <li>Automating through code the configuration and provisioning of resources</li> </ul>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#1-what-is-terraform","title":"1 What is Terraform?","text":"<p>Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing, popular service providers and custom in-house solutions.</p> <p>Configuration files describe to Terraform the components needed to run a single application or your entire data center. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform can determine what changed and create incremental execution plans that can be applied.</p> <p>The infrastructure Terraform can manage includes both low-level components such as compute instances, storage, and networking, and high-level components such as DNS entries and SaaS features.</p>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#12-key-features","title":"1.2 Key features","text":"<p>Infrastructure as code Infrastructure is described using a high-level configuration syntax. This allows a blueprint of your data center to be versioned and treated as you would any other code. Additionally, infrastructure can be shared and re-used.</p> <p>Execution plans Terraform has a planning step in which it generates an execution plan. The execution plan shows what Terraform will do when you execute the apply command. This lets you avoid any surprises when Terraform manipulates infrastructure.</p> <p>Resource graph Terraform builds a graph of all your resources and parallelizes the creation and modification of any non-dependent resources. Because of this, Terraform builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure.</p> <p>Change automation Complex changesets can be applied to your infrastructure with minimal human interaction. With the previously mentioned execution plan and resource graph, you know exactly what Terraform will change and in what order, which helps you avoid many possible human errors.</p>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-demos","title":"Lab Demos","text":""},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-1-simple-terraform","title":"Lab 1: Simple Terraform","text":"<p>You will be performing the following steps in Cloud Shell. Launch Cloud Shell from Google Cloud Console</p> <p>Step 1 Ensure that you have a GCP Project and Billing enabled on the project. </p> <pre><code>gcloud config set project &lt;PROJECT_ID&gt;\n</code></pre> <p>Step 2  Clone the following gitlab repository to your Cloud Shell, which going to use for our work:</p> <pre><code>git clone https://github.com/Cloud-Architects-Program/ycit020_2022.git\ncd ycit020_2022/tf-samples/terraform-basics-1\n</code></pre> <p>Step 3 First we will explore a simple Terraform configuration. It has a single main.tf file. In this file you will have a section called provider. By stating the provider as \"google\", Terraform knows to invoke Google API using the provider to provision resources  </p> <pre><code>provider \"google\" {\n    project = PROJECT_ID\n    region = \"us-central1\"\n}\n</code></pre> <p>In the next section,  you will see the declaration and configuration of compute_instance resource, which will be used to provision a virtual machine. Now, lets execute the terraform using the commands</p> <pre><code>terraform init \n</code></pre> <p>You can notice how the provider plugin has been initialized</p> <pre><code>Initializing provider plugins...\n- Finding latest version of hashicorp/google...\n- Installing hashicorp/google v4.36.0...\n- Installed hashicorp/google v4.36.0 (signed by HashiCorp)\n</code></pre> <p>Step 4 You can now run the validate command to check if the Terraform file and syntax are valid </p> <pre><code>terraform validate \n</code></pre> <p>You will find the following error</p> <pre><code>\u2502 Error: Invalid reference\n\u2502 \n\u2502   on main.tf line 3, in provider \"google\":\n\u2502    3:     project = PROJECT_ID\n</code></pre> <p>Open the main.tf file and modify the PROJECT_ID with you actual project-id and run <code>terraform validate</code> again</p> <p>Step 5 Next step is to preview the resources that are going to be built as part of the Terraform Configuration </p> <pre><code>terraform plan\n</code></pre> <p>Scroll through the output to see the resource type and values for each (identify few key ones you would like to know the output for eg: network_ip)</p> <pre><code>Plan: 1 to add, 0 to change, 0 to destroy.\n</code></pre> <p>Step 6 Perform a terraform deployment </p> <pre><code>terraform apply\n</code></pre> <p>When terraform tries to deploy your resource, you might run into a few issues</p> <pre><code>If its a Permission issue --&gt; make sure you are authenticated to google cloud. You might have already been prompted with login prompt before \nIf its an API based error --&gt; then you might not have enabled Compute Engine API in your GCP Project \nIf its a invalid configuration error ---&gt; check if you have provided the correct project id \n</code></pre> <p>You can delete the resource created using the destroy command</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-2-terraform-with-multiple-config-files-and-variables","title":"Lab 2: Terraform with multiple config files and variables","text":"<p>In the next terraform we will look at how Terraform configurations can be separated as multiple files, but can be built together. We will also look at how to use <code>variables.tf</code> file and <code>.tfvars</code> files for ease of configuration</p> <p>Step 1 Navigate to the second lab folder </p> <pre><code>cd..\ncd terraform-basic-2\n</code></pre> <p>Analyze the files and the contents of the files.</p> <pre><code>firewall.tf\nmain.tf\nprovider.tf\nterraform.tfvars\nvariables.tf\n</code></pre> <p>Look at the configs on both <code>firewall.tf</code> and <code>provider.tf</code> to under resource definition segregation.All of the variable definition and default values are listed in <code>variables.tf</code> file. The run time values for these variables are provided in the <code>.tfvars</code> file</p> <p>Step 2 Open the main.tf file and modify the PROJECT_ID with you actual project-id and run </p> <pre><code>terraform init \nterraform validate\nterraform plan\n</code></pre> <p>Even though there are terraform files, there is only one planfile. Terraform automatically will create resources in the right order, if output of one is declared as input for other. You can deploy the terraform using </p> <pre><code>terraform apply\n</code></pre> <p>You can delete the resource created using the destroy command</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-3-terraform-deployment-to-multiple-environments-and-the-use-of-terraform-workspaces","title":"Lab 3: Terraform deployment to multiple environments and the use of terraform workspaces","text":"<p>In this lab, we will look at storing the terraform state file in a central storage (GCS backend). We will also look at how to deploy the same terraform code to mulitple environments, similar to how we use environment files in high level programming languages.</p> <p>Step 1 Navigate to the right folder and take a note of the 3 different <code>.tfvars</code> file, one for each environment</p> <pre><code>cd..\ncd terraform-basic-3\nls\n</code></pre> <p>Step 2  You can also notice that we have a new file called <code>backend.tf</code> to store our state files in a central bucket instead of on the local filesystem. This ensure consistency when multiple people are working on the terraform file that the state file is synchronized with changes </p> <p>Create a storage bucket in the project </p> <pre><code>gsutil mb gs://&lt;PROJECT-ID&gt;-tfstate\n</code></pre> <p>Update the <code>backend.tf</code> file with the appropiriate storage bucket name </p> <pre><code>terraform {\n  backend \"gcs\" {\n    bucket = \"PROJECT_ID-tfstate\"\n  }\n}\n</code></pre> <p>Step 3 Next, observe closely the different <code>.tfvars</code> file created for the corresponding environment. Notice the VM name change.  </p> <pre><code>region=\"us-central1\"\nproject=PROJECT_ID\nname=\"flask-server-dev\"\nmachine_type=\"f1-micro\"\nzone=\"us-central1-c\"\n</code></pre> <p>Update the <code>project</code> field to your project_id on the <code>dev.tfvars</code> file. You can deploy terraform config to different environments by passing the respective .tfvars file as below</p> <pre><code>terraform init\nterraform plan -var-file=\"dev.tfvars\"\nterraform apply -var-file=\"dev.tfvars\"\n</code></pre> <p>Modify the <code>project</code> field in <code>uat.tfvars</code> file and proceed with next step</p> <pre><code>terraform plan -var-file=\"uat.tfvars\"\n</code></pre> <p>If you try to deploy similarly to another environment, you will notice that terraform will force a replacement of the current deployment. </p> <p>Because there is only one state file and the state file has a single resource, its trying to replace the resource with new config. We will now use Terraform workspace to get it working without replace You can use </p> <pre><code>terraform workspace list\n</code></pre> <p>to identify the current workspace. By default it uses the default workspace. Lets create worksapce corresponding to each environment. </p> <pre><code>terraform workspace new uat\nterraform init\nterraform plan -var-file=\"uat.tfvars\"\nterraform apply -var-file=\"uat.tfvars\"\n</code></pre> <p>In order to ensure that the state file doesnt get overwritten due to env changes, you need to use separate workspace for each environment. Also naviagte to the GCS bucket and you will be able to see different state file for each environment </p> <pre><code>gsutil ls gs://&lt;PROJECT-ID&gt;-tfstate\n  gs://course-valavan-tfstate/default.tfstate\n  gs://course-valavan-tfstate/dev.tfstate\n  gs://course-valavan-tfstate/prod.tfstate\n  gs://course-valavan-tfstate/uat.tfstate\n</code></pre> <p>You can now perform the terraform destroy on these deployments</p> <pre><code>terraform destroy\nterraform workspace select default\nterraform destroy\n</code></pre> <p>You can also delete the newly created GCS state files. </p>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-4-building-terraform-configuration-using-devops-pipeline-cloudbuild","title":"Lab 4: Building Terraform configuration using DevOps Pipeline (Cloudbuild)","text":"<p>In this lab, we will look to how DevOps team normally deploys the terraform configuration in a pipeline. We will take a closer look at how to build for different environments using the same pipeline and the base terraform code </p> <p>Step 1 Navigate to the right folder and take a note of the 3 different <code>.tfvars</code> file, one for each environment</p> <pre><code>cd..\ncd terraform-basic-4\nls\n</code></pre> <p>Step 2  Lets look at the cloudbuild.yaml pipeline file </p> <pre><code>steps:\n- id: 'tf init'\n  name: 'hashicorp/terraform:1.0.3'\n  entrypoint: 'sh'\n  args: \n  - '-c'\n  - |\n      echo \"Branch '$BRANCH_NAME'\"\n      terraform init\n      terraform workspace select $BRANCH_NAME || terraform workspace new $BRANCH_NAME\n\n# [START tf-plan]\n- id: 'tf plan'\n  name: 'hashicorp/terraform:1.0.3'\n  entrypoint: 'sh'\n  args: \n  - '-c'\n  - | \n      terraform workspace select $BRANCH_NAME || terraform workspace new $BRANCH_NAME\n      terraform plan -var-file=\"$BRANCH_NAME.tfvars\" -lock=false\n# [END tf-plan]\n\n# [START tf-apply]\n- id: 'tf apply'\n  name: 'hashicorp/terraform:1.0.3'\n  entrypoint: 'sh'\n  args: \n  - '-c'\n  - | \n      terraform workspace select $BRANCH_NAME || terraform workspace new $BRANCH_NAME\n      terraform apply -var-file=\"$BRANCH_NAME.tfvars\" -auto-approve -lock=false\n</code></pre> <p>Step 3 Before deploying the pipeline, you need to make sure that in the GCP Project IAM console that the <code>cloudbuild</code> service account needs to have Compute Admin Permissions as it uses the service account to deploy the VM</p> <p>Step 4 Modify the <code>project</code> field in the following files </p> <pre><code>backend.tf\ndev.tfvars\nuat.tfvars\nprod.tfvars\n</code></pre> <p>Step 5 On the last line, you should also take note of the <code>auto-approve</code> flag, which will not wait for the user input before terraform deplyoment. You can trigger the pipeline by specifying the tfvars file to use for the build as Paramter as part of the cloud build job initiation</p> <pre><code> gcloud builds submit --substitutions=BRANCH_NAME=\"dev\"\n gcloud builds submit --substitutions=BRANCH_NAME=\"uat\"\n gcloud builds submit --substitutions=BRANCH_NAME=\"prod\"\n</code></pre> <p>If you need to destroy a deployment you can invoke cloudbuild, by specifying the destroy cloudbuild.yaml</p> <pre><code>gcloud builds submit --substitutions=BRANCH_NAME=\"dev\" --config=cloudbuild-destroy.yaml\n</code></pre>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-5-instructor-demo-only-building-terraform-configuration-using-devops-pipeline-gitlab","title":"Lab 5: [Instructor Demo Only] Building Terraform configuration using DevOps Pipeline (GITLAB)","text":"<p>In the previous pipeline, we werent able to verify the plan output before applying. You will need to build seperate triggers for each and manully deploy the respective pipeline. Gitlab pipelines reduce this complexity to pause between steps in a pipeline and also provides a nice UI to trigger and view the status of deployments. </p> <p>Before deploying the gitlab pipeline, you need to make sure that Gitlab can deploy to your project by using a service account. You will need to create a service account first, download the key and upload it in the Gitlab pipeline variables. Before the gitlab pipeline executes the first step, it will authenticate to the project using the Service account key</p> <p></p> <p>Sample Terraform Gitlab Pipeline Code </p> <pre><code>image:\n  name: hashicorp/terraform:light\n  entrypoint:\n    - '/usr/bin/env'\n    - 'PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\n\nbefore_script:\n  - rm -rf .terraform\n  - terraform --version\n  #- mkdir -p ./creds\n  #- echo $SERVICEACCOUNT | base64 -d &gt; ./creds/serviceaccount.json\n  - export GOOGLE_APPLICATION_CREDENTIALS=${SERVICEACCOUNT}\n  - terraform init\n\nstages:\n  - validate\n  - plan\n  - apply\n  - destroy\n\nvalidate:\n  stage: validate\n  script:\n    - terraform validate\n\nplan:\n  stage: plan\n  script:\n    - terraform plan -out \"planfile\"\n  dependencies:\n    - validate\n  artifacts:\n    paths:\n      - planfile\n\napply:\n  stage: apply\n  script:\n    - terraform apply -input=false \"planfile\"\n  dependencies:\n    - plan\n  when: manual\n\n\ndestroy: \n  stage: destroy\n  script:\n    - terraform destroy -auto-approve\n  when: manual\n</code></pre> <p>Output and manual trigger options, to be shown by the instructor during the class</p>"},{"location":"020_Module_2_Lab_Terraform_Fundamentals/#lab-6-terrform-deployment-using-modules","title":"Lab 6: Terrform deployment using modules","text":"<p>In this section, we will look at the advantages provided by Terraform Modules. Modules help with abstraction of complexity, encapsulaion of serveral resources into a simple format and also provides re-usability and ease of use. Similar in concepts to OOO Programming and Classes in higher level languages. </p> <p>Step 1Navigate to the right folder and take a note of the 3 different <code>.tfvars</code> file, one for each environment</p> <pre><code>cd..\ncd terraform-sql\ncat main.tf\n</code></pre> <p>Here we are building a private MYSQL instance. There are about 4 resouces that get created - including a private network, reserved IP, Service Peering, SQL instance. We need to build all of the configuration ourselves and know that to build a Private Cloud SQL, you will need all 4 resources. It doesnt include best practice configuration like having a Cloud SQL Proxy, configure default username for logging to the database and so on. </p> <p>Lets use a pre-built module that has all of the configurations ready and we will use this module to pass input paramaters alone and module can take care of the resource deployment in the backend.</p> <pre><code>cd ..\ngit clone https://github.com/terraform-google-modules/terraform-google-sql-db.git\ncd terraform-google-sql-db/examples\nls\n</code></pre> <p>Prebuilt configuration on how to use the specific module for certain configurations can be found.</p> <pre><code>mssql-public\nmysql-backup-create-service-account\nmysql-ha\nmysql-private\nmysql-public\npostgresql-backup-provided-service-account\npostgresql-ha\npostgresql-public-iam\npostgresql-public\n</code></pre> <p>You will need to how to use the module and what the various parameters in the module mean. But you dont have to reasearch terraform website on how to build each of the resource configuration yourself. </p> <p>Apply the configuration using the commands below. Since these are database creations, they generally take about 15-20 mins to complete deployment</p> <pre><code>cd mysql-private\nterraform init\nterraform plan   ## Enter the project id when prompted\nterraform apply  ## Enter the project id when prompted\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE/","title":"020 Module 3 Assignment Production GKE","text":"<p>Objective:</p> <ul> <li>GKE Regional, Private Standard Cluster for Production Deployment</li> <li>GKE Autopilot Cluster </li> </ul>"},{"location":"020_Module_3_Assignment_Production_GKE/#1-creating-production-gke-cluster","title":"1 Creating Production GKE Cluster","text":"<p>In Part 1 of the Assignment we going to deploy <code>GKE</code> Standard cluster for production usage. This cluster will have <code>Regional</code> control plain for high availability. Also following production security Best Practices we going to setup <code>GKE</code> with <code>private</code> nodes (No Public IP) and learn how to setup <code>Cloud Nat</code> which allows Private cluster's Pods access securely Internet.</p>"},{"location":"020_Module_3_Assignment_Production_GKE/#11-locate-assignment-2","title":"1.1 Locate Assignment 2","text":"<p>Step 1  Clone <code>ycit020</code> repo with Kubernetes manifests, which going to use for our work:</p> <pre><code>cd ~/ycit020_2022/\ngit pull       # Pull latest Mod3_assignment\n</code></pre> <p>In case you don't have this folder clone it as following:</p> <pre><code>cd ~\ngit clone https://github.com/Cloud-Architects-Program/ycit020_2022\ncd ~/ycit020_2022/module3_assignment/\nls\n</code></pre> <p>Result</p> <p>You can see Kubernetes manifests </p> <p>Step 2 Go into your personal Google Cloud Source Repository:</p> <pre><code>export student_name=&lt;write_your_name_here_and_remove_brakets&gt;\n</code></pre> <p>Note</p> <p>Replace $student_id with your ID</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git pull                              # Pull latest code from you repo\n</code></pre> <p>Step 3 Copy <code>module3_assignment</code> folder to your repo:</p> <pre><code>cp -r ~/ycit020_2022/module3_assignment ycit020_module3\n</code></pre> <p>Step 4 Create <code>production_gke.txt</code> file that will have instructions how to deploy Production GKE Clusters.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\ncat &gt; production_gke.txt &lt;&lt; EOF\n# Creating Production GKE Cluster\n\n**Step 1** Create a vpc network:\n\n**Step 2** Create a subnet VPC design:\n\n**Step 3** Create a subnet with primary and secondary ranges:\n\n**Step 4** Create a Private GKE Cluster:\n\n**Step 5** Create a Cloud Router:\n\n**Step 6** Create a Cloud Nat Gateway:\n\n**Step 7** Create an Autopilot GKE Cluster:\n\nEOF\n</code></pre> <p>Important</p> <p>This document will be graded.</p> <p>Step 5 Commit <code>ycit020_module3</code> folder using the following Git commands:</p> <pre><code>cd ~/$student_name-notepad\ngit status \ngit add .\ngit commit -m \"adding documentation for ycit020 module 3 assignment\"\n</code></pre> <p>Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command:</p> <pre><code>git push origin master\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE/#12-creating-a-gcp-project","title":"1.2 Creating a GCP project","text":"<p>Note</p> <p>We are going to create a temporary project for this assignment. While you going to store code in existing project that we've used so far in class</p> <p>Set variables:</p> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\nexport PROJECT_PREFIX=1   # Project has to be unique\nexport PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX\n</code></pre> <p>Create a project:</p> <pre><code>gcloud projects create $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX\n</code></pre> <p>List billings and take note of <code>ACCOUNT_ID</code>:</p> <pre><code>ACCOUNT_ID=$(gcloud beta billing accounts list | grep -B2 True  | head -1 | grep ACCOUNT_ID  |awk '{ print $2}') \n</code></pre> <p>Check account ID was set properly as variable:</p> <pre><code>echo $ACCOUNT_ID\n</code></pre> <p>Attach your project to the correct billing account:</p> <pre><code>gcloud beta billing projects link $PROJECT_ID --billing-account=$ACCOUNT_ID\n</code></pre> <p>Set the newly created project as default for gcloud:</p> <pre><code>gcloud config set project $PROJECT_ID\n</code></pre> <p>Enable <code>compute</code>, <code>container</code>, <code>cloudresourcemanager</code> APIs:</p> <pre><code>gcloud services enable container.googleapis.com\ngcloud services enable compute.googleapis.com \ngcloud services enable cloudresourcemanager.googleapis.com\n</code></pre> <p>Note</p> <p>Enabling GCP Service APIs very important step in automation (e.g. terraform)</p> <p>Get a list of services that enabled in your project:</p> <pre><code>gcloud services list\n</code></pre> <p>Note</p> <p>Some services APIs enabled by default during project creation</p> <pre><code>gcloud config set compute/region us-central1\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE/#12-deleting-default-vpc-and-associated-firewall-rules-to-it","title":"1.2 Deleting Default VPC and associated Firewall Rules to it","text":"<p>Observe Asset Inventory in GCP UI:</p> <pre><code>Products -&gt; IAM &amp; Admin -&gt; Asset Inventory -&gt; Overview\n</code></pre> <p>Select resource type <code>compute.Subnetwork</code></p> <p>Result</p> <p>You can see that <code>vpc</code> default network spans across all GCP Regions, which for many companies will not be acceptable practice (e.g. GDPR)</p> <p>List all networks in a project:</p> <pre><code>gcloud compute networks list\n</code></pre> <p>Output:</p> <pre><code>NAME     SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4\ndefault  AUTO         REGIONAL\n</code></pre> <p>Review existing firewall rules for <code>default</code> vpc:</p> <pre><code>gcloud compute firewall-rules list\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>VPC Network-&gt;Firewall\n</code></pre> <p>Delete firewall rules associated with <code>default</code> vpc network:</p> <pre><code>gcloud compute firewall-rules delete default-allow-internal\ngcloud compute firewall-rules delete default-allow-ssh\ngcloud compute firewall-rules delete default-allow-rdp\ngcloud compute firewall-rules delete default-allow-icmp\n</code></pre> <p>Delete the <code>Default</code> network, following best practices:</p> <pre><code>gcloud compute networks delete default\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE/#13-creating-a-custom-mode-network-vpc","title":"1.3 Creating a custom mode network (VPC)","text":"<p>Task N1: Using reference doc: Creating a custom mode network. Create a new custom mode VPC network using gcloud command with following parameters:</p> <ul> <li>Network name: <code>$ORG-$PRODUCT-$ENV-vpc</code></li> <li>Subnet mode: <code>custom</code></li> <li>Bgp routing mode: <code>regional</code></li> <li>MTUs: <code>default</code></li> </ul> <p>Step 1: Create a new custom mode VPC network:</p> <p>with name <code>$ORG-$PRODUCT-$ENV-vpc</code>, with subnet mode <code>custom</code>, and <code>regional</code>.</p> <p>Step 1: Create a new custom mode VPC network:</p> <p>Set variables:</p> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\n</code></pre> <pre><code>TODO: gcloud create network\n</code></pre> <p>Review created network:</p> <pre><code>gcloud compute networks list\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>Networking-&gt;VPC Networks\n</code></pre> <p>Document a command to create <code>vpc</code> network in <code>production_gke.txt</code> doc under <code>step 1</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Important</p> <p>If <code>production_gke.txt</code> is not updated with commands this will reduce score for you assignment</p> <p>Step 2: Create firewall rules  <code>default-allow-ssh</code>:</p> <pre><code>gcloud compute firewall-rules create $ORG-$PRODUCT-$ENV-allow-tcp-ssh-icmp --network $ORG-$PRODUCT-$ENV-vpc --allow tcp:22,tcp:3389,icmp\n</code></pre> <p>Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/firewall-rules</p> <p>Review created firewall rules:</p> <pre><code>gcloud compute firewall-rules list\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>Networking-&gt;Firewalls\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE/#14-design-and-create-a-user-managed-subnet","title":"1.4 Design and create a <code>user-managed</code> subnet","text":"<p>After you've created VPC network, it is require to add subnet to it.</p> <p>Task N2: In order to ensure that GKE clusters in you organization doesn't overlap each other, design VPC subnet ranges for <code>dev</code>, <code>stg</code>, <code>prd</code> VPC-native GKE clusters, where</p> <ul> <li> <p>2 GKE Clusters per Project</p> </li> <li> <p>Max 200 <code>Nodes</code> per cluster and primary range belongs to Class A and starting from (10.130.0.0/x) </p> </li> <li> <p>Max 110 <code>Pods</code> per node and pod secondary ranges  belongs to Class A and starting from (10.0.0.0/x) </p> </li> <li> <p>Max 500 <code>Services</code> per cluster and service secondary ranges  belongs to Class A and starting from (10.100.0.0/x)</p> </li> <li> <p>CIDR range for master-ipv4-cidr (k8s api range) belongs to Class B (172.16.0.0/x)</p> </li> </ul> <p>Use following reference docs:</p> <ol> <li>VPC-native clusters</li> <li>GKE address management </li> </ol> <p>Using tables from VPC-native clusters document and online subnet calculator, create a table for <code>dev</code>, <code>stg</code>, <code>prd</code> in the following format and store result under <code>production_gke.txt</code>:</p> <pre><code>Project   | Subnet Name |     subnet          |    pod range     |    srv range   | kubectl api range\napp 1 Dev | dev-1       |     (10.130.0.0/x)  |     IP range     |     IP range   |     IP range    \n          | dev-2       |     (10.130.0.0/x)  |     IP range     |     IP range   |     IP range      \n</code></pre> <p>Document a subnet VPC design in <code>production_gke.txt</code> doc under <code>step 2</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Task N3: Create a <code>user-managed</code> subnet.</p> <p>Create a subnet for <code>dev</code> cluster, taking in consideration VPC subnet ranges created in above table, where:</p> <p>Subnet N1 for <code>GKE Standard Cluster</code>:</p> <ul> <li> <p>Subnet name: <code>gke-standard-$ORG-$PRODUCT-$ENV-subnet</code></p> </li> <li> <p>Region: <code>us-central1</code></p> </li> <li> <p>Node Range: See column <code>subnet</code> in above table for <code>dev-1</code> cluster</p> </li> <li> <p>Secondary Ranges:</p> <ul> <li> <p>Service range name: <code>gke-standard-services</code></p> </li> <li> <p>Service range CIDR: See column <code>srv range</code> in above table for <code>dev-1</code> cluster</p> </li> <li> <p>Pods range name: <code>gke-standard-pods</code></p> </li> <li> <p>Pods range CIDR: See column <code>pod range</code> in above table for <code>dev-1</code> cluster</p> </li> </ul> </li> <li> <p>Features:</p> <ul> <li>Flow Logs</li> <li>Private IP Google Access</li> </ul> </li> </ul> <p>Subnet N2 for <code>GKE Autopilot</code>:</p> <ul> <li>Subnet name: <code>gke-auto-$ORG-$PRODUCT-$ENV-subnet</code></li> <li>Region: <code>us-central1</code></li> <li>Node Range: See column <code>subnet</code> in above table for <code>dev-2</code> cluster</li> <li>Secondary Ranges:<ul> <li>Service range name: <code>gke-auto-services</code></li> <li>Service range CIDR: See column <code>srv range</code> in above table for <code>dev-2</code> cluster</li> <li>Pods range name: <code>gke-auto-pods</code></li> <li>Pods range CIDR: See column <code>pod range</code> in above table for <code>dev-2</code> cluster</li> </ul> </li> <li>Features:<ul> <li>Flow Logs</li> <li>Private IP Google Access</li> </ul> </li> </ul> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\n</code></pre> <pre><code>TODO: gcloud compute networks subnets create gke-standard-$ORG-$PRODUCT-$ENV-subnet\n\nTODO: gcloud compute networks subnets create gke-auto-$ORG-$PRODUCT-$ENV-subnet\n</code></pre> <p>Reference docs:</p> <ol> <li>Creating a private cluster with custom subnet</li> <li>VPC-native clusters</li> <li>Use <code>gcloud subnets create</code> command reference for all available options.</li> </ol> <p>Review created subnet:</p> <pre><code>gcloud compute networks subnets list\n</code></pre> <p>Also check in Google cloud UI:</p> <pre><code>Networking-&gt;VPC Networks -&gt; Click VPC network and check `Subnet` tab\n</code></pre> <p>Document a subnet VPC creation command in <code>production_gke.txt</code> doc under <code>step 3</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p>"},{"location":"020_Module_3_Assignment_Production_GKE/#15-creating-a-private-regional-and-vpc-native-gke-cluster","title":"1.5 Creating a Private, Regional and VPC Native GKE Cluster","text":"<p>Task N4: Create GKE Standard Cluster with  Private Nodes and following values:</p> <ul> <li>Cluster name: <code>$ORG-$PRODUCT-$ENV-standard-cluster</code></li> <li>VPC <code>network</code> name: <code>$ORG-$PRODUCT-$ENV-vpc</code></li> <li>VPC <code>subnet</code> name: <code>gke-standard-$ORG-$PRODUCT-$ENV-subnet</code></li> <li>Secondary <code>pod</code> range with name: <code>gke-standard-pods</code></li> <li>Secondary <code>service</code> range with name: <code>gke-standard-services</code></li> <li>VM size: <code>e2-micro</code></li> <li>Node count 1 per zone: <code>num-nodes</code></li> <li>GKE Control plane is replicated across three zones of a region: <code>us-central1</code></li> <li>GKE Release channel: <code>regular</code></li> <li>Enable Cilium based Networking DataplaneV2: <code>enable-dataplane-v2</code></li> <li>GKE version: \"1.22.12-gke.300\"</li> <li>Cluster Node Communication <code>VPC Native</code>: <code>enable-ip-alias</code></li> <li>Cluster Nodes access: Private Node GKE Cluster with Public API endpoint</li> <li>For <code>master-ipv4-cidr</code> ranges: See column <code>kubectl api range</code> in above table for <code>dev-1</code> cluster</li> </ul> <p>Use following reference docs:</p> <ol> <li>Creating a private cluster</li> <li>GKE Release Channels</li> <li>Use <code>gcloud container clusters create</code> command reference for all available options.</li> </ol> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\n</code></pre> <pre><code>#TODO gcloud clusters create $ORG-$PRODUCT-$ENV-cluster\n</code></pre> <p>Document a GKE cluster creation command in <code>production_gke.txt</code> doc under <code>step 4</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Result</p> <p>The private cluster is now created. <code>gcloud</code> has set up <code>kubectl</code> to authenticate with the private cluster but the cluster's K8s API server will only accept connections from the primary range of your subnetwork, and the secondary range of your subnetwork that is used for pods. That means nobody can access K8s API at this point of time, until we specify allowed ranges.</p> <p>Step 3 Authenticate to the cluster.</p> <pre><code>gcloud container clusters get-credentials $ORG-$PRODUCT-$ENV-standard-cluster --region us-central1 \n</code></pre> <p>Step 4: Connecting to a Private Cluster</p> <p>Let's try to connect to the cluster:</p> <pre><code>kubectl get pods\n</code></pre> <pre><code>Unable to connect to the server i/o timeout\n</code></pre> <p>Fail</p> <p>This fails because private clusters firewall traffic to the master by default. In order to connect to the cluster you need to make use of the master authorized networks feature. </p> <p>Step 4: Enable Master Authorized networks on you cluster</p> <p>Suppose you have a group of machines, outside of your VPC network, belongs to your organization. You could authorize ONLY those machines to access the public endpoint (Kubernetes API).</p> <p>Here we will enable master authorized networks and whitelist the IP address for our Cloud Shell instance, to allow access to the master:</p> <pre><code>gcloud container clusters update $ORG-$PRODUCT-$ENV-standard-cluster --enable-master-authorized-networks --master-authorized-networks $(curl ipinfo.io/ip)/32 --region us-central1\n</code></pre> <p>Now we can access the API server using <code>kubectl</code> from GCP console:</p> <p>Note</p> <p>In real life it could be CIDR Range for you company, so only engineers or CI/CD systems from you company can connect to <code>kubectl</code> apis in secure manner.</p> <p>Now we can access the API server using kubectl:</p> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>No resources found in default namespace.\n</code></pre> <p>Let's deploy basic application on the GKE Standard Cluster:</p> <pre><code>kubectl run hello-web --image=gcr.io/google-samples/hello-app:1.0 --port=8080\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME        READY   STATUS    RESTARTS   AGE\nhello-web   1/1     Running   0          7s\n</code></pre> <p>Result</p> <p>We can deploy Pods to our Private Cluster.</p> <pre><code>kubectl delete pod hello-web\n</code></pre> <p>Step 2: Testing Outbound Traffic</p> <p>Outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads.</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: wget\nspec:\n  containers:\n  - name: wget\n    image: alpine\n    command: ['wget', '-T', '5', 'http://www.example.com/']\n  restartPolicy: Never\nEOF\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME   READY   STATUS   RESTARTS   AGE\nwget   0/1     Error    0          4m41s\n</code></pre> <pre><code>kubectl logs wget\n</code></pre> <p>Output:</p> <pre><code>Connecting to www.example.com (93.184.216.34:80)\nwget: download timed out\nwget: bad address 'www.example.com'\n</code></pre> <p>Results</p> <p>Pods can't access internet, because it's a private cluster</p> <p>Note</p> <p>Normally, if cluster is Private and doesn't have Cloud Nat configured, users can't even deploy images from Docker Hub. See troubleshooting details here</p> <p>Delete test pod:</p> <pre><code>kubectl delete pods wget\n</code></pre>"},{"location":"020_Module_3_Assignment_Production_GKE/#16-create-a-google-cloud-nat","title":"1.6 Create a Google Cloud Nat","text":"<p>Private clusters give you the ability to isolate nodes from having inbound and outbound connectivity to the public internet. This isolation is achieved as the nodes have internal IP addresses only.</p> <p>If you want to provide outbound internet access for certain private nodes, you can use Cloud NAT</p> <p>Cloud NAT is a distributed, software-defined managed service. It's not based on proxy VMs or appliances.</p> <p>You configure a NAT gateway on a Cloud Router, which provides the control plane for NAT, holding configuration parameters that you specify. Google Cloud runs and maintains processes on the physical machines that run your Google Cloud VMs.</p> <p>Cloud NAT can be configured to automatically scale the number of NAT IP addresses that it uses, and it supports VMs that belong to managed instance groups, including those with autoscaling enabled.</p> <p>Step 1: Create a Cloud NAT configuration using Cloud Router</p> <p>Create Cloud Router in the same region as the instances that use Cloud NAT. Cloud NAT is only used to place NAT information onto the VMs. It is not used as part of the actual NAT gateway.</p> <p>Task N5: Create <code>Cloud Router</code></p> <p>Step 1a: Create a <code>Cloud Router</code>:</p> <pre><code>gcloud compute routers create gke-nat-router \\\n    --network $ORG-$PRODUCT-$ENV-vpc \\\n    --region us-central1\n</code></pre> <p>Verify created Cloud Router:</p> <pre><code>Networking -&gt; Hybrid Connectivity -&gt; Cloud Routers\n</code></pre> <p>Document <code>Cloud Router</code> creation  command in <code>production_gke.txt</code> doc under <code>step 5</code></p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Task N6: Create <code>Cloud Nat</code> Gateway</p> <p>Step 1b: Create a <code>Cloud Nat</code> Gateway:</p> <pre><code>gcloud compute routers nats create nat-config \\\n    --router-region us-central1 \\\n    --router gke-nat-router \\\n    --nat-all-subnet-ip-ranges \\\n    --auto-allocate-nat-external-ips\n</code></pre> <p>Verify created <code>Cloud Nat</code>:</p> <pre><code>Networking -&gt; Network Services -&gt; Cloud NAT\n</code></pre> <p>Note</p> <p>Cloud NAT uses Cloud Router only to group NAT configuration information (control plane). Cloud NAT does not direct a Cloud Router to use BGP or to add routes. NAT traffic does not pass through a Cloud Router (data plane).</p> <p>Document Cloud Nat creation  command in <code>production_gke.txt</code> doc under <code>step 6</code>:</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Step 2: Testing Outbound Traffic</p> <p>Most outbound traffic is not routable in private clusters so access to the internet is limited. This isolates pods that are running sensitive workloads.</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: wget\nspec:\n  containers:\n  - name: wget\n    image: alpine\n    command: ['wget', '-T', '5', 'http://www.example.com/']\n  restartPolicy: Never\nEOF\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME                              READY   STATUS      RESTARTS   AGE\nwget                              0/1     Completed   0          2m53s\n</code></pre> <pre><code>kubectl logs wget\n</code></pre> <p>Output:</p> <pre><code>Connecting to www.example.com (93.184.216.34:80)\nsaving to 'index.html'\nindex.html           100% |********************************|  1256  0:00:00 ETA\n'index.html' saved\n</code></pre> <p>Results</p> <p>Pods can access internet, thanks to our configured Cloud Nat.</p>"},{"location":"020_Module_3_Assignment_Production_GKE/#17-delete-default-node-pool-and-create-custom-node-pool","title":"1.7 Delete Default Node Pool and Create Custom Node Pool","text":"<p>A Kubernetes Engine cluster consists of a master and nodes. Kubernetes doesn't handle provisioning of nodes, so Google Kubernetes Engine handles this for you with a concept called node pools.</p> <p>A node pool is a subset of node instances within a cluster that all have the same configuration. They map to instance templates in Google Compute Engine, which provides the VMs used by the cluster. By default a Kubernetes Engine cluster has a single node pool, but you can add or remove them as you wish to change the shape of your cluster.</p> <p>In the previous example, you've created a Kubernetes Engine cluster. This gave us three nodes (three e2-micro (2 vCPUs, 1 GB memory), 100 GB of disk each) in a single node pool (called default-pool). Let's inspect the node pool:</p> <pre><code>gcloud config set compute/region us-central1\ngcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Output:</p> <pre><code>NAME: default-pool\nMACHINE_TYPE: e2-micro\nDISK_SIZE_GB: 100\nNODE_VERSION: 1.22.12-gke.300\n</code></pre> <p>If you want to add more nodes of this type, you can grow this node pool. If you want to add more nodes of a different type, you can add other node pools.</p> <p>A common method of moving a cluster to larger nodes is to add a new node pool, move the work from the old nodes to the new, and delete the old node pool.</p> <p>Let's add a second node pool. This time we will use the larger <code>e2-medium</code>  (2 vCPUs, 4 GB memory), 100 GB of disk each machine type.</p> <p>Task N7: Create Create Custom Node Pool and Delete Default Node Pool </p> <pre><code>gcloud container node-pools create custom-pool --cluster $ORG-$PRODUCT-$ENV-standard-cluster \\\n    --machine-type e2-medium --num-nodes 1\n</code></pre> <p>Output:</p> <pre><code>NAME: custom-pool\nMACHINE_TYPE: e2-medium\nDISK_SIZE_GB: 100\nNODE_VERSION: 1.22.12-gke.300\n</code></pre> <pre><code>gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Output:</p> <pre><code>NAME: default-pool\nMACHINE_TYPE: e2-micro\nDISK_SIZE_GB: 100\nNODE_VERSION: 1.22.12-gke.300\n\nNAME: custom-pool\nMACHINE_TYPE: e2-medium\nDISK_SIZE_GB: 100\nNODE_VERSION: 1.22.12-gke.300\n</code></pre> <p>You can now delete the original <code>default-pool</code> node pool:</p> <pre><code>gcloud container node-pools delete default-pool --cluster $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Check that node-pool has been deleted:</p> <pre><code>gcloud container node-pools list --cluster $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Result</p> <p>GKE cluster running only on <code>custom-pool</code> Node pool.</p>"},{"location":"020_Module_3_Assignment_Production_GKE/#2-creating-gke-autopilot-clusters","title":"2 Creating GKE Autopilot Clusters","text":"<p>In Part 2 of the Assignment we going to deploy GKE Autopilot Clusters for Production Usage. Autopilot Clusters provides easy way to configure and manage GKE Clusters. They configured with best practices and  high availability out of the box.  And can additionally configured with Private Clusters. </p> <p>Task N8: Create GKE Autopilot Cluster with following subnet ranges:</p> <ul> <li>Cluster name: <code>$ORG-$PRODUCT-$ENV-auto-cluster</code></li> <li>In <code>region</code>: <code>us-central1</code></li> <li>VPC <code>network</code> name: <code>$ORG-$PRODUCT-$ENV-vpc</code></li> <li>VPC <code>subnet</code> name: <code>gke-auto-$ORG-$PRODUCT-$ENV-subnet</code></li> <li>Secondary <code>pod</code> range with name: <code>gke-auto-pods</code></li> <li>Secondary <code>service</code> range with name: <code>gke-auto-services</code></li> </ul> <p>Use following reference docs:</p> <ol> <li>Creating a GKE Autopilot cluster</li> </ol> <pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\n</code></pre> <pre><code>#TODO gcloud container clusters create-auto $ORG-$PRODUCT-$ENV-auto-cluster\n</code></pre> <p>Result</p> <p>Production ready GKE Autopilot Cluster has been created</p> <p>Document a GKE Autopilot cluster creation command in <code>production_gke.txt</code> doc under <code>step 8</code>.</p> <pre><code>cd ~/$student_name-notepad/ycit020_module3\nedit production_gke.txt\n</code></pre> <p>Save and go to next step.</p> <p>Authenticate to GKE Autopilot cluster:</p> <pre><code>gcloud container clusters get-credentials $ORG-$PRODUCT-$ENV-auto-cluster --region us-central1 \n</code></pre> <p>GKE Autopilot starts with 2 small Nodes, that is not charged to Customers:</p> <pre><code>kubectl get nodes\n</code></pre> <pre><code>gk3-archy-notepad-gcloud-default-pool-5c17f5f3-lp4p   Ready    &lt;none&gt;   11m   v1.22.12-gke.300\ngk3-archy-notepad-gcloud-default-pool-fae5625e-cwxx   Ready    &lt;none&gt;   11m   v1.22.12-gke.300\n</code></pre> <p>This Nodes are used to run GKE system containers such as various agents and networking pods:</p> <pre><code>kubectl get pods -o wide --all-namespaces\n</code></pre> <p>Note</p> <p>With GKE Autopilot, you don't need to worry about nodes or nodepools at all. Autopilot will spin nodes up and down based on the resources needed by your deployments, but you will only be charged for the resources requested by your actual deployments.</p> <p>Let's deploy basic application on the GKE Standard Cluster:</p> <pre><code>kubectl run hello-web --image=gcr.io/google-samples/hello-app:1.0 --port=8080\n</code></pre> <p>Let's verify that Pod deployed:</p> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME        READY   STATUS    RESTARTS   AGE\nhello-web   0/1     Pending   0          9s\n</code></pre> <p>After about 1 munute you should see:</p> <pre><code>NAME        READY   STATUS              RESTARTS   AGE\nhello-web   0/1     ContainerCreating   0          101s\n</code></pre> <p>Note</p> <p>That the application takes longer than usual to start. This is because Autopilot NAP system starting off new Nodes for application.</p> <p>Let's verify that Pod deployment after few minutes:</p> <pre><code>kubectl get pods\n</code></pre> <p>Output:</p> <pre><code>NAME        READY   STATUS    RESTARTS   AGE\nhello-web   1/1     Running   0          7s\n</code></pre> <p>Let's verify that Nodes deployed:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Output:</p> <pre><code>NAME                                                  STATUS   ROLES    AGE     VERSION\ngk3-archy-notepad-gcloud-default-pool-5c17f5f3-lp4p   Ready    &lt;none&gt;   24m     v1.22.12-gke.300\ngk3-archy-notepad-gcloud-default-pool-5c17f5f3-t89j   Ready    &lt;none&gt;   5m35s   v1.22.12-gke.300\ngk3-archy-notepad-gcloud-default-pool-fae5625e-cwxx   Ready    &lt;none&gt;   24m     v1.22.12-gke.300\n</code></pre> <p>Result</p> <p>You will see 3d node has been created. GKE Autopilot created a Node based on workload requirement</p> <p>Note</p> <p>If you don't specify resource requests in your deployment spec, then Autopilot will set <code>CPU</code> to 2000m and <code>Memory</code> to 4gb. If your app requires less resources than that, make sure you set the resource request for your deployment. The minimum CPU is 250m and the minimum memory is 512mb.</p>"},{"location":"020_Module_3_Assignment_Production_GKE/#3-commit-terraform-configurations-to-repository-and-share-it-with-instructorteacher","title":"3 Commit Terraform configurations to repository and share it with Instructor/Teacher","text":"<p>Step 1 Commit <code>ycit020_module3</code> folder using the following Git commands:</p> <pre><code>cd ~/$student_name-notepad\n</code></pre> <pre><code>git add .\ngit commit -m \"Gcloud Documentation for Module 3 Assignment\"\n</code></pre> <p>Step 2 Push commit to the Cloud Source Repositories:</p> <pre><code>git push origin master\n</code></pre> <p>Result</p> <p>Your instructor will be able to review your code and grade it.</p>"},{"location":"020_Module_3_Assignment_Production_GKE/#4-cleanup","title":"4 Cleanup","text":"<pre><code>export ORG=$student_name\n</code></pre> <pre><code>export PRODUCT=notepad\nexport ENV=gcloud\nexport PROJECT_PREFIX=1   # Project has to be unique\nexport PROJECT_ID=$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX\n</code></pre> <p>Delete GKE standard Cluster:</p> <pre><code>gcloud container clusters delete $ORG-$PRODUCT-$ENV-standard-cluster\n</code></pre> <p>Delete GKE Autopilot Cluster:</p> <pre><code>gcloud container clusters delete $ORG-$PRODUCT-$ENV-auto-cluster\n</code></pre> <p>Important</p> <p>We are going to delete the <code>$ORG-$PRODUCT-$ENV-$PROJECT_PREFIX</code> only, please do not delete your project with Source Code Repo.</p> <pre><code>gcloud projects delete $ORG-$PRODUCT-$ENV-$PROJECT_PREFIX\n</code></pre>"}]}