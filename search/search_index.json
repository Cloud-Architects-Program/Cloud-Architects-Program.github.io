{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud Architecture \u00b6 Class Objectives: The power of containerization and its distinction from virtualization; features of the Linux kernel underpinning containerization; how to set up a Docker environment, build containers, and use an orchestration tool, namely, Kubernetes; additional tools for monitoring, sharing, and deploying applications.","title":"Home"},{"location":"#cloud-architecture","text":"Class Objectives: The power of containerization and its distinction from virtualization; features of the Linux kernel underpinning containerization; how to set up a Docker environment, build containers, and use an orchestration tool, namely, Kubernetes; additional tools for monitoring, sharing, and deploying applications.","title":"Cloud Architecture"},{"location":"Lab_2_Docker_basics/","text":"Lab 2 Docker basics Objective: Practice to run Docker containers 1 Docker basics \u00b6 1.1 Show running containers \u00b6 Step 1 Run docker ps to show running containers: docker ps Step 2 The output shows that there are no running containers at the moment. Use the command docker ps -a to list all containers including the ones has been stopped: docker ps -a Output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e6db2a24a8e hello-world \"/hello\" 15 minutes ago Exited (0) 15 min dreamy_nobel Review the collumns CONTAINER ID , STATUS , COMMAND , PORTS , NAMES . In the previous section we started one container and the command docker ps -a shows it as Exited . Note You can name your own containers with --name when you use docker run. If you do not provide a name, Docker will generate a random one like the one you have. Question Why Docker names are random? How docker containers named? Step 3 Let\u2019s run the command docker images to show all the images on your local system: docker images As you see, there is only one image that was downloaded from the Docker Hub. 1.2 Specify a container main process \u00b6 Step 1 Let\u2019s run our own \"hello world\" container. For that we will use the official Ubuntu image : docker run ubuntu /bin/echo 'Hello world' Output: Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:latest Hello world As you see, Docker downloaded the image ubuntu because it was not on the local machine. Step 2 Let\u2019s run the command docker images again: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Step 3 If you run the same \"hello world\" container again, Docker will use a local copy of the image: docker run ubuntu /bin/echo 'Hello world' Output: Hello world Question Compare Ubuntu Docker image with ISO image or with Cloud VM image. Why the size is so different ? Summary Pulling docker images from Docker Hub takes sometime. This time depends on: How large is the image? How fast is the network to Internet ? However, it is still much faster than booting traditional OS with Ubuntu on VM. If image already pulled on local host it takes fraction of a second to start a container. Running application in docker containers considered as a best practice for running CI/CD pipelines as it considerably faster than using VMs and reduce time for deploying a test environments. 1.3 Specify an image version \u00b6 Step 1 As you see, Docker has downloaded the ubuntu:latest image. You can see Ubuntu version by running the following command: docker run ubuntu /bin/cat /etc/issue.net Output: Ubuntu 16.04 LTS Let\u2019s say you need a previous Ubuntu LTS release. In this case, you can specify the version you need: docker run ubuntu:14.04 /bin/cat /etc/issue.net Output: Unable to find image 'ubuntu:14.04' locally 14.04: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:14.04 Ubuntu 14.04.4 LTS Step 2 The docker images command should show that we have 3 Ubuntu images downloaded locally: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB ubuntu 14.04 0ccb13bf1954 11 days ago 188 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Tip Running CI/CD pipeline with Docker using latest tag considered as a Bad Practice. Instead consider using: Versioning SHA tagging. 1.4 Run an interactive container \u00b6 Step 1 Let\u2019s use the ubuntu image to run an interactive bash session and inspect what is running inside our docker image. To achive that we going to use -i and -t flags. The -i is shorthand for --interactive , which instructs Docker to keep stdin open so that we can send commands to the sprocess. The -t flag is short for --tty and allocates a pseudo-TTY or terminal inside of the session. docker run -it ubuntu /bin/bash root@17d8bdeda98e:/# Result We get a bash shell prompt inside of the container. Note Bash prompt is not availabe for all docker images. Step 2 Let's print the system information of the latest Ubuntu image: root@17d8bdeda98e:/# uname -a Linux 17d8bdeda98e 3.19.0-31-generic ... Step 3 Let's verify what Ubuntu version is run by latest image of ubuntu: root@17d8bdeda98e:/# lsb_release -a bash: lsb_release: command not found Failure Why the standard Ubuntu command that checks version of OS is not working as expeced ? Step 4 Let's verify Ubuntu version using alternative way by checking /etc/lsb-release file. root@8cbcbd0fe8d2:/# cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.3 LTS\" Step 5 Let's compare the number of executable binaries availabe inside of the docker image versus Cloud VM that we running our class environment. First, run ls command on /bin and /usr/bin directories inside of the running ubuntu container as well as dpkg --list command that shows total number of installed packages: root@8cbcbd0fe8d2:/# ls /bin | wc -l 86 root@8cbcbd0fe8d2:/# ls /usr/bin | wc -l 233 root@eb11cd0b4106:/# dpkg --list | wc -l 101 Step 6 Use the exit command or press Ctrl-D to exit the interactive bash session back to Cloud VM. root@eb11cd0b4106:/# exit Step 7 Now run ls command on /bin and /usr/bin directories on Cloud VM that we using as our class environment: cca-user@userx-docker-vm:~$ ls /bin | wc -l 171 cca-user@userx-docker-vm:~$ ls /usr/bin | wc -l 660 cca-user@userx-docker-vm:~$ dpkg --list | wc -l 463 Result Official Docker container has much less binaries and packages installed vs Ubuntu Cloud Image. Summary Some of the use cases running docker containers in interactive mode are: Troubleshooting containerized applications Deploying and running containerized application on the existing production systems without affecting it. We've also learned that an official Docker \"minimal\" ubuntu image, does not include lsb_release command, as well as many other commands and packages that can be found in Official Ubuntu ISO image . The docker images are ment to contain only required core system commands and functions to make Images as light as possible. That say you can still install required packages using apt-get install , however this may increase size of docker image considerably. Hint While Docker Ubuntu image we used so far or Docker Centos image are very familiar to users and can be good starting point for learning docker containers. Using them in production or development considered as a Bad Practice. This is due those images still considered as heavy and potentially contain a lot more valnurabilities compare to specialized images. To reduce image pull time from docker hub and follow the best secuirity practices consider using specialized images that works well with you underlining code (Node image for NodeJS applications and etc.). Examples of specialized images are: Alpine Linux Node Atomic In fact, not so long ago all the official Docker Images in Docker-Hub has been moved to use Alpine Image . Step 8 Finally let\u2019s check that when the shell process has finished, the container stops: docker ps 1.5 Run a container in a background \u00b6 Now we know how to connect to running container and execute commands in it. However in most cases you just want run a container in a background so it can do a specific action. Step 1 Run a container in a background using the -d command line argument: docker run -d ubuntu /bin/sh -c \"while true; do date; echo hello world; sleep 1; done\" Result Command should return the container ID. Step 2 Let\u2019s use the docker ps command to see running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac231579e57f ubuntu \"/bin/sh -c 'while tr\" 1 minute ago Up 11 minute evil_golick Note Container id is going to be different in your case Hint Instead of using full container-id when building commands, it is possible simply type first few characters of container-id, to make things nice and easy. Step 3 Let\u2019s use container-id to show the container standard output: docker logs <container-id> Thu Jan 26 00:23:45 UTC 2017 hello world Thu Jan 26 00:23:46 UTC 2017 hello world Thu Jan 26 00:23:47 UTC 2017 hello world ... As you can see, in the docker ps command output, the auto generated container name is evil_golick (your container can have a different name). Step 4 Now, instead of using docker contaier-id use container name to show the container standard output: docker logs <name> Thu Jan 26 00:23:51 UTC 2017 hello world Thu Jan 26 00:23:52 UTC 2017 hello world Thu Jan 26 00:23:53 UTC 2017 hello world ... Step 5 Finally, let\u2019s stop our container: docker stop <name> Step 6 Check, that there are no running containers: docker ps Summary docker logs is a very usefull command to troubleshoot containers, and going to be used very often both for Docker and Kubernertes troubleshooting. 1.6 Accessing Containers from the Internet \u00b6 Step 1 Let\u2019s run a simple web application. We will use the existing image training/webapp, which contains a Python Flask application: docker run -d -P training/webapp python app.py ... Status: Downloaded newer image for training/webapp:latest 6e88f42d3d853762edcbfe1fe73fdc5c48865275bc6df759b83b0939d5bd2456 In the command above we specified the main process (python app.py), the -d command line argument, which tells Docker to run the container in the background. The -P command line argument tells Docker to map any required network ports inside our container to our host. This allows us to access the web application in the container. Step 2 Use the docker ps command to list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e88f42d3d85 training/webapp \"python app.py\" 3 minutes ago Up 3 minutes 0.0.0.0:32768->5000/tcp determined_torvalds The PORTS column contains the mapped ports. In our case, Docker has exposed port 5000 (the default Python Flask port) on port 32768 (can be different in your case). Step 3 The docker port command shows the exposed port. We will use the container name (determined_torvalds in the example above, it can be different in your case): docker port <name> 5000 0.0.0.0:32768 Step 4 Let\u2019s check that we can access the web application exposed port: curl http://localhost:<port>/ Result Hello world! Step 5 Let\u2019s stop our web application for now: docker stop <name> Step 6 We want to manually specify the local port to expose (-p argument). Let\u2019s use the standard HTTP port 80. We also want to specify the container name (--name argument): docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 7 Let\u2019s check that the port 80 is exposed: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp curl http://localhost/ Result `Hello world!`` Step 8 You can also observe Hello world! webapp from you laptop, for that you need to use you public VM IP that can be gather from VMs list: Your_VM_Public_IP Than paste VM Public IP address in you browser. Result Our web-app can be accessed from Internet! 1.7 Restart a container \u00b6 Step 1 Let\u2019s stop the container with web application: docker stop webapp The main process inside of the container will receive SIGTERM, and after a grace period, SIGKILL. Step 2 You can start the container later using the docker start command: docker start webapp Step 3 Check that the web application works: curl http://localhost/ Hello world! Step 4 You also can restart the running container using the docker restart command. docker restart webapp Step 4 Run docker ps command and check STATUS field: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS 6e400179070f training/webapp \"python app.py\" 25 minutes ago Up 3 seconds 1.8 Ensuring Container Uptime \u00b6 Docker considers any containers to exit with a non-zero exit code to have crashed. By default a crashed container will remain stopped. Step 1 Start the container that outputs a message and then exits with code 1 to simulate a crash. docker run -d --name restart-default scrapbook/docker-restart-example docker ps -a | grep restart-default CONTAINER ID IMAGE CREATED STATUS NAMES c854289d2f39 scrapbook/docker-restart-example 5 seconds ago Exited 3 sec ago restart-default $ docker logs restart-default Sun Sep 17 20:34:55 UTC 2017 Booting up... Result Container crushed and exited. However, there are several ways to ensure that you container up and running even if it\u2019s restarts. Step 2 The option --restart=on-failure : allows you to say how many times Docker should try again: docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example docker logs restart-3 Thu Apr 20 14:01:27 UTC 2017 Booting up... Thu Apr 20 14:01:28 UTC 2017 Booting up... Thu Apr 20 14:01:29 UTC 2017 Booting up... Thu Apr 20 14:01:31 UTC 2017 Booting up... Step 3 Finally, Docker can always restart a failed container. In this case, Docker will keep trying until the container is explicitly told to stop. docker run -d --name restart-always --restart=always scrapbook/docker-restart-example docker logs restart-always Step 4 After sometime stop running docker container, as it will be keep failing and starting again: docker stop restart-always 1.9 Inspect a container \u00b6 Step 1 You can use the docker inspect command to see the configuration and status information for the specified container: docker inspect webapp [ { \"Id\": \"249476631f7d...\", \"Created\": \"2016-08-02T23:42:56.932135327Z\", \"Path\": \"python\", \"Args\": [ \"app.py\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 16055, \"ExitCode\": 0, \"Error\": \"\", ... Step 2 You can specify a filter (-f command line argument) to show only specific elements. For example: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp 172.17.0.2 The command returns the IP address of the container. 1.10 Interacting with containers \u00b6 In some cases using docker log is not enough to undertand issues and you want to login inside of running VM. Also sometimes you package you applicaiton and in order to run it you need to login inside of container and execute and leave it running in background. Below provded few ways to interacting with containers that can help to achive descrined use cases. 1.10.1 Detach from Interactive container \u00b6 In Module, 1.4 Run an interactive container we run an Ubuntu container with -it flag and able directly login inside of the container to interact with it, however after we exited contianer using Ctrl-D or exit command container stopped. However you can exit from Interactive mode without stoping a container. Let's demonstrate how this works: Step 1 Start Ubunu container in interactive mode: docker run -it ubuntu /bin/bash Step 2 Run watch date command inside running container in order to exit date command every 2 seconds. root@1d688a9f4ed4:/# watch date Step 3 Detach from a container and leave it running using the CTRL-p CTRL-q key sequence. Step 4 Verify that Ubuntu container is still running: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS NAMES 1d688a9f4ed4 ubuntu \"/bin/bash\" 1 minutes ago Up 1 minutes admiring_lovelace Result Great you were able to detach from Docker container without stopping it, while it is executing a process in it. What about attaching back to container ? Important CTRL-p CTRL-q sequence key only works if docker contaienr started with -it command! 1.11.2 Attach to a container \u00b6 Now let's get back and attach to our running Ubuntu image. For that docker provides docker attach command. docker attach <container name> Every 2.0s: date Mon Sep 18 00:08:57 2017 Summary docker attach attaches your contairs terminal\u2019s standard input, output, and error (or any combination of the 3) to a running container. This allows you to view its ongoing output or to control it interactively, as though the commands were running directly in your terminal. 1.11.3 Execute a process in a container \u00b6 Step 1 Let verify if webapp container is still running docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp If not running start it with following command: $ docker run -d -p 80:5000 --name webapp training/webapp python app.py other wise skip to next step . Step 2 Use the docker exec command to execute a command in the running container. For example: docker exec webapp ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 26 0.0 0.0 15572 2104 ? Rs 00:12 0:00 ps aux The same command with the -it command line argument can be used to run an interactive session in the container: docker exec -it webapp bash root@249476631f7d:/opt/webapp# ps auxw ps auxw USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 32 0.0 0.0 18144 3064 ? Ss 00:14 0:00 bash root 47 0.0 0.0 15572 2076 ? R+ 00:16 0:00 ps auxw Step 2 Use the exit command or press Ctrl-D to exit the interactive bash session: root@249476631f7d:/opt/webapp# exit Summary docker exec is one of the most usefull docker commands used for troubleshooting containers. 1.12 Copy files to/from container \u00b6 The docker cp command allows you to copy files from the container to the local machine or from the local file system to the container. This command works for a running or stopped container. Step 1 Let\u2019s copy the container\u2019s app.py file to the local machine: docker cp webapp:/opt/webapp/app.py . Step 2 Edit the local app.py file. For example, change the line return 'Hello '+provider+'!' to return 'Hello '+provider+'!!!'. Copy the modified file back and restart the container: docker cp app.py webapp:/opt/webapp/ docker restart webapp Step 3 Check that the modified web application works:: curl http://localhost/ Result `Hello world!!!`` 1.12 Remove containers \u00b6 Now let's clean up the environment and at the same time learn how delete containers. Step 1 First list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 81c4c66baaf9 training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp Step 2 Than try to delete running container using docker rm <container_id> docker rm $container_id Error response from daemon: You cannot remove a running container 81c4c66baaf9. Stop the container before attempting removal or force remove. Failure Docker containers needs to be first stopped or deleted using --force flag. docker rm $container_id -f Alternatively, you can run stop and rm in sequence: docker stop 81c4c66baaf9 docker rm 81c4c66baaf9 Summary We've learned a lot of docker commands which are very handy to know both when using Docker and Kubernetes. In the next Module we going to deep dive in to details of how networking and storage works in Docker.","title":"Lab 2 Docker Basics"},{"location":"Lab_2_Docker_basics/#1-docker-basics","text":"","title":"1 Docker basics"},{"location":"Lab_2_Docker_basics/#11-show-running-containers","text":"Step 1 Run docker ps to show running containers: docker ps Step 2 The output shows that there are no running containers at the moment. Use the command docker ps -a to list all containers including the ones has been stopped: docker ps -a Output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e6db2a24a8e hello-world \"/hello\" 15 minutes ago Exited (0) 15 min dreamy_nobel Review the collumns CONTAINER ID , STATUS , COMMAND , PORTS , NAMES . In the previous section we started one container and the command docker ps -a shows it as Exited . Note You can name your own containers with --name when you use docker run. If you do not provide a name, Docker will generate a random one like the one you have. Question Why Docker names are random? How docker containers named? Step 3 Let\u2019s run the command docker images to show all the images on your local system: docker images As you see, there is only one image that was downloaded from the Docker Hub.","title":"1.1 Show running containers"},{"location":"Lab_2_Docker_basics/#12-specify-a-container-main-process","text":"Step 1 Let\u2019s run our own \"hello world\" container. For that we will use the official Ubuntu image : docker run ubuntu /bin/echo 'Hello world' Output: Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:latest Hello world As you see, Docker downloaded the image ubuntu because it was not on the local machine. Step 2 Let\u2019s run the command docker images again: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Step 3 If you run the same \"hello world\" container again, Docker will use a local copy of the image: docker run ubuntu /bin/echo 'Hello world' Output: Hello world Question Compare Ubuntu Docker image with ISO image or with Cloud VM image. Why the size is so different ? Summary Pulling docker images from Docker Hub takes sometime. This time depends on: How large is the image? How fast is the network to Internet ? However, it is still much faster than booting traditional OS with Ubuntu on VM. If image already pulled on local host it takes fraction of a second to start a container. Running application in docker containers considered as a best practice for running CI/CD pipelines as it considerably faster than using VMs and reduce time for deploying a test environments.","title":"1.2 Specify a container main process"},{"location":"Lab_2_Docker_basics/#13-specify-an-image-version","text":"Step 1 As you see, Docker has downloaded the ubuntu:latest image. You can see Ubuntu version by running the following command: docker run ubuntu /bin/cat /etc/issue.net Output: Ubuntu 16.04 LTS Let\u2019s say you need a previous Ubuntu LTS release. In this case, you can specify the version you need: docker run ubuntu:14.04 /bin/cat /etc/issue.net Output: Unable to find image 'ubuntu:14.04' locally 14.04: Pulling from library/ubuntu ... Status: Downloaded newer image for ubuntu:14.04 Ubuntu 14.04.4 LTS Step 2 The docker images command should show that we have 3 Ubuntu images downloaded locally: docker images Output: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 42118e3df429 11 days ago 124.8 MB ubuntu 14.04 0ccb13bf1954 11 days ago 188 MB hello-world latest c54a2cc56cbb 4 weeks ago 1.848 kB Tip Running CI/CD pipeline with Docker using latest tag considered as a Bad Practice. Instead consider using: Versioning SHA tagging.","title":"1.3 Specify an image version"},{"location":"Lab_2_Docker_basics/#14-run-an-interactive-container","text":"Step 1 Let\u2019s use the ubuntu image to run an interactive bash session and inspect what is running inside our docker image. To achive that we going to use -i and -t flags. The -i is shorthand for --interactive , which instructs Docker to keep stdin open so that we can send commands to the sprocess. The -t flag is short for --tty and allocates a pseudo-TTY or terminal inside of the session. docker run -it ubuntu /bin/bash root@17d8bdeda98e:/# Result We get a bash shell prompt inside of the container. Note Bash prompt is not availabe for all docker images. Step 2 Let's print the system information of the latest Ubuntu image: root@17d8bdeda98e:/# uname -a Linux 17d8bdeda98e 3.19.0-31-generic ... Step 3 Let's verify what Ubuntu version is run by latest image of ubuntu: root@17d8bdeda98e:/# lsb_release -a bash: lsb_release: command not found Failure Why the standard Ubuntu command that checks version of OS is not working as expeced ? Step 4 Let's verify Ubuntu version using alternative way by checking /etc/lsb-release file. root@8cbcbd0fe8d2:/# cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.3 LTS\" Step 5 Let's compare the number of executable binaries availabe inside of the docker image versus Cloud VM that we running our class environment. First, run ls command on /bin and /usr/bin directories inside of the running ubuntu container as well as dpkg --list command that shows total number of installed packages: root@8cbcbd0fe8d2:/# ls /bin | wc -l 86 root@8cbcbd0fe8d2:/# ls /usr/bin | wc -l 233 root@eb11cd0b4106:/# dpkg --list | wc -l 101 Step 6 Use the exit command or press Ctrl-D to exit the interactive bash session back to Cloud VM. root@eb11cd0b4106:/# exit Step 7 Now run ls command on /bin and /usr/bin directories on Cloud VM that we using as our class environment: cca-user@userx-docker-vm:~$ ls /bin | wc -l 171 cca-user@userx-docker-vm:~$ ls /usr/bin | wc -l 660 cca-user@userx-docker-vm:~$ dpkg --list | wc -l 463 Result Official Docker container has much less binaries and packages installed vs Ubuntu Cloud Image. Summary Some of the use cases running docker containers in interactive mode are: Troubleshooting containerized applications Deploying and running containerized application on the existing production systems without affecting it. We've also learned that an official Docker \"minimal\" ubuntu image, does not include lsb_release command, as well as many other commands and packages that can be found in Official Ubuntu ISO image . The docker images are ment to contain only required core system commands and functions to make Images as light as possible. That say you can still install required packages using apt-get install , however this may increase size of docker image considerably. Hint While Docker Ubuntu image we used so far or Docker Centos image are very familiar to users and can be good starting point for learning docker containers. Using them in production or development considered as a Bad Practice. This is due those images still considered as heavy and potentially contain a lot more valnurabilities compare to specialized images. To reduce image pull time from docker hub and follow the best secuirity practices consider using specialized images that works well with you underlining code (Node image for NodeJS applications and etc.). Examples of specialized images are: Alpine Linux Node Atomic In fact, not so long ago all the official Docker Images in Docker-Hub has been moved to use Alpine Image . Step 8 Finally let\u2019s check that when the shell process has finished, the container stops: docker ps","title":"1.4 Run an interactive container"},{"location":"Lab_2_Docker_basics/#15-run-a-container-in-a-background","text":"Now we know how to connect to running container and execute commands in it. However in most cases you just want run a container in a background so it can do a specific action. Step 1 Run a container in a background using the -d command line argument: docker run -d ubuntu /bin/sh -c \"while true; do date; echo hello world; sleep 1; done\" Result Command should return the container ID. Step 2 Let\u2019s use the docker ps command to see running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac231579e57f ubuntu \"/bin/sh -c 'while tr\" 1 minute ago Up 11 minute evil_golick Note Container id is going to be different in your case Hint Instead of using full container-id when building commands, it is possible simply type first few characters of container-id, to make things nice and easy. Step 3 Let\u2019s use container-id to show the container standard output: docker logs <container-id> Thu Jan 26 00:23:45 UTC 2017 hello world Thu Jan 26 00:23:46 UTC 2017 hello world Thu Jan 26 00:23:47 UTC 2017 hello world ... As you can see, in the docker ps command output, the auto generated container name is evil_golick (your container can have a different name). Step 4 Now, instead of using docker contaier-id use container name to show the container standard output: docker logs <name> Thu Jan 26 00:23:51 UTC 2017 hello world Thu Jan 26 00:23:52 UTC 2017 hello world Thu Jan 26 00:23:53 UTC 2017 hello world ... Step 5 Finally, let\u2019s stop our container: docker stop <name> Step 6 Check, that there are no running containers: docker ps Summary docker logs is a very usefull command to troubleshoot containers, and going to be used very often both for Docker and Kubernertes troubleshooting.","title":"1.5 Run a container in a background"},{"location":"Lab_2_Docker_basics/#16-accessing-containers-from-the-internet","text":"Step 1 Let\u2019s run a simple web application. We will use the existing image training/webapp, which contains a Python Flask application: docker run -d -P training/webapp python app.py ... Status: Downloaded newer image for training/webapp:latest 6e88f42d3d853762edcbfe1fe73fdc5c48865275bc6df759b83b0939d5bd2456 In the command above we specified the main process (python app.py), the -d command line argument, which tells Docker to run the container in the background. The -P command line argument tells Docker to map any required network ports inside our container to our host. This allows us to access the web application in the container. Step 2 Use the docker ps command to list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e88f42d3d85 training/webapp \"python app.py\" 3 minutes ago Up 3 minutes 0.0.0.0:32768->5000/tcp determined_torvalds The PORTS column contains the mapped ports. In our case, Docker has exposed port 5000 (the default Python Flask port) on port 32768 (can be different in your case). Step 3 The docker port command shows the exposed port. We will use the container name (determined_torvalds in the example above, it can be different in your case): docker port <name> 5000 0.0.0.0:32768 Step 4 Let\u2019s check that we can access the web application exposed port: curl http://localhost:<port>/ Result Hello world! Step 5 Let\u2019s stop our web application for now: docker stop <name> Step 6 We want to manually specify the local port to expose (-p argument). Let\u2019s use the standard HTTP port 80. We also want to specify the container name (--name argument): docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 7 Let\u2019s check that the port 80 is exposed: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp curl http://localhost/ Result `Hello world!`` Step 8 You can also observe Hello world! webapp from you laptop, for that you need to use you public VM IP that can be gather from VMs list: Your_VM_Public_IP Than paste VM Public IP address in you browser. Result Our web-app can be accessed from Internet!","title":"1.6 Accessing Containers from the Internet"},{"location":"Lab_2_Docker_basics/#17-restart-a-container","text":"Step 1 Let\u2019s stop the container with web application: docker stop webapp The main process inside of the container will receive SIGTERM, and after a grace period, SIGKILL. Step 2 You can start the container later using the docker start command: docker start webapp Step 3 Check that the web application works: curl http://localhost/ Hello world! Step 4 You also can restart the running container using the docker restart command. docker restart webapp Step 4 Run docker ps command and check STATUS field: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS 6e400179070f training/webapp \"python app.py\" 25 minutes ago Up 3 seconds","title":"1.7 Restart a container"},{"location":"Lab_2_Docker_basics/#18-ensuring-container-uptime","text":"Docker considers any containers to exit with a non-zero exit code to have crashed. By default a crashed container will remain stopped. Step 1 Start the container that outputs a message and then exits with code 1 to simulate a crash. docker run -d --name restart-default scrapbook/docker-restart-example docker ps -a | grep restart-default CONTAINER ID IMAGE CREATED STATUS NAMES c854289d2f39 scrapbook/docker-restart-example 5 seconds ago Exited 3 sec ago restart-default $ docker logs restart-default Sun Sep 17 20:34:55 UTC 2017 Booting up... Result Container crushed and exited. However, there are several ways to ensure that you container up and running even if it\u2019s restarts. Step 2 The option --restart=on-failure : allows you to say how many times Docker should try again: docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example docker logs restart-3 Thu Apr 20 14:01:27 UTC 2017 Booting up... Thu Apr 20 14:01:28 UTC 2017 Booting up... Thu Apr 20 14:01:29 UTC 2017 Booting up... Thu Apr 20 14:01:31 UTC 2017 Booting up... Step 3 Finally, Docker can always restart a failed container. In this case, Docker will keep trying until the container is explicitly told to stop. docker run -d --name restart-always --restart=always scrapbook/docker-restart-example docker logs restart-always Step 4 After sometime stop running docker container, as it will be keep failing and starting again: docker stop restart-always","title":"1.8 Ensuring Container Uptime"},{"location":"Lab_2_Docker_basics/#19-inspect-a-container","text":"Step 1 You can use the docker inspect command to see the configuration and status information for the specified container: docker inspect webapp [ { \"Id\": \"249476631f7d...\", \"Created\": \"2016-08-02T23:42:56.932135327Z\", \"Path\": \"python\", \"Args\": [ \"app.py\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 16055, \"ExitCode\": 0, \"Error\": \"\", ... Step 2 You can specify a filter (-f command line argument) to show only specific elements. For example: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp 172.17.0.2 The command returns the IP address of the container.","title":"1.9 Inspect a container"},{"location":"Lab_2_Docker_basics/#110-interacting-with-containers","text":"In some cases using docker log is not enough to undertand issues and you want to login inside of running VM. Also sometimes you package you applicaiton and in order to run it you need to login inside of container and execute and leave it running in background. Below provded few ways to interacting with containers that can help to achive descrined use cases.","title":"1.10 Interacting with containers"},{"location":"Lab_2_Docker_basics/#1101-detach-from-interactive-container","text":"In Module, 1.4 Run an interactive container we run an Ubuntu container with -it flag and able directly login inside of the container to interact with it, however after we exited contianer using Ctrl-D or exit command container stopped. However you can exit from Interactive mode without stoping a container. Let's demonstrate how this works: Step 1 Start Ubunu container in interactive mode: docker run -it ubuntu /bin/bash Step 2 Run watch date command inside running container in order to exit date command every 2 seconds. root@1d688a9f4ed4:/# watch date Step 3 Detach from a container and leave it running using the CTRL-p CTRL-q key sequence. Step 4 Verify that Ubuntu container is still running: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS NAMES 1d688a9f4ed4 ubuntu \"/bin/bash\" 1 minutes ago Up 1 minutes admiring_lovelace Result Great you were able to detach from Docker container without stopping it, while it is executing a process in it. What about attaching back to container ? Important CTRL-p CTRL-q sequence key only works if docker contaienr started with -it command!","title":"1.10.1 Detach from Interactive container"},{"location":"Lab_2_Docker_basics/#1112-attach-to-a-container","text":"Now let's get back and attach to our running Ubuntu image. For that docker provides docker attach command. docker attach <container name> Every 2.0s: date Mon Sep 18 00:08:57 2017 Summary docker attach attaches your contairs terminal\u2019s standard input, output, and error (or any combination of the 3) to a running container. This allows you to view its ongoing output or to control it interactively, as though the commands were running directly in your terminal.","title":"1.11.2 Attach to a container"},{"location":"Lab_2_Docker_basics/#1113-execute-a-process-in-a-container","text":"Step 1 Let verify if webapp container is still running docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 249476631f7d training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp If not running start it with following command: $ docker run -d -p 80:5000 --name webapp training/webapp python app.py other wise skip to next step . Step 2 Use the docker exec command to execute a command in the running container. For example: docker exec webapp ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 26 0.0 0.0 15572 2104 ? Rs 00:12 0:00 ps aux The same command with the -it command line argument can be used to run an interactive session in the container: docker exec -it webapp bash root@249476631f7d:/opt/webapp# ps auxw ps auxw USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 52320 17384 ? Ss 00:11 0:00 python app.py root 32 0.0 0.0 18144 3064 ? Ss 00:14 0:00 bash root 47 0.0 0.0 15572 2076 ? R+ 00:16 0:00 ps auxw Step 2 Use the exit command or press Ctrl-D to exit the interactive bash session: root@249476631f7d:/opt/webapp# exit Summary docker exec is one of the most usefull docker commands used for troubleshooting containers.","title":"1.11.3 Execute a process in a container"},{"location":"Lab_2_Docker_basics/#112-copy-files-tofrom-container","text":"The docker cp command allows you to copy files from the container to the local machine or from the local file system to the container. This command works for a running or stopped container. Step 1 Let\u2019s copy the container\u2019s app.py file to the local machine: docker cp webapp:/opt/webapp/app.py . Step 2 Edit the local app.py file. For example, change the line return 'Hello '+provider+'!' to return 'Hello '+provider+'!!!'. Copy the modified file back and restart the container: docker cp app.py webapp:/opt/webapp/ docker restart webapp Step 3 Check that the modified web application works:: curl http://localhost/ Result `Hello world!!!``","title":"1.12 Copy files to/from container"},{"location":"Lab_2_Docker_basics/#112-remove-containers","text":"Now let's clean up the environment and at the same time learn how delete containers. Step 1 First list running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 81c4c66baaf9 training/webapp \"python app.py\" 1 minute ago Up 1 minute 0.0.0.0:80->5000/tcp webapp Step 2 Than try to delete running container using docker rm <container_id> docker rm $container_id Error response from daemon: You cannot remove a running container 81c4c66baaf9. Stop the container before attempting removal or force remove. Failure Docker containers needs to be first stopped or deleted using --force flag. docker rm $container_id -f Alternatively, you can run stop and rm in sequence: docker stop 81c4c66baaf9 docker rm 81c4c66baaf9 Summary We've learned a lot of docker commands which are very handy to know both when using Docker and Kubernetes. In the next Module we going to deep dive in to details of how networking and storage works in Docker.","title":"1.12 Remove containers"},{"location":"Lab_3_Advanced_Docker/","text":"Lab 3 Docker Networking, Persistence, Monitoring and Logging Objective: Networks Docker basics User-defined private Networks Persistence Data Volumes 1 Docker Networking \u00b6 1.1 Docker Networking Basics \u00b6 Step 1: The Docker Network Command The docker network command is the main command for configuring and managing container networks. Run the docker network command from the first terminal. docker network Usage: docker network COMMAND Manage networks Options: --help Print usage Commands: connect Connect a container to a network create Create a network disconnect Disconnect a container from a network inspect Display detailed information on one or more networks ls List networks prune Remove all unused networks rm Remove one or more networks Run 'docker network COMMAND --help' for more information on a command. The command output shows how to use the command as well as all of the docker network sub-commands. As you can see from the output, the docker network command allows you to create new networks, list existing networks, inspect networks, and remove networks. It also allows you to connect and disconnect containers from networks. Step 2 Run a docker network ls command to view existing container networks on the current Docker host. docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local The output above shows the container networks that are created as part of a standard installation of Docker. New networks that you create will also show up in the output of the docker network ls command. You can see that each network gets a unique ID and NAME . Each network is also associated with a single driver. Notice that the \"bridge\" network and the \"host\" network have the same name as their respective drivers. Step 3: The docker network inspect command is used to view network configuration details. These details include; name, ID, driver, IPAM driver, subnet info, connected containers, and more. Use docker network inspect <network> to view configuration details of the container networks on your Docker host. The command below shows the details of the network called bridge . docker network inspect bridge [ { \"Name\": \"bridge\", \"Id\": \"3430ad6f20bf1486df2e5f64ddc93cc4ff95d81f59b6baea8a510ad500df2e57\", \"Created\": \"2017-04-03T16:49:58.6536278Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Containers\": {}, \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" }, \"Labels\": {} } ] Note The syntax of the docker network inspect command is docker network inspect <network> , where <network> can be either network name or network ID. In the example above we are showing the configuration details for the network called \"bridge\". Do not confuse this with the \"bridge\" driver. Step 4 Now, list Docker supported network driver plugins. For that run docker info command, that shows a lot of interesting information about a Docker installation. Run the docker info command and locate the list of network plugins. docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ee-3 Storage Driver: aufs <Snip> Plugins: Volume: local Network: bridge host macvlan null overlay Swarm: inactive Runtimes: runc <Snip> The output above shows the bridge , host , macvlan , null , and overlay drivers. Summary We've quickly reviewed available docker networking commands as well as found what drivers current docker setup supports. 1.2 Default bridge network \u00b6 Every clean installation of Docker comes with a pre-built network called Default bridge network . Let's explore in more details how it works. Step 1 Verify this with the docker network ls . docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local Result The output above shows that the bridge network is associated with the bridge driver. It's important to note that the network and the driver are connected, but they are not the same. In this example the network and the driver have the same name - but they are not the same thing! The output above also shows that the bridge network is scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the bridge driver - the bridge driver provides single-host networking. All networks created with the bridge driver are based on a Linux bridge (a.k.a. a virtual switch). Step 5 Start webapp in Default bridge network docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 6 Check that the webapp and db containers are running: Command: docker ps 1.3 User-defined Private Networks \u00b6 So far we\u2019ve learned how Docker networking works with Docker default bridge network . With the introduction of user-defined networking in Docker 1.9, it is now possible to create multiple Docker bridges to allow network segregation within the same host or multi-host networking to allow communicate Docker containers between hosts. The commands are available through the Docker Engine CLI are: docker network create docker network connect docker network ls docker network rm docker network disconnect docker network inspect Let's demonstrate how to create a custom bridge network. Step 1 By default, Docker runs containers in the bridge network. You may want to isolate one or more containers in a separate network. Let\u2019s create a new network: docker network create my-network \\ -d bridge \\ --subnet 172.19.0.0/16 The -d bridge command line argument specifies the bridge network driver and the --subnet command line argument specifies the network segment in CIDR format. If you do not specify a subnet when creating a network, then Docker assigns a subnet automatically, so it is a good idea to specify a subnet to avoid potential conflicts with the existing networks. Below are some other options that are available with the bridge Driver: com.docker.network.bridge.enable_ip_masquerade: This instructs the Docker host to hide or masquerade all containers in this network behind the Docker host's interfaces if the container attempts to route off the local host . com.docker.network.bridge.name: This is the name you wish to give to the bridge. com.docker.network.bridge.enable_icc: This turns on or off Inter-Container Connectivity (ICC) mode for the bridge. com.docker.network.bridge.host_binding_ipv4: This defines the host interface that should be used for port binding. com.docker.network.driver.mtu: This sets MTU for containers attached to this bridge. Step 2 To check that the new network is created, execute docker network ls: docker network ls NETWORK ID NAME DRIVER SCOPE d428e49e4869 bridge bridge local 0d1f78528cc5 host host local 56ef0481820d my-network bridge local 4a07cef84617 none null local Step 3 Let\u2019s inspect the new network: docker network inspect my-network [ { \"Name\": \"my-network\", \"Id\": \"56ef0481820d...\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"172.19.0.0/16\" } ] }, \"Internal\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ] Step 4 As expected, there are no containers connected to the my-network. Let\u2019s recreate the db container in the my-network: docker rm -f db docker run -d --network=my-network --name db training/postgres Step 5 Inspect the my-network again: docker network inspect my-network Output: \"Containers\": { \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... As you see, the db container is connected to the my-network and has 172.19.0.2 address. Step 6 Let\u2019s start an interactive session in the db container and ping the IP address of the webapp again: Note Quick reminder how to locate webapp ip: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp docker exec -it db bash Once inside of container run: root@c3afff20019a:/# ping -c 1 172.17.0.3 PING 172.17.0.3 (172.17.0.3) 56(84) bytes of data. --- 172.17.0.3 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms As expected, the webapp container is no longer accessible from the db container, because they are connected to different networks. Summary Using Multi-host networking provides network isolation within a Docker host via network namepsaces. This is can be used if you want to deploy different applications on same host for isolation or resource duplicate prevention. Step 7 Let\u2019s connect the webapp container to the my-network: docker network connect my-network webapp Step 8 Check that the webapp container now is connected to the my-network: docker network inspect my-network Output: ... \"Containers\": { \"62ed4a627356...\": { \"Name\": \"webapp\", \"EndpointID\": \"ae95b0103bbc...\", \"MacAddress\": \"02:42:ac:12:00:03\", \"IPv4Address\": \"172.19.0.3/16\", \"IPv6Address\": \"\" }, \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... The output shows that two containers are connected to the my-network and the webapp container has 172.19.0.3 address in that network. Step 9 Check that the webapp container is accessible from the db container using its new IP address: docker exec -it db bash root@c3afff20019a:/# ping -c 1 172.19.0.3 PING 172.19.0.3 (172.19.0.3) 56(84) bytes of data. 64 bytes from 172.19.0.3: icmp_seq=1 ttl=64 time=0.136 ms --- 172.19.0.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms Success As expected containers can communicate with each other. Step 10 You can now remove the existing container. You should stop the container before removing it. Alternatively you can use the -f command line argument: docker rm -f webapp docker rm -f db docker network rm my-network Hint Use below command to delete running containers in bulk : docker rm -f $(docker ps -q) Summary It is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses 1.4 Access containers from outside \u00b6 External Access to the Containers can be configured via publishing mechanism. Docker provides 2 options to publish ports: -P flag publishes all exposed ports -p flag allows you to specify specific ports and interfaces to use when mapping ports. The -p flag can take several different forms with the syntax looking like this: Specify the host port and container port: \u2013p <host port>:<container port> Specify the host interface, host port, and container port: \u2013p <host IP interface>:<host port>:<container port> Specify the host interface, have Docker choose a random host port, and specify the container port: \u2013p <host IP interface>::<container port> Specify only a container port and have Docker use a random host port: \u2013p <container port> Let's test exposing containers. For that let's start a new NGINX container and map port 8080 on the Docker host to port 80 inside of the container. This means that traffic that hits the Docker host on port 8080 will be passed on to port 80 inside the container. Note If you start a new container from the official NGINX image without specifying a command to run, the container will run a basic web server on port 80. Step 1 Start a new container based off the official NGINX image by running docker run --name web1 -d -p 8080:80 nginx . docker run --name web1 -d -p 8080:80 nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 6d827a3ef358: Pull complete b556b18c7952: Pull complete 03558b976e24: Pull complete 9abee7e1ef9d: Pull complete Digest: sha256:52f84ace6ea43f2f58937e5f9fc562e99ad6876e82b99d171916c1ece587c188 Status: Downloaded newer image for nginx:latest 4e0da45b0f169f18b0e1ee9bf779500cb0f756402c0a0821d55565f162741b3e Step 2 Review the container status and port mappings by running docker ps . docker ps CONTAINER ID IMAGE COMMAND PORTS NAMES 4e0da45b0f16 nginx \"nginx -g 'daemon ...\" 443/tcp, 0.0.0.0:8080->80/tcp web1 Result The top line shows the new web1 container running NGINX. Take note of the command the container is running as well as the port mapping - 0.0.0.0:8080->80/tcp maps port 8080 on all host interfaces to port 80 inside the web1 container. This port mapping is what effectively makes the containers web service accessible from external sources (via the Docker hosts IP address on port 8080). Step 3 Test connectivity to the NGINX web server, by pasting <Public_IP:8080> of VM to the browser. Note In order to locate Public IP see the list of VMs. Alternatively from inside of VM run curl 127.0.0.1:8080 command. curl 127.0.0.1:8080 <!DOCTYPE html> <html> <Snip> <head> <title>Welcome to nginx!</title> <Snip> <p><em>Thank you for using nginx.</em></p> </body> </html> Success Both CLI and UI method works! If you try and curl the IP address on a different port number it will fail. Summary Docker provides easy way to expose containers outside of the Docker Node. This can ber used for connecting containers between each other: Between networks on the same host Between networks on different host Accessing containers from outside (e.g web site) However, port mapping is implemented via port address translation (PAT) unlike in Kubernetes which we learn soon, exposes applications via service IPs and communicates via POD IPs using (NAT) Step 4 Cleanup environment docker rm -f $(docker ps -q) 2 Persistant Volumes \u00b6 2.1 Storage driver \u00b6 We've discussed several Storage drivers (graphdrivers) during the class. Let's find out what graphdriver is running in our Lab environment. docker info | grep Storage WARNING: No swap limit support Storage Driver: aufs Result Our Classroom is running aufs storage driver. Not a suprise as we running our Lab on Ubuntu VM. Summary Systems runnng Ubuntu or Debian ,going to run aufs graphdriver by default and will most likely meet the majority of your needs. In future overlay2 may replace aufs stay tunned! 2.2 Persisting Data Using Volumes \u00b6 Docker Volumes are created and assigned when containers are started. Data Volumes allow you to map a host directory to a container for sharing data. This mapping is bi-directional. It allows data stored on the host to be accessed from within the container. It also means data saved by the process inside the container is persisted on the host. 2.2.1 Create and manage volumes \u00b6 Step 1 Create a volume: docker volume create --name my-vol Step 2 List volumes: docker volume ls Output: local my-vol Step 3 Inspect a volume: docker volume inspect my-vol [ { \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\", \"Name\": \"my-vol\", \"Options\": {}, \"Scope\": \"local\" } ] Step 3 Add some data to the Mountpoint of the volume: sudo touch /var/lib/docker/volumes/my-vol/_data/test_vol sudo ls /var/lib/docker/volumes/my-vol/_data/ Step 4 Create a container busybox alpine image and attach created my-vol volume in to it: docker run -it -v my-vol:/world busybox / # ls /world test_vol / # Result Volume is mounted and test_vol file is under /world folder as expected Step 5 Try to delete the volume: docker volume rm my-vol Error response from daemon: unable to remove volume: remove my-vol: volume is in use - [6ef3055b516b306847150af8fcea796c02cd90578967802ac29c39d3a2c90102] Failure Deleting container that is attached is not permited. However you can delete with -f option Step 5 Busybox container stopped, howerver it is not deleted. Let's locate stopped busybox container and delete it: docker ps -a | grep busybox docker rm $docker_id Step 6 You can now delete my-vol Note Volume is still avaiable if needed to be reattached any time docker volume ls docker volume rm my-vol docker volume ls Summary Volumes can be craeted and managed separately from containers. 2.2.2 Start a container with a volume \u00b6 If you start a container with a volume that does not yet exist, Docker creates the volume for you. Step 1 Add a data volume to a container: docker run -d -P --name webapp -v /webapp training/webapp python app.py Result Command started a new container and created a new volume inside the container at /webapp. Step 2 Locate the volume on the host using the docker inspect command: docker inspect webapp | grep -A9 Mounts ``` **Output:** ``` \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\", \"Source\": \"/var/lib/docker/volumes/39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d/_data\", \"Destination\": \"/webapp\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" Step 3 List container docker volume ls Output: DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Step 5 Alternatively, you can specify a host directory you want to use as a data volume: mkdir db docker run -d --name db -v ~/db:/db training/postgres Step 2 Start an interactive session in the db container and create a new file in the /db directory: docker exec -it db bash Type inside docker containers console: root@9a7a4fbcc929:/# cd /db root@9a7a4fbcc929:/db# touch hello_from_db_container root@9a7a4fbcc929:/db# exit Step 4 Check that the local db directory contains the new file: ls db hello_from_db_container Step 5 Check that the data volume is persistent. Remove the db container: docker rm -f db Step 6 Create the db container again: docker run -d --name db -v ~/db:/db training/postgres Step 7 Check that its /db directory contains the hello_from_db_container file: docker exec -it db bash Run commands inside container: root@47a60c01590e:/# ls /db hello_from_db_container root@47a60c01590e:/# exit 2.2.3 Use a read-only volume \u00b6 Step 1 Mounting Volumes gives the container full read and write access to the directory. You can specify read-only permissions on the directory by adding the permissions :ro to the mount. If the container attempts to modify data within the directory it will error. docker run -d --name db1 -v ~/db:/db:ro training/postgres docker exec -it db1 bash cd db touch test Result touch: cannot touch 'test': Read-only file system $ exit Step 2 Clean up containers and volumes: docker rm -f $(docker ps -q) docker volume ls Output DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d docker volume rm 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Summary We've learned how to manage volumes with containers Hint If you Docker host has several Storage plugins configured (e.g. ceph, gluster) you can specify via --opt type=btrfs, nfs or --driver=glusterfs during docker volume creation.","title":"Lab 3 Advanced Docker"},{"location":"Lab_3_Advanced_Docker/#1-docker-networking","text":"","title":"1 Docker Networking"},{"location":"Lab_3_Advanced_Docker/#11-docker-networking-basics","text":"Step 1: The Docker Network Command The docker network command is the main command for configuring and managing container networks. Run the docker network command from the first terminal. docker network Usage: docker network COMMAND Manage networks Options: --help Print usage Commands: connect Connect a container to a network create Create a network disconnect Disconnect a container from a network inspect Display detailed information on one or more networks ls List networks prune Remove all unused networks rm Remove one or more networks Run 'docker network COMMAND --help' for more information on a command. The command output shows how to use the command as well as all of the docker network sub-commands. As you can see from the output, the docker network command allows you to create new networks, list existing networks, inspect networks, and remove networks. It also allows you to connect and disconnect containers from networks. Step 2 Run a docker network ls command to view existing container networks on the current Docker host. docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local The output above shows the container networks that are created as part of a standard installation of Docker. New networks that you create will also show up in the output of the docker network ls command. You can see that each network gets a unique ID and NAME . Each network is also associated with a single driver. Notice that the \"bridge\" network and the \"host\" network have the same name as their respective drivers. Step 3: The docker network inspect command is used to view network configuration details. These details include; name, ID, driver, IPAM driver, subnet info, connected containers, and more. Use docker network inspect <network> to view configuration details of the container networks on your Docker host. The command below shows the details of the network called bridge . docker network inspect bridge [ { \"Name\": \"bridge\", \"Id\": \"3430ad6f20bf1486df2e5f64ddc93cc4ff95d81f59b6baea8a510ad500df2e57\", \"Created\": \"2017-04-03T16:49:58.6536278Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Containers\": {}, \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" }, \"Labels\": {} } ] Note The syntax of the docker network inspect command is docker network inspect <network> , where <network> can be either network name or network ID. In the example above we are showing the configuration details for the network called \"bridge\". Do not confuse this with the \"bridge\" driver. Step 4 Now, list Docker supported network driver plugins. For that run docker info command, that shows a lot of interesting information about a Docker installation. Run the docker info command and locate the list of network plugins. docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ee-3 Storage Driver: aufs <Snip> Plugins: Volume: local Network: bridge host macvlan null overlay Swarm: inactive Runtimes: runc <Snip> The output above shows the bridge , host , macvlan , null , and overlay drivers. Summary We've quickly reviewed available docker networking commands as well as found what drivers current docker setup supports.","title":"1.1 Docker Networking Basics"},{"location":"Lab_3_Advanced_Docker/#12-default-bridge-network","text":"Every clean installation of Docker comes with a pre-built network called Default bridge network . Let's explore in more details how it works. Step 1 Verify this with the docker network ls . docker network ls NETWORK ID NAME DRIVER SCOPE 3430ad6f20bf bridge bridge local a7449465c379 host host local 06c349b9cc77 none null local Result The output above shows that the bridge network is associated with the bridge driver. It's important to note that the network and the driver are connected, but they are not the same. In this example the network and the driver have the same name - but they are not the same thing! The output above also shows that the bridge network is scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the bridge driver - the bridge driver provides single-host networking. All networks created with the bridge driver are based on a Linux bridge (a.k.a. a virtual switch). Step 5 Start webapp in Default bridge network docker run -d -p 80:5000 --name webapp training/webapp python app.py Step 6 Check that the webapp and db containers are running: Command: docker ps","title":"1.2 Default bridge network"},{"location":"Lab_3_Advanced_Docker/#13-user-defined-private-networks","text":"So far we\u2019ve learned how Docker networking works with Docker default bridge network . With the introduction of user-defined networking in Docker 1.9, it is now possible to create multiple Docker bridges to allow network segregation within the same host or multi-host networking to allow communicate Docker containers between hosts. The commands are available through the Docker Engine CLI are: docker network create docker network connect docker network ls docker network rm docker network disconnect docker network inspect Let's demonstrate how to create a custom bridge network. Step 1 By default, Docker runs containers in the bridge network. You may want to isolate one or more containers in a separate network. Let\u2019s create a new network: docker network create my-network \\ -d bridge \\ --subnet 172.19.0.0/16 The -d bridge command line argument specifies the bridge network driver and the --subnet command line argument specifies the network segment in CIDR format. If you do not specify a subnet when creating a network, then Docker assigns a subnet automatically, so it is a good idea to specify a subnet to avoid potential conflicts with the existing networks. Below are some other options that are available with the bridge Driver: com.docker.network.bridge.enable_ip_masquerade: This instructs the Docker host to hide or masquerade all containers in this network behind the Docker host's interfaces if the container attempts to route off the local host . com.docker.network.bridge.name: This is the name you wish to give to the bridge. com.docker.network.bridge.enable_icc: This turns on or off Inter-Container Connectivity (ICC) mode for the bridge. com.docker.network.bridge.host_binding_ipv4: This defines the host interface that should be used for port binding. com.docker.network.driver.mtu: This sets MTU for containers attached to this bridge. Step 2 To check that the new network is created, execute docker network ls: docker network ls NETWORK ID NAME DRIVER SCOPE d428e49e4869 bridge bridge local 0d1f78528cc5 host host local 56ef0481820d my-network bridge local 4a07cef84617 none null local Step 3 Let\u2019s inspect the new network: docker network inspect my-network [ { \"Name\": \"my-network\", \"Id\": \"56ef0481820d...\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"172.19.0.0/16\" } ] }, \"Internal\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ] Step 4 As expected, there are no containers connected to the my-network. Let\u2019s recreate the db container in the my-network: docker rm -f db docker run -d --network=my-network --name db training/postgres Step 5 Inspect the my-network again: docker network inspect my-network Output: \"Containers\": { \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... As you see, the db container is connected to the my-network and has 172.19.0.2 address. Step 6 Let\u2019s start an interactive session in the db container and ping the IP address of the webapp again: Note Quick reminder how to locate webapp ip: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' webapp docker exec -it db bash Once inside of container run: root@c3afff20019a:/# ping -c 1 172.17.0.3 PING 172.17.0.3 (172.17.0.3) 56(84) bytes of data. --- 172.17.0.3 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms As expected, the webapp container is no longer accessible from the db container, because they are connected to different networks. Summary Using Multi-host networking provides network isolation within a Docker host via network namepsaces. This is can be used if you want to deploy different applications on same host for isolation or resource duplicate prevention. Step 7 Let\u2019s connect the webapp container to the my-network: docker network connect my-network webapp Step 8 Check that the webapp container now is connected to the my-network: docker network inspect my-network Output: ... \"Containers\": { \"62ed4a627356...\": { \"Name\": \"webapp\", \"EndpointID\": \"ae95b0103bbc...\", \"MacAddress\": \"02:42:ac:12:00:03\", \"IPv4Address\": \"172.19.0.3/16\", \"IPv6Address\": \"\" }, \"93af62cdab64...\": { \"Name\": \"db\", \"EndpointID\": \"b1e8e314cff0...\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.19.0.2/16\", \"IPv6Address\": \"\" } }, ... The output shows that two containers are connected to the my-network and the webapp container has 172.19.0.3 address in that network. Step 9 Check that the webapp container is accessible from the db container using its new IP address: docker exec -it db bash root@c3afff20019a:/# ping -c 1 172.19.0.3 PING 172.19.0.3 (172.19.0.3) 56(84) bytes of data. 64 bytes from 172.19.0.3: icmp_seq=1 ttl=64 time=0.136 ms --- 172.19.0.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms Success As expected containers can communicate with each other. Step 10 You can now remove the existing container. You should stop the container before removing it. Alternatively you can use the -f command line argument: docker rm -f webapp docker rm -f db docker network rm my-network Hint Use below command to delete running containers in bulk : docker rm -f $(docker ps -q) Summary It is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses","title":"1.3 User-defined Private Networks"},{"location":"Lab_3_Advanced_Docker/#14-access-containers-from-outside","text":"External Access to the Containers can be configured via publishing mechanism. Docker provides 2 options to publish ports: -P flag publishes all exposed ports -p flag allows you to specify specific ports and interfaces to use when mapping ports. The -p flag can take several different forms with the syntax looking like this: Specify the host port and container port: \u2013p <host port>:<container port> Specify the host interface, host port, and container port: \u2013p <host IP interface>:<host port>:<container port> Specify the host interface, have Docker choose a random host port, and specify the container port: \u2013p <host IP interface>::<container port> Specify only a container port and have Docker use a random host port: \u2013p <container port> Let's test exposing containers. For that let's start a new NGINX container and map port 8080 on the Docker host to port 80 inside of the container. This means that traffic that hits the Docker host on port 8080 will be passed on to port 80 inside the container. Note If you start a new container from the official NGINX image without specifying a command to run, the container will run a basic web server on port 80. Step 1 Start a new container based off the official NGINX image by running docker run --name web1 -d -p 8080:80 nginx . docker run --name web1 -d -p 8080:80 nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 6d827a3ef358: Pull complete b556b18c7952: Pull complete 03558b976e24: Pull complete 9abee7e1ef9d: Pull complete Digest: sha256:52f84ace6ea43f2f58937e5f9fc562e99ad6876e82b99d171916c1ece587c188 Status: Downloaded newer image for nginx:latest 4e0da45b0f169f18b0e1ee9bf779500cb0f756402c0a0821d55565f162741b3e Step 2 Review the container status and port mappings by running docker ps . docker ps CONTAINER ID IMAGE COMMAND PORTS NAMES 4e0da45b0f16 nginx \"nginx -g 'daemon ...\" 443/tcp, 0.0.0.0:8080->80/tcp web1 Result The top line shows the new web1 container running NGINX. Take note of the command the container is running as well as the port mapping - 0.0.0.0:8080->80/tcp maps port 8080 on all host interfaces to port 80 inside the web1 container. This port mapping is what effectively makes the containers web service accessible from external sources (via the Docker hosts IP address on port 8080). Step 3 Test connectivity to the NGINX web server, by pasting <Public_IP:8080> of VM to the browser. Note In order to locate Public IP see the list of VMs. Alternatively from inside of VM run curl 127.0.0.1:8080 command. curl 127.0.0.1:8080 <!DOCTYPE html> <html> <Snip> <head> <title>Welcome to nginx!</title> <Snip> <p><em>Thank you for using nginx.</em></p> </body> </html> Success Both CLI and UI method works! If you try and curl the IP address on a different port number it will fail. Summary Docker provides easy way to expose containers outside of the Docker Node. This can ber used for connecting containers between each other: Between networks on the same host Between networks on different host Accessing containers from outside (e.g web site) However, port mapping is implemented via port address translation (PAT) unlike in Kubernetes which we learn soon, exposes applications via service IPs and communicates via POD IPs using (NAT) Step 4 Cleanup environment docker rm -f $(docker ps -q)","title":"1.4 Access containers from outside"},{"location":"Lab_3_Advanced_Docker/#2-persistant-volumes","text":"","title":"2 Persistant Volumes"},{"location":"Lab_3_Advanced_Docker/#21-storage-driver","text":"We've discussed several Storage drivers (graphdrivers) during the class. Let's find out what graphdriver is running in our Lab environment. docker info | grep Storage WARNING: No swap limit support Storage Driver: aufs Result Our Classroom is running aufs storage driver. Not a suprise as we running our Lab on Ubuntu VM. Summary Systems runnng Ubuntu or Debian ,going to run aufs graphdriver by default and will most likely meet the majority of your needs. In future overlay2 may replace aufs stay tunned!","title":"2.1 Storage driver"},{"location":"Lab_3_Advanced_Docker/#22-persisting-data-using-volumes","text":"Docker Volumes are created and assigned when containers are started. Data Volumes allow you to map a host directory to a container for sharing data. This mapping is bi-directional. It allows data stored on the host to be accessed from within the container. It also means data saved by the process inside the container is persisted on the host.","title":"2.2 Persisting Data Using Volumes"},{"location":"Lab_3_Advanced_Docker/#221-create-and-manage-volumes","text":"Step 1 Create a volume: docker volume create --name my-vol Step 2 List volumes: docker volume ls Output: local my-vol Step 3 Inspect a volume: docker volume inspect my-vol [ { \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\", \"Name\": \"my-vol\", \"Options\": {}, \"Scope\": \"local\" } ] Step 3 Add some data to the Mountpoint of the volume: sudo touch /var/lib/docker/volumes/my-vol/_data/test_vol sudo ls /var/lib/docker/volumes/my-vol/_data/ Step 4 Create a container busybox alpine image and attach created my-vol volume in to it: docker run -it -v my-vol:/world busybox / # ls /world test_vol / # Result Volume is mounted and test_vol file is under /world folder as expected Step 5 Try to delete the volume: docker volume rm my-vol Error response from daemon: unable to remove volume: remove my-vol: volume is in use - [6ef3055b516b306847150af8fcea796c02cd90578967802ac29c39d3a2c90102] Failure Deleting container that is attached is not permited. However you can delete with -f option Step 5 Busybox container stopped, howerver it is not deleted. Let's locate stopped busybox container and delete it: docker ps -a | grep busybox docker rm $docker_id Step 6 You can now delete my-vol Note Volume is still avaiable if needed to be reattached any time docker volume ls docker volume rm my-vol docker volume ls Summary Volumes can be craeted and managed separately from containers.","title":"2.2.1  Create and manage volumes"},{"location":"Lab_3_Advanced_Docker/#222-start-a-container-with-a-volume","text":"If you start a container with a volume that does not yet exist, Docker creates the volume for you. Step 1 Add a data volume to a container: docker run -d -P --name webapp -v /webapp training/webapp python app.py Result Command started a new container and created a new volume inside the container at /webapp. Step 2 Locate the volume on the host using the docker inspect command: docker inspect webapp | grep -A9 Mounts ``` **Output:** ``` \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d\", \"Source\": \"/var/lib/docker/volumes/39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d/_data\", \"Destination\": \"/webapp\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" Step 3 List container docker volume ls Output: DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Step 5 Alternatively, you can specify a host directory you want to use as a data volume: mkdir db docker run -d --name db -v ~/db:/db training/postgres Step 2 Start an interactive session in the db container and create a new file in the /db directory: docker exec -it db bash Type inside docker containers console: root@9a7a4fbcc929:/# cd /db root@9a7a4fbcc929:/db# touch hello_from_db_container root@9a7a4fbcc929:/db# exit Step 4 Check that the local db directory contains the new file: ls db hello_from_db_container Step 5 Check that the data volume is persistent. Remove the db container: docker rm -f db Step 6 Create the db container again: docker run -d --name db -v ~/db:/db training/postgres Step 7 Check that its /db directory contains the hello_from_db_container file: docker exec -it db bash Run commands inside container: root@47a60c01590e:/# ls /db hello_from_db_container root@47a60c01590e:/# exit","title":"2.2.2 Start a container with a volume"},{"location":"Lab_3_Advanced_Docker/#223-use-a-read-only-volume","text":"Step 1 Mounting Volumes gives the container full read and write access to the directory. You can specify read-only permissions on the directory by adding the permissions :ro to the mount. If the container attempts to modify data within the directory it will error. docker run -d --name db1 -v ~/db:/db:ro training/postgres docker exec -it db1 bash cd db touch test Result touch: cannot touch 'test': Read-only file system $ exit Step 2 Clean up containers and volumes: docker rm -f $(docker ps -q) docker volume ls Output DRIVER VOLUME NAME local 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d docker volume rm 39885c52758dcf7516513be2d44a17560e42b6da75aba30bc66d4af41df5384d Summary We've learned how to manage volumes with containers Hint If you Docker host has several Storage plugins configured (e.g. ceph, gluster) you can specify via --opt type=btrfs, nfs or --driver=glusterfs during docker volume creation.","title":"2.2.3 Use a read-only volume"},{"location":"Lab_4_Docker_Images/","text":"Lab 4 Managing Docker Images Objective: Learn to build docker images using Dockerfiles. Store images in Docker Hub Learn alternative registry solutions (GCR) Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1 Distributing Docker images with Container Registry \u00b6 In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: https://github.com/Cloud-Architects-Program/ycit019/tree/main/Module4/flask-app 1.1 Create DOCKERFILE \u00b6 Step 1 Clone git repo on you laptop: git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\" , \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\" , \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\" , \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\" , \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\" , \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\" , \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\" , \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\" , \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\" , \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==2.0.0 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # Official Python Alpine Base image using Simple Tags # Image contains Python 3 and pip pre-installed, so no need to install them FROM python:3.9.5-alpine3.12 # Specify Working directory WORKDIR /usr/src/app # COPY requirements.txt /usr/src/app/ COPY requirements.txt ./ # Install Python Flask used by the Python app RUN pip install --no-cache-dir -r requirements.txt # Copy files required for the app to run COPY app.py ./ COPY templates/index.html ./templates/ # Make a record that the port number the container should be expose is: EXPOSE 5000 # run the application CMD [ \"python\" , \"./app.py\" ] 1.2 Build a Docker image \u00b6 Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step to your own Docker Hub. docker build -t <Docker-hub-user-name>/myfirstapp . Result Image has been buit Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 8080:5000 --name myfirstapp <Docker-hub-user-name>/myfirstapp Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Stop the container and remove it: docker rm -f myfirstapp 1.2.2 Publish Docker Image to Docker Hub \u00b6 One of the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <Docker-hub-user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry Note Image Tag of the created myfirstapp : docker images Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag <Docker-hub-user-name>/myfirstapp:v1 docker push <Docker-hub-user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <Docker-hub-user-name>/myfirstapp:latest docker pull <Docker-hub-user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <Docker-hub-user-name>/myfirstapp:v1 1.2.3 Pushing images to gcr.io \u00b6 In a similar manner we need to tag the image to prepare it to be pushed to gcr.io. We just need to change the registry, which is for gcr.io formatted as gcr.io/PROJECT_ID. Step 1 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 2 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 3 Tag the image: Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag gcr.io/${PROJECT_ID}/myfirstapp:v1 $docker_image_tag Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/myfirstapp:v1 Step 4 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 1.2.3 Pushing images to Local Repository \u00b6 First, we need to spin up a local docker registry. This could be a use case if you want to deploy basic registry On-Prem. This registry will luck security features such as Authentication, SSL, scanning. If you interested to use Enterprise ready solution On-Prem consider: Jfrog Artifactory, RedHa's Clair, Docker Enterprise or open source CNCF project Harbor. Step 1 Deploy local registry docker run -d -p 5000:5000 --name registry registry:2.7.1 Step 2 In order to upload an image to a registry, we need to tag it properly Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag localhost:5000/myfirstapp:v1 Step 3 Now that we have an image tagged correctly, we can push it to our local registry docker push localhost:5000/myfirstapp:v1 Step 4 Let\u2019s now delete the local image, and pull it again from the local registry To delete the image, we need to first remove the container that depends on that image. Run docker ps and get the Container_ID for the container that uses myfirstapp:v1 Kill and delete that container by running the following command, but make sure to replace CONTAINER_ID, with the actual ID. docker rm CONTAINER_ID Result: The command will print back the container ID, which is an indication it was successful. Step 5 Run docker images to validate docker images Step 6 Now we can delete the docker image docker rmi localhost:5000/myfirstapp:v1 Step 7 Although the image is deleted locally, it is still in the registry and we can pull it back, or use it to deploy containers. docker run -dp 8080:5000 --name myfirstapp localhost:5000/myfirstapp:v1 Run docker images again to check how the image is available locally again. docker images Step 8 Cleanup: docker rm -f myfirstapp 2 Follow Docker Best Practices \u00b6 2.1 Inspecting Dockerfiles with dockle \u00b6 Dockle - Container Image Linter for Security, Helping build the Best-Practice Docker Image, Easy to start Dockle helps you: Build Best Practice Docker images Build secure Docker images Checkpoints includes CIS Benchmarks Step 1 Install Dockle $ VERSION=$( curl --silent \"https://api.github.com/repos/goodwithtech/dockle/releases/latest\" | \\ grep '\"tag_name\":' | \\ sed -E 's/.*\"v([^\"]+)\".*/\\1/' \\ ) && curl -L -o dockle.deb https://github.com/goodwithtech/dockle/releases/download/v${VERSION}/dockle_${VERSION}_Linux-64bit.deb $ sudo dpkg -i dockle.deb && rm dockle.deb Step 2 Experiment with existing applications we've created in the class: $ dockle [YOUR_IMAGE_NAME] e.g. dockle archy/myfirstapp output: WARN - CIS-DI-0001: Create a user for the container * Last user should not be root WARN - DKL-DI-0006: Avoid latest tag * Avoid 'latest' tag INFO - CIS-DI-0005: Enable Content trust for Docker * export DOCKER_CONTENT_TRUST=1 before docker pull/build INFO - CIS-DI-0006: Add HEALTHCHECK instruction to the container image * not found HEALTHCHECK statement INFO - DKL-LI-0003: Only put necessary files * Suspicious directory : tmp 2.2 Automated Builds with Google Cloud Build \u00b6 Live Demo: GCR Image scanning Setting Up Docker Image Auto-Build with Google Cloud Build based on Push to Branch Auto Deployment of Image to Cloud Run","title":"Lab 4 Managing Docker Images"},{"location":"Lab_4_Docker_Images/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"Lab_4_Docker_Images/#1-distributing-docker-images-with-container-registry","text":"In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a Docker image registry maintained by Docker Inc. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: https://github.com/Cloud-Architects-Program/ycit019/tree/main/Module4/flask-app","title":"1 Distributing Docker images with Container Registry"},{"location":"Lab_4_Docker_Images/#11-create-dockerfile","text":"Step 1 Clone git repo on you laptop: git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://media.giphy.com/media/mlvseq9yvZhba/giphy.gif\" , \"https://media.giphy.com/media/13CoXDiaCcCoyk/giphy.gif\" , \"https://media.giphy.com/media/LtVXu5s7KwlK8/giphy.gif\" , \"https://media.giphy.com/media/PekRU0CYIpXS8/giphy.gif\" , \"https://media.giphy.com/media/11quO2C07Sh2oM/giphy.gif\" , \"https://media.giphy.com/media/12HZukMBlutpoQ/giphy.gif\" , \"https://media.giphy.com/media/1HKaikaFqDt7i/giphy.gif\" , \"https://media.giphy.com/media/v6aOjy0Qo1fIA/giphy.gif\" , \"https://media.giphy.com/media/12bjQ7uASAaCKk/giphy.gif\" , \"https://media.giphy.com/media/HFcl9uhuCqzGU/giphy.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==2.0.0 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # Official Python Alpine Base image using Simple Tags # Image contains Python 3 and pip pre-installed, so no need to install them FROM python:3.9.5-alpine3.12 # Specify Working directory WORKDIR /usr/src/app # COPY requirements.txt /usr/src/app/ COPY requirements.txt ./ # Install Python Flask used by the Python app RUN pip install --no-cache-dir -r requirements.txt # Copy files required for the app to run COPY app.py ./ COPY templates/index.html ./templates/ # Make a record that the port number the container should be expose is: EXPOSE 5000 # run the application CMD [ \"python\" , \"./app.py\" ]","title":"1.1 Create DOCKERFILE"},{"location":"Lab_4_Docker_Images/#12-build-a-docker-image","text":"Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step to your own Docker Hub. docker build -t <Docker-hub-user-name>/myfirstapp . Result Image has been buit Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 8080:5000 --name myfirstapp <Docker-hub-user-name>/myfirstapp Step 4 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 5 Stop the container and remove it: docker rm -f myfirstapp","title":"1.2 Build a Docker image"},{"location":"Lab_4_Docker_Images/#122-publish-docker-image-to-docker-hub","text":"One of the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <Docker-hub-user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry Note Image Tag of the created myfirstapp : docker images Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag <Docker-hub-user-name>/myfirstapp:v1 docker push <Docker-hub-user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <Docker-hub-user-name>/myfirstapp:latest docker pull <Docker-hub-user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <Docker-hub-user-name>/myfirstapp:v1","title":"1.2.2 Publish Docker Image to Docker Hub"},{"location":"Lab_4_Docker_Images/#123-pushing-images-to-gcrio","text":"In a similar manner we need to tag the image to prepare it to be pushed to gcr.io. We just need to change the registry, which is for gcr.io formatted as gcr.io/PROJECT_ID. Step 1 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 2 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 3 Tag the image: Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag gcr.io/${PROJECT_ID}/myfirstapp:v1 $docker_image_tag Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/myfirstapp:v1 Step 4 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"1.2.3 Pushing images to gcr.io"},{"location":"Lab_4_Docker_Images/#123-pushing-images-to-local-repository","text":"First, we need to spin up a local docker registry. This could be a use case if you want to deploy basic registry On-Prem. This registry will luck security features such as Authentication, SSL, scanning. If you interested to use Enterprise ready solution On-Prem consider: Jfrog Artifactory, RedHa's Clair, Docker Enterprise or open source CNCF project Harbor. Step 1 Deploy local registry docker run -d -p 5000:5000 --name registry registry:2.7.1 Step 2 In order to upload an image to a registry, we need to tag it properly Modify $docker_image_tag with myfirstapp:v1 image tag value: docker tag $docker_image_tag localhost:5000/myfirstapp:v1 Step 3 Now that we have an image tagged correctly, we can push it to our local registry docker push localhost:5000/myfirstapp:v1 Step 4 Let\u2019s now delete the local image, and pull it again from the local registry To delete the image, we need to first remove the container that depends on that image. Run docker ps and get the Container_ID for the container that uses myfirstapp:v1 Kill and delete that container by running the following command, but make sure to replace CONTAINER_ID, with the actual ID. docker rm CONTAINER_ID Result: The command will print back the container ID, which is an indication it was successful. Step 5 Run docker images to validate docker images Step 6 Now we can delete the docker image docker rmi localhost:5000/myfirstapp:v1 Step 7 Although the image is deleted locally, it is still in the registry and we can pull it back, or use it to deploy containers. docker run -dp 8080:5000 --name myfirstapp localhost:5000/myfirstapp:v1 Run docker images again to check how the image is available locally again. docker images Step 8 Cleanup: docker rm -f myfirstapp","title":"1.2.3 Pushing images to Local Repository"},{"location":"Lab_4_Docker_Images/#2-follow-docker-best-practices","text":"","title":"2 Follow Docker Best Practices"},{"location":"Lab_4_Docker_Images/#21-inspecting-dockerfiles-with-dockle","text":"Dockle - Container Image Linter for Security, Helping build the Best-Practice Docker Image, Easy to start Dockle helps you: Build Best Practice Docker images Build secure Docker images Checkpoints includes CIS Benchmarks Step 1 Install Dockle $ VERSION=$( curl --silent \"https://api.github.com/repos/goodwithtech/dockle/releases/latest\" | \\ grep '\"tag_name\":' | \\ sed -E 's/.*\"v([^\"]+)\".*/\\1/' \\ ) && curl -L -o dockle.deb https://github.com/goodwithtech/dockle/releases/download/v${VERSION}/dockle_${VERSION}_Linux-64bit.deb $ sudo dpkg -i dockle.deb && rm dockle.deb Step 2 Experiment with existing applications we've created in the class: $ dockle [YOUR_IMAGE_NAME] e.g. dockle archy/myfirstapp output: WARN - CIS-DI-0001: Create a user for the container * Last user should not be root WARN - DKL-DI-0006: Avoid latest tag * Avoid 'latest' tag INFO - CIS-DI-0005: Enable Content trust for Docker * export DOCKER_CONTENT_TRUST=1 before docker pull/build INFO - CIS-DI-0006: Add HEALTHCHECK instruction to the container image * not found HEALTHCHECK statement INFO - DKL-LI-0003: Only put necessary files * Suspicious directory : tmp","title":"2.1 Inspecting Dockerfiles with dockle"},{"location":"Lab_4_Docker_Images/#22-automated-builds-with-google-cloud-build","text":"Live Demo: GCR Image scanning Setting Up Docker Image Auto-Build with Google Cloud Build based on Push to Branch Auto Deployment of Image to Cloud Run","title":"2.2 Automated Builds with Google Cloud Build"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/","text":"Lab 4 Managing Docker Images Objective: Learn to build docker images using Dockerfiles. Store images in Docker Hub Learn alternative registry solutions (Quya.io) Automate image build process with Docker Cloud 1 Building Docker Images \u00b6 In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a registry of Docker images. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: git clone https://github.com/archyufa/k8scanada 1.1 Create DOCKERFILE \u00b6 Step 1 Clone git repo on you laptop: git clone https://github.com/archyufa/k8scanada cd k8scanada/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26388-1381844103-11.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr01/15/9/anigif_enhanced-buzz-31540-1381844535-8.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26390-1381844163-18.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-1376-1381846217-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3391-1381844336-26.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-29111-1381845968-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3409-1381844582-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr02/15/9/anigif_enhanced-buzz-19667-1381844937-10.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26358-1381845043-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-18774-1381844645-6.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-25158-1381844793-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/10/anigif_enhanced-buzz-11980-1381846269-1.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==0.10.1 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # our base image FROM alpine:3.5 # Install python and pip RUN apk add --update py2-pip # upgrade pip RUN pip install --upgrade pip # install Python modules needed by the Python app COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt # copy files required for the app to run COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ # tell the port number the container should expose EXPOSE 5000 # run the application CMD [ \"python\" , \"/usr/src/app/app.py\" ] 1.2 Build a Docker image \u00b6 Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step. docker build -t <user-name>/myfirstapp . Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 80:5000 --name myfirstapp <user-name>/myfirstapp Step 4 Use your browser to open the address http:// and check that the application works. Step 5 Stop the container and remove it: docker stop myfirstapp docker rm myfirstapp myfirstapp 1.2.1 Share docker images with tar files \u00b6 Now ideally you want to share you freshly build docker image with someone or run it in different environment which for some reason don\u2019t have internet access so images can not be pulled from Online Docker registries. In that case Docker images can be shared as we share traditionally regular files by creating tarballs using docker save command as following Step 1 Create a tar file using docker save command: docker save <user-name>/myfirstapp > myfirstapp.tar Or docker save --output myfirstapp1.tar archyufa/myfirstapp ls -trh | grep tar Step 2 Transfer images to another environment using scp command. Hint You can also store this images in Object storages, e.g. Swift or Amazon S3 using version control. Step 3 Now you can restore this images using docker load, that will load a tarred repository from a file or the standard input stream. It restores both images and tags. docker load < myfirstapp.tar 23b9c7b43573: Loading layer [==================================================>] 4.23MB/4.23MB b3b5c1214f71: Loading layer [==================================================>] 52.87MB/52.87MB f877d8dd64d3: Loading layer [==================================================>] 8.636MB/8.636MB bba871f91589: Loading layer [==================================================>] 3.584kB/3.584kB 1c131e92eb5f: Loading layer [==================================================>] 5.053MB/5.053MB 3f6463bcb64c: Loading layer [==================================================>] 5.12kB/5.12kB 47c61110467a: Loading layer [==================================================>] 4.096kB/4.096kB Loaded image: archyufa/myfirstapp:latest docker images 1.2.2 Publish Docker Image to Docker Hub \u00b6 However the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry docker tag 5b45ce063cea <user-name>/myfirstapp:v1 docker push <user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <user-name>/myfirstapp:latest docker pull <user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <user-name>/myfirstapp:v1 1.2.3 Automated Builds with Docker Cloud \u00b6 Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud and notifications to slac Automated Tests with PR 1.2.4 Push Docker Images to quay.io \u00b6 Prerequisite: Register to quay.io with your Github user Step 1 Login to quay.io from CLI docker login quay.io Step 2 Build image with quay prefix docker build -t quay.io/archyufa/myfirstapp . Step 3 Push image to quay registry docker push quay.io/archyufa/myfirstapp 1.2.5 Demo quay.io \u00b6 Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud / Quai.io","title":"Lab 4 Docker Images Docker Hub Quya io"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#1-building-docker-images","text":"In the previous modules, we learned how to use Docker images to run Docker containers. Docker images that we used have been downloaded from the Docker Hub, a registry of Docker images. In this section we will create a simple web application from scratch. We will use Flask ( http://flask.pocoo.org/ ), a microframework for Python. Our application for each request will display a random picture from the defined set. In the next session we will create all necessary files for our application, build docker image and then push to Docker Hub and Quay. The code for this application is also available in GitHub: git clone https://github.com/archyufa/k8scanada","title":"1 Building Docker Images"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#11-create-dockerfile","text":"Step 1 Clone git repo on you laptop: git clone https://github.com/archyufa/k8scanada cd k8scanada/Module4/flask-app/ Step 2 In this directory, we see following files: flask-app/ Dockerfile app.py requirements.txt templates/ index.html Step 3 Let\u2019s review file app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26388-1381844103-11.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr01/15/9/anigif_enhanced-buzz-31540-1381844535-8.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26390-1381844163-18.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-1376-1381846217-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3391-1381844336-26.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-29111-1381845968-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3409-1381844582-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr02/15/9/anigif_enhanced-buzz-19667-1381844937-10.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26358-1381845043-13.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-18774-1381844645-6.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-25158-1381844793-0.gif\" , \"http://ak-hdl.buzzfed.com/static/2013-10/enhanced/webdr03/15/10/anigif_enhanced-buzz-11980-1381846269-1.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) Step 4 Below is the content of requirements.txt file: Flask==0.10.1 Step 5 Under directory templates observe index.html with the following content: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > Step 6 Let\u2019s review content of the Dockerfile: # our base image FROM alpine:3.5 # Install python and pip RUN apk add --update py2-pip # upgrade pip RUN pip install --upgrade pip # install Python modules needed by the Python app COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt # copy files required for the app to run COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ # tell the port number the container should expose EXPOSE 5000 # run the application CMD [ \"python\" , \"/usr/src/app/app.py\" ]","title":"1.1 Create DOCKERFILE"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#12-build-a-docker-image","text":"Step 1 Now let\u2019s build our Docker image. In the command below, replace with your user name. This user name should be the same as you created when you registered on Docker Hub. Because we will publish our build image in the next step. docker build -t <user-name>/myfirstapp . Step 2 Where is your built image? It\u2019s in your machine\u2019s local Docker image registry, you can check that your image exists with command below: docker images Step 3 Now run a container in a background and expose a standard HTTP port (80), which is redirected to the container\u2019s port 5000: docker run -dp 80:5000 --name myfirstapp <user-name>/myfirstapp Step 4 Use your browser to open the address http:// and check that the application works. Step 5 Stop the container and remove it: docker stop myfirstapp docker rm myfirstapp myfirstapp","title":"1.2 Build a Docker image"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#121-share-docker-images-with-tar-files","text":"Now ideally you want to share you freshly build docker image with someone or run it in different environment which for some reason don\u2019t have internet access so images can not be pulled from Online Docker registries. In that case Docker images can be shared as we share traditionally regular files by creating tarballs using docker save command as following Step 1 Create a tar file using docker save command: docker save <user-name>/myfirstapp > myfirstapp.tar Or docker save --output myfirstapp1.tar archyufa/myfirstapp ls -trh | grep tar Step 2 Transfer images to another environment using scp command. Hint You can also store this images in Object storages, e.g. Swift or Amazon S3 using version control. Step 3 Now you can restore this images using docker load, that will load a tarred repository from a file or the standard input stream. It restores both images and tags. docker load < myfirstapp.tar 23b9c7b43573: Loading layer [==================================================>] 4.23MB/4.23MB b3b5c1214f71: Loading layer [==================================================>] 52.87MB/52.87MB f877d8dd64d3: Loading layer [==================================================>] 8.636MB/8.636MB bba871f91589: Loading layer [==================================================>] 3.584kB/3.584kB 1c131e92eb5f: Loading layer [==================================================>] 5.053MB/5.053MB 3f6463bcb64c: Loading layer [==================================================>] 5.12kB/5.12kB 47c61110467a: Loading layer [==================================================>] 4.096kB/4.096kB Loaded image: archyufa/myfirstapp:latest docker images","title":"1.2.1 Share docker images with tar files"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#122-publish-docker-image-to-docker-hub","text":"However the most popular way to share and work with you images is to push them to the Docker Hub. Docker Hub is a registry of Docker images. You can think of the registry as a directory of all available Docker images. Step 1 (Optional) If you don\u2019t have a Docker account, sign up for one here . Make a note of your username and password. Step 2 Log in to your local machine. docker login Step 3 Now, publish your image to docker Hub. docker push <user-name>/myfirstapp Step 4 Login to https://hub.docker.com and verify simage and tags. Result Image been pushed and can be observed in Docker Hub, with the tag latest. Step 5 It is also possible to specify a custom tag for image prior to push it to the registry docker tag 5b45ce063cea <user-name>/myfirstapp:v1 docker push <user-name>/myfirstapp:v1 Result Image been pushed and can be observed in Docker Hub. You can now observe 2 docker image one with the tag latest and another with tag v1 Step 6 You can now pull or run specified Docker images from any other location where docker engine is installed with following commands: docker pull <user-name>/myfirstapp:latest docker pull <user-name>/myfirstapp:v1 Result Images stored locally docker images Output: myfirstapp v1 f50f9524513f 1 hour ago 22 MB myfirstapp latest f50f9524513f 1 hour ago 22 MB Finally run images with specific tag: docker run <user-name>/myfirstapp:v1","title":"1.2.2 Publish Docker Image to Docker Hub"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#123-automated-builds-with-docker-cloud","text":"Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud and notifications to slac Automated Tests with PR","title":"1.2.3 Automated Builds with Docker Cloud"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#124-push-docker-images-to-quayio","text":"Prerequisite: Register to quay.io with your Github user Step 1 Login to quay.io from CLI docker login quay.io Step 2 Build image with quay prefix docker build -t quay.io/archyufa/myfirstapp . Step 3 Push image to quay registry docker push quay.io/archyufa/myfirstapp","title":"1.2.4 Push Docker Images to quay.io"},{"location":"Lab_4_Docker_Images_Docker_Hub_Quya_io/#125-demo-quayio","text":"Live Demo: Docker Security scanning Setting Up Auto-Build in Docker Cloud / Quai.io","title":"1.2.5 Demo quay.io"},{"location":"Lab_5_Docker_Compose/","text":"Lab 5 Docker Compose and Docker Security Objective: Practice to use Docker Compose, 1 Docker Security \u00b6 1.1 Scan images with Trivy \u00b6 Trivy (tri pronounced like trigger, vy pronounced like envy) is a simple and comprehensive vulnerability scanner for containers and other artifacts. A software vulnerability is a glitch, flaw, or weakness present in the software or in an Operating System. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and application dependencies (Bundler, Composer, npm, yarn, etc.). Trivy is easy to use. Just install the binary and you're ready to scan. All you need to do for scanning is to specify a target such as an image name of the container. Step 1 Install Trivy sudo apt-get install wget apt-transport-https gnupg lsb-release wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add - echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list sudo apt-get update sudo apt-get install trivy Step 2 Specify an image name (and a tag). $ trivy image [YOUR_IMAGE_NAME] For example: $ trivy image python:3.4-alpine 2019-05-16T01:20:43.180+0900 INFO Updating vulnerability database... 2019-05-16T01:20:53.029+0900 INFO Detecting Alpine vulnerabilities... python:3.4-alpine3.9 (alpine 3.9.2) =================================== Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0) +---------+------------------+----------+-------------------+---------------+--------------------------------+ | LIBRARY | VULNERABILITY ID | SEVERITY | INSTALLED VERSION | FIXED VERSION | TITLE | +---------+------------------+----------+-------------------+---------------+--------------------------------+ | openssl | CVE-2019-1543 | MEDIUM | 1.1.1a-r1 | 1.1.1b-r1 | openssl: ChaCha20-Poly1305 | | | | | | | with long nonces | +---------+------------------+----------+-------------------+---------------+--------------------------------+ Step 3 Explore local images in your environment. 2 Docker Compose \u00b6 In this module, will guide you through the process of building a multi-container application using docker compose. The application code is available at GitHub: https://github.com/Cloud-Architects-Program/ycit019 2.1 Deploy Guestbook app with Compose \u00b6 Let\u2019s build another application. This time we going to create famous Guestbook application. Guestbook consists of three services. A redis-master node, a set of redis-slave that can be scaled and find the redis-master via its DNS name. And a PHP frontend that exposes itself on port 80. The resulting application allows you to leave short messages which are stored in the redis cluster. Step 1 Change directory to the guestbook cd ~/ycit019/Module5/guestbook/ ls Step 2 Let\u2019s review the docker-guestbook.yml file version: \"2\" services: redis-master: image: gcr.io/google_containers/redis:e2e ports: - \"6379\" redis-slave: image: gcr.io/google_samples/gb-redisslave:v1 ports: - \"6379\" environment: - GET_HOSTS_FROM=dns frontend: image: gcr.io/google-samples/gb-frontend:v4 ports: - \"80:80\" environment: - GET_HOSTS_FROM=dns Step 3 Let\u2019s run docker-guestbook.yml with compose export LD_LIBRARY_PATH=/usr/local/lib docker-compose -f docker-guestbook.yml up -d Creating network \"examples_default\" with the default driver Creating examples_redis-slave_1 Creating examples_frontend_1 Creating examples_redis-master_1 Note -d - Detached mode: Run containers in the background, print new container names. -f - Specify an alternate compose file (default: docker-compose.yml) Step 4 Check that all containers are running: docker ps CONTAINER ID IMAGE COMMAND d1006d1beee5 gcr.io/google-samples/gb-frontend:v4 \"apache2-foreground\" fb3a15fde23f gcr.io/google_containers/redis:e2e \"redis-server /etc...\" 326b94d4cdd7 gcr.io/google_samples/gb-redisslave:v1 \"/entrypoint.sh /b...\" Step 5 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Success Nice you now have compose stuck up and running! Step 6 Cleanup environment: docker-compose -f docker-guestbook.yml down Stopping guestbook_frontend_1 ... done Stopping guestbook_redis-master_1 ... done Stopping guestbook_redis-slave_1 ... done Removing guestbook_frontend_1 ... done Removing guestbook_redis-master_1 ... done Removing guestbook_redis-slave_1 ... done Removing network guestbook_default 2.2 Deploy Voting App using Compose \u00b6 Step 1 Switch to Module5/example-voting-app folder : cd ~/ycit019/Module5/example-voting-app/ Step 2 The existing file docker-compose.yml defines several images: A voting-app container based on a Python image A result-app container based on a Node.js image A Redis container based on a redis image, to temporarily store the data. A worker app based on a dotnet image A Postgres container based on a postgres image App Architecture: Note that three of the containers are built from Dockerfiles, while the other two are images on Docker Hub. Let's review them closely: Step 3 Review files that going to be deployed with tree command. Alternatively view the files in gitrepo page here sudo apt install tree tree Step 5 Let\u2019s change the default port to expose. Edit the docker-compose.yml file and find the following lines: ports: - \"5000:80\" Change 5000 to 8080: ports: - \"8080:80\" Step 4 Verify Docker Compose version: docker-compose version Step 5 Use the docker-compose tool to launch your application: docker-compose up -d Step 6 Check that all containers are running, volumes created. Check compose state and logs : #Docker state docker ps docker volumes #Docker compose state docker-compose ps docker-compose logs Step 7 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 8 Cleanup up. docker-compose down Stopping examplevotingapp_worker_1 ... done Stopping examplevotingapp_redis_1 ... done Stopping examplevotingapp_result_1 ... done Stopping examplevotingapp_db_1 ... done Stopping examplevotingapp_vote_1 ... done Removing examplevotingapp_worker_1 ... done Removing examplevotingapp_redis_1 ... done Removing examplevotingapp_result_1 ... done Removing examplevotingapp_db_1 ... done Removing examplevotingapp_vote_1 ... done Removing network examplevotingapp_default Step 9 You Boss told you that the application has a bug. Update the the app by editing the vote/app.py file and change the following lines near the top of the file: vim vote/app.py Press 'i' option_a = os.getenv('OPTION_A', \"Cats\") option_b = os.getenv('OPTION_B', \"Dogs\") Step 10 Replace \u201cCats\u201d and \u201cDogs\u201d with two options of your choice. For example: option_a = os.getenv('OPTION_A', \"Java\") option_b = os.getenv('OPTION_B', \"Python\") Press 'wq!' Step 11 Use docker-compose tool to launch your Update application: docker-compose up -d Check the UI Bingo Let's see who wins the battle of Orchestrations! Step 8 Cleanup up docker-compose down Congratulations You are now docker expert! We were able to start 2 microservices application with docker compose. First microservice had 3 services. Second microservice had 5 servics written in 3 different languages and able to talk to each other. Summary So far we've learned docker-compose v2. docker-compose v3 is out of scope for this Lab. However you got the idea! Read the Docker-Compose documentation on new syntax. Also example of v3 version of voting-app is here for you reference.","title":"Lab 5 Docker Compose"},{"location":"Lab_5_Docker_Compose/#1-docker-security","text":"","title":"1 Docker Security"},{"location":"Lab_5_Docker_Compose/#11-scan-images-with-trivy","text":"Trivy (tri pronounced like trigger, vy pronounced like envy) is a simple and comprehensive vulnerability scanner for containers and other artifacts. A software vulnerability is a glitch, flaw, or weakness present in the software or in an Operating System. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and application dependencies (Bundler, Composer, npm, yarn, etc.). Trivy is easy to use. Just install the binary and you're ready to scan. All you need to do for scanning is to specify a target such as an image name of the container. Step 1 Install Trivy sudo apt-get install wget apt-transport-https gnupg lsb-release wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add - echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list sudo apt-get update sudo apt-get install trivy Step 2 Specify an image name (and a tag). $ trivy image [YOUR_IMAGE_NAME] For example: $ trivy image python:3.4-alpine 2019-05-16T01:20:43.180+0900 INFO Updating vulnerability database... 2019-05-16T01:20:53.029+0900 INFO Detecting Alpine vulnerabilities... python:3.4-alpine3.9 (alpine 3.9.2) =================================== Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0) +---------+------------------+----------+-------------------+---------------+--------------------------------+ | LIBRARY | VULNERABILITY ID | SEVERITY | INSTALLED VERSION | FIXED VERSION | TITLE | +---------+------------------+----------+-------------------+---------------+--------------------------------+ | openssl | CVE-2019-1543 | MEDIUM | 1.1.1a-r1 | 1.1.1b-r1 | openssl: ChaCha20-Poly1305 | | | | | | | with long nonces | +---------+------------------+----------+-------------------+---------------+--------------------------------+ Step 3 Explore local images in your environment.","title":"1.1 Scan images with Trivy"},{"location":"Lab_5_Docker_Compose/#2-docker-compose","text":"In this module, will guide you through the process of building a multi-container application using docker compose. The application code is available at GitHub: https://github.com/Cloud-Architects-Program/ycit019","title":"2 Docker Compose"},{"location":"Lab_5_Docker_Compose/#21-deploy-guestbook-app-with-compose","text":"Let\u2019s build another application. This time we going to create famous Guestbook application. Guestbook consists of three services. A redis-master node, a set of redis-slave that can be scaled and find the redis-master via its DNS name. And a PHP frontend that exposes itself on port 80. The resulting application allows you to leave short messages which are stored in the redis cluster. Step 1 Change directory to the guestbook cd ~/ycit019/Module5/guestbook/ ls Step 2 Let\u2019s review the docker-guestbook.yml file version: \"2\" services: redis-master: image: gcr.io/google_containers/redis:e2e ports: - \"6379\" redis-slave: image: gcr.io/google_samples/gb-redisslave:v1 ports: - \"6379\" environment: - GET_HOSTS_FROM=dns frontend: image: gcr.io/google-samples/gb-frontend:v4 ports: - \"80:80\" environment: - GET_HOSTS_FROM=dns Step 3 Let\u2019s run docker-guestbook.yml with compose export LD_LIBRARY_PATH=/usr/local/lib docker-compose -f docker-guestbook.yml up -d Creating network \"examples_default\" with the default driver Creating examples_redis-slave_1 Creating examples_frontend_1 Creating examples_redis-master_1 Note -d - Detached mode: Run containers in the background, print new container names. -f - Specify an alternate compose file (default: docker-compose.yml) Step 4 Check that all containers are running: docker ps CONTAINER ID IMAGE COMMAND d1006d1beee5 gcr.io/google-samples/gb-frontend:v4 \"apache2-foreground\" fb3a15fde23f gcr.io/google_containers/redis:e2e \"redis-server /etc...\" 326b94d4cdd7 gcr.io/google_samples/gb-redisslave:v1 \"/entrypoint.sh /b...\" Step 5 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Success Nice you now have compose stuck up and running! Step 6 Cleanup environment: docker-compose -f docker-guestbook.yml down Stopping guestbook_frontend_1 ... done Stopping guestbook_redis-master_1 ... done Stopping guestbook_redis-slave_1 ... done Removing guestbook_frontend_1 ... done Removing guestbook_redis-master_1 ... done Removing guestbook_redis-slave_1 ... done Removing network guestbook_default","title":"2.1 Deploy Guestbook app with Compose"},{"location":"Lab_5_Docker_Compose/#22-deploy-voting-app-using-compose","text":"Step 1 Switch to Module5/example-voting-app folder : cd ~/ycit019/Module5/example-voting-app/ Step 2 The existing file docker-compose.yml defines several images: A voting-app container based on a Python image A result-app container based on a Node.js image A Redis container based on a redis image, to temporarily store the data. A worker app based on a dotnet image A Postgres container based on a postgres image App Architecture: Note that three of the containers are built from Dockerfiles, while the other two are images on Docker Hub. Let's review them closely: Step 3 Review files that going to be deployed with tree command. Alternatively view the files in gitrepo page here sudo apt install tree tree Step 5 Let\u2019s change the default port to expose. Edit the docker-compose.yml file and find the following lines: ports: - \"5000:80\" Change 5000 to 8080: ports: - \"8080:80\" Step 4 Verify Docker Compose version: docker-compose version Step 5 Use the docker-compose tool to launch your application: docker-compose up -d Step 6 Check that all containers are running, volumes created. Check compose state and logs : #Docker state docker ps docker volumes #Docker compose state docker-compose ps docker-compose logs Step 7 Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Step 8 Cleanup up. docker-compose down Stopping examplevotingapp_worker_1 ... done Stopping examplevotingapp_redis_1 ... done Stopping examplevotingapp_result_1 ... done Stopping examplevotingapp_db_1 ... done Stopping examplevotingapp_vote_1 ... done Removing examplevotingapp_worker_1 ... done Removing examplevotingapp_redis_1 ... done Removing examplevotingapp_result_1 ... done Removing examplevotingapp_db_1 ... done Removing examplevotingapp_vote_1 ... done Removing network examplevotingapp_default Step 9 You Boss told you that the application has a bug. Update the the app by editing the vote/app.py file and change the following lines near the top of the file: vim vote/app.py Press 'i' option_a = os.getenv('OPTION_A', \"Cats\") option_b = os.getenv('OPTION_B', \"Dogs\") Step 10 Replace \u201cCats\u201d and \u201cDogs\u201d with two options of your choice. For example: option_a = os.getenv('OPTION_A', \"Java\") option_b = os.getenv('OPTION_B', \"Python\") Press 'wq!' Step 11 Use docker-compose tool to launch your Update application: docker-compose up -d Check the UI Bingo Let's see who wins the battle of Orchestrations! Step 8 Cleanup up docker-compose down Congratulations You are now docker expert! We were able to start 2 microservices application with docker compose. First microservice had 3 services. Second microservice had 5 servics written in 3 different languages and able to talk to each other. Summary So far we've learned docker-compose v2. docker-compose v3 is out of scope for this Lab. However you got the idea! Read the Docker-Compose documentation on new syntax. Also example of v3 version of voting-app is here for you reference.","title":"2.2 Deploy Voting App using Compose"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/","text":"Deploy Kubernetes \u00b6 In this Lab, we are going to: Deploy Single Node Kubernetes cluster using kubeadm on a Google Compute Engine node Deploy an application to Kubernetes 1. Deploy Kubernetes and Calico with Kubeadm \u00b6 In general to deploy Kubernetes with kubeadm it is required to use following official Kubernetes documentation . 1.1 Create a VM, and ssh into it \u00b6 Step 1 Create the instance gcloud compute instances create k8s-cluster \\ --zone us-central1-c \\ --machine-type=e2-standard-4 \\ --image=ubuntu-1804-bionic-v20210514 \\ --image-project=ubuntu-os-cloud Step 2 Capture the private IP address of the VM Record the IP address of the node, as we will need later. Step 3 Once the vm is created, you can ssh into it gcloud compute ssh k8s-cluster --zone us-central1-c You can also ssh into the node through the console if you prefer that. Step 4 Define the NODE_IP as an environment variable. export NODE_IP=<REPLACE_WITH_NODE_PRIVATE_IP> 1.2 Let iptables see bridged traffic \u00b6 cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 1.3 Install Docker \u00b6 Step 1 Update the apt package index and install packages: sudo su apt-get update apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Step 2 Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Step 3 Setup the stable repository echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Step 4 Install Docker Engine and containerd apt-get update apt-get install docker-ce docker-ce-cli containerd.io Step 5 Configure the Docker daemon to use systemd for the management of the container's cgroups. mkdir /etc/docker cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF Step 6 Restart Docker and enable on boot: systemctl enable docker systemctl daemon-reload systemctl restart docker 1.4 Install kubeadm and prerequisite packages on each node \u00b6 The next step is to install kubeadm and prerequisite packages as showed here. Step 1 Deploy kubeadm and prerequisite packages apt-get update && apt-get install -y apt-transport-https ca-certificates curl curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list apt-get update && apt-get install -y kubelet=1.20.6-00 kubeadm=1.20.6-00 kubectl=1.20.6-00 apt-mark hold kubelet kubeadm kubectl Step 2 Verify kubeadm version kubeadm version Output: kubeadm version: &version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:26:21Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"} Result Latest available version of Kubernetes/kubeadm has been installed from (GitHub Kubernetes repo release page.)[https://github.com/kubernetes/kubernetes/releases] 1.5 'Kubeadm init' the Master \u00b6 Run On the Master node only: Step 1: Build kubeadm Custom Config cat <<EOF > kubeadm.conf kind: ClusterConfiguration apiVersion: kubeadm.k8s.io/v1beta2 apiServer: extraArgs: advertise-address: $NODE_IP kubernetesVersion: v1.20.6 --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd EOF Make sure the IP address was updated: cat kubeadm.conf Note To expose custom Config you can create a kubeadm.conf and specify during kubecadm init execution. For instance: * ControllerManager configs * Custom Subnet * Custom version * Apiserver configs such as authentication, authorization and etc. Step 2: Create a cluster kubeadm init --config=kubeadm.conf Result Once the command completes, configure the KUBECONFIG env variable with the path to admin.conf (recommend adding it to your .bashrc): mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config export KUBECONFIG=$HOME/.kube/config Let's validate that the installation was successful. You should now be able to run kubectl commands and see that all cluster Pods are running (except DNS one): watch kubectl get pods --all-namespaces -o wide To exit back to the terminal, press ctrl+c 1.6 Deploy Calico Networking (via self-hosted, as a daemon set) \u00b6 Step 1 Lets download the Calico manifest: curl https://docs.projectcalico.org/manifests/calico.yaml -O Note You can customize Calico deployments based on you needs. For instance by changing: (Optional) In the manifest, change CALICO_IPV4POOL_IPIP from \"always\" to \"cross-subnet\". (Optional) In the manifest, change FELIX_IPINIPMTU to match your ethernet interface mtu (Optional) If you want a different address range for containers, change CALICO_IPV4POOL_CIDR to the same cidr range used in kubeadm init, for e.g., \"10.6.0.0/16\" This time we are not going to modify defaults Calico values. Step 2 Now lets deploy Calico as a daemon set. kubectl apply -f calico.yaml Watch the Calico/node pod for the master get created (hopefully successfully) watch kubectl get pods --all-namespaces -o wide 1.7 Join worker node \u00b6 If you have other nodes around you can run the 'kubeadm join ...' command from the output of kubeadm init on each worker node (incl token). Watch the calico/node pods get created for each worker node automatically. e.g. kubeadm join --token **** For this lab, we are creating a one node kubernetes clusters, so in order to be able to deploy applications on the same node as the control plane, we need to remove the taint that prevent such deployment. kubectl taint nodes --all node-role.kubernetes.io/master- 1.8 Now lets create a test deployment with 2 replicas \u00b6 kubectl create deployment nginx --replicas=2 --image=nginx --port=8080 Lets get some more detail about the deployment: kubectl describe deployment nginx kubectl get deployment nginx And pods that has been created by nginx deployment: kubectl get pods Congrats. Now you have a working Kubernetes+Calico cluster. 2.1 Verify Kubernetes components deployed by kubeadm \u00b6 2.1.1 Check Kubernetes version \u00b6 Step 1 Verify that Kubernetes is deployed and working. kubectl get nodes Result Kubernetes has single node for workload scheduling. Kubernetes running version 1.20.6 Note At Kubernetes community, we define 3 types of Kubernetes releases: Major (x.0.0) Minor (x.x.0) Patch (x.x.x) Note At a single point of time, we develop the new \"Major\"/\"Minor\" version of Kubernetes (today - Kubernetes 1.21), and we support three existing releases as the \"Patch\" releases (today - 1.19.x, 1.20.x and 1.21.x). 2.1.2 Verify Cluster default namespaces. \u00b6 Step 1 Verify namespaces created in K8s systems $ kubectl get ns NAME STATUS AGE default Active 5h50m kube-node-lease Active 5h50m kube-public Active 5h50m kube-system Active 5h50m Info Namespaces are intendent to isolate groups/teams and give them access to a set of resources. They avoid name collisions between resources. Namespaces provides with a soft Multitenancy, meaning they not provide full isolation. Result By default Kubernetes deployed by kubeadm starts with 4 namespaces: default The default namespace for objects with no other namespace. When listing resources with the kubectl get command, we\u2019ve never specified the namespace explicitly, so kubectl always defaulted to the default namespace, showing us just the objects inside that namespace. kube-system The namespace for objects created by the Kubernetes system kube-public Readable by all users, and mostly reserved for cluster usage. kube-node-lease This namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales. 2.1.3 Verify kubelet \u00b6 Step 1 Verify that kubelet installed in K8s Cluster: systemctl -l | grep kubelet systemctl status kubelet Note Service and its config file can be found in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Step 2 Find manifests file for other master Node components: Once kubelet is deployed, all the rest master node components are deployed as a static pods on Kubernetes Master node. Setting --pod-manifest-path= specifies from where to read Static Pod manifests used for spinning up the control plane. Step 3 List K8s components manifest files that is going to be used for cluster deployment and run as Static Pods by kubelet : sudo ls /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml Result We see etcd, api-server, controller-manager and scheduler that has been used to deploy on this cluster and managed by kubelet . Step 4 Verify K8s Components deployed as containers on K8s: kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-6d7b4db76c-h242g 1/1 Running 0 27m calico-node-gwnng 1/1 Running 0 27m coredns-74ff55c5b-5s7rp 1/1 Running 0 5h59m coredns-74ff55c5b-l6hd4 1/1 Running 0 5h59m etcd-k8s-cluster 1/1 Running 0 5h59m kube-apiserver-k8s-cluster 1/1 Running 0 5h59m kube-controller-manager-k8s-cluster 1/1 Running 0 5h59m kube-proxy-f8647 1/1 Running 0 5h59m kube-scheduler-k8s-cluster 1/1 Running 0 5h59m Result We can see that Kubernetes components: etcd, api-server, controller-manager and scheduler deployed on K8s cluster via kubelet. Calico Networking including calico-etcd, calico-node, calico-policy-controller has been deployed as a last step of kubeadm installation 2.1.4 Verify etcd database deployment. \u00b6 Step 1 Verify etcd config file sudo cat /etc/kubernetes/manifests/etcd.yaml Step 2 Overview etcd pod deployed on K8s cluster: kubectl get pods -n kube-system | grep etcd kubectl describe pods/etcd-k8s-cluste -n kube-system Result etcd has been deployed as a static pod. Annotation Priority Class Name: system-node-critical tells to K8s that this Pod is critical and will have highest QOS . Step 3 Check the location of etcd db and snapshot dumps. sudo ls /var/lib/etcd/member Result The data directory has two sub-directories in it: wal: write ahead log files are stored here. snap: log snapshots are stored here. When first started, etcd stores its configuration into a data directory specified by the data-dir configuration parameter. Configuration is stored in the write ahead log and includes: the local member ID, cluster ID, and initial cluster configuration. The write ahead log and snapshot files are used during member operation and to recover after a restart. 2.1.5 Verify api-server deployment on the K8s cluster. \u00b6 Step 1 Review configuration file: sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml Step 2 Overview api-server pod and its parameters. kubectl describe pods/kube-apiserver-k8s-cluster -n kube-system 2.1.6 Verify Controller-manager and scheduler deployment. \u00b6 Step 1 Controller-manager and scheduler deployed on K8s cluster via kubelet the same way api-server . Verify both configuration files and pods running on K8s Cluster. Summary K8s is an orchestration system for containers. Since most of the k8s components are the go binaries that can be containerized, K8s has been designed to run itself. This makes system itself HA, easily deployable, scaleable and upgradable.","title":"Lab 6 Deploy Kubernetes Cluster with Kubeadm"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#deploy-kubernetes","text":"In this Lab, we are going to: Deploy Single Node Kubernetes cluster using kubeadm on a Google Compute Engine node Deploy an application to Kubernetes","title":"Deploy Kubernetes"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#1-deploy-kubernetes-and-calico-with-kubeadm","text":"In general to deploy Kubernetes with kubeadm it is required to use following official Kubernetes documentation .","title":"1. Deploy Kubernetes and Calico with Kubeadm"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#11-create-a-vm-and-ssh-into-it","text":"Step 1 Create the instance gcloud compute instances create k8s-cluster \\ --zone us-central1-c \\ --machine-type=e2-standard-4 \\ --image=ubuntu-1804-bionic-v20210514 \\ --image-project=ubuntu-os-cloud Step 2 Capture the private IP address of the VM Record the IP address of the node, as we will need later. Step 3 Once the vm is created, you can ssh into it gcloud compute ssh k8s-cluster --zone us-central1-c You can also ssh into the node through the console if you prefer that. Step 4 Define the NODE_IP as an environment variable. export NODE_IP=<REPLACE_WITH_NODE_PRIVATE_IP>","title":"1.1 Create a VM, and ssh into it"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#12-let-iptables-see-bridged-traffic","text":"cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system","title":"1.2 Let iptables see bridged traffic"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#13-install-docker","text":"Step 1 Update the apt package index and install packages: sudo su apt-get update apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Step 2 Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Step 3 Setup the stable repository echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Step 4 Install Docker Engine and containerd apt-get update apt-get install docker-ce docker-ce-cli containerd.io Step 5 Configure the Docker daemon to use systemd for the management of the container's cgroups. mkdir /etc/docker cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF Step 6 Restart Docker and enable on boot: systemctl enable docker systemctl daemon-reload systemctl restart docker","title":"1.3 Install Docker"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#14-install-kubeadm-and-prerequisite-packages-on-each-node","text":"The next step is to install kubeadm and prerequisite packages as showed here. Step 1 Deploy kubeadm and prerequisite packages apt-get update && apt-get install -y apt-transport-https ca-certificates curl curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list apt-get update && apt-get install -y kubelet=1.20.6-00 kubeadm=1.20.6-00 kubectl=1.20.6-00 apt-mark hold kubelet kubeadm kubectl Step 2 Verify kubeadm version kubeadm version Output: kubeadm version: &version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:26:21Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"} Result Latest available version of Kubernetes/kubeadm has been installed from (GitHub Kubernetes repo release page.)[https://github.com/kubernetes/kubernetes/releases]","title":"1.4 Install kubeadm and prerequisite packages on each node"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#15-kubeadm-init-the-master","text":"Run On the Master node only: Step 1: Build kubeadm Custom Config cat <<EOF > kubeadm.conf kind: ClusterConfiguration apiVersion: kubeadm.k8s.io/v1beta2 apiServer: extraArgs: advertise-address: $NODE_IP kubernetesVersion: v1.20.6 --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd EOF Make sure the IP address was updated: cat kubeadm.conf Note To expose custom Config you can create a kubeadm.conf and specify during kubecadm init execution. For instance: * ControllerManager configs * Custom Subnet * Custom version * Apiserver configs such as authentication, authorization and etc. Step 2: Create a cluster kubeadm init --config=kubeadm.conf Result Once the command completes, configure the KUBECONFIG env variable with the path to admin.conf (recommend adding it to your .bashrc): mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config export KUBECONFIG=$HOME/.kube/config Let's validate that the installation was successful. You should now be able to run kubectl commands and see that all cluster Pods are running (except DNS one): watch kubectl get pods --all-namespaces -o wide To exit back to the terminal, press ctrl+c","title":"1.5 'Kubeadm init' the Master"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#16-deploy-calico-networking-via-self-hosted-as-a-daemon-set","text":"Step 1 Lets download the Calico manifest: curl https://docs.projectcalico.org/manifests/calico.yaml -O Note You can customize Calico deployments based on you needs. For instance by changing: (Optional) In the manifest, change CALICO_IPV4POOL_IPIP from \"always\" to \"cross-subnet\". (Optional) In the manifest, change FELIX_IPINIPMTU to match your ethernet interface mtu (Optional) If you want a different address range for containers, change CALICO_IPV4POOL_CIDR to the same cidr range used in kubeadm init, for e.g., \"10.6.0.0/16\" This time we are not going to modify defaults Calico values. Step 2 Now lets deploy Calico as a daemon set. kubectl apply -f calico.yaml Watch the Calico/node pod for the master get created (hopefully successfully) watch kubectl get pods --all-namespaces -o wide","title":"1.6 Deploy Calico Networking (via self-hosted, as a daemon set)"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#17-join-worker-node","text":"If you have other nodes around you can run the 'kubeadm join ...' command from the output of kubeadm init on each worker node (incl token). Watch the calico/node pods get created for each worker node automatically. e.g. kubeadm join --token **** For this lab, we are creating a one node kubernetes clusters, so in order to be able to deploy applications on the same node as the control plane, we need to remove the taint that prevent such deployment. kubectl taint nodes --all node-role.kubernetes.io/master-","title":"1.7 Join worker node"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#18-now-lets-create-a-test-deployment-with-2-replicas","text":"kubectl create deployment nginx --replicas=2 --image=nginx --port=8080 Lets get some more detail about the deployment: kubectl describe deployment nginx kubectl get deployment nginx And pods that has been created by nginx deployment: kubectl get pods Congrats. Now you have a working Kubernetes+Calico cluster.","title":"1.8 Now lets create a test deployment with 2 replicas"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#21-verify-kubernetes-components-deployed-by-kubeadm","text":"","title":"2.1 Verify Kubernetes components deployed by kubeadm"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#211-check-kubernetes-version","text":"Step 1 Verify that Kubernetes is deployed and working. kubectl get nodes Result Kubernetes has single node for workload scheduling. Kubernetes running version 1.20.6 Note At Kubernetes community, we define 3 types of Kubernetes releases: Major (x.0.0) Minor (x.x.0) Patch (x.x.x) Note At a single point of time, we develop the new \"Major\"/\"Minor\" version of Kubernetes (today - Kubernetes 1.21), and we support three existing releases as the \"Patch\" releases (today - 1.19.x, 1.20.x and 1.21.x).","title":"2.1.1 Check Kubernetes version"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#212-verify-cluster-default-namespaces","text":"Step 1 Verify namespaces created in K8s systems $ kubectl get ns NAME STATUS AGE default Active 5h50m kube-node-lease Active 5h50m kube-public Active 5h50m kube-system Active 5h50m Info Namespaces are intendent to isolate groups/teams and give them access to a set of resources. They avoid name collisions between resources. Namespaces provides with a soft Multitenancy, meaning they not provide full isolation. Result By default Kubernetes deployed by kubeadm starts with 4 namespaces: default The default namespace for objects with no other namespace. When listing resources with the kubectl get command, we\u2019ve never specified the namespace explicitly, so kubectl always defaulted to the default namespace, showing us just the objects inside that namespace. kube-system The namespace for objects created by the Kubernetes system kube-public Readable by all users, and mostly reserved for cluster usage. kube-node-lease This namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales.","title":"2.1.2 Verify Cluster default namespaces."},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#213-verify-kubelet","text":"Step 1 Verify that kubelet installed in K8s Cluster: systemctl -l | grep kubelet systemctl status kubelet Note Service and its config file can be found in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Step 2 Find manifests file for other master Node components: Once kubelet is deployed, all the rest master node components are deployed as a static pods on Kubernetes Master node. Setting --pod-manifest-path= specifies from where to read Static Pod manifests used for spinning up the control plane. Step 3 List K8s components manifest files that is going to be used for cluster deployment and run as Static Pods by kubelet : sudo ls /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml Result We see etcd, api-server, controller-manager and scheduler that has been used to deploy on this cluster and managed by kubelet . Step 4 Verify K8s Components deployed as containers on K8s: kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-6d7b4db76c-h242g 1/1 Running 0 27m calico-node-gwnng 1/1 Running 0 27m coredns-74ff55c5b-5s7rp 1/1 Running 0 5h59m coredns-74ff55c5b-l6hd4 1/1 Running 0 5h59m etcd-k8s-cluster 1/1 Running 0 5h59m kube-apiserver-k8s-cluster 1/1 Running 0 5h59m kube-controller-manager-k8s-cluster 1/1 Running 0 5h59m kube-proxy-f8647 1/1 Running 0 5h59m kube-scheduler-k8s-cluster 1/1 Running 0 5h59m Result We can see that Kubernetes components: etcd, api-server, controller-manager and scheduler deployed on K8s cluster via kubelet. Calico Networking including calico-etcd, calico-node, calico-policy-controller has been deployed as a last step of kubeadm installation","title":"2.1.3 Verify kubelet"},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#214-verify-etcd-database-deployment","text":"Step 1 Verify etcd config file sudo cat /etc/kubernetes/manifests/etcd.yaml Step 2 Overview etcd pod deployed on K8s cluster: kubectl get pods -n kube-system | grep etcd kubectl describe pods/etcd-k8s-cluste -n kube-system Result etcd has been deployed as a static pod. Annotation Priority Class Name: system-node-critical tells to K8s that this Pod is critical and will have highest QOS . Step 3 Check the location of etcd db and snapshot dumps. sudo ls /var/lib/etcd/member Result The data directory has two sub-directories in it: wal: write ahead log files are stored here. snap: log snapshots are stored here. When first started, etcd stores its configuration into a data directory specified by the data-dir configuration parameter. Configuration is stored in the write ahead log and includes: the local member ID, cluster ID, and initial cluster configuration. The write ahead log and snapshot files are used during member operation and to recover after a restart.","title":"2.1.4 Verify etcd database deployment."},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#215-verify-api-server-deployment-on-the-k8s-cluster","text":"Step 1 Review configuration file: sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml Step 2 Overview api-server pod and its parameters. kubectl describe pods/kube-apiserver-k8s-cluster -n kube-system","title":"2.1.5 Verify api-server deployment on the K8s cluster."},{"location":"Lab_6_Deploy_Kubernetes_kubeadm/#216-verify-controller-manager-and-scheduler-deployment","text":"Step 1 Controller-manager and scheduler deployed on K8s cluster via kubelet the same way api-server . Verify both configuration files and pods running on K8s Cluster. Summary K8s is an orchestration system for containers. Since most of the k8s components are the go binaries that can be containerized, K8s has been designed to run itself. This makes system itself HA, easily deployable, scaleable and upgradable.","title":"2.1.6 Verify Controller-manager and scheduler deployment."},{"location":"Lab_7_Kubernetes_Concepts/","text":"Kubernetes Concepts \u00b6 Objective: Learn basic Kubernetes concepts: Create a GKE Cluster Pods Labels, Selectors and Annotations Create Deployments Create Services namespaces 0 Create GKE Cluster \u00b6 Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c 1 Pods \u00b6 1.1 Create a Pod with manifest \u00b6 Reference: Pod Overview Step 1 Printout explanation of the object and lists of attributes: kubectl explain pods See all possible fields available for the pods: kubectl explain pods.spec --recursive Note It's not require to provide all possible fields for the Pods or any other resources. Most of the fields will be added by default if not specified. For the Pods at minimum it is required to specify image , name , ports inside of spec.containers. Step 2 Define a new pod in the file echoserver-pod.yaml : cat <<EOF > echoserver-pod.yaml apiVersion: v1 kind: Pod metadata: name: echoserver labels: app: echoserver spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.10 ports: - containerPort: 8080 EOF Here, we use the existing image echoserver . This is a simple server that responds with the http headers it received. It runs on nginx server and implemented using lua in the nginx configuration: https://github.com/kubernetes/contrib/tree/master/ingress/echoheaders Step 3 Create the echoserver pod: kubectl apply -f echoserver-pod.yaml Step 4 Use kubectl get pods to watch the pod get created: kubectl get pods Result: NAME READY STATUS RESTARTS AGE echoserver 1/1 Running 0 5s Step 5 Use kubectl describe pods/podname to watch the details about scheduled pod: kubectl describe pods/echoserver Note Review and discuss the following fields: Namespace Status Containers QoS Class Events Step 6 Now let\u2019s get the pod definition back from Kubernetes: kubectl get pods echoserver -o yaml > echoserver-pod-created.yaml cat echoserver-pod-created.yaml Compare echoserver-pod.yaml and echoserver-pod-created.yaml to see additional properties that have been added to the original pod definition. 2 Labels & Selectors \u00b6 Organizing pods and other resources with labels. 2.1 Label and Select Pods \u00b6 Reference: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ Step 1 Label Pod hello-world with label app=hello and env=test kubectl label pods echoserver dep=sales kubectl label pods echoserver env=test Step 2 See all Pods and all their Labels. kubectl get pods --show-labels Step 3 Select all Pods with labels env=test kubectl get pods -l env=test 2.2 Labels Nodes \u00b6 Reference: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ Step 1 List available nodes kubectl get nodes Step 2 List a detailed view of nodes kubectl get nodes -o wide Step 3 List Nodes and their labels kubectl get nodes --show-labels Step 5 Label the node as size: small . Make sure to replace YOUR_NODE_NAME with one of the nodes you have. kubectl label node YOUR_NODE_NAME size=small Step 6 Check the labels for this node kubectl get node YOUR_NODE_NAME --show-labels | grep size Note In the upcoming classes we will use node labels to make sure our applications run on eligible nodes only. 3 Services \u00b6 3.1 Create a Service \u00b6 We have three running echoserver pods, but we cannot access them yet, because the container ports are not accessible. Let\u2019s define a new service that will expose echoserver ports and make them accessible. Step 1 Create a new file echoserver-service.yaml with the following content: cat <<EOF > echoserver-service.yaml apiVersion: v1 kind: Service metadata: name: echoserver spec: selector: app: echoserver type: \"NodePort\" ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: echoserver EOF Step 2 Create a new service: kubectl create -f echoserver-service.yaml Step 3 Check the service details: kubectl describe services/echoserver Output: Name: echoserver Namespace: default Labels: <none> Selector: app=echoserver Type: NodePort IP: ... Port: <unset> 8080/TCP NodePort: <unset> 30366/TCP Endpoints: ...:8080,...:8080,..:8080 Session Affinity: None No events. Note Above output contains three endpoints and a node port. It is 30366 in the output above, but it can be different in your case. Remember this port to use it in the next step. Step 4 We need to open the node port on one of the cluster nodes to be able to access the service externally. Let's first find the exteran IP address of one of the nodes. kubectl get nodes -o wide Output: NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gke-k8s-concepts-default-pool-ad96fd50-1rf1 Ready <none> 20m v1.19.9-gke.1400 10.128.0.32 34.136.1.22 gke-k8s-concepts-default-pool-ad96fd50-jpd2 Ready <none> 20m v1.19.9-gke.1400 10.128.0.31 34.69.114.67 Step 5 Create a firewall rule to allow TCP traffic on your node port. Make sure to replace YOUR_NODE_PORT. gcloud compute firewall-rules create echoserver-node-port --allow tcp:YOUR_NODE_PORT Step 6 To access a service exposed via a node port, specify the node port from the previous step and use one of the IP addresses of the cluster nodes. Make sure to replace both NODE_IP and YOUR_NODE_PORT curl http://NODE_IP:YOUR_NODE_PORT 3.2 Cleanup Services and Pods \u00b6 Step 1 Before diving into Kubernetes deployment, let\u2019s delete our service and pods. To delete the service execute the following command: kubectl delete service echoserver Step 2 delete the pod kubectl delete pod echoserver Step 3 Check that there are no running pods: kubectl get pods 4 Deployments \u00b6 4.1 Deploy hello-app on Kubernetes using Deployments \u00b6 4.1.1 Create a Deployment \u00b6 Step 1 The simplest way to create a new deployment for a single-container pod is to use kubectl run : kubectl create deployment hello-app \\ --image=gcr.io/google-samples/hello-app:1.0 \\ --port=8080 \\ --replicas=2 Note --port Deployment opens port 8080 for use by the Pods. --replicas number of replicas. Step 2 Check pods: kubectl get pods Step 3 To access the hello-app deployment, create a new service of type LoadBalancer this time using kubectl expose deployment : kubectl expose deployment hello-app --type=LoadBalancer To get the external IP for the loadbalancer that got created: kubectl get services/hello-app The Loadbalancer might take few minutes to get created, and it'll show pending status. Step 4 Check that the hello-app is accessible: Make sure to replace the LB_IP. curl http://LB_IP:8080 Output: Hello, world! Version: 1.0.0 Hostname: hello-app-76f778987d-rdhr7 Step 5 You can open the app in the browser by navigating to LB_IP:8080 Summary We learned how to create a deployment and expose our container. 4.1.2 Scale a Deployment \u00b6 Now, let's scale our application as our website get popular. Step 1 Deployments using replica set (RS) to scale the containers. Let's check how replica set (RS) looks like: kubectl get rs,deploy NAME DESIRED CURRENT READY AGE replicaset.apps/hello-app-76f778987d 2 2 2 5m12s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hello-app 2/2 2 2 5m12s Step 2 Let\u2019s scale number of pods in replica for the deployment. Use going to use kubectl scale to change the number of replicas to 5: kubectl scale deployment hello-app --replicas=5 Step 3 View the deployment details: kubectl describe deployment hello-app Step 4 Check that there are 5 running pods: kubectl get pods 4.1.3 Rolling Update of Containers \u00b6 To perform rolling upgrade we need a new version of our application and then perform Rolling Upgrade using deployments Step 4 Use kubectl rollout history deployment to see revisions of the deployment: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> Result Since we've just deployed there is only 1 revision that currenly running. Step 5 Now we want to replace our hello-app with a new implementation. We want to use a new version of hello-app image. We are going to use kubectl set command this time around. Hint kubectl set used only to change image name/version. You can use this for command for CI/CD pipeline. Suppose that we want to update the webk8sbirthday Pods to use the hello-app:2.0 image instead of the hello-app:1.0 image. kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:1.0 --record kubectl get pods Note It is a good practice to paste --record at the end of the rolling upgrade command as it will record the action in the rollout history Result: We can see that the Rolling Upgraded was recorded: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> 2 kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record=true Step 6 Refresh browser and see new version of app deployed http://LB_IP:8080 Step 7 Let's assume there was something wrong with this new version and we need to rollback with kubectl rollout undo our deployment: kubectl rollout undo deployment/hello-app Refresh the browser again to see how we rolledback to version 1.0.0 We have successfully rolled back the deployment and our pods are based on the hello-app:1.0.0 image again. Step 8 Let's delete the deployment and the service: kubectl delete deployment hello-app kubectl delete services/hello-app Success You are now up to speed with Kubernetes Concepts such as Pods, Services and Deployments. Let's move on to Kubernetes Features to learn what else Kubernetes is capable of! 5 NameSpace \u00b6 Namespace can be used for: Splitting complex systems with several components into smaller groups Separating resources in a multi-tenant env: production, development and QA environments Separating resources per productio Separating per-user or per-department or any other logical group Some other rules and regulations: Resource names only need to be unique within a namespace. Two different namespaces can contain resources of the same name. Most of the Kubernetes resources (e.g. pods, svc, rcs, and others) are namespaced. However, some resource can be cluster-wide e.g nodes, persistentVolumes and PodSecurityPolicy. 5.1 Viewing namespaces \u00b6 Step 1 List the current namespaces in a cluster using: kubectl get ns Output: NAME STATUS AGE default Active 71m kube-node-lease Active 71m kube-public Active 71m kube-system Active 71m Step 2 You can also get the summary of a specific namespace using: kubectl get namespaces <name> Or you can get detailed information with: kubectl describe namespaces <name> A namespace can be in one of two phases: Active the namespace is in use Terminating the namespace is being deleted, and can not be used for new objects Note These details show both resource quota (if present) as well as resource and limit ranges. Resource quota tracks aggregate usage of resources in the Namespace and allows cluster operators to define Hard resource usage limits that a Namespace may consume. A limit range defines min/max constraints on the amount of resources a single entity can consume in a Namespace . Step 3 Let\u2019s have a look at the pods that belong to the kube-system namespace, by telling kubectl to list pods in that namespace: kubectl get po --namespace kube-system Output: NAME READY STATUS RESTARTS AGE event-exporter-gke-67986489c8-5fsdv 2/2 Running 0 71m fluentbit-gke-fqcsx 2/2 Running 0 71m fluentbit-gke-ppb9j 2/2 Running 0 71m gke-metrics-agent-5vl7t 1/1 Running 0 71m gke-metrics-agent-bxt2r 1/1 Running 0 71m kube-dns-5d54b45645-9srx6 4/4 Running 0 71m kube-dns-5d54b45645-b7njm 4/4 Running 0 71m kube-dns-autoscaler-58cbd4f75c-2scrv 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-1rf1 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-jpd2 1/1 Running 0 71m l7-default-backend-66579f5d7-dsbdt 1/1 Running 0 71m metrics-server-v0.3.6-6c47ffd7d7-mtls4 2/2 Running 0 71m pdcsi-node-knlqp 2/2 Running 0 71m pdcsi-node-vh4tx 2/2 Running 0 71m stackdriver-metadata-agent-cluster-level-6f7d66dc98-zcd25 2/2 Running 0 71m Tip You can also use -n instead of --namespace Yot may already know some of the pods, the rest we will cover later. It\u2019s clear from the name of the namespace, that resources inside kube-system related to the Kubernetes system itself. By having them in this separate namespace, it keeps everything nicely organized. If they were all in the default namespace, mixed in with the resources we create ourselves, we\u2019d have a hard time seeing what belongs where and we might inadvertently delete some system resources. Step 4 Now you know how to view resources in specific namespaces. Additionally, it is also possible to view list all resources in all namespaces. For example below is example to list all pods in all namespaces: kubectl get pods --all-namespaces 5.2 Creating Namespaces \u00b6 A namespace is a Kubernetes resource, therefore it is possible to create it by posting a YAML file to the Kubernetes API server or using kubectl create ns . Step 1 First, create a custom-namespace.yaml file with the following content: cat <<EOF > custom-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: custom-namespace EOF Step 2 Than, use kubectl to post the file to the Kubernetes API server: kubectl create -f custom-namespace.yaml Step 3 A much easier and faster way to create a namespaces using kubectl create ns command, as shown below: kubectl create namespace custom-namespace2 5.3 Deleting Namepsaces \u00b6 Step 1 Delete a namespace with kubectl delete namespaces custom-namespace kubectl delete namespaces custom-namespace2 Warning In Kubernetes when you delete namespace it deletes everything under it (pods, svc, rc, and etc.)! This is called resource garbage collection in Kubernetes. Delete process is asynchronous, so you may see Terminating state for some time. 5.4 Create a pod in a different namespace \u00b6 Create test namespace: kubectl create ns test Create a pod in this namespaces: cat <<EOF > echoserver-pod_ns.yaml apiVersion: v1 kind: Pod metadata: name: echoserverns spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.10 ports: - containerPort: 8080 EOF Create Pod in namespaces: kubectl create -f echoserver-pod_ns.yaml -n test Verify Pods created in specified namespaces: kubectl get pods -n test 6 Cleaning Up \u00b6 Step 1 Delete the cluster gcloud container clusters delete k8s-concepts Step 2 Delete the firewall rule gcloud compute firewall-rules delete echoserver-node-port","title":"Lab 7 Kubernetes Concepts"},{"location":"Lab_7_Kubernetes_Concepts/#kubernetes-concepts","text":"Objective: Learn basic Kubernetes concepts: Create a GKE Cluster Pods Labels, Selectors and Annotations Create Deployments Create Services namespaces","title":"Kubernetes Concepts"},{"location":"Lab_7_Kubernetes_Concepts/#0-create-gke-cluster","text":"Step 1 Enbale the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with two nodes: gcloud container clusters create k8s-concepts \\ --zone us-central1-c \\ --num-nodes 2 Output: NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS k8s-concepts us-central1-c 1.19.9-gke.1400 34.121.222.83 e2-medium 1.19.9-gke.1400 2 RUNNING Step 3 Authenticate to the cluster. gcloud container clusters get-credentials k8s-concepts --zone us-central1-c","title":"0 Create GKE Cluster"},{"location":"Lab_7_Kubernetes_Concepts/#1-pods","text":"","title":"1 Pods"},{"location":"Lab_7_Kubernetes_Concepts/#11-create-a-pod-with-manifest","text":"Reference: Pod Overview Step 1 Printout explanation of the object and lists of attributes: kubectl explain pods See all possible fields available for the pods: kubectl explain pods.spec --recursive Note It's not require to provide all possible fields for the Pods or any other resources. Most of the fields will be added by default if not specified. For the Pods at minimum it is required to specify image , name , ports inside of spec.containers. Step 2 Define a new pod in the file echoserver-pod.yaml : cat <<EOF > echoserver-pod.yaml apiVersion: v1 kind: Pod metadata: name: echoserver labels: app: echoserver spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.10 ports: - containerPort: 8080 EOF Here, we use the existing image echoserver . This is a simple server that responds with the http headers it received. It runs on nginx server and implemented using lua in the nginx configuration: https://github.com/kubernetes/contrib/tree/master/ingress/echoheaders Step 3 Create the echoserver pod: kubectl apply -f echoserver-pod.yaml Step 4 Use kubectl get pods to watch the pod get created: kubectl get pods Result: NAME READY STATUS RESTARTS AGE echoserver 1/1 Running 0 5s Step 5 Use kubectl describe pods/podname to watch the details about scheduled pod: kubectl describe pods/echoserver Note Review and discuss the following fields: Namespace Status Containers QoS Class Events Step 6 Now let\u2019s get the pod definition back from Kubernetes: kubectl get pods echoserver -o yaml > echoserver-pod-created.yaml cat echoserver-pod-created.yaml Compare echoserver-pod.yaml and echoserver-pod-created.yaml to see additional properties that have been added to the original pod definition.","title":"1.1 Create a Pod with manifest"},{"location":"Lab_7_Kubernetes_Concepts/#2-labels-selectors","text":"Organizing pods and other resources with labels.","title":"2 Labels &amp; Selectors"},{"location":"Lab_7_Kubernetes_Concepts/#21-label-and-select-pods","text":"Reference: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ Step 1 Label Pod hello-world with label app=hello and env=test kubectl label pods echoserver dep=sales kubectl label pods echoserver env=test Step 2 See all Pods and all their Labels. kubectl get pods --show-labels Step 3 Select all Pods with labels env=test kubectl get pods -l env=test","title":"2.1 Label and Select Pods"},{"location":"Lab_7_Kubernetes_Concepts/#22-labels-nodes","text":"Reference: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ Step 1 List available nodes kubectl get nodes Step 2 List a detailed view of nodes kubectl get nodes -o wide Step 3 List Nodes and their labels kubectl get nodes --show-labels Step 5 Label the node as size: small . Make sure to replace YOUR_NODE_NAME with one of the nodes you have. kubectl label node YOUR_NODE_NAME size=small Step 6 Check the labels for this node kubectl get node YOUR_NODE_NAME --show-labels | grep size Note In the upcoming classes we will use node labels to make sure our applications run on eligible nodes only.","title":"2.2 Labels Nodes"},{"location":"Lab_7_Kubernetes_Concepts/#3-services","text":"","title":"3 Services"},{"location":"Lab_7_Kubernetes_Concepts/#31-create-a-service","text":"We have three running echoserver pods, but we cannot access them yet, because the container ports are not accessible. Let\u2019s define a new service that will expose echoserver ports and make them accessible. Step 1 Create a new file echoserver-service.yaml with the following content: cat <<EOF > echoserver-service.yaml apiVersion: v1 kind: Service metadata: name: echoserver spec: selector: app: echoserver type: \"NodePort\" ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: echoserver EOF Step 2 Create a new service: kubectl create -f echoserver-service.yaml Step 3 Check the service details: kubectl describe services/echoserver Output: Name: echoserver Namespace: default Labels: <none> Selector: app=echoserver Type: NodePort IP: ... Port: <unset> 8080/TCP NodePort: <unset> 30366/TCP Endpoints: ...:8080,...:8080,..:8080 Session Affinity: None No events. Note Above output contains three endpoints and a node port. It is 30366 in the output above, but it can be different in your case. Remember this port to use it in the next step. Step 4 We need to open the node port on one of the cluster nodes to be able to access the service externally. Let's first find the exteran IP address of one of the nodes. kubectl get nodes -o wide Output: NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gke-k8s-concepts-default-pool-ad96fd50-1rf1 Ready <none> 20m v1.19.9-gke.1400 10.128.0.32 34.136.1.22 gke-k8s-concepts-default-pool-ad96fd50-jpd2 Ready <none> 20m v1.19.9-gke.1400 10.128.0.31 34.69.114.67 Step 5 Create a firewall rule to allow TCP traffic on your node port. Make sure to replace YOUR_NODE_PORT. gcloud compute firewall-rules create echoserver-node-port --allow tcp:YOUR_NODE_PORT Step 6 To access a service exposed via a node port, specify the node port from the previous step and use one of the IP addresses of the cluster nodes. Make sure to replace both NODE_IP and YOUR_NODE_PORT curl http://NODE_IP:YOUR_NODE_PORT","title":"3.1 Create a Service"},{"location":"Lab_7_Kubernetes_Concepts/#32-cleanup-services-and-pods","text":"Step 1 Before diving into Kubernetes deployment, let\u2019s delete our service and pods. To delete the service execute the following command: kubectl delete service echoserver Step 2 delete the pod kubectl delete pod echoserver Step 3 Check that there are no running pods: kubectl get pods","title":"3.2 Cleanup Services and Pods"},{"location":"Lab_7_Kubernetes_Concepts/#4-deployments","text":"","title":"4 Deployments"},{"location":"Lab_7_Kubernetes_Concepts/#41-deploy-hello-app-on-kubernetes-using-deployments","text":"","title":"4.1 Deploy hello-app on Kubernetes using Deployments"},{"location":"Lab_7_Kubernetes_Concepts/#411-create-a-deployment","text":"Step 1 The simplest way to create a new deployment for a single-container pod is to use kubectl run : kubectl create deployment hello-app \\ --image=gcr.io/google-samples/hello-app:1.0 \\ --port=8080 \\ --replicas=2 Note --port Deployment opens port 8080 for use by the Pods. --replicas number of replicas. Step 2 Check pods: kubectl get pods Step 3 To access the hello-app deployment, create a new service of type LoadBalancer this time using kubectl expose deployment : kubectl expose deployment hello-app --type=LoadBalancer To get the external IP for the loadbalancer that got created: kubectl get services/hello-app The Loadbalancer might take few minutes to get created, and it'll show pending status. Step 4 Check that the hello-app is accessible: Make sure to replace the LB_IP. curl http://LB_IP:8080 Output: Hello, world! Version: 1.0.0 Hostname: hello-app-76f778987d-rdhr7 Step 5 You can open the app in the browser by navigating to LB_IP:8080 Summary We learned how to create a deployment and expose our container.","title":"4.1.1 Create a Deployment"},{"location":"Lab_7_Kubernetes_Concepts/#412-scale-a-deployment","text":"Now, let's scale our application as our website get popular. Step 1 Deployments using replica set (RS) to scale the containers. Let's check how replica set (RS) looks like: kubectl get rs,deploy NAME DESIRED CURRENT READY AGE replicaset.apps/hello-app-76f778987d 2 2 2 5m12s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hello-app 2/2 2 2 5m12s Step 2 Let\u2019s scale number of pods in replica for the deployment. Use going to use kubectl scale to change the number of replicas to 5: kubectl scale deployment hello-app --replicas=5 Step 3 View the deployment details: kubectl describe deployment hello-app Step 4 Check that there are 5 running pods: kubectl get pods","title":"4.1.2 Scale a Deployment"},{"location":"Lab_7_Kubernetes_Concepts/#413-rolling-update-of-containers","text":"To perform rolling upgrade we need a new version of our application and then perform Rolling Upgrade using deployments Step 4 Use kubectl rollout history deployment to see revisions of the deployment: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> Result Since we've just deployed there is only 1 revision that currenly running. Step 5 Now we want to replace our hello-app with a new implementation. We want to use a new version of hello-app image. We are going to use kubectl set command this time around. Hint kubectl set used only to change image name/version. You can use this for command for CI/CD pipeline. Suppose that we want to update the webk8sbirthday Pods to use the hello-app:2.0 image instead of the hello-app:1.0 image. kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:1.0 --record kubectl get pods Note It is a good practice to paste --record at the end of the rolling upgrade command as it will record the action in the rollout history Result: We can see that the Rolling Upgraded was recorded: kubectl rollout history deployment hello-app Output: deployment.apps/hello-app REVISION CHANGE-CAUSE 1 <none> 2 kubectl set image deployment/hello-app hello-app=gcr.io/google-samples/hello-app:2.0 --record=true Step 6 Refresh browser and see new version of app deployed http://LB_IP:8080 Step 7 Let's assume there was something wrong with this new version and we need to rollback with kubectl rollout undo our deployment: kubectl rollout undo deployment/hello-app Refresh the browser again to see how we rolledback to version 1.0.0 We have successfully rolled back the deployment and our pods are based on the hello-app:1.0.0 image again. Step 8 Let's delete the deployment and the service: kubectl delete deployment hello-app kubectl delete services/hello-app Success You are now up to speed with Kubernetes Concepts such as Pods, Services and Deployments. Let's move on to Kubernetes Features to learn what else Kubernetes is capable of!","title":"4.1.3 Rolling Update of Containers"},{"location":"Lab_7_Kubernetes_Concepts/#5-namespace","text":"Namespace can be used for: Splitting complex systems with several components into smaller groups Separating resources in a multi-tenant env: production, development and QA environments Separating resources per productio Separating per-user or per-department or any other logical group Some other rules and regulations: Resource names only need to be unique within a namespace. Two different namespaces can contain resources of the same name. Most of the Kubernetes resources (e.g. pods, svc, rcs, and others) are namespaced. However, some resource can be cluster-wide e.g nodes, persistentVolumes and PodSecurityPolicy.","title":"5 NameSpace"},{"location":"Lab_7_Kubernetes_Concepts/#51-viewing-namespaces","text":"Step 1 List the current namespaces in a cluster using: kubectl get ns Output: NAME STATUS AGE default Active 71m kube-node-lease Active 71m kube-public Active 71m kube-system Active 71m Step 2 You can also get the summary of a specific namespace using: kubectl get namespaces <name> Or you can get detailed information with: kubectl describe namespaces <name> A namespace can be in one of two phases: Active the namespace is in use Terminating the namespace is being deleted, and can not be used for new objects Note These details show both resource quota (if present) as well as resource and limit ranges. Resource quota tracks aggregate usage of resources in the Namespace and allows cluster operators to define Hard resource usage limits that a Namespace may consume. A limit range defines min/max constraints on the amount of resources a single entity can consume in a Namespace . Step 3 Let\u2019s have a look at the pods that belong to the kube-system namespace, by telling kubectl to list pods in that namespace: kubectl get po --namespace kube-system Output: NAME READY STATUS RESTARTS AGE event-exporter-gke-67986489c8-5fsdv 2/2 Running 0 71m fluentbit-gke-fqcsx 2/2 Running 0 71m fluentbit-gke-ppb9j 2/2 Running 0 71m gke-metrics-agent-5vl7t 1/1 Running 0 71m gke-metrics-agent-bxt2r 1/1 Running 0 71m kube-dns-5d54b45645-9srx6 4/4 Running 0 71m kube-dns-5d54b45645-b7njm 4/4 Running 0 71m kube-dns-autoscaler-58cbd4f75c-2scrv 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-1rf1 1/1 Running 0 71m kube-proxy-gke-k8s-concepts-default-pool-ad96fd50-jpd2 1/1 Running 0 71m l7-default-backend-66579f5d7-dsbdt 1/1 Running 0 71m metrics-server-v0.3.6-6c47ffd7d7-mtls4 2/2 Running 0 71m pdcsi-node-knlqp 2/2 Running 0 71m pdcsi-node-vh4tx 2/2 Running 0 71m stackdriver-metadata-agent-cluster-level-6f7d66dc98-zcd25 2/2 Running 0 71m Tip You can also use -n instead of --namespace Yot may already know some of the pods, the rest we will cover later. It\u2019s clear from the name of the namespace, that resources inside kube-system related to the Kubernetes system itself. By having them in this separate namespace, it keeps everything nicely organized. If they were all in the default namespace, mixed in with the resources we create ourselves, we\u2019d have a hard time seeing what belongs where and we might inadvertently delete some system resources. Step 4 Now you know how to view resources in specific namespaces. Additionally, it is also possible to view list all resources in all namespaces. For example below is example to list all pods in all namespaces: kubectl get pods --all-namespaces","title":"5.1 Viewing namespaces"},{"location":"Lab_7_Kubernetes_Concepts/#52-creating-namespaces","text":"A namespace is a Kubernetes resource, therefore it is possible to create it by posting a YAML file to the Kubernetes API server or using kubectl create ns . Step 1 First, create a custom-namespace.yaml file with the following content: cat <<EOF > custom-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: custom-namespace EOF Step 2 Than, use kubectl to post the file to the Kubernetes API server: kubectl create -f custom-namespace.yaml Step 3 A much easier and faster way to create a namespaces using kubectl create ns command, as shown below: kubectl create namespace custom-namespace2","title":"5.2 Creating Namespaces"},{"location":"Lab_7_Kubernetes_Concepts/#53-deleting-namepsaces","text":"Step 1 Delete a namespace with kubectl delete namespaces custom-namespace kubectl delete namespaces custom-namespace2 Warning In Kubernetes when you delete namespace it deletes everything under it (pods, svc, rc, and etc.)! This is called resource garbage collection in Kubernetes. Delete process is asynchronous, so you may see Terminating state for some time.","title":"5.3 Deleting Namepsaces"},{"location":"Lab_7_Kubernetes_Concepts/#54-create-a-pod-in-a-different-namespace","text":"Create test namespace: kubectl create ns test Create a pod in this namespaces: cat <<EOF > echoserver-pod_ns.yaml apiVersion: v1 kind: Pod metadata: name: echoserverns spec: containers: - name: echoserver image: gcr.io/google_containers/echoserver:1.10 ports: - containerPort: 8080 EOF Create Pod in namespaces: kubectl create -f echoserver-pod_ns.yaml -n test Verify Pods created in specified namespaces: kubectl get pods -n test","title":"5.4 Create a pod in a different namespace"},{"location":"Lab_7_Kubernetes_Concepts/#6-cleaning-up","text":"Step 1 Delete the cluster gcloud container clusters delete k8s-concepts Step 2 Delete the firewall rule gcloud compute firewall-rules delete echoserver-node-port","title":"6 Cleaning Up"},{"location":"ass1/","text":"1 Containerize Applications \u00b6 Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1.1 Overview of the Sample Application \u00b6 This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application. 1.1 Build Dockers image for frontend application \u00b6 Step 1 Locate and review the go source code: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Setup vim editor with appropriate color schema Note Skip this step if you use another editor. echo \"colo elflord\" > ~/.vimrc Step 3 Write Dockerfile for your frontend application cd ~/ycit019/Assignment1/gowebapp Create a file named Dockerfile in this directory for the frontend Go app. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1 1.2 Build Docker image for backend application \u00b6 Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally 1.3 Test application by running with Docker Engine. \u00b6 Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network named gowebapp with subnet range 172.19.0.0/16 in which to run the frontend and backend containers: #TODO docker xxx Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Container needs to run on network: `gowebapp` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine #TODO Container needs to run on network: `gowebapp` Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit; 1.5 Cleanup running applications and unused networks \u00b6 ### TODO docker xxx","title":"Assignment1"},{"location":"ass1/#1-containerize-applications","text":"Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process","title":"1 Containerize Applications"},{"location":"ass1/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ass1/#11-overview-of-the-sample-application","text":"This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application.","title":"1.1 Overview of the Sample Application"},{"location":"ass1/#11-build-dockers-image-for-frontend-application","text":"Step 1 Locate and review the go source code: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Setup vim editor with appropriate color schema Note Skip this step if you use another editor. echo \"colo elflord\" > ~/.vimrc Step 3 Write Dockerfile for your frontend application cd ~/ycit019/Assignment1/gowebapp Create a file named Dockerfile in this directory for the frontend Go app. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1","title":"1.1 Build Dockers image for frontend application"},{"location":"ass1/#12-build-docker-image-for-backend-application","text":"Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally","title":"1.2 Build Docker image for backend application"},{"location":"ass1/#13-test-application-by-running-with-docker-engine","text":"Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network named gowebapp with subnet range 172.19.0.0/16 in which to run the frontend and backend containers: #TODO docker xxx Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Container needs to run on network: `gowebapp` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine #TODO Container needs to run on network: `gowebapp` Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit;","title":"1.3 Test application by running with Docker Engine."},{"location":"ass1/#15-cleanup-running-applications-and-unused-networks","text":"### TODO docker xxx","title":"1.5 Cleanup running applications and unused networks"},{"location":"ass1_sol/","text":"1 Containerize Applications \u00b6 Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1.1 Overview of the Sample Application \u00b6 This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application. 1.1 Build Dockers image for frontend application \u00b6 Step 1 Locate and review the go source code folders: https://github.com/Cloud-Architects-Program/k8s cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Write Dockerfile for your frontend application Create a file named Dockerfile in this directory for the frontend Go application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: cd ~/ycit019/Assignment1/gowebapp vim Dockerfile FROM golang:1.15.11 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 3 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. docker build -t <user-name>/gowebapp:v1 . 1.2 Build Docker image for backend application \u00b6 Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1//gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Build gowebapp-mysql Docker image locally docker build -t <user-name>/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally 1.3 Store images in the Dockerhub \u00b6 docker login docker push <user-name>/gowebapp-mysql docker push <user-name>/gowebapp 1.4 Test application by running with Docker Engine. \u00b6 Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd <user-name>/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp <user-name>/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: docker exec -it gowebapp-mysql bash Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit; 1.5 Cleanup running applications and unused networks \u00b6 docker rm -f $(docker ps -q)","title":"Assignment1 - Solution"},{"location":"ass1_sol/#1-containerize-applications","text":"Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process","title":"1 Containerize Applications"},{"location":"ass1_sol/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ass1_sol/#11-overview-of-the-sample-application","text":"This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application.","title":"1.1 Overview of the Sample Application"},{"location":"ass1_sol/#11-build-dockers-image-for-frontend-application","text":"Step 1 Locate and review the go source code folders: https://github.com/Cloud-Architects-Program/k8s cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Write Dockerfile for your frontend application Create a file named Dockerfile in this directory for the frontend Go application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: cd ~/ycit019/Assignment1/gowebapp vim Dockerfile FROM golang:1.15.11 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 3 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. docker build -t <user-name>/gowebapp:v1 .","title":"1.1 Build Dockers image for frontend application"},{"location":"ass1_sol/#12-build-docker-image-for-backend-application","text":"Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1//gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Build gowebapp-mysql Docker image locally docker build -t <user-name>/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally","title":"1.2 Build Docker image for backend application"},{"location":"ass1_sol/#13-store-images-in-the-dockerhub","text":"docker login docker push <user-name>/gowebapp-mysql docker push <user-name>/gowebapp","title":"1.3 Store images in the Dockerhub"},{"location":"ass1_sol/#14-test-application-by-running-with-docker-engine","text":"Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd <user-name>/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp <user-name>/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: docker exec -it gowebapp-mysql bash Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit;","title":"1.4 Test application by running with Docker Engine."},{"location":"ass1_sol/#15-cleanup-running-applications-and-unused-networks","text":"docker rm -f $(docker ps -q)","title":"1.5 Cleanup running applications and unused networks"},{"location":"ass2/","text":"1 Containerize Applications \u00b6 Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1 Configure Cloud Source Repository \u00b6 Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console. 1.1 Create a repository with Cloud Source Repository \u00b6 Step 1 Start a new session in Cloud Shell and clone repository with Assignment2: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 Result Two folders with go app and mysql config has been reviewed. Step 2 Run the following command to create a new Cloud Source Repository named $student_id-notepad, where $student_id - is you mcgill student-ID: gcloud source repos create $student_id-notepad You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: MY_REPO=$student_id-notepad gcloud source repos clone $MY_REPO The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: cd $MY_REPO Step 4 Copy Assignment 2 code to you repo: cp -r ~/ycit019/Assignment2/* . Step 5 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" git add . git commit -m \"Repo Structure\" Output: [master (root-commit) 48c4f03] Repo Structure 82 files changed, 3686 insertions(+) ... Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Git pushes the sample application files from the master branch to the origin remote: Output: Enumerating objects: 119, done. Counting objects: 100% (119/119), done. Delta compression using up to 4 threads Compressing objects: 100% (104/104), done. Writing objects: 100% (119/119), 224.31 KiB | 5.47 MiB/s, done. Total 119 (delta 22), reused 0 (delta 0) remote: Resolving deltas: 100% (22/22) To https://source.developers.google.com/p/mcgil-stundent/r/notepad-gowebapp * [new branch] master -> master Browse files in the Google Cloud Source repository Step 7 Browse files in the Google Cloud Source repository Use the Google Cloud Source Repositories source code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Assignment 2 files you pushed to the repository by opening the Navigation menu and selecting Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. Step 8 View a file in the Google Cloud repository Click $MY_REPO > gowebapp to view the file's contents in the source code browser Click $MY_REPO > gowebapp-mysql to view the file's contents in the source code browser 2 Build and push Docker images to Google Container Registry (GCR) \u00b6 2.1 Build and push gowebapp-mysql Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 2.2 Build and push gowebapp Image to GCR \u00b6 Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry 2.3 Test application by running with Docker Engine. \u00b6 Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. 2.4 Cleanup running applications and unused networks \u00b6 ### TODO docker xxx 3 Docker Compose \u00b6 3.1 Test application locally with Docker Compose \u00b6 Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v2 documentations Note Starting from Docker Compose 1.27.0+, v2 and v3 format has been merged, there for if you using latest version above 1.27.0+ you do NOT need to specify version of docker-compose, otherwise you need to specify version 2 or 3. Where version 3 mainly used for docker swarm deployment. Implementation Ensure that mysql start first and then webapp services Ensure that mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Use version 2 or 3 of compose Step 1 Create compose file cd ~/$MY_REPO/ vim docker-compose.yaml Create structure as following, replace #TODO with correct values. version: '2.4' services: gowebapp-mysql: build: #TODO ... gowebapp: build: #TODO ... networks: #TODO docker-compose up -d Step 3 Test application Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!! 3.2 Commit docker-compose file to repository and share it with Instructor/Teacher \u00b6 Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master 3.3 Grant viewing permissions for a repository to Instructors/Teachers \u00b6 Reference document gcloud projects add-iam-policy-binding mcgil-stundent --member='user:ayrat.khayretdinov@gmail.com' --role=roles/browser gcloud projects add-iam-policy-binding mcgil-stundent --member='user:dima.kassab@gmail.com' --role=roles/browser Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"Assignment2"},{"location":"ass2/#1-containerize-applications","text":"Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose","title":"1 Containerize Applications"},{"location":"ass2/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"ass2/#1-configure-cloud-source-repository","text":"Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console.","title":"1 Configure Cloud Source Repository"},{"location":"ass2/#11-create-a-repository-with-cloud-source-repository","text":"Step 1 Start a new session in Cloud Shell and clone repository with Assignment2: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 Result Two folders with go app and mysql config has been reviewed. Step 2 Run the following command to create a new Cloud Source Repository named $student_id-notepad, where $student_id - is you mcgill student-ID: gcloud source repos create $student_id-notepad You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: MY_REPO=$student_id-notepad gcloud source repos clone $MY_REPO The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: cd $MY_REPO Step 4 Copy Assignment 2 code to you repo: cp -r ~/ycit019/Assignment2/* . Step 5 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" git add . git commit -m \"Repo Structure\" Output: [master (root-commit) 48c4f03] Repo Structure 82 files changed, 3686 insertions(+) ... Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Git pushes the sample application files from the master branch to the origin remote: Output: Enumerating objects: 119, done. Counting objects: 100% (119/119), done. Delta compression using up to 4 threads Compressing objects: 100% (104/104), done. Writing objects: 100% (119/119), 224.31 KiB | 5.47 MiB/s, done. Total 119 (delta 22), reused 0 (delta 0) remote: Resolving deltas: 100% (22/22) To https://source.developers.google.com/p/mcgil-stundent/r/notepad-gowebapp * [new branch] master -> master Browse files in the Google Cloud Source repository Step 7 Browse files in the Google Cloud Source repository Use the Google Cloud Source Repositories source code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Assignment 2 files you pushed to the repository by opening the Navigation menu and selecting Source Repositories > Source Code. Result The console shows the files in the master branch at the most recent commit. Step 8 View a file in the Google Cloud repository Click $MY_REPO > gowebapp to view the file's contents in the source code browser Click $MY_REPO > gowebapp-mysql to view the file's contents in the source code browser","title":"1.1 Create a repository with Cloud Source Repository"},{"location":"ass2/#2-build-and-push-docker-images-to-google-container-registry-gcr","text":"","title":"2 Build and push Docker images to Google Container Registry (GCR)"},{"location":"ass2/#21-build-and-push-gowebapp-mysql-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp-mysql Step 2 Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ Step 2 Get the Project ID: PROJECT_ID=$(gcloud config get-value project) Step 3 Enable the required APIs: gcloud services enable containerregistry.googleapis.com Step 4 Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"2.1 Build and push gowebapp-mysql Image to GCR"},{"location":"ass2/#22-build-and-push-gowebapp-image-to-gcr","text":"Step 1 Locate folder with mysql config cd ~/$MY_REPO/gowebapp Step 2 Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp Step 4 Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally Step 5 Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 Step 6 Login to GCP console -> Container Registry -> Images Result Docker images has been pushed to GCR registry","title":"2.2 Build and push gowebapp Image to GCR"},{"location":"ass2/#23-test-application-by-running-with-docker-engine","text":"Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed.","title":"2.3 Test application by running with Docker Engine."},{"location":"ass2/#24-cleanup-running-applications-and-unused-networks","text":"### TODO docker xxx","title":"2.4 Cleanup running applications and unused networks"},{"location":"ass2/#3-docker-compose","text":"","title":"3 Docker Compose"},{"location":"ass2/#31-test-application-locally-with-docker-compose","text":"Task: Automate local testing with Docker Compose by creating docker-compose.yaml file which contains: User-defined network gowebapp1 Service gowebapp-mysql Service gowebapp Reference Docker Compose v2 documentations Note Starting from Docker Compose 1.27.0+, v2 and v3 format has been merged, there for if you using latest version above 1.27.0+ you do NOT need to specify version of docker-compose, otherwise you need to specify version 2 or 3. Where version 3 mainly used for docker swarm deployment. Implementation Ensure that mysql start first and then webapp services Ensure that mysql database is fully up prior to start webapp services using healthcheck feature of docker compose. Ensure that webapp-mysql and webapp build with Docker-Compose Ensure that environment variable MYSQL_ROOT_PASSWORD is set inside of the docker compose file. Use version 2 or 3 of compose Step 1 Create compose file cd ~/$MY_REPO/ vim docker-compose.yaml Create structure as following, replace #TODO with correct values. version: '2.4' services: gowebapp-mysql: build: #TODO ... gowebapp: build: #TODO ... networks: #TODO docker-compose up -d Step 3 Test application Step 4 Tear down environment docker-compose down Step 5 Cleanup created networks docker network ls Important Make sure gowebapp and gowebapp1 networks has been deleted!!!","title":"3.1 Test application locally with Docker Compose"},{"location":"ass2/#32-commit-docker-compose-file-to-repository-and-share-it-with-instructorteacher","text":"Step 1 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" Step 2 Push commit to the Cloud Source Repositories: git push origin master","title":"3.2 Commit docker-compose file to repository and share it with Instructor/Teacher"},{"location":"ass2/#33-grant-viewing-permissions-for-a-repository-to-instructorsteachers","text":"Reference document gcloud projects add-iam-policy-binding mcgil-stundent --member='user:ayrat.khayretdinov@gmail.com' --role=roles/browser gcloud projects add-iam-policy-binding mcgil-stundent --member='user:dima.kassab@gmail.com' --role=roles/browser Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"3.3 Grant viewing permissions for a repository to Instructors/Teachers"},{"location":"ass2_solution/","text":"1 Containerize Applications \u00b6 Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. Cloud Source Repositories: Qwik Start 1 Configure Cloud Source Repository \u00b6 Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console. 1.1 Create a repository with Cloud Source Repository \u00b6 Step 1 Start a new session in Cloud Shell and clone repository with Assignment2: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 Result Two folders with go app and mysql config has been reviewed. Step 2 Run the following command to create a new Cloud Source Repository named $student_id-notepad, where $student_id - is you mcgill student-ID: gcloud source repos create $student_id-notepad You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: MY_REPO=$student_id-notepad gcloud source repos clone $MY_REPO The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: cd $MY_REPO Step 4 Copy Assignment 2 code to you repo: cp -r ~/ycit019/Assignment2/* . Step 5 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" git add . git commit -m \"Repo Structure\" Output: [master (root-commit) 48c4f03] Repo Structure 82 files changed, 3686 insertions(+) ... Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Git pushes the sample application files from the master branch to the origin remote: Output: Enumerating objects: 119, done. Counting objects: 100% (119/119), done. Delta compression using up to 4 threads Compressing objects: 100% (104/104), done. Writing objects: 100% (119/119), 224.31 KiB | 5.47 MiB/s, done. Total 119 (delta 22), reused 0 (delta 0) remote: Resolving deltas: 100% (22/22) To https://source.developers.google.com/p/mcgil-stundent/r/notepad-gowebapp * [new branch] master -> master ``` Browse files in the Google Cloud Source repository **Step 7** Browse files in the Google Cloud Source repository Use the Google Cloud Source Repositories source code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Assignment 2 files you pushed to the repository by opening the Navigation menu and selecting Source Repositories > Source Code. !!! result The console shows the files in the master branch at the most recent commit. **Step 8** View a file in the Google Cloud repository Click $MY_REPO > gowebapp to view the file's contents in the source code browser Click $MY_REPO > gowebapp-mysql to view the file's contents in the source code browser ## 2 Build and push Docker images to Google Container Registry (GCR) ### 2.1 Build and push `gowebapp-mysql` Image to GCR **Step 1** Locate folder with mysql config cd ~/$MY_REPO/gowebapp-mysql **Step 2** Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ **Step 2** Get the Project ID: PROJECT_ID=$(gcloud config get-value project) **Step 3** Enable the required APIs: gcloud services enable containerregistry.googleapis.com **Step 4** Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally **Step 5** Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 **Step 6** Login to GCP console -> Container Registry -> Images !!! result Docker images has been pushed to GCR registry ## 2.1 Build and push `gowebapp` Image to GCR **Step 1** Locate folder with mysql config cd ~/$MY_REPO/gowebapp **Step 2** Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp **Step 4** Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally **Step 5** Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 **Step 6** Login to GCP console -> Container Registry -> Images !!! result Docker images has been pushed to GCR registry ### 2.3 Test application by running with Docker Engine. **Step 1** Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 **Step 2** Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 **Step 3** Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 **Step 4** Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud `Web Preview` Console: ![alt text](images/web_preview.png \"Web Preview\") !!! note Web Preview using port `8080` by default. If you application using other port, you can edit this as needed. ### 2.4 Cleanup running applications and unused networks TODO docker xxx \u00b6 ## 3 Docker Compose ### 3.1 Test application locally with Docker Compose **Task:** Automate local testing with `Docker Compose` by creating `docker-compose.yaml` file which contains: * User-defined network `gowebapp1` * Service `gowebapp-mysql` * Service `gowebapp` !!! reference Docker Compose v2 [documentations](https://docs.docker.com/compose/compose-file/compose-file-v2/) !!! note Starting from Docker Compose 1.27.0+, v2 and v3 format has been merged, there for if you using latest version above 1.27.0+ you do need to specify version of docker-compose. !!! implementation 1. Ensure that `Mysql` start first and then `webapp` services 2. Ensure that `Mysql` database is fully up prior to start `webapp` services using [healthcheck](https://docs.docker.com/compose/compose-file/compose-file-v2/#healthcheck) feature of docker compose. 3. Ensure that `webapp-mysql`and `webapp` build with Docker-Compose 4. Ensure that environment variable `MYSQL_ROOT_PASSWORD` is set inside of the docker compose file. **Step 1** Create compose file cd ~/$MY_REPO/ vim docker-compose.yaml Create structure as following: version: '2.4' services: gowebapp-mysql: build: ./gowebapp-mysql environment: MYSQL_ROOT_PASSWORD: rootpasswd hostname: gowebapp-mysql healthcheck: test: \"/usr/bin/mysql --user=root --password=rootpasswd --execute \\\"SHOW DATABASES;\\\"\" interval: 2s timeout: 5s retries: 30 networks: - gowebapp1 gowebapp: build: ./gowebapp hostname: gowebapp ports: - \"8080:80\" depends_on: gowebapp-mysql: condition: service_healthy networks: - gowebapp1 networks: gowebapp1: driver: bridge !!! note In this case we using Compose v2.4 version due to support of healthcheck feature. **Step 2** Run compose file export CLOUDSDK_PYTHON=/usr/bin/python # https://github.com/google-github-actions/setup-gcloud/issues/128 docker-compose up -d **Step 3** Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud `Web Preview` Console: ![alt text](images/web_preview.png \"Web Preview\") !!! note Web Preview using port `8080` by default. If you application using other port, you can edit this as needed. **Step 4** Tear down environment docker-compose down **Step 5** Cleanup created networks docker network ls !!! important Make sure `gowebapp` and `gowebapp1` networks has been deleted!!! ### 3.2 Commit docker-compose file to repository and share it with Instructor/Teacher **Step 1** Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" **Step 2** Push commit to the Cloud Source Repositories: git push origin master ### 3.3 Grant viewing permissions for a repository to Instructors/Teachers Reference [document](https://cloud.google.com/source-repositories/docs/granting-users-access) gcloud projects add-iam-policy-binding mcgil-stundent --member='user:ayrat.khayretdinov@gmail.com' --role=roles/browser gcloud projects add-iam-policy-binding mcgil-stundent --member='user:dima.kassab@gmail.com' --role=roles/browser Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO ``` e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"1 Containerize Applications"},{"location":"ass2_solution/#1-containerize-applications","text":"Objective: Use GCP Cloud Source Repositories Push Images to GCR and DockerHub Automate local Development with Docker-Compose","title":"1 Containerize Applications"},{"location":"ass2_solution/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. Cloud Source Repositories: Qwik Start","title":"Prepare Lab Environment"},{"location":"ass2_solution/#1-configure-cloud-source-repository","text":"Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service. In this lab, you will create a local Git repository that contains a sample file, add a Google Source Repository as a remote, and push the contents of the local repository. You will use the source browser included in Source Repositories to view your repository files from within the Cloud Console.","title":"1 Configure Cloud Source Repository"},{"location":"ass2_solution/#11-create-a-repository-with-cloud-source-repository","text":"Step 1 Start a new session in Cloud Shell and clone repository with Assignment2: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 Result Two folders with go app and mysql config has been reviewed. Step 2 Run the following command to create a new Cloud Source Repository named $student_id-notepad, where $student_id - is you mcgill student-ID: gcloud source repos create $student_id-notepad You can safely ignore any billing warnings for creating repositories. Step 3 Clone the contents of your new Cloud Source Repository to a local repo in your Cloud Shell session: MY_REPO=$student_id-notepad gcloud source repos clone $MY_REPO The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository. Step 3 Go into the local repository you've created: cd $MY_REPO Step 4 Copy Assignment 2 code to you repo: cp -r ~/ycit019/Assignment2/* . Step 5 Commit gowebapp and gowebapp-mysql folders using the following Git commands: git config --global user.email \"you@example.com\" #You GCP Account User git config --global user.name \"Your Name\" git add . git commit -m \"Repo Structure\" Output: [master (root-commit) 48c4f03] Repo Structure 82 files changed, 3686 insertions(+) ... Step 6 Once you've committed code to the local repository, add its contents to Cloud Source Repositories using the git push command: git push origin master Git pushes the sample application files from the master branch to the origin remote: Output: Enumerating objects: 119, done. Counting objects: 100% (119/119), done. Delta compression using up to 4 threads Compressing objects: 100% (104/104), done. Writing objects: 100% (119/119), 224.31 KiB | 5.47 MiB/s, done. Total 119 (delta 22), reused 0 (delta 0) remote: Resolving deltas: 100% (22/22) To https://source.developers.google.com/p/mcgil-stundent/r/notepad-gowebapp * [new branch] master -> master ``` Browse files in the Google Cloud Source repository **Step 7** Browse files in the Google Cloud Source repository Use the Google Cloud Source Repositories source code browser to view repository files. You can filter your view to focus on a specific branch, tag, or comment. Browse the Assignment 2 files you pushed to the repository by opening the Navigation menu and selecting Source Repositories > Source Code. !!! result The console shows the files in the master branch at the most recent commit. **Step 8** View a file in the Google Cloud repository Click $MY_REPO > gowebapp to view the file's contents in the source code browser Click $MY_REPO > gowebapp-mysql to view the file's contents in the source code browser ## 2 Build and push Docker images to Google Container Registry (GCR) ### 2.1 Build and push `gowebapp-mysql` Image to GCR **Step 1** Locate folder with mysql config cd ~/$MY_REPO/gowebapp-mysql **Step 2** Review the existing Dockerfile cat Dockerfile output: FROM mysql:8.0 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp-sql \"v1\" COPY gowebapp.sql /docker-entrypoint-initdb.d/ **Step 2** Get the Project ID: PROJECT_ID=$(gcloud config get-value project) **Step 3** Enable the required APIs: gcloud services enable containerregistry.googleapis.com **Step 4** Build gowebapp-mysql Docker image with GCR registry address locally docker build -t gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally **Step 5** Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 **Step 6** Login to GCP console -> Container Registry -> Images !!! result Docker images has been pushed to GCR registry ## 2.1 Build and push `gowebapp` Image to GCR **Step 1** Locate folder with mysql config cd ~/$MY_REPO/gowebapp **Step 2** Review the existing Dockerfile cat Dockerfile output: FROM golang:1.16.4 LABEL maintainer \"student@mcgill.ca\" LABEL gowebapp \"v1\" EXPOSE 80 ENV GO111MODULE=auto ENV GOPATH=/go COPY /code $GOPATH/src/gowebapp/ WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install ENTRYPOINT $GOPATH/bin/gowebapp **Step 4** Build gowebapp Docker image with GCR registry address l locally docker build -t gcr.io/${PROJECT_ID}/gowebapp:v1 . Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally **Step 5** Push the image to gcr.io: docker push gcr.io/${PROJECT_ID}/gowebapp:v1 **Step 6** Login to GCP console -> Container Registry -> Images !!! result Docker images has been pushed to GCR registry ### 2.3 Test application by running with Docker Engine. **Step 1** Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network in which to run the frontend and backend containers: docker network create gowebapp \\ -d bridge \\ --subnet 172.19.0.0/16 **Step 2** Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql \\ -d -e MYSQL_ROOT_PASSWORD=rootpasswd gcr.io/${PROJECT_ID}/gowebapp-mysql:v1 **Step 3** Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: docker run -p 8080:80 --net gowebapp -d --name gowebapp \\ --hostname gowebapp gcr.io/${PROJECT_ID}/gowebapp:v1 **Step 4** Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud `Web Preview` Console: ![alt text](images/web_preview.png \"Web Preview\") !!! note Web Preview using port `8080` by default. If you application using other port, you can edit this as needed. ### 2.4 Cleanup running applications and unused networks","title":"1.1 Create a repository with Cloud Source Repository"},{"location":"ass2_solution/#todo-docker-xxx","text":"## 3 Docker Compose ### 3.1 Test application locally with Docker Compose **Task:** Automate local testing with `Docker Compose` by creating `docker-compose.yaml` file which contains: * User-defined network `gowebapp1` * Service `gowebapp-mysql` * Service `gowebapp` !!! reference Docker Compose v2 [documentations](https://docs.docker.com/compose/compose-file/compose-file-v2/) !!! note Starting from Docker Compose 1.27.0+, v2 and v3 format has been merged, there for if you using latest version above 1.27.0+ you do need to specify version of docker-compose. !!! implementation 1. Ensure that `Mysql` start first and then `webapp` services 2. Ensure that `Mysql` database is fully up prior to start `webapp` services using [healthcheck](https://docs.docker.com/compose/compose-file/compose-file-v2/#healthcheck) feature of docker compose. 3. Ensure that `webapp-mysql`and `webapp` build with Docker-Compose 4. Ensure that environment variable `MYSQL_ROOT_PASSWORD` is set inside of the docker compose file. **Step 1** Create compose file cd ~/$MY_REPO/ vim docker-compose.yaml Create structure as following: version: '2.4' services: gowebapp-mysql: build: ./gowebapp-mysql environment: MYSQL_ROOT_PASSWORD: rootpasswd hostname: gowebapp-mysql healthcheck: test: \"/usr/bin/mysql --user=root --password=rootpasswd --execute \\\"SHOW DATABASES;\\\"\" interval: 2s timeout: 5s retries: 30 networks: - gowebapp1 gowebapp: build: ./gowebapp hostname: gowebapp ports: - \"8080:80\" depends_on: gowebapp-mysql: condition: service_healthy networks: - gowebapp1 networks: gowebapp1: driver: bridge !!! note In this case we using Compose v2.4 version due to support of healthcheck feature. **Step 2** Run compose file export CLOUDSDK_PYTHON=/usr/bin/python # https://github.com/google-github-actions/setup-gcloud/issues/128 docker-compose up -d **Step 3** Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud `Web Preview` Console: ![alt text](images/web_preview.png \"Web Preview\") !!! note Web Preview using port `8080` by default. If you application using other port, you can edit this as needed. **Step 4** Tear down environment docker-compose down **Step 5** Cleanup created networks docker network ls !!! important Make sure `gowebapp` and `gowebapp1` networks has been deleted!!! ### 3.2 Commit docker-compose file to repository and share it with Instructor/Teacher **Step 1** Commit gowebapp and gowebapp-mysql folders using the following Git commands: git add . git commit -m \"adding docker-compose.yml\" **Step 2** Push commit to the Cloud Source Repositories: git push origin master ### 3.3 Grant viewing permissions for a repository to Instructors/Teachers Reference [document](https://cloud.google.com/source-repositories/docs/granting-users-access) gcloud projects add-iam-policy-binding mcgil-stundent --member='user:ayrat.khayretdinov@gmail.com' --role=roles/browser gcloud projects add-iam-policy-binding mcgil-stundent --member='user:dima.kassab@gmail.com' --role=roles/browser Submit link to your Cloud Source Repository to LMS, replace with you values https://source.cloud.google.com/${PROJECT_ID}/$MY_REPO ``` e.g: https://source.cloud.google.com/ycit019-project/ayratk-notepad","title":"TODO docker xxx"},{"location":"assignment1/","text":"1 Containerize Applications \u00b6 Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process Prepare Lab Environment \u00b6 This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab. 1.1 Overview of the Sample Application \u00b6 This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application. 1.1 Build Dockers image for frontend application \u00b6 Step 1 Locate and review the go source code: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Setup vim editor with appropriate color schema Note Skip this step if you use another editor. echo \"colo elflord\" > ~/.vimrc Step 3 Write Dockerfile for your frontend application cd ~/ycit019/Assignment1/gowebapp Create a file named Dockerfile in this directory for the frontend Go app. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1 1.2 Build Docker image for backend application \u00b6 Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally 1.3 Test application by running with Docker Engine. \u00b6 Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network named gowebapp with subnet range 172.19.0.0/16 in which to run the frontend and backend containers: #TODO docker xxx Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Container needs to run on network: `gowebapp` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine #TODO Container needs to run on network: `gowebapp` Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit; 1.5 Cleanup running applications and unused networks \u00b6 ### TODO docker xxx","title":"1 Containerize Applications"},{"location":"assignment1/#1-containerize-applications","text":"Objective: Review process of containerizing of applications Review creation of Docker Images Review build image process","title":"1 Containerize Applications"},{"location":"assignment1/#prepare-lab-environment","text":"This lab can be executed in you GCP Cloud Environment using Google Cloud Shell. Open the Google Cloud Shell by clicking on the icon on the top right of the screen: Once opened, you can use it to run the instructions for this lab.","title":"Prepare Lab Environment"},{"location":"assignment1/#11-overview-of-the-sample-application","text":"This package contains two application components which will be used throughout the course to demonstrate features of Kubernetes: gowebapp This directory contains a simple Go-based note-taking web application. It includes a home page, registration page, login page, about page, and note management page. Configuration for this application is stored in code/config/config.json. This file is used to configure the backing data store for the application, which in this course is MySQL. Later in the course, we will externalize this configuration in order to take advantage of Kubernetes Secrets and ConfigMaps. This will include some minor modifications to the Go source code. Go programming experience is not required to complete the exercises. For more details about the internal design and implementation of the Go web application, see code/README.md. gowebapp-mysql This directory contains the schema file used to setup the backing MySQL database for the Go web application.","title":"1.1 Overview of the Sample Application"},{"location":"assignment1/#11-build-dockers-image-for-frontend-application","text":"Step 1 Locate and review the go source code: cd ~ git clone https://github.com/Cloud-Architects-Program/ycit019 cd ~/ycit019/Assignment1/ Result Two folders with go app and mysql config has been reviewed. Step 2 Setup vim editor with appropriate color schema Note Skip this step if you use another editor. echo \"colo elflord\" > ~/.vimrc Step 3 Write Dockerfile for your frontend application cd ~/ycit019/Assignment1/gowebapp Create a file named Dockerfile in this directory for the frontend Go app. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"golang\" base image. Use version `1.15.11` or lower for `golang` #https://hub.docker.com/_/golang/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. #TODO --- Define a version label for this image #https://docs.docker.com/engine/reference/builder/#label EXPOSE 80 ENV GOPATH=/go #TODO --- Copy source code in the local /code directory into $GOPATH/src/gowebapp #https://docs.docker.com/engine/reference/builder/#copy WORKDIR $GOPATH/src/gowebapp/ RUN go get && go install #TODO --- Define an entrypoint for this image which executes the compiled application in $GOPATH/bin/gowebapp when the container starts #https://docs.docker.com/engine/reference/builder/#entrypoint Step 4 Build gowebapp Docker image locally Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. #TODO Build image `<your-github-user>/gowebapp:v1","title":"1.1 Build Dockers image for frontend application"},{"location":"assignment1/#12-build-docker-image-for-backend-application","text":"Step 1 Locate folder with mysql config cd ~/ycit019/Assignment1/gowebapp-mysql Step 2 Write Dockerfile for your backend application Create a file named Dockerfile in this directory for the backend MySQL database application. Use vi or any preferred text editor. vim Dockerfile The template below provides a starting point for defining the contents of this file. Replace TODO comments with the appropriate commands: #TODO --- Define this image to inherit from the \"mysql\" version 8.0 base image #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#from #TODO Set a label corresponding to the MAINTAINER field you could use, so that it wil be visible from docker inspect with the other labels. #MAINTAINER should be you student e-mail. LABEL gowebapp-mysql \"v1\" #TODO --- Investigate the \"Initializing a Fresh Instance\" instructions for the mysql parent image, and copy the local gowebapp.sql file to the proper container directory to be automatically executed when the container starts up #https://hub.docker.com/_/mysql/ #https://docs.docker.com/engine/reference/builder/#copy Step 2 Build gowebapp-mysql Docker image locally #TODO Build image <your-github-user>/gowebapp-mysql:v1 Build the image locally. Make sure to include \".\" at the end. Make sure the build runs to completion without errors. You should get a success message. Run and test Docker images locally","title":"1.2 Build Docker image for backend application"},{"location":"assignment1/#13-test-application-by-running-with-docker-engine","text":"Before putting our app in production let's run the Docker images locally, to ensure that the frontend and backend containers run and integrate properly. Step 1 Create Docker user-defined network To facilitate cross-container communication, let's first define a user-defined network named gowebapp with subnet range 172.19.0.0/16 in which to run the frontend and backend containers: #TODO docker xxx Step 2 Launch backend container Next, let's launch a frontend and backend container using the Docker CLI. First, we launch the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable: #TODO Launch `backend` container in background #TODO Use this settings: `--name gowebapp-mysql` `--hostname gowebapp-mysql` #TODO Container needs to run on network: `gowebapp` #TODO Include following Env Variable in the command: `MYSQL_ROOT_PASSWORD=rootpasswd` Step 3 Launch frontend container Now launch a frontend container, mapping the container port 80 - where the web application is exposed - to port 8080 on the host machine: #TODO Launch `frontend` container in background #TODO Use this settings: `--name gowebapp` `--hostname gowebapp` #TODO Map the container port 80 - to port 8080 on the host machine #TODO Container needs to run on network: `gowebapp` Step 4 Test the application locally Now that we've launched the application containers, let's try to test the web application locally. You should be able to access the application at Google Cloud Web Preview Console: Note Web Preview using port 8080 by default. If you application using other port, you can edit this as needed. Once you can see the application loaded. Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container. Task Take a screenshot of running application. Step 5 Inspect the MySQL database Let's connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly: #TODO docker xxx Step 6 Once inside the container, connect to MySQL database: mysql -u root -p password: Note Use password that has beed used in MYSQL_ROOT_PASSWORD env variable. Step 7 Once connected, run some simple SQL commands to inspect the database tables and persistence: #Simple SQL to navigate SHOW DATABASES; USE gowebapp; SHOW TABLES; SELECT * FROM <table_name>; exit;","title":"1.3 Test application by running with Docker Engine."},{"location":"assignment1/#15-cleanup-running-applications-and-unused-networks","text":"### TODO docker xxx","title":"1.5 Cleanup running applications and unused networks"}]}